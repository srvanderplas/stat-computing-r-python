{
  "hash": "d1e0bb2bf3b2017585cc6a5d98331d56",
  "result": {
    "engine": "knitr",
    "markdown": "# Simulation {#simulation}\n\n## {{< fa bullseye >}} Objectives\n\n-   Understand the limitations of pseudorandom number generation\n-   Understand the connection between sampling methods such as inverse probability sampling and rejection sampling and theoretical statistics\n-   Implement sampling methods as required for distributions that do not have closed form approximations\n-   Use Monte Carlo methods for integration and other simulation tasks\n-   Determine the appropriate data structure to use for storing simulation task results\n\n\nSimulation is an extremely important part of computational statistics.\nBayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models.\nIn this module, we're going to touch on a few foundational pieces of simulation in computing. Hopefully, you will get more exposure to simulation in both theory and methods courses down the line.\n\n## Pseudorandom Number Generation\n\nComputers are almost entirely deterministic, which makes it very difficult to come up with \"random\" numbers.\nIn addition to the deterministic nature of computing, it's also somewhat important to be able to run the same code and get the same results every time, which isn't possible if you rely on truly random numbers.\n\nHistorically, **pseudorandom** numbers were generated using linear congruential generators (LCGs) [@wikipediacontributorsLinearCongruentialGenerator2022].\nThese algorithms aren't typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem \"random\" but are actually deterministic.\nLCGs use modular arithmetic: $$X_{n+1} = (aX_n + c) \\mod m$$ where $X_0$ is the start value (the seed), $a$ is the multiplier, $c$ is the increment, and $m$ is the modulus.\nWhen using a LCG, the user generally specifies only the seed.\n\n![LCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia)](https://upload.wikimedia.org/wikipedia/commons/a/a3/Lcg_3d.gif){fig-align=\"center\"}\n\nThe important thing to note here is that if you specify the same generator values ($a$, $c$, $m$, and $X_0$), you will always get the same series of numbers.\nSince $a$, $c$, $m$ are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time.\n\n::: callout-warning\nIt is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.\n:::\n\nOnce you set your seed, the remaining results will only be reproducible if you generate the same amount of random numbers every time.\n\n::: column-margin\nI once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds).\nWe got brilliant results, but couldn't ever reproduce them, because I hadn't set the seed first and we never could figure out what the original seed was.\nðŸ˜­\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Example: Setting Seeds for Reproducibility\n\n:::: panel-tabset\n#### R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(50))\n## [1] -0.1095366\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  4 57 69 10 76 15 67  1  3 91\n```\n:::\n\n\n\n\n\n#### Python\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 500 std normal draws and take the mean\nnp.mean(rng.standard_normal(500))\n## np.float64(-0.008197259441979758)\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([33, 38,  3, 95,  3, 58, 79,  3, 77, 23])\n```\n:::\n\n\n\n\n\n::::\n\nCompare the results above to these results:\n\n:::: panel-tabset\n#### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(30))\n## [1] -0.1936645\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  49  37   6  34   9   3 100  43   7  29\n```\n:::\n\n\n\n\n\n#### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 30 std normal draws and take the mean\nnp.mean(rng.standard_normal(30))\n## np.float64(0.3016849078747997)\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([21, 49, 21, 99, 45,  1, 56, 70, 15, 82])\n```\n:::\n\n\n\n\n\n::::\n\nNotice how the results have changed?\n:::\n\nTo make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I've already set the seed earlier.\nThis introduces certain \"fixed points\" where results won't change immediately after I've re-set the seed.\nThis is particularly important when I'm generating bootstrap estimates, fitting models, or simulating data for graphics experiments.\n\nPick your seed in any way you want.\nI tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people's birthdays, the current time in hhmmss format... basically, you can use anything, so long as it's a valid integer. \n\n\n## Simulating from Probability Distributions\n\n### Using Built-in Simulation Functions\n\nOften, we can get away with simulating data from a known distribution. \nIn these cases, there is absolutely no point in DIY -- use the implementation that is available in R or Python, as it will be more numerically stable and much faster due to someone else having optimized the underlying C code. \n\n::: panel-tabset\n#### R\n\nYou can see the various distribution options using `?Distributions`.\nIn general, `dxxx` is the PDF/PMF, `pxxx` is the CDF, `qxxx` is the quantile function, and `rxxx` gives you random numbers generated from the distribution.\n(`xxx`, obviously, is whatever distribution you're looking to use.)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nset.seed(109025879)\n\ntibble(\n  norm = rnorm(500),\n  gamma = rgamma(500, shape = 3, scale = 1),\n  exp = rexp(500, rate = 1), # R uses a exp(-ax) \n  t = rt(500, df = 5),\n  chisq = rchisq(500, 5)\n) %>%\n  pivot_longer(1:5, names_to = \"dist\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = \"free\", nrow = 1)\n```\n\n::: {.cell-output-display}\n![Density curves created from 500 samples from each of the Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions.](01-simulation_files/figure-html/fig-built-in-sims-r-1.png){#fig-built-in-sims-r fig-alt='A plot with 5 panels, where each panel contains a density curve created from 500 simulated values. Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions are shown.' width=3000}\n:::\n:::\n\n\n\n\n\n#### Python\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\nrandom.seed(109025879)\n\nimport pandas as pd\nimport numpy as np\n\nwide_df = pd.DataFrame({\n  \"norm\": np.random.normal(size=500),\n  \"gamma\": np.random.gamma(size=500, shape = 3, scale = 1),\n  \"exp\": np.random.exponential(size = 500, scale = 1),\n  \"t\": np.random.standard_t(df = 5, size = 500),\n  \"chisq\": np.random.chisquare(df = 5, size = 500)\n})\n\nlong_df = pd.melt(wide_df, id_vars = None, var_name = \"dist\", value_name = \"value\")\n\nfrom plotnine import *\n\np = (ggplot(long_df, aes(x = \"value\")) + geom_density() + facet_wrap(\"dist\", scales=\"free\", nrow = 1) + theme(figure_size=(10,2)))\np.show()\n```\n\n::: {.cell-output-display}\n![Density curves created from 500 samples from each of the Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions.](01-simulation_files/figure-html/fig-built-in-sims-py-1.png){#fig-built-in-sims-py fig-alt='A plot with 5 panels, where each panel contains a density curve created from 500 simulated values. Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions are shown.' width=960}\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n#### Try it out\n\n:::: panel-tabset\n##### Problem\n\nGenerate variables x and y, where x is a sequence from -10 to 10 and y is equal to $x + \\epsilon$, $\\epsilon \\sim N(0, 1)$.\nFit a linear regression to your simulated data (in R, `lm`, in Python, `sklearn.linear_model`'s `LinearRegression`).\n\nHint: Sample code for regression using sklearn [@menonLinearRegressionLines2018].\n\n##### R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20572983)\ndata <- tibble(x = seq(-10, 10, .1), \n               y = x + rnorm(length(x)))\nregression <- lm(y ~ x, data = data)\nsummary(regression)\n## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.14575 -0.70986  0.03186  0.65429  2.40305 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.01876    0.06869  -0.273    0.785    \n## x            0.99230    0.01184  83.823   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9738 on 199 degrees of freedom\n## Multiple R-squared:  0.9725,\tAdjusted R-squared:  0.9723 \n## F-statistic:  7026 on 1 and 199 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n\n\n\n##### Python\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nrandom.seed(20572983)\n\ndata = pd.DataFrame({'x': np.arange(-10, 10, .1)})\ndata['y'] = data.x + np.random.normal(size = data.x.size)\n\n# Fitting the regression and predictions\n# scikit-learn requires that we reshape everything into\n# nparrays before we pass them into the model.fit() function.\nmodel = LinearRegression().\\\n  fit(data.x.values.reshape(-1, 1),\\\n      data.y.values.reshape(-1, 1))\ndata['pred'] = model.predict(data.x.values.reshape(-1, 1))\n\n# Plotting the results\nimport matplotlib.pyplot as plt\nplt.clf()\n\nplt.scatter(data.x, data.y)\nplt.plot(data.x, data.pred, color='red')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n::::\n:::\n\nSimulation from different distributions can be used to determine which estimators are most appropriate for a given scenario, to determine how likely it is to observe a specific value in a sample of size $n$, and for many other applications. \n\n:::: {.callout-caution collapse=true}\n\n#### Example: Mean vs. Median\n\nSuppose you want to determine which estimator is better - mean or median - for $t$-distributions with different degrees of freedom and different numbers of observations.\n\nYou consider degrees of freedom $\\nu = 2, 5, 10, 30$ and samples of size $n = 10, 20, 30, 50$. \nFor each sample, you calculate the mean and median, $\\hat\\theta$, and you repeat this process $N=1000$ times. \nYou then calculate the bias of the sample estimator $\\hat\\theta$ (mean, median) and the MSE of that estimator.\nThe **bias** is $\\text{Bias}(\\hat\\theta, \\theta) = \\text{Bias}_\\theta\\left[\\hat\\theta\\right] = E_{x|\\theta}\\left[\\hat\\theta\\right]- \\theta = E_{x|\\theta} \\left[\\hat\\theta - \\theta\\right]$. \nThe **MSE** (mean squared error) of $\\hat\\theta$ is $\\text{MSE}(\\hat\\theta) = E\\left[(\\hat\\theta-\\theta)^2\\right]$. \nWe know that the MSE is related to the bias and the variance of $\\hat\\theta$: $\\text{MSE}(\\hat\\theta) = \\left(\\text{Bias}(\\hat\\theta,\\theta)\\right)^2+\\text{Var}(\\hat\\theta)$ (the Bias-Variance tradeoff). \n\nHow can you use this information to determine which estimator is preferable in each $\\nu, n$ situation?\n\n::: panel-tabset\n\n##### R\n\nFirst, let's conduct the simulation. \nI find it helpful to make a function to simulate the data and calculate the necessary quantities, and then to use the `purrr` package to run my function $N$ times with each parameter set.\n\nI always save the original simulated data, even when it's not explicitly necessary, because I want to know exactly what results contributed, and this is the easiest way to get it. \nThis is *not* the most memory efficient way to go, so if you are concerned about overflowing your computer's memory, you might omit saving the data once you're confident the simulation function works as planned.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Write a function to draw a single sample\ndraw_sample <- function(n = 10, v = 2) {\n  my_samp <- rt(n, v)\n  tibble(data = list(my_samp), mean = mean(my_samp), median = median(my_samp))\n}\n\npars <- expand_grid(n = c(10, 20, 30, 50), v = c(2, 5, 10, 30), id = 1:1000)\n\n# Only recompute results if they're not present already\nif (!file.exists(\"data/t-sim-results.RDS\")) {\n  results <- pars |>\n  rowwise() |>\n  mutate(res = map2(n, v, draw_sample))\n  \n  results <- unnest(results, \"res\")\n  saveRDS(results, file = \"data/t-sim-results.RDS\")\n} else {\n  results <- readRDS(\"data/t-sim-results.RDS\")\n}\n\n\nresult_sum <- results |>\n  pivot_longer(mean:median, names_to = \"estimator\", values_to = \"value\") |>\n  group_by(n, v, estimator) |>\n  summarize(bias = mean(value), variance = var(value), mse = mean(value^2))\n\n```\n:::\n\n\n\n\n\nNow that I have the simulation summaries, I can think about plotting the results to focus on the difference in Bias/MSE between estimators under different parameter sets.\nAs our goal is to determine whether mean or median is preferable, we should focus on making that comparison as easy as possible by plotting mean and median lines on the same plot. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nresult_sum |> \n  pivot_longer(bias:mse, names_to = \"quantity\", values_to = \"value\") |>\n  mutate(quantity = factor(quantity, levels = c(\"bias\", \"variance\", \"mse\"), ordered = T)) |>\n  ggplot(aes(x = n, y = value, linetype = estimator, color = factor(v))) + \n  geom_line() + \n  guides(color = 'none') + \n  facet_grid(v~quantity, scales = \"free_y\", space = \"free_y\") + \n  xlab(\"Sample Size\") + ylab(\"Expected Value\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Summary chart of t-distribution estimator simulation results. Across $n$ and $\nu$, we see that the bias is around zero - that is, both mean and median are unbiased estimators (which is sensible given that the $t$-distribution is symmetrical). We see that when $\nu=2$, the variance component is much higher than $\nu=5,10,30$, and as MSE is the sum of the bias and variance, the MSE is also higher in this case. At $\nu=5$, the mean and median perform fairly similarly; but for $\nu=2$, the median has lower variance and thus lower MSE than the mean, which is due to the very heavy tails of the $t$-distribution with low degrees of freedom; the mean is influenced by the higher chances of extreme observations. Thus, when $\nu=2$, we would prefer the median as an estimator. When $\nu=10,30$, however, the mean has lower variance and thus MSE than the median across all values of $n$, indicating that when there are sufficient degrees of freedom, we should prefer the mean as an estimator.](01-simulation_files/figure-html/t-dist-bias-sim-results-r-chart-1.png){fig-alt='A chart showing lines comparing the mean and median performance under simulation. The chart has three columns and four rows, with columns corresponding to the bias, variance, and mse and rows to  degrees of freedom (2, 5, 10, 30) shown as rows. The y-axis of each subplot is the expected value of the corresponding measurement (bias, variance, mse), and the x-axis of each subplot is the number of observations in the sample (10, 20, 30, 50). In each subplot, there are two lines showing the mean and median. The first row is allocated much more space than subsequent rows, with y values ranging between 0 and .75, while subsequent rows range between 0 and 0.15. The lines representing bias (first column) are very flat and close to 0, though there is more variability around the 0 line when degrees of freedom = 2. The variance and MSE tend to decrease as n increases across all panels, but again when DF=2, there is more variability and the trend is less clear. When DF=2, the mean line is higher than the median line for the variance and MSE over all values of n, indicating that the variance of the mean is higher than the variance of the median (the MSE is the bias + variance, and is technically redundant here). When DF=5 and we look at the variance or MSE, the lines are very close and it is difficult to determine which estimator would be better. When DF=10 or DF=20 and we consider the variance or MSE, the median line is higher than the mean across all values of n, indicating that in these situations we would prefer the mean as an estimator over the median.' width=2100}\n:::\n:::\n\n\n\n\n\n##### Python\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import t # t distribution class\nimport numpy as np # mean, median\nimport pandas as pd\n\nrv = t(2)\nn_opts = [10, 20, 30, 50]\nv_opts = [2, 5, 10, 30]\n\npars = [(n, v, i) for n in n_opts for v in v_opts for i in range(1000)]\n# Simulate t-data for each parameter combination and iteration\n# IMO, it's easier to use list comprehension for this, \n# but you could absolutely write a function instead\ndata = [t(x[1]).rvs(x[0]) for x in pars]\nmedian= [np.median(dat) for dat in data]\nmean = [np.mean(dat) for dat in data]\n\n# Construct a data structure to hold all results together\nresults = pd.DataFrame(pars, columns = ['n', 'df', 'i'])\nresults['data'] = data\nresults['median'] = median\nresults['mean'] = mean\n\ndef mse(x, x0=0):\n  return np.mean((x - x0)**2)\n\n# summarize\nres_sum = results.groupby(['n', 'df']).agg({'median': ['mean', 'var', mse], 'mean': ['mean', 'var', mse]})\n# Merge column indices together\nres_sum.columns = ['_'.join(col) for col in res_sum.columns]\n# \"Ungroup\" python style\nres_sum = res_sum.reset_index()\n# Melt to long form\nres_sum_long = pd.melt(res_sum, id_vars=['n', 'df'], var_name = 'est', value_name = 'value')\nres_sum_long[['estimator', 'measure']] = res_sum_long.est.str.split(\"_\", expand = True)\nres_sum_long['measure'] = res_sum_long['measure'].str.replace('mean', 'bias', regex = True)\nres_sum_long = res_sum_long.drop('est', axis = 1)\n```\n:::\n\n\n\n\n\nNow that I have the simulation summaries, I can think about plotting the results to focus on the difference in Bias/MSE between estimators under different parameter sets.\nAs our goal is to determine whether mean or median is preferable, we should focus on making that comparison as easy as possible by plotting mean and median lines on the same plot. \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt\n(\n  so.Plot(res_sum_long, x = \"n\", y = \"value\", linestyle = 'estimator')\n  .add(so.Line())\n  .facet('measure', 'df')\n  .share(y=\"row\")\n  .label(x = \"Sample Size\", y = \"Value\")\n  .show()\n)\n```\n\n::: {.cell-output-display}\n![Summary chart of t-distribution estimator simulation results. Across $n$ and $\nu$, we see that the bias is around zero - that is, both mean and median are unbiased estimators (which is sensible given that the $t$-distribution is symmetrical). We see that when $\nu=2$, the variance component is much higher than $\nu=5,10,30$, and as MSE is the sum of the bias and variance, the MSE is also higher in this case. At $\nu=5$, the mean and median perform fairly similarly; but for $\nu=2$, the median has lower variance and thus lower MSE than the mean, which is due to the very heavy tails of the $t$-distribution with low degrees of freedom; the mean is influenced by the higher chances of extreme observations. Thus, when $\nu=2$, we would prefer the median as an estimator. When $\nu=10,30$, however, the mean has lower variance and thus MSE than the median across all values of $n$, indicating that when there are sufficient degrees of freedom, we should prefer the mean as an estimator.](01-simulation_files/figure-html/t-dist-bias-sim-results-py-chart-1.png){fig-alt='A chart showing lines comparing the mean and median performance under simulation. The chart has three columns and four rows, with columns corresponding to the bias, variance, and mse and rows to  degrees of freedom (2, 5, 10, 30) shown as rows. The y-axis of each subplot is the expected value of the corresponding measurement (bias, variance, mse), and the x-axis of each subplot is the number of observations in the sample (10, 20, 30, 50). In each subplot, there are two lines showing the mean and median. The first row has a much higher range than subsequent rows, with y values ranging between 0 and .75, while subsequent rows range between 0 and 0.15. The lines representing bias (first column) are very flat and close to 0, though there is more variability around the 0 line when degrees of freedom = 2. The variance and MSE tend to decrease as n increases across all panels, but again when DF=2, there is more variability and the trend is less clear. When DF=2, the mean line is higher than the median line for the variance and MSE over all values of n, indicating that the variance of the mean is higher than the variance of the median (the MSE is the bias + variance, and is technically redundant here). When DF=5 and we look at the variance or MSE, the lines are very close and it is difficult to determine which estimator would be better. When DF=10 or DF=20 and we consider the variance or MSE, the median line is higher than the mean across all values of n, indicating that in these situations we would prefer the mean as an estimator over the median.' width=614}\n:::\n:::\n\n\n\n\n\n:::\n\nConsidering the results we obtained, it seems clear that when we have a very low ($\\nu<=5$) degrees of freedom, the median is a preferable estimator because it has lower variance; when we have a higher number of degrees of freedom, the mean is a preferable estimator on the basis of variance. \nBoth estimators are asymptotically unbiased, and are unbiased even for small sample sizes when the distribution is symmetric. \n\n::::\n\n## Simulating from Non-standard distributions\n\nIn some cases, you may want to simulate from a probability distribution that isn't already implemented in your software of choice. \nTo decide how to do this efficiently requires that you first take stock of what you actually have describing your distribution. \n\nDo you have a: \n\n- method for getting from a standard distribution to your distribution via e.g. censoring, combining variables, etc.?\n- probability density/mass function?\n- cumulative density function?\n\nIf you have a PDF/PMF, can it be integrated to get a CDF, either analytically or computationally?\n\nIf your PDF/PMF cannot be easily integrated, can you come up with a more standard distribution with a similar shape that you can use to get samples from your distribution?\n\n\nIf you have a method of getting from a standard distribution via censoring or combinations of variables, this is often the most intuitive way of generating a random sample from a complex distribution. \nIn addition, this can be a good mechanism for generating distributions that mimic real-world processes that are not always natural to translate into functional probability distributions. \n\nIf you have a CDF, and the CDF can be easily inverted, it is often easiest to try inverse probability sampling first.\nIn some cases, however, it is difficult to get an analytical form for the CDF (or the inverse CDF); in these cases, it may be preferable to work with the PDF/PMF instead. \nWhen working with a probability density or mass function, it is often natural to try rejection sampling before moving on to other methods. \n\n### Inverse Probability Sampling\n\nIf you have a cumulative distribution function $F$ that is nondecreasing, you can use that $F$ to get samples from your distribution by generating uniform random variables $u \\sim U[0,1]$ and computing $x = F^{-1}(u)$. \nThis method relies on the [probability integral transform](https://en.wikipedia.org/wiki/Probability_integral_transform). \n\nLet's consider how this might work in the case of a distribution with the following CDF: $$F(x\\leq X) = x^2, \\ \\ 0 < x < 1.$$\n\n\nSteps: \n\n1. Generate a random number $u \\sim U[0,1]$\n2. Find the inverse of the desired CDF, $x^\\ast = F^{-1}_X(u)$\n\n\nThis procedure can often be done in a vectorized manner, if the CDF and inverse CDF (or quantile function) are written with vectorization in mind. \n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Steps in Inverse Probability Sampling. First, we consider the CDF, which has a range of [0,1] over the domain of $x$. A uniform[0,1] random variable can be used to select a random position along the CDF's range, corresponding to the first step of the inverse probability sampling algorithm. Inverting the CDF produces a value $X^\u0007st$, which is a random sample from the distribution specified by the CDF $F(x)$.](01-simulation_files/figure-html/fig-inverse-prob-sketch-3.png){#fig-inverse-prob-sketch fig-alt='A three-panel figure showing the same CDF, which is 0 for x < 0, 1 for x > 1, and x^2 for 0 < x < 1. The first panel shows only the CDF and is titled \\'Step 0: Cumulative Density Function\\'. The second panel shows a horizontal line drawn at a randomly generated value between 0 and 1 that extends from 0 to the CDF. The line is labeled \\'Generated  u ~ U[0,1]\\', and the plot is labeled \\'Step 1: Generate Uniform RV\\'. The third panel shows an additional vertical line drawn from the horizontal line\\'s intersection with the CDF to the x-axis, and is labeled \\'Generated X*=F^{-1}(u)\\'. The panel is titled \\'Step 2: Invert to generate X*~F\\'.' width=2400}\n:::\n:::\n\n\n\n\n\n### Rejection Sampling\n\nIn some cases, you have a PDF but not an easily obtainable or invert-able CDF. \nYou could numerically integrate your PDF and use inverse probability sampling, but it may also be easier to use rejection sampling.\n\nRejection sampling, which is also sometimes called 'acceptance-rejection' sampling works for any distribution with a density function. \nRejection sampling operates by generating random samples from a proposal distribution, and accepting those proposals with a certain probability determined by the relationship between the proposal and target distribution at that point.\n\nIt is easiest to illustrate how this works by showing a probability distribution with a defined domain (say, $[0,10]$). \nIn this case, I've created a nonstandard probability distribution by sketching a bimodal distribution on the interval and converting it to a polynomial. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Intuition for Rejection Sampling. First, we consider the density function. If we sample points from the rectangle enclosing the density function, we can keep only points below the density, and those points would have $x$ values corresponding to the probability distribution $f(x)$.](01-simulation_files/figure-html/fig-rejection-crude-sketch-1.png){#fig-rejection-crude-sketch fig-alt='A three-panel figure showing the same bimodal PDF with unspecified form over 0 < x < 10. The first panel shows only the PDF. The second panel shows 2000 dots scattered over the rectangle, with points above $f(x)$ as red circles and points below $f(x)$ as blue triangles. The third panel shows a blue histogram with the black PDF overlaid; the histogram is formed from the x coordinates of the points below $f(x)$ in the previous panel.' width=2400}\n:::\n:::\n\n\n\n\n\nThink about throwing darts at this PDF (who hasn't wanted to do that?).\nAny darts that land below the line describing $f(x)$ would be distributed uniformly within the area under the curve, and their $x$ values would be a random sample from $f(x)$. \nRejection sampling is essentially a formalization of this idea. \n\nIn the crudest case, rejection sampling is essentially a Monte Carlo sampling method -- we generate random proposals from $\\mathbb{R}^n$ and reject anything outside of our probability density. \nBut, we can usually improve over generating over the full $\\mathbb{R}^n$ -- in most cases, we can find a better proposal space. \nIn @fig-rejection-crude-sketch, we generated 2000 points and rejected 1034 of those points, for an acceptance rate of 48.3%. \n\nBut, what if we were a bit smarter about how we proposed points?\nBimodal distributions are irritating, because there's always that space in the middle between the modes, but, what if we could at least not \"throw darts\" at the top corners?\n\nThe idea here is to get an \"envelope distribution\" - a function that is greater than or equal to $f(x)$ at every point where $f(x) > 0$, and use that function to generate a proposal value.  Then, we can accept or reject that proposal value based on the relationship between our envelope distribution and $f(x)$. \nThis is essentially the same thing that we did before, but our \"envelope\" was very rectangular. \n\nWhat is interesting is that we can generate an \"envelope distribution\" from a distribution that isn't, strictly speaking, greater than $f(x)$, by using some fancy scaling.\n\nAn envelope distribution $g(x)$ can be used for rejection sampling if  $\\displaystyle \\frac{f(x)}{g(x)} \\leq M$ for some constant $M<\\infty$.\n\nSteps:\n\n1. Generate $u\\sim U[0,1]$\n2. Generate $x\\sim g(x)$, a proposal value\n3. Accept $x$ if $u < \\displaystyle\\frac{f(x)}{M g(x)}$; otherwise, start over. \n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Rejection Sampling with a Better Envelope. First, we consider the density function f(x) and an envelope function g(x) such that M g(x) > f(x) for a constant M < infinity. We can determine $M$ by the maximum of the ratio between f(x) and g(x) on the full domain $x$. Then, if we sample points from g(x) and accept those points when a uniform(0,1) RV u < f(x)/(M g(x)), this gives us a sample from f(x). Essentially, we keep the sample g(x) with probability scaled by the likelihood ratio between f and g at x. This is more adaptive than the rectangular proposal envelope but functionally works the same way.](01-simulation_files/figure-html/fig-rejection-better-sketch-1.png){#fig-rejection-better-sketch fig-alt='A three-panel figure showing the same bimodal PDF with unspecified form over 0 < x < 10. The first panel shows only the PDF and a proposal density that has a similar shape to the original. The second panel shows the ratio between the proposal and the original density, with the maximum value shown as a horizontal line indicating the value of the constant $M$. The third panel shows dots under M g(x), with those which are also under f(x) shown as blue triangles, where those above f(x) are shown as red circles. The blue triangles represent a sample from $f(x)$' width=2400}\n:::\n:::\n\n\n\n\n\n\nIn @fig-rejection-better-sketch, where the proposal density is a mixture of two truncated normal densities, we generated 2000 points and rejected 390 of those points, for an acceptance rate of 80.5%. \nI'm entirely sure I could get the proposal density much closer to $f(x)$ if necessary, but it's better to be able to illustrate how the process works. \nIt is possible to use any given $M$ for the proposal density, so long as the $M$ you use meets the condition of $M \\geq \\frac{f(x)}{g(x)}$ for all $x$ with $P(x) > 0$. \nUsing a larger $M$ will simply decrease the acceptance probability, which will be less efficient but will still produce a valid sample. \n\n\n\n\n\n## Simulation to test model assumptions\n\nOne of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model.\nSuppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors.\nHow bad will your results be?\nWhere can you trust the results, and where should you be cautious?\n\n::: column-margin\n\nThe `purrr::map` notation specifies that we are using the `map` function from the `purrr` package.\nWhen functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function.\n\n`purrr::map` takes an argument and for each \"group\" calls the compute_interval function, storing the results in `res`.\nSo each row in `res` is a 1x2 tibble with columns lb and ub.\n\nThis pattern is very useful in all sorts of applications.\nYou can read more about purrr in @sec-lists.\n:::\n\n\n::: {.callout-caution collapse=true}\n### Example: Confidence Interval coverage rates\n\n::: panel-tabset\n#### Problem\n\nSuppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our 25 observations.\n\nYou want to assess the coverage probability of a confidence interval computed under two different modeling scenarios:\n\n1.  Working with the log-transformed values, ln(x), and then transform the computed interval back\n2.  Working with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal.\n\n**Scenario 1**:\n\n- the expected value of the standard normal deviates is 0\n- the variance of the data is 1\n- the SE($\\overline x$) is $\\sqrt\\frac{1}{25} = \\frac{1}{5}$\n\nOur theoretical interval should be $(\\exp(-1.96/5), \\exp(1.96/5)) = (0.6757, 1.4799)$.\n\n**Scenario 2**\n\n- The expected value of the lognormal distribution is $\\exp(1/2) = 1.6487213$\n- The variance of the data is $(\\exp(1) - 1)(\\exp(1)) = 4.6707743$ \n- The SE($\\overline x$) is thus $\\sqrt{\\frac{(e^1 - 1)e^1}{25}} = \\frac{\\sqrt{(e^1 - 1)e^1}}{5} = 0.4322$\n\nOur theoretical interval should be $(0.8015, 2.4959)$.\nThis interval could, if the circumstances were slightly different, contain 0, which is implausible for lognormally distributed data.\n\nOur expected values are different under scenario 1 and scenario 2: \n\n- In scenario 1 we are computing an interval for $\\mu$\n- In scenario 2, we are computing an interval for the population mean, which is $\\exp(\\mu + .5\\sigma^2)$\n\nBoth are valid quantities we might be interested in, but they do not mean the same thing.\n\n\n#### Simulate Data\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(40295023)\n\nsim <- tibble(\n  id = rep(1:100, each = 25), # generate 100 samples of 25 points each\n  ln_x = rnorm(25*100), # generate 25 normal deviates for each sample\n  x = exp(ln_x), # transform into lognormal deviates\n) %>%\n  # this creates a 100-row data frame, with one row for each id. \n  # the columns x, ln_x are stored in the data list-column as a tibble.\n  nest(data = c(x, ln_x))\n  \nhead(sim)\n## # A tibble: 6 Ã— 2\n##      id data             \n##   <int> <list>           \n## 1     1 <tibble [25 Ã— 2]>\n## 2     2 <tibble [25 Ã— 2]>\n## 3     3 <tibble [25 Ã— 2]>\n## 4     4 <tibble [25 Ã— 2]>\n## 5     5 <tibble [25 Ã— 2]>\n## 6     6 <tibble [25 Ã— 2]>\nsim$data[[1]]\n## # A tibble: 25 Ã— 2\n##        x    ln_x\n##    <dbl>   <dbl>\n##  1 0.310 -1.17  \n##  2 0.622 -0.475 \n##  3 0.303 -1.19  \n##  4 1.05   0.0525\n##  5 0.529 -0.636 \n##  6 1.09   0.0891\n##  7 1.97   0.676 \n##  8 8.94   2.19  \n##  9 0.598 -0.514 \n## 10 0.183 -1.70  \n## # â„¹ 15 more rows\n```\n:::\n\n\n\n\n\n\n\n#### Function\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_interval <- function(x) {\n  s1 <- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x)))\n  s2 <- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x))\n  tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n         mean = c(1, exp(1/2)),\n         lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]),\n         in_interval = (lb < mean) & (ub > mean))\n}\n```\n:::\n\n\n\n\n\n#### Apply to Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsim_long <- sim %>%\n  # This line takes each data entry and computes an interval for x.\n  # .$x is code for take the argument you passed in to map and get the x column\n  mutate(res = purrr::map(data, ~compute_interval(.$x))) %>%\n  # this \"frees\" res and we end up with two columns: lb and ub, for each scenario\n  unnest(res)\n  \n\nci_df <- tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n                mu = c(1, exp(1/2)),\n                lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5),\n                ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5))\n\n```\n:::\n\n\n\n\n\n#### Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), \n            data = ci_df,\n            fill = \"grey\", alpha = .5, color = NA) + \n  geom_vline(aes(xintercept = mu), data = ci_df) + \n  geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval),\n               data = sim_long) + \n  scale_color_manual(values = c(\"red\", \"black\")) + \n  theme_bw() + \n  facet_wrap(~scenario)\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-11-1.png){width=2100}\n:::\n:::\n\n\n\n\n:::\n\nFrom this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain.\n:::\n\n::: column-margin\n[Here is a similar example worked through in SAS with IML](https://blogs.sas.com/content/iml/2016/09/08/coverage-probability-confidence-intervals.html).\nNote the use of BY-group processing to analyze each group at once - this is very similar to the use of `purrr::map()` in the R code.\n:::\n\n::: {.callout-caution collapse=true}\n\n### Example: Multilevel Regression and Post Stratification simulation\n\n[Multilevel regression and post-stratification simulation with toddler bedtimes](https://tellingstorieswithdata.com/15-mrp.html#simulation) [@alexanderTellingStoriesData2023]\n\nThis example talks about how to take a biased sample and then recover the original unbiased estimates -- which is something you have to test using simulation to be sure it works, because you never actually know what the true population features are when you are working with real world data.\nWhen reading this example, you may not be all that interested with the specific model - but focus on the **process of simulating data for your analysis** so that you understand how and why you would want to simulate data in order to test a computational method.\n:::\n\n::: {.callout-caution collapse=true}\n\n### Example: Regression and high-leverage points\n\nWhat happens if we have one high-leverage point (e.g. a point which is an outlier in both x and y)?\nHow pathological do our regression coefficient estimates get?\n\nThe challenging part here is to design a data generating mechanism.\n\n::: panel-tabset\n#### Data Generation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngen_data <- function(o = 1, n = 30, error_sd = 2) {\n  # generate the main part of the regression data\n  data <- tibble(x = rnorm(n = n - o, \n                           mean = seq(-10, 10, length.out = n - o), \n                           sd = .1),\n                 y = x + rnorm(length(x), \n                               mean = 0, \n                               sd = error_sd))\n  # generate the outlier - make it at ~(-10, 5)\n  outdata <- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd))\n  bind_rows(data, outdata)\n}\n\nsim_data <- crossing(id = 1:100, outliers = 0:2) %>%\n  mutate(\n  # call gen_data for each row in sim_data, \n  # but don't use id as a parameter.\n  data = purrr::map(outliers, gen_data) \n)\n\n```\n:::\n\n\n\n\n\n#### Data Checking\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sim_data)\n## # A tibble: 6 Ã— 3\n##      id outliers data             \n##   <int>    <int> <list>           \n## 1     1        0 <tibble [30 Ã— 2]>\n## 2     1        1 <tibble [30 Ã— 2]>\n## 3     1        2 <tibble [30 Ã— 2]>\n## 4     2        0 <tibble [30 Ã— 2]>\n## 5     2        1 <tibble [30 Ã— 2]>\n## 6     2        2 <tibble [30 Ã— 2]>\n\n# plot a few datasets just to check they look like we expect:\nsim_data %>%\n  filter(id %% 100 < 3) %>%\n  unnest(data) %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  facet_grid(id ~ outliers, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-13-1.png){width=2100}\n:::\n:::\n\n\n\n\n\n#### Model Fitting\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom) # the broom package cleans up model objects to tidy form\n\nsim_data <- sim_data %>%\n  # fit linear regression\n  mutate(model = purrr::map(data, ~lm(y ~ x, data = .)))  %>%\n  mutate(tidy_model = purrr::map(model, tidy))\n```\n:::\n\n\n\n\n\n#### Results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the coefficients out\ntidy_coefs <- select(sim_data, id, outliers, tidy_model) %>%\n  unnest(tidy_model) %>%\n  mutate(group = case_when(outliers == 0 ~ \"No HLPs\",\n                           outliers == 1 ~ \"1 HLP\",\n                           outliers == 2 ~ \"2 HLPs\") %>%\n           factor(levels = c(\"No HLPs\", \"1 HLP\", \"2 HLPs\")))\n\nggplot(tidy_coefs, aes(x = estimate, color = group)) + \n  facet_grid(term ~ .) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-15-1.png){width=2100}\n:::\n:::\n\n\n\n\n\n:::\n\nObviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0).\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### Try it out\n\n:::: panel-tabset\n#### Problem\n\nLet's explore what happens to estimates when certain observations are censored.\n\nSuppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102$^\\circ F$; for these temperatures, the thermometer will record a value of 102.0.\n\nIt is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness.\nGiven that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer's censoring. \n\n#### Hint\n\nIf most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution?\nWhat if you assume that 101 and 102.5 are the 2SD bounds?\n\n#### General Solution\n\nIf 101 and 102.5 are the anchor points we have, let's assume that 95% of normal pet temperatures fall in that range.\nSo our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375.\n\nWe can simulate 1000 observations from $N(101.75, 0.375)$, create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are.\n\n#### R Code\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(204209527)\ndogtemp <- tibble(\n  actual = rnorm(1000, 101.75, 0.375),\n  read = pmin(actual, 102)\n) \ndogtemp %>%\n  summarize_all(mean) %>%\n  diff()\n## Error in r[i1] - r[-length(r):-(length(r) - lag + 1L)]: non-numeric argument to binary operator\n```\n:::\n\n\n\n\n\nThe effect of the thermometer's censoring is around 0.06 degrees F for animals that are not ill.\n\n#### Python Code\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(204209527)\ndogtemp = pd.DataFrame({\n  \"actual\": np.random.normal(size = 1000, loc = 101.75, scale = 0.375)\n})\ndogtemp['read'] = np.minimum(dogtemp.actual, 102)\n\nnp.diff(dogtemp.mean())\n## array([-0.05928393])\n```\n:::\n\n\n\n\n\nThe effect of the thermometer's censoring is around 0.06 degrees F for animals that are not ill.\n::::\n:::\n\n## Monte Carlo methods\n\n[Monte carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method) [@MonteCarloMethod2022] are methods which rely on repeated random sampling in order to solve numerical problems.\nOften, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically.\n\nIn general, a MC problem involves these steps:\n\n1.  Define the input domain\n2.  Generate inputs randomly from an appropriate probability distribution\n3.  Perform a computation using those inputs\n4.  Aggregate the results.\n\n::: {.callout-caution collapse=\"true\"}\n### Example: Sum of Uniform Random Variables\n\n::: panel-tabset\n#### Problem\n\nLet's try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1.\n\nMore precisely, if $u_i \\sim U(0,1)$, where \\sum\\_{i=1}\\^k u_i \\> 1, what is the expected value of $k$?\n\n#### Defining Steps\n\n1.  In this simulation, our input domain is \\[0,1\\].\n2.  Our input is $u_i \\sim U(0,1)$\n3.  We generate new $u_i$ until $\\sum_{i=1}^k > 1$ and save the value of $k$\n4.  We average the result of $N$ such simulations.\n\n#### R Code\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# It's easier to think through the code if we write it inefficiently first\nsim_fcn <- function() {\n  usum <- 0\n  k <- 0\n  # prevent infinite loops by monitoring the value of k as well\n  while (usum < 1 & k < 15) {\n    usum <- runif(1) + usum\n    k <- k + 1\n  }\n  return(k)\n}\n\nset.seed(302497852)\nres <- tibble(k = replicate(1000, sim_fcn(), simplify = T))\n\nmean(res$k)\n## [1] 2.717\n```\n:::\n\n\n\n\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20417023)\n\nsim_res <- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) \n\nsim_res <- sim_res %>%\n  mutate(running_avg_est = cummean(samp),\n         N = row_number())\n\nggplot(aes(x = N, y = running_avg_est), data = sim_res) + \n  geom_hline(yintercept = exp(1), color = \"red\") + \n  geom_line()\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-20-1.png){width=2100}\n:::\n:::\n\n\n\n\n\n#### Python Code\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport random\nimport pandas as pd\n\n\ndef sim_fcn():\n  usum = 0\n  k = 0\n  # prevent infinite loops by monitoring the value of k as well\n  while usum < 1 and k < 15:\n    # print(\"k = \", k)\n    usum = np.random.uniform(size=1) + usum\n    k += 1\n  return k\n\nrandom.seed(302497852)\nres = pd.DataFrame({\"k\": [sim_fcn() for _ in range(1000)]})\n```\n:::\n\n\n\n\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(20417023)\n\nsim_res = pd.DataFrame({\"k\": [sim_fcn() for _ in range(250000)]})\nsim_res['running_avg_est'] = sim_res.k.expanding().mean()\nsim_res['N'] = np.arange(len(sim_res))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = sim_res, x = 'N', y = 'running_avg_est', color = \"black\")\ngraph.axhline(y = np.exp(1), xmin = 0, xmax = 1, color = \"red\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\n#### Learn More\n\nThe expected number of uniform RV draws required to sum to 1 is $e$!\n\n[Explanation of why this works](https://doi.org/10.2307/2685243)\n:::\n:::\n\nMonte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods).\n\n::: {.callout-caution collapse=\"true\"}\n### Example: Integration\n\n::: panel-tabset\n#### Problem\n\nSuppose you want to integrate $$\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) dx$$\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![f(x) over the interval [0,1].](01-simulation_files/figure-html/unnamed-chunk-23-3.png){fig-alt='A function that oscillates around 0 very rapidly from 0 to 0.12 and then reaches a minimum at approximately x = 0.25 before increasing at an approximately linear rate through x = 1.0.' width=50%}\n:::\n:::\n\n\n\n\n\nYou could set up Riemann integration and evaluate the integral using a sum over $K$ points, but that approach only converges for smooth functions (and besides, that's boring Calc 2 stuff, right?).\n\nInstead, let's observe that this is equivalent to $\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) \\cdot 1 dx$, where $p(x) = 1$ for a uniform random variable.\nThat is, this integral can be written as the expected value of the function over the interval $[0,1]$.\nWhat if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at those points, and average the result?\n\nYou can use the law of large numbers to prove that this approach will converge. [@chenLectureMonteCarlo2017]\n\n#### R Code\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20491720)\nfn <- function(x)  x^2 * sin(1/x)\n\nsim_data <- tibble(x = runif(100000),\n                   y = fn(x))\nmean(sim_data$y)\n## [1] 0.28607461\n```\n:::\n\n\n\n\n\n#### Python Code\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(20491720)\n\ndef fn(x):\n  return x**2 * np.sin(1/x)\n\nsim_data = pd.DataFrame({\"x\": np.random.uniform(size = 100000)})\nsim_data['y'] = fn(sim_data.x)\n\nsim_data.y.mean()\n## np.float64(0.2869765648091521)\n```\n:::\n\n\n\n\n\n#### Riemann R Code\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfn <- function(x)  x^2 * sin(1/x)\n\nriemann <- tibble(x = seq(0, 1, length.out = 10000)[-1],\n                  y = fn(x))\nmean(riemann$y)\n## [1] 0.28657161\n```\n:::\n\n\n\n\n\n:::\n:::\n\n\n::: {.callout-caution collapse=\"true\"}\n### Example: Integration in 2d\n\n::: panel-tabset\n#### Problem\n\nLet's say that you want to find an estimate for $\\pi$, and you know that a circle with radius 1 has an area of exactly that. You also know, that all of the points on this circle can be written as $x^2 + y^2 \\le 1$. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The unit circle.](01-simulation_files/figure-html/unnamed-chunk-27-1.png){fig-alt='A circle centered in (0,0) with radius 1.' width=50%}\n:::\n:::\n\n\n\n\n\nEvaluating the area of the circle mathematically, would need us to either change to polar-coordinates or separate the graph into suitable functions (half-circles), and evaluate the integral between the top and the bottom:\n$$\n\\int_{-1}^1 2 \\sqrt{1-x^2} dx\n$$\nInstead, we note that the circle is encapsulated in a square with side length 2. We can reach all points in that square by using two independent uniform random random variables over the interval $[-1,1]$, i.e. when we generate two random values from U[-1,1], and use one as the $x$ coordinate and one as the $y$ coordinate, we get a point in the square. \nIf the sum of the squares of the coordinates are less than 1, the point will also fall inside the circle. If not, the point falls in one of the four corners of the square that are outside the circle.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The unit circle  is encapsulated by a square and overlaid with uniform points from U[-1,1] x U[-1,1]. ](01-simulation_files/figure-html/unnamed-chunk-28-1.png){fig-alt='A circle centered in (0,0) with radius 1 overlaid with randomly generated points. The points inside the circle are drawn in a different color from the ones outside the circle.' width=50%}\n:::\n:::\n\n\n\n\n\nHow do we get to an estimate of $\\pi$ from there? We know that the area of the square is simply $2^2 = 4$. The area of the circle is then directly proportional to the rate at which points fall into the circle, ie. \n\n$$\n\\hat{\\pi} = 4 \\times \\frac{\\text{Number of points with } x^2+y^2 \\le 1}{\\text{Number of points generated}}.\n$$\nThe more points we generate, the closer our estimate will be to the real value. \n\n\n\nThis problem is an example for Monte-Carlo Integration using an Acceptance-Rejection approach: we can slightly re-write the simulation and think of the generation of a new point in the circle as a two step process, where we first generate a value for $x$ from U[-1,1], and in second step generate a candidate $c$ for $y$ from U[-1, 1], which we will only accept as $y$, if  $|c| \\le \\sqrt{1-x^2}$.  Acceptance-Rejection sampling is the basis of a lot of [Markov-Chain Monte-Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) methods, such as e.g. the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\n\n#### R Code\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20491720)\n\ncalculate_pi <- function(R) {\n  x = runif(R, min=-1, max=1)\n  y = runif(R, min=-1, max=1)\n  in_circle = x^2+y^2<1\n  \n  4 * sum(in_circle) / R\n}\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## [1] 3.16\ncalculate_pi(100)\n## [1] 3.2\ncalculate_pi(100)\n## [1] 2.96\n\n# Better with 10,000\ncalculate_pi(10000)\n## [1] 3.126\ncalculate_pi(10000)\n## [1] 3.1392\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## [1] 3.14344\n\npi\n## [1] 3.1415927\n```\n:::\n\n\n\n\n\n#### Python Code\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(20491720)\n\ndef calculate_pi(R):\n  x = np.random.uniform(size = R)\n  y = np.random.uniform(size = R)\n  in_circle = x**2+y**2<1\n  \n  return 4 * sum(in_circle) / R\n\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## np.float64(3.08)\ncalculate_pi(100)\n## np.float64(3.52)\ncalculate_pi(100)\n## np.float64(3.32)\n\n# Better with 10,000\ncalculate_pi(10000)\n## np.float64(3.1408)\ncalculate_pi(10000)\n## np.float64(3.1472)\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## np.float64(3.142272)\n\nnp.pi\n## 3.141592653589793\n```\n:::\n\n\n\n\n\n#### Numeric integration\n\nWe can compare our results to the results we would get using a more typical  numerical integration function, such as `integrate` in R. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20491720)\nfn <- function(x)  2*sqrt(1-x^2)\n\nintegrate(fn, lower=-1, upper=1)\n## 3.1415927 with absolute error < 2e-09\n\npi\n## [1] 3.1415927\n```\n:::\n\n\n\n\n\n:::\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n### Try it out\n\n::: panel-tabset\n#### Problem\n\nBuffon's needle is a mathematical problem which can be boiled down to a simple physical simulation.\nRead [this science friday description of the problem](https://www.sciencefriday.com/articles/estimate-pi-by-dropping-sticks/) and develop a monte carlo simulation method which estimates $\\pi$ using the Buffon's needle method.\nYour method should be a function which\n\n-   allows the user to specify how many sticks are dropped\n-   plots the result of the physical simulation\n-   prints out a numerical estimate of pi.\n\n#### R code\n\nLet's start out with horizontal lines at 0 and 1, and set our stick length to 1.\nWe need to randomly generate a position (of one end of the stick) and an angle.\nThe position in $x$ doesn't actually make much of a difference (since what we care about is the $y$ coordinates), but we can draw a picture if we generate $x$ as well.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneedle_sim <- function(sticks = 100) {\n  df <- tibble(xstart = runif(sticks, 0, 10), \n         ystart = runif(sticks, 0, 1), \n         angle = runif(sticks, 0, 360),\n         xend = xstart + cos(angle/180*pi), \n         yend = ystart + sin(angle/180*pi)\n  ) %>%\n    # We can see if a stick crosses a line if the floor() function of ystart is \n    # different than floor(yend). \n    # Note this only works for integer line values.\n  mutate(crosses_line = floor(ystart) != floor(yend)) \n  \n  \n  gg <- ggplot() + \n  geom_hline(yintercept = c(0, 1)) + \n  geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend,\n                   color = crosses_line), data = df) + \n  coord_fixed()\n  \n  return(list(est = 2 * sticks / sum(df$crosses_line), plot = gg))\n}\n\nneedle_sim(10)\n## $est\n## [1] 3.3333333\n## \n## $plot\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-32-1.png){width=2100}\n:::\n\n```{.r .cell-code}\n\nneedle_sim(100)\n## $est\n## [1] 3.125\n## \n## $plot\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-32-2.png){width=2100}\n:::\n\n```{.r .cell-code}\n\nneedle_sim(1000)\n## $est\n## [1] 3.1446541\n## \n## $plot\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-32-3.png){width=2100}\n:::\n\n```{.r .cell-code}\n\nneedle_sim(10000)\n## $est\n## [1] 3.1730922\n## \n## $plot\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-32-4.png){width=2100}\n:::\n:::\n\n\n\n\n\n#### Python Code\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef needle_sim(sticks = 100):\n  df = pd.DataFrame({\n    \"xstart\": np.random.uniform(0, 10, size = sticks),\n    \"ystart\": np.random.uniform(0, 1, size = sticks),\n    \"angle\": np.random.uniform(0, 360, size = sticks)\n  })\n  \n  df['xend'] = df.xstart + np.cos(df.angle/180*np.pi)\n  df['yend'] = df.ystart + np.sin(df.angle/180*np.pi)\n  df['crosses_line'] = np.floor(df.ystart) != np.floor(df.yend)\n  \n  return df\n\ndata = needle_sim(100000)\ndata['N'] = np.arange(len(data)) + 1\ndata['cum_est'] = 2*data.N / data.crosses_line.expanding().sum()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = data, x = \"N\", y = \"cum_est\", color = \"black\")\ngraph.axhline(y = np.pi, xmin = 0, xmax = 1, color = \"red\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-simulation_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n\n\n## Other Resources\n\n-   [Simulation](https://bookdown.org/rdpeng/rprogdatascience/simulation.html) (R programming for Data Science chapter)\n\n-   [Simulation](http://rstudio-pubs-static.s3.amazonaws.com/302783_75485bd9eb4646698f534a4833a026e5.html#_simulation_) - R Studio lesson\n\n-   [Simulation, focusing on statistical modeling](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/resources/R/simRreg.pdf) (R)\n\n-   [Simulating Data with SAS](https://support.sas.com/content/dam/SAS/support/en/books/simulating-data-with-sas/65378_excerpt.pdf) (Excerpt)\n\n-   [Simulating a Drunkard's Walk in 2D in SAS](https://blogs.sas.com/content/iml/2015/08/12/2d-drunkards-walk.html)\n\n-   [Simulation from a triangle distribution (SAS)](https://blogs.sas.com/content/iml/2015/07/22/sim-triangular-distrib.html)\n\n-   [Simulating the Monty Hall problem (SAS)](https://blogs.sas.com/content/iml/2015/04/01/monty-hall.html)\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}