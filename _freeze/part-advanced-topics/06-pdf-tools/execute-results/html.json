{
  "hash": "d3ac4e1c34fb59ba0b088ba40a875e3a",
  "result": {
    "engine": "knitr",
    "markdown": "# Working with PDFs {#sec-pdf-data}\n\nWhen I started my first job out of graduate school, one particular process used by my coworkers completely mystified me: they would print out a document, and then immediately scan it back in, with the scan emailed to themselves.\nThis seemed like a waste of toner, paper, and time to me -- why would anyone do such a thing?\nEventually, I found out that the printer would automatically recognize the text and add a text layer to a PDF that previously couldn't be highlighted. \nSo, my coworkers found a handy workaround to manually typing out the numbers they needed from older PDF documents!\nAs clever as this was, it was also unnecessary. A lot of paper, toner, and time could have been saved if the company had just provided **Optical Character Recognition** programs and made them available to workers. \n\nIn this chapter, you'll learn about PDF document structure, as well as how to use OCR programs.\n\n::: column-margin\n\n![DOWNSIDES: Adobe people may periodically email your newsroom to ask you to call it an 'Adobe® PDF document,' but they'll reverse course once they learn how sarcastically you can pronounce the registered trademark symbol.<br>CC-A-NC2.5 by Randall Munroe. [source](https://xkcd.com/2304/)](../images/advanced/xkcd-2304-preprint.png){fig-alt='The image is a comic strip depicting a person sitting at a news desk. The person has simplistic features with a round head, short hair, and no facial features drawn. A speech bubble above them says \"According to a new PDF...\" Behind them is a sign reading \"Breaking News.\" Crossed out phrases above them include \"According to a new preprint...\", \"...an unpublished study...\", and \"According to a new paper uploaded to a preprint server, but which has not undergone peer review...\". Below the main image, there\\'s a section titled \"Benefits of just saying \\'a PDF\\':\" followed by three bullet points. • AVOIDS IMPLICATIONS ABOUT PUBLICATION STATUS • IMMEDIATELY RAISES QUESTIONS ABOUT AUTHOR(S) • STILL IMPLIES \"THIS DOCUMENT WAS PROBABLY PREPARED BY A PROFESSIONAL, BECAUSE NO NORMAL HUMAN TRYING TO COMMUNICATE IN 2020 WOULD CHOOSE THIS RIDICULOUS FORMAT\".'}\n\n:::\n\n## Objectives {-}\n\n- Identify the type of PDF and the data it contains.\n- Develop a strategy to extract the data from the PDF programmatically, using strategies to improve the success of Optical Character Recognition (OCR) if necessary.\n- Augment the PDF files with OCR to add a text layer, if necessary, before extracting information.\n- Extract information from PDFs programmatically and format the information appropriately.\n- Implement quality control and data cleaning measures which handle the most common OCR errors elegantly. \n\n## Introduction\n\nOver the objections of open data organizations, data archivists [@klindtPDFConsideredHarmful2017], and programmers [@edwardsWhyExtractingData2025;@zhangDocumentParsingUnveiled2024], companies and government agencies frequently use PDF (portable document format) documents to store and release data.\nElection results ([Oregon, 2024](https://sos.oregon.gov/elections/Documents/results/november-general-2024-results.pdf)), property appraisals ([Lancaster County, Nebraska](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28719-2025-147568.pdf)), public health reports ([Centers for Disease Control Morbidity and Mortality Weekly Reports](../data/cdc-mmwr-2025-07-10.pdf)), and more, locked up [@harbertTappingPowerUnstructured2021] in PDF format instead of stored properly in nicely formatted spreadsheets or databases. \nEven though we object to the storage mechanism, learning how to deal with data stored in PDF format is a valuable skill for the aspiring data scientist. \nEven if you never work with PDF data in a professional capacity (and I hope you're that lucky), these skills are very useful for public data side projects. \n\n### PDF File Format\n\nThe PDF file format was created in 1993 by Adobe and was a proprietary format until 2008, when the format became an open standard under the control of the International Standards Organization [@wikimediacontributorsHistoryPDF2024]. \nAs the acronym suggests, Portable Document Format is intended to be readable on any computer. \nThis was something of a novel idea in the 1990s, when Mac users used one document creation software and Windows users another, and there was not a version of e.g. Microsoft Office available for Mac.\n\nThe technical details of a PDF file are complex [@gnupdfprojectIntroductionPDF2014;@hodsonPDFSuccinctly2012].\nHowever, conceptually, there are four required components to a PDF document, as shown in @fig-pdf-doc-components: \n\n::: column-margin\n![High-level required components of a PDF document.](../images/advanced/pdf-document-components.png){#fig-pdf-doc-components fig-alt=\"A box containing four colored boxes: Header, Body, Cross-ref table, and Trailer.\" .lightbox}\n\n![Required components of a PDF Body](../images/advanced/pdf-body-components.png){#fig-pdf-body-components fig-alt=\"A network diagram showing the Catalog, with an arrow leading to the Page Tree. Below the Page tree are two page nodes, Page 1 and Page 2. Arrows connect the Page tree to the pages, and the pages to the page tree. The final layer of the network contains page 1 content, resources, and page 2 content. Arrows point from page 1 and 2 to the resources node, as well as to the corresponding page content.\" .lightbox}\n:::\n\n- Header\n    - PDF version number\n    - arbitrary sequence of binary data to prevent applications from opening the document as a text file (which would corrupt the file)\n- Body (relationships between body components shown in @fig-pdf-body-components)\n    - Page tree - serves as the root of the document, and may be as simple as a list of pages.\n    - Pages - each page is defined independently and contains its own metadata, links to resources, and content (defined separately). \n    - Resources - objects required to render a page, such as fonts.\n    - Content - text and graphics which appear visually on the page. \n    - Catalog - an indication to programs as to where to start reading the document. Often this is just a link to the root page tree.\n- Cross-reference table - Records the location in the file of each object in the body of the file so that when viewing a page, only objects from that page are loaded into memory. \n- Trailer - tells applications how to read the file\n    - A reference to the catalog which links to the document root\n    - Location of the cross-reference table\n    - Size of the cross-reference table\n\n@kingAdobeIntroductionInsides2005 has some good examples showing pages and the PDF document code that create the pages. \n\nWithin the page, **streams** are often used to define the page's appearance (the other option is **lattices**, which can be used to divide the page up into sections). \nTo add text, commands are issued to define the font, position the text cursor, and type the text onto the page. \nText is positioned from the bottom left corner, with $Y$ defining the vertical and $X$ defining the horizontal location. \nLine breaks and other formatting features are not a part of the PDF format -- these operations are performed by another program before the file is saved as PDF. \nAs a result, text commands in PDF can be fragmented, leading to a continuous paragraph of words being written in the PDF file as separate lines, with other page elements present in between (like figure captions, page numbers, and images). \nIn addition, PDF documents allow for changing the kerning of text (space between letters) in specific ways that may make it difficult for the characters to be separated visually. \nOne common example of this is the sequence of characters `ff` or `fi` in a document, which can sometimes be read in interesting ways by OCR: sometimes, as unicode ﬀ or ﬁ, sometimes as Cyrillic characters, and sometimes left out entirely or misplaced. \n\n\n\n::: demo\n\n### Demo: Hybrid PDF Fun {-}\n\nConsider a [Home appraisal record from Lancaster County, NE](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28719-2025-147568.pdf). \nOpening the card in a PDF reader and selecting all the text yields a [disorganized text file](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28719-2025-147568.txt) (and this PDF is actually created using modern methods and relatively clean!)\n\nA few observations, marked up in @fig-pdf-layout-text: \n\n- The title on page 1 is on line 39 of the file, and it appears that the data from the first column is on lines 1-38. \n- When there are multi-column tables, as in the \"Inspection History\" table in the middle of the first page, the values are listed by column, but missing values (the times of inspection) are not indicated at all!\n- The Appraised Values table has columns Land, Building, Total, and Method. Line 142 of the text file shows an entry for \"Total Method\", and it is clear that the text for the two columns has been combined. \n\n![Comparing text of PDF document to the rendered PDF document.](../images/advanced/Lancaster-County-NE-Real-Estate-AppraisalCard-28719-2025-147568.svg){#fig-pdf-layout-text .lightbox fig-alt=\"A screenshot of the first page of the PDF, with boxes highlighting the locations corresponding to the points shown above. In the Appraised Values table, arrows are shown connecting the Appraised Values header, Land, Building, Total Method, Current, $45,000, $111,000, $156,000 IDXVAL, Prior, $45,000, $111,000, and $156,000 IDXVAL sequentially.\"}\n\nNow, perhaps we could write a script that would disentangle some of this information and format it properly, though I think the missing values would still be unrecoverable without someone visually mapping the data to the corresponding lines. \n\nThe arrangement of the text you get from selecting all text is different in different PDF applications -- I tried it with [Okular](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568.txt) and [Firefox](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-Firefox.txt), and got totally different orders of text boxes. \n\nThe scope of the actual problem only becomes visible when you look at a second [PDF document](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568.pdf) and the corresponding [text file](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568.txt). \n@fig-pdf-layout-text-parallel shows the two PDF files and their corresponding text files, with the comparable portion of each PDF and text file highlighted. \n\n![Lancaster county, NE real estate appraisal cards and text, with correpsonding sections highlighted across two appraisal cards. The text corresponding to the same space on the PDF has different formatting because one row of the table is blank on one of the cards.](../images/advanced/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568.png){#fig-pdf-layout-text-parallel .lightbox fig-alt=\"A screenshot of the text files and PDF files for both real estate appraisal cards. The recent appeal history and assessed value history table headers and first row of text are highlighted in both sets of documents.\"}\n\n\n:::\n\nHopefully you're beginning to understand how challenging this whole extracting data from PDFs thing can be!\n\n### PDF Layers\n\nPractically, we can think of PDFs as having a text layer, an image layer, or both (hybrid PDFs). \nA PDF with a text layer will allow you to select embedded text and copy it into a text file, while a PDF that just has an image layer does not. \nIt is also possible to have a PDF that has an image layer with a corresponding text layer on top. \nOptical Character Recognition takes a PDF with only image layers and creates a text file (or layer, depending on the tool) by  identifying the characters in the document and converting those characters to text with a corresponding $(x,y)$ location in the document.\nDifferent OCR programs use different conventions for this process, and the quality of the image matters a lot as well - some images are just not good enough to produce a passable transcription of the text using automatic methods. \n\nWhat about Tables? \nIn my opinion, the `pdftools` package documentation says it best...\n\n> Data scientists are often interested in data from tables. Unfortunately the pdf format is pretty dumb and does not have notion of a table (unlike for example HTML). Tabular data in a pdf file is nothing more than strategically positioned lines and text, which makes it difficult to extract the raw data... [@oomsPdftoolsTextExtraction2025]\n\nTable extraction from PDFs is a challenge: hopefully, this demonstration will help you understand why it's so challenging.\n\n:::: demo\n\n### Demo: Types of PDF Files and Layers {-}\n\nFor this demo, I've converted the first page of one of the Lancaster county, NE property appraisal PDFs into an [image only PDF](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf) and a [text-based PDF (the original form)](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf). \nOpen these up in your favorite PDF editor and try to highlight the text in each. How does it work?\n\nWe can also examine the format of a PDF file using R and python libraries. \n\n::: panel-tabset\n\n#### R {-}\nYou'll need the `pdftools` package, which you can install with `install.packages(\"pdftools\")`. \nThis may require you to install `libpoppler` on Linux <i class=\"fa-brands fa-linux\" aria-hidden=\"true\"></i>, but versions for other operating systems should be self-contained. \n\nThe `pdf_info` function gives us information from the PDF header, and the `pdf_text` function tries to extract the text, if it exists. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pdftools)\nlibrary(stringr)\n\npdf_info(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\")$version\npdf_text(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\") |>\n  str_split(\"\\n\") |>  # <1>\n  unlist() |> # <2>\n  head() # <3>\n## [1] \"1.3\"\n## [1] \"                                                                   LANCASTER COUNTY APPRAISAL CARD\"                                                                                                        \n## [2] \"     Parcel ID: 10-24-201-025-000                                         Tax Year: 2025                                                Run Date: 7/15/2025 12:23:27 PM                  Page       1 of 2\"\n## [3] \"    OWNER NAME AND MAILING ADDRESS                                                                                 SALES INFORMATION\"                                                                      \n## [4] \"EASTDALE RENTALS LLC                           Date                Type            Sale Amount          Validity              Multi           Inst.Type                            Instrument #\"           \n## [5] \"Attn: JEFF & ANITA EASTMAN                     05/20/2022          Improved                   $0        Disqualified                          Warranty Deed                        2022025796\"             \n## [6] \"2501 S 74 ST                                   04/23/1996          Improved              $37,000        Disqualified                          Warranty Deed                        1996016959\"\n```\n:::\n\n1. Split text into lines\n2. Remove from list structure -- make a vector\n3. Show first few lines\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf_info(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\")$version\n## [1] \"1.5\"\npdf_text(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\") \n## [1] \"\"\n```\n:::\n\nNotice that the text does not exist for the image-based PDF. \n\n#### Python {-}\n\nYou will need the `pdfplumber` package [@singer-vinePdfplumber2025], which you can install with pip. \n\nThe `pdf` object contains `metadata` and `pages`, and page text (if it exists) can be accessed with the `pdf.pages[i].extract_text()` method.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pdfplumber # <1>\nwith pdfplumber.open(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\") as pdf: # <2>\n  metadata = pdf.metadata # <3>\n  first_page =  pdf.pages[0] # <4>\n  text = first_page.extract_text() # <5>\n  pdf.close()  # <6>\n\nmetadata\ntext.split(\"\\n\")[0:5]  # <7>\n## {'CreationDate': \"D:20250715122327-05'00'\", 'Producer': 'iText# by Gerald Henson (r0.95 of lowagie.com, based on version Paulo build 103)'}\n## ['LANCASTER COUNTY APPRAISAL CARD', 'Parcel ID:10-24-201-025-000 Tax Year: 2025 Run Date: 7/15/2025 12:23:27 PM Page 1 of 2', 'OWNER NAME AND MAILING ADDRESS SALES INFORMATION', 'EASTDALE RENTALS LLC Date Type Sale Amount Validity Multi Inst.Type Instrument #', 'Attn: JEFF & ANITA EASTMAN 05/20/2022 Improved $0 Disqualified Warranty Deed 2022025796']\n```\n:::\n\n1. Use the `pdfplumber` library\n2. Open the file and call it `pdf`\n3. Get the metadata\n4. Get the data for the first page\n5. Extract the text from the first page\n6. Close the file (important to release memory)\n7. Split the text by `\\n` and show the first few lines\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith pdfplumber.open(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\") as pdf:\n  metadata = pdf.metadata\n  first_page =  pdf.pages[0]\n  text = first_page.extract_text()\n  pdf.close()\n\nmetadata\n## {'Producer': 'cairo 1.16.0 (https://cairographics.org)', 'CreationDate': \"D:20250715133207-05'00\"}\n\ntext.split(\"\\n\")[0:5]\n## ['']\n```\n:::\n\n\n:::\n\nNow, I created the image version of the PDF by opening the PDF in an image editor and saving the resulting file as PDF within that image editor, so it's not surprising that the text layer is gone in that version of the file. \n\n::: {.callout collapse=true}\n#### Preview: OCR\n\nMost free OCR tools are based on the `tesseract` library, which you can access using `pytesseract` in python or the `tesseract` R package. \nYou can also run `tesseract` from the command line, if you [install the library](https://tesseract-ocr.github.io/tessdoc/Installation.html) for your operating system and language. \n\n::: panel-tabset\n\n##### Bash {-}\n\nFor the sake of shorter commands, let's assume I'm working with a 1-page PDF file named `file.pdf` and want to create `file.txt` which contains the text of the image-based PDF.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Convert PDF to PNG to work with Tesseract\n# If file has more than one page, \n# this will create file-1.png ... file-n.png images\npdftoppm -png file.pdf file\n\n# Extract the text\ntesseract  -l eng file-1.png file-1.txt\n```\n:::\n\n\nHere are the [PNG](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image-1.png) and [text file](../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text-1.txt) created by using tesseract on one of the Lancaster county appraisal data sheets. \nNote that with this text document, you can search the PDF for text, but it doesn't provide all of the features that a hybrid PDF with both an image and a text layer provides -- the text isn't associated with the $(x,y)$ location on the page(s). \n\n##### R {-}\n\nOn Mac and Linux, you will likely need to install some system packages to make the `tesseract` package installable. \n```\nTry installing:\n * deb: libtesseract-dev libleptonica-dev (Debian, Ubuntu, etc)\n * rpm: tesseract-devel leptonica-devel (Fedora, CentOS, RHEL)\n * brew: tesseract (Mac OSX)\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tesseract\")\nlibrary(tesseract)\nlibrary(pdftools)\n\npdf_convert(\"../data/file.pdf\", dpi = 600) # <1>\neng <- tesseract(\"eng\")\n\ntext <- ocr(\"../data/file-1.png\", engine = eng)\ntext <- str_split(text, \"\\n\", simplify = T)\ntext[1,1:5]\n## Converting page 1 to file_1.png... done!\n## [1] \"file_1.png\"\n## [1] \"LANCASTER COUNTY APPRAISAL CARD\"                                                         \n## [2] \"Parcel ID: 10-24-201-025-000 Tax Year: 2025 Run Date: 7/15/2025 12:23:27 PM Page 1 of 2\" \n## [3] \"EASTDALE RENTALS LLC Date Tvpe Sale Amount Validitv Multi Inst.Type Instrument #\"        \n## [4] \"Attn: JEFF & ANITA EASTMAN 05/20/2022 Improved $0 Disqualified Warranty Deed 2022025796,\"\n## [5] \"2501 S 74ST 04/23/1996 Improved $37,000 Disqualified Warranty Deed 1996016959\"\n```\n:::\n\n1. Testing\n\n\n##### Python {-}\n\nIf you already have the image created, it's simple to get the text out with `pytesseract`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom PIL import Image\nimport pytesseract\n\npytesseract.image_to_string(Image.open('../data/file-1.png')).split(\"\\n\")[0:5]\n## ['Parcel ID: 10-24-201-025-000', '', 'EASTDALE RENTALS LLC', 'Attn: JEFF & ANITA EASTMAN,', '2501 S 74 ST']\n```\n:::\n\n\nIf you don't have the image created, you have to first create an image (`imgBlob`) for each page, and then run `image_to_string` on that image. \nNote that this method does not require that you write the image out to a separate file: the image is only stored in memory.\nThis might be preferable over the bash method, which produces one file for each PDF page. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pdf2image import convert_from_path\n\npages = convert_from_path(\"../data/file.pdf\")\nfor pageNum,imgBlob in enumerate(pages):\n  text = pytesseract.image_to_string(imgBlob, lang='eng')\n  with open(f'../data/file-py-{pageNum}.txt', 'a') as the_file:\n    the_file.write(text)\n## 1796\n```\n:::\n\n\nHere is the [text file](../data/file-py-0.txt) created by Python using `pytesseract`. \n\n:::\n\n:::\n\n\n::: {.callout collapse=true}\n#### Preview: Creating Hybrid PDFs\n\nWhen I need OCR and don't necessarily want to bother with R, I prefer to use a program called [`ocrmypdf`](https://github.com/ocrmypdf/OCRmyPDF) that is based on tesseract and available for most distributions.\nI started using `ocrmypdf` before I realized that it's actually a python package that can just be called from the Linux command line. \n\n\n::: {.cell}\n\n```{.bash .cell-code}\nocrmypdf -l eng ../data/file.pdf ../data/file-ocrmypdf.pdf\n```\n:::\n\n\n\nThe output of `ocrmypdf` is a [hybrid PDF](../data/file-ocrmypdf.pdf) that has the text and image data superimposed (the text is invisible until you highlight it). \n\n\n:::\n\n::::\n\n\n## Working with PDFs Programmatically\n\n### Reading Tabular Data from Text or Hybrid PDFs\n\nAs mentioned above, tabular data is a particular challenge to read from PDF files, as the PDF specification doesn't actually have any way to represent structured text. \nThere are two common open-source libraries recommended for extracting tabular data from PDFs - `tabula`, which is a Java library, and `camelot`, a Python library. \nThere are interfaces to the `tabula` library in both R and Python (`tabulapdf` and `tabula-py`, respectively), but there is no R interface to `camelot`, as far as I am aware. \n\nInstallation of `tabulapdf` in R depends on `rJava`, which can be a bit tricky, particularly on Windows. The [tabulapdf](https://github.com/ropensci/tabulapdf/) github page has more detailed instructions for [how to install Java for Windows using Chocolatey](https://github.com/ropensci/tabulapdf/#installing-java-on-windows-with-chocolatey). \n\n:::: example\n\n#### Example: BLS \n\nThe Bureau of Labor Statistics provides monthly Consumer Price Index news releases. \nAn archive of these releases is available at https://www.bls.gov/bls/news-release/cpi.htm. \nLet's acquire 2 years worth of monthly CPI reports, and focus on trying to extract the first table, \"Consumer Price Index for All Urban Consumers (CPI-U): US city average, by expenditure category\". \n\n::: {.callout collapse=true}\n#### Acquiring 2 years of BLS CPI news releases\n\nAs this material is covered in @sec-data-web, I'm just going to provide the code to do this in R -- you can see equivalent python commands in @sec-data-web. \nAfter having done this in R, I realized I probably could have accomplished the same task with a single `wget` command in `bash`, the lesson being that it is important to pick your tools wisely. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(stringr)\n\nsave_dir <- \"../data/bls-pdfs/\"\ndir.create(save_dir, showWarnings = F)\n\nurl <- \"https://www.bls.gov/bls/news-release/cpi.htm\"\n\nsession <- read_html_live(url)\n\n# PDFs are the 2nd link in each entry\nlinks <- session$html_elements(\"li a:nth-child(2)\")  \n\n# Get the last 2 years of entries\nlink_tbl <- tibble(link = html_attr(links, \"href\"), \n                   date = str_extract(link, \"\\\\d{8}\")) |>\n  na.omit() |>\n  mutate(datestr = date, date = mdy(date)) |>\n  filter(today() - years(2) <= date)\n\n\nua <- \"Mozilla/5.0 (Windows NT x.y; Win64; x64; rv:10.0) Gecko/20100101 Firefox/10.0\"\noptions(HTTPUserAgent = ua)\n\nfilelist <- paste0(\"https://www.bls.gov\", link_tbl$link)\nfilesave <- paste0(save_dir, basename(link_tbl$link))\n\n# The site is finicky about user agents, so we need to \n# specifically pass that in to the download.file method.\nwalk2(filelist, filesave, ~download.file(.x, destfile = .y, method = \"wget\", extra = paste0(\"-U \\\"\", ua, \"\\\"\")))\n```\n:::\n\n\n:::\n\n::: column-margin\n\n![A screenshot of Table 1 of the CPI monthly report from December 2023](../images/advanced/bls-cpi-table1-screenshot.png){.lightbox #fig-cpi-tab1-screenshot fig-alt=\"A screenshot of Table 1 of the BLS CPI report showing components of the consumer price index for urban consumers, averaged over US cities.\"}\n\n:::\n\nYou can either run the code above (assuming you have `wget` on your machine), or you can [download a zip file of the PDFs](../data/bls-pdfs/bls-cpi-pdfs.zip). \n\n\n::: panel-tabset\n##### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tabulapdf\")\nlibrary(tabulapdf)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(stringr)\n\nfiles <- list.files(path = \"../data/bls-pdfs\", pattern = \".pdf$\", full.names=T)\n\nfind_page_number <- function(file) {\n  txt <- pdf_text(file)\n  txt_by_page <- map_chr(txt, ~paste(., collapse=\" \"))\n  which(str_detect(txt_by_page, \"Table 1\"))\n}\n\n# page_numbers <- map_int(files, find_page_number)\npage_numbers <- c(9, 10, 8, 9, 8, 8, 9, 8, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 9, 10, 9)\n\ntables <- extract_tables(files[1], page = page_numbers[1],  output = \"tibble\")[[1]]\n\nhead(tables)\n## # A tibble: 6 × 5\n##   ...1                 ...2    ...3  `Unadjusted percent` Seasonally adjusted …¹\n##   <chr>                <chr>   <chr> <chr>                <chr>                 \n## 1 <NA>                 Relati… Unad… change               change                \n## 2 <NA>                 impor-  <NA>  <NA>                 <NA>                  \n## 3 Expenditure category tance   <NA>  Dec. Nov.            Sep. Oct. Nov.        \n## 4 <NA>                 Nov.    Dec.… 2022- 2023-          2023- 2023- 2023-     \n## 5 <NA>                 2023    2022… Dec. Dec.            Oct. Nov. Dec.        \n## 6 <NA>                 <NA>    <NA>  2023 2023            2023 2023 2023        \n## # ℹ abbreviated name: ¹​`Seasonally adjusted percent`\n```\n:::\n\n\nIt appears that `tabulapdf` isn't separating the columns the way we'd prefer. \nLet's see if we can fix that....\n\nThere's a function, `locate_areas()`, that works interactively - it opens a viewer tab and you select the table using the mouse, as in @fig-tabulapdf-locate-area. \n\n::: column-margin\n![A screenshot of one page of one of the PDFs, showing how `locate_areas` works.](../images/advanced/tabula-pdf-locate-area.png){#fig-tabulapdf-locate-area .lightbox fig-alt=\"A screenshot of the `tabulapdf` `locate_area()` function interactive window, showing a highlighted table region.\"}\n:::\n\n`locate_areas` provides a sequence of coordinates that are relatively consistent across multiple full-page tables, so we might try to use those coordinates to improve our table parsing. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocate_areas(files[1], pages = rep(page_numbers[1], 5))\n```\n:::\n\n\n\nHere's what I got running this 5 times for the first PDF in the list -- this gives me boundaries for each column (without including the header). \n\n```\nListening on http://127.0.0.1:6481\n[[1]]\n     top     left   bottom    right \n128.1042 209.2557 583.8484 244.2939 \n\n[[2]]\n     top     left   bottom    right \n128.1042 289.0649 584.8223 327.9962 \n\n[[3]]\n     top     left   bottom    right \n128.1042 369.8473 586.7699 410.7252 \n\n[[4]]\n     top     left   bottom    right \n128.1042 452.5763 586.7699 489.5611 \n\n[[5]]\n     top     left   bottom    right \n126.1566 535.3054 586.7699 575.2099 \n\n```\n\nWe can actually run this for each column, keeping track of the left and right values, to get an even more precise way to read our data in. \nHere are my rough column alignments, using `cpi_01112024.pdf` as a test. \n\n- Table Start - 35 \n- Col2 - 209\n- Col3 - 244\n- Col4 - 289\n- Col5 - 328\n- Col6 - 370\n- Col7 - 411\n- Col8 - 453\n- Col9 - 490\n- Col10 - 535\n- Table End - 575\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntables <- extract_tables(\n  files[1], page = page_numbers[1], \n  guess = F,\n  col_names = F, \n  area = list(c(128, 35, 586, 575)), \n  columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535))\n)[[1]]\n\nhead(tables)\n## # A tibble: 6 × 10\n##   X1                          X2    X3    X4    X5    X6    X7    X8    X9   X10\n##   <chr>                    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 All items.. . . . . . … 100     297.  307.  307.   3.4  -0.1   0     0.1   0.3\n## 2 Food.. . . . . . . . .…  13.4   317.  325.  325.   2.7   0.1   0.3   0.2   0.2\n## 3 Food at home.. . . . .…   8.55  299.  303.  303.   1.3  -0.1   0.3   0.1   0.1\n## 4 Cereals and bakery pro…   1.16  345.  356.  354.   2.6  -0.7   0.2   0.5  -0.3\n## 5 Meats, poultry, fish, …   1.78  320.  320.  320.  -0.1   0.1   0.7  -0.2   0.5\n## 6 Dairy and related prod…   0.78  271.  268.  268.  -1.3   0.1   0.3   0.1   0.3\n```\n:::\n\n\nOk, that looks good - let's apply it to the rest of the reports now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntables <- map2(files, page_numbers, ~extract_tables(\n    .x, page = .y, \n    guess = F,\n    col_names = F, \n    area = list(c(128, 35, 586, 575)), \n    columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535))\n    )[[1]]\n  )\n```\n:::\n\n\n\nThen, we can read in the dates that are present in the header row, assuming that the major dividers stay the same between reports.\nI used `locate_areas()` to get the coordinates of each header that we care about.\n\n::: {.callout collapse=true}\n###### `locate_areas()` output\n\n```\n> locate_areas(files[1], pages = page_numbers[1])\n\nListening on http://127.0.0.1:7339\n[[1]]\n      top      left    bottom     right \n 89.15168  36.98473 127.13037 572.29009 \n```\n \n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nheaders <- map2( # <1>\n  files, page_numbers,  # <1>\n  ~extract_tables( # <1>\n    .x, page = .y,  # <1>\n    guess = F, # <1>\n    col_names = F,  # <1>\n    area = list(c(89, 35, 128, 575)),  # <1>\n    columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535)) # <1>\n  )[[1]] # <1>\n)  # <1>\n\nfix_headers <- function(tbl) { # <2>\nmodifiers <- c(\"\", \"Rel_imp.\", rep(\"Unadj_idx.\", 3), rep(\"Unadj_pct_chg.\", 2), rep(\"Seas_adj_pct_chg.\", 3)) # <3>\n  \n  vars <- c(tbl[1,1],   # <4>\n    paste(unlist(tbl[2:3, 2]), collapse=\"\"), # <4>\n    paste(unlist(tbl[2:3, 3]), collapse=\"\"), # <4>\n    paste(unlist(tbl[2:3, 4]), collapse=\"\"), # <4>\n    paste(unlist(tbl[2:3, 5]), collapse=\"\"), # <4>\n    paste(unlist(tbl[1:4, 6]), collapse=\"\"), # <4>\n    paste(unlist(tbl[1:4, 7]), collapse=\"\"), # <4>\n    paste(unlist(tbl[1:4, 8]), collapse=\"\"), # <4>\n    paste(unlist(tbl[1:4, 9]), collapse=\"\"), # <4>\n    paste(unlist(tbl[1:4, 10]), collapse=\"\") # <4>\n    ) |> \n    str_remove_all(\"\\\\.\") |> # <5>\n    str_replace_all(\"[ -]\", \"_\") # <5>\n  \n  paste0(modifiers, vars) # <6>\n}\n\nheaders_fixed = map(headers, fix_headers) # <7>\n\nlibrary(magrittr)\ntables <- map2(tables, headers_fixed, ~set_names(.x, .y)) # <8>\n\ntables[[1]]\n## # A tibble: 41 × 10\n##    Expenditure_category      Rel_imp.Nov2023 Unadj_idx.Dec2022 Unadj_idx.Nov2023\n##    <chr>                               <dbl>             <dbl>             <dbl>\n##  1 All items.. . . . . . . …          100                 297.              307.\n##  2 Food.. . . . . . . . . .…           13.4               317.              325.\n##  3 Food at home.. . . . . .…            8.55              299.              303.\n##  4 Cereals and bakery produ…            1.16              345.              356.\n##  5 Meats, poultry, fish, an…            1.78              320.              320.\n##  6 Dairy and related produc…            0.78              271.              268.\n##  7 Fruits and vegetables. .…            1.47              349.              351.\n##  8 Nonalcoholic beverages a…           NA                  NA                NA \n##  9 materials. . . . . . . .…            1.03              210.              216.\n## 10 Other food at home.. . .…            2.33              263.              270.\n## # ℹ 31 more rows\n## # ℹ 6 more variables: Unadj_idx.Dec2023 <dbl>,\n## #   Unadj_pct_chg.Dec2022_Dec2023 <dbl>, Unadj_pct_chg.Nov2023_Dec2023 <dbl>,\n## #   Seas_adj_pct_chg.Sep2023_Oct2023 <dbl>,\n## #   Seas_adj_pct_chg.Oct2023_Nov2023 <dbl>,\n## #   Seas_adj_pct_chg.Nov2023_Dec2023 <dbl>\n```\n:::\n\n1. Pull headers out using roughly the same command as we used to get the tables, but with a different top and bottom area.\n2. Write a function to clean the headers up a bit\n3. `modifiers` are the top row of the variable names that aren't captured by our headers object. They're consistent from report to report. We'll separate the modifier from the dates using `.`, for easier cleaning.\n4. Extract only the components of the headers object that are needed -- this depends on whether we're talking about a single month-to-month comparison, or a time span. In the first column, we only need the \"expenditure category\" object. \n5. Remove all `.` characters from the names so they don't mess up our delimiter, and replace spaces and dashes with `_`. \n6. Paste the two vectors together to get the names.\n7. Apply the function to each header\n8. Set the names of the variables in each table to the corresponding header.\n\nThen, we just need to clean things up a bit more.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\nreport_date <- str_replace(basename(files), \"cpi_(.*)\\\\.pdf\", \"\\\\1\") |> mdy() # <1>\n\ncpi_data <- map2(tables, report_date, ~mutate(.x, report_date = .y)) # <2>\n\ncpi_data <- map(\n  cpi_data, \n  ~pivot_longer(., -c(Expenditure_category, report_date), # <3>\n                names_to=\"var\",  # <3>\n                values_to = \"val\") |> # <3>\n    separate(var, c(\"varname\", \"vardate\"), sep = \"\\\\.\") |> # <4>\n    pivot_wider(id_cols = c(\"Expenditure_category\", \"report_date\", \"vardate\"), names_from = \"varname\", values_from = \"val\") # <5>\n)\n\ncpi_data <- cpi_data |>\n  bind_rows() |>  # <6>\n  mutate(Expenditure_category = str_remove_all(Expenditure_category, \"[\\\\. ]{1,}$\") |>\n           str_trim())  # <7>\n\ndim(cpi_data)\ncpi_data\n## [1] 6601    7\n## # A tibble: 6,601 × 7\n##    Expenditure_category report_date vardate      Rel_imp Unadj_idx Unadj_pct_chg\n##    <chr>                <date>      <chr>          <dbl>     <dbl>         <dbl>\n##  1 All items            2024-01-11  Nov2023        100        307.          NA  \n##  2 All items            2024-01-11  Dec2022         NA        297.          NA  \n##  3 All items            2024-01-11  Dec2023         NA        307.          NA  \n##  4 All items            2024-01-11  Dec2022_Dec…    NA         NA            3.4\n##  5 All items            2024-01-11  Nov2023_Dec…    NA         NA           -0.1\n##  6 All items            2024-01-11  Sep2023_Oct…    NA         NA           NA  \n##  7 All items            2024-01-11  Oct2023_Nov…    NA         NA           NA  \n##  8 Food                 2024-01-11  Nov2023         13.4      325.          NA  \n##  9 Food                 2024-01-11  Dec2022         NA        317.          NA  \n## 10 Food                 2024-01-11  Dec2023         NA        325.          NA  \n## # ℹ 6,591 more rows\n## # ℹ 1 more variable: Seas_adj_pct_chg <dbl>\n```\n:::\n\n1. Determine the date of the report from the filename\n2. Add a column with the corresponding report date to each table\n3. Convert each table to long form with expenditure category and report date as ID columns. \n4. Split the variable names from the period over which the variable is calculated. In theory, we should be able to determine the lag for each of these and not care about the date, but I don't trust that the report has been that consistent over 2 years... paranoia. \n5. Pivot wider, so that there's a column for each variable name.\n6. Bind all the tables together into a single table\n7. Clean up the expenditure category names so that the dots are gone. \n\nWe could probably get this data cleaner -- the lagged columns should be specified better, but this will do for now. \nLet's at least do something interesting with this data that wouldn't have been possible without reading data in from the tables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\ncpi_data |>\n  filter(Expenditure_category %in% c(\"Energy\", \"Food\", \"Shelter\", \"Medical care services\", \"commodities\", \"Transportation services\")) |>\n  mutate(Category = str_replace_all(Expenditure_category, c(\"commodities\"= \"Non-food Goods\", \"Medical care services\" = \"Medical\", \"Transportation services\" =\"Transportation\")) |>\n           factor(levels = c(\"Shelter\", \"Non-food Goods\", \"Energy\", \"Food\", \"Medical\", \"Transportation\"))) |>\n  select(Category, report_date, Rel_imp) |>\n  na.omit() |>\n  ggplot(aes(x = report_date, y = Rel_imp, color = Category)) + geom_line() + \n  xlab(\"Date\") + ylab(\"Relative Importance in CPI-U Calculation\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](06-pdf-tools_files/figure-html/unnamed-chunk-16-1.png){width=2100}\n:::\n:::\n\n\n\n##### Python {-}\n\nIf we learned anything from doing this in R, it's that it probably won't work the first time. \nSo this code saves a bit of evaluation time by using some of the info we got from R, like the page numbers (I've adjusted these to match python indexing). \n\n::: {.cell}\n\n```{.python .cell-code}\nimport pdfplumber\nimport tabula\nfrom glob import glob\nimport numpy as np\n\nfiles = glob(\"../data/bls-pdfs/*.pdf\")\n\ndef find_page_number(file):\n  pdf = pdfplumber.open(file)\n  has_tb1 = [\"Table 1\" in i.extract_text() for i in pdf.pages]\n  pdf.close()\n  return int(np.where(has_tb1)[0][0])\n\n# page_numbers = [find_page_number(i) for i in files]\n# page_numbers = [i[0] for i in page_numbers]\npage_numbers = [8, 8, 7, 8, 8, 8, 7, 8, 9, 8, 8, 7, 8, 8, 8, 9, 8, 8, 8, 7, 7, 9, 8]\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntables = tabula.read_pdf(files[0], pages=page_numbers[0]+1)\n## RuntimeError: Unable to start JVM\ntables\n## NameError: name 'tables' is not defined\n```\n:::\n\nOk, so this time, we get 5 columns, which isn't quite right - it seems as if the major headers are determining the column structure. \n\nLet's see if we can define the table area and help things out. \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntbl = tabula.read_pdf(files[0], pages=page_numbers[0]+1, area=[128, 35, 586, 575], pandas_options={'header': None})[0]\n## RuntimeError: Unable to start JVM\ntbl\n## NameError: name 'tbl' is not defined\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom itertools import chain\nimport pandas as pd\n\nheader = tabula.read_pdf(files[0], pages=page_numbers[0]+1, area=[89, 35, 128, 575], pandas_options={'header': None})[0]\n## RuntimeError: Unable to start JVM\n\ndef fix_headers(tbl):\n  modifiers=[[\"\", \"Rel_imp.\"], [\"Unadj_idx.\"]*3, [\"Unadj_pct_chg.\"]*2, [\"Seas_adj_pct_chg.\"]*3]\n  modifiers=list(chain.from_iterable(modifiers))\n  # https://stackoverflow.com/questions/11860476/how-to-unnest-a-nested-list\n  \n  spans=pd.Series([tbl.loc[0,0], \n  ''.join(tbl.loc[1:2,1]),\n  ''.join(tbl.loc[1:2,2]),\n  ''.join(tbl.loc[1:2,3]),\n  ''.join(tbl.loc[1:2,4]),\n  ''.join(tbl.loc[0:3,5]),\n  ''.join(tbl.loc[0:3,6]),\n  ''.join(tbl.loc[0:3,7]),\n  ''.join(tbl.loc[0:3,8]),\n  ''.join(tbl.loc[0:3,9])])\n  spans=spans.str.replace(\"\\.\", \"\", regex = True)\n  spans=spans.str.replace(\"[ -]\", \"_\", regex = True)\n  \n  return modifiers + spans\n\nheader = fix_headers(header)\n## NameError: name 'header' is not defined\n\n\ndef read_table_1(file, page_number): \n  tbl=tabula.read_pdf(file, pages=page_number+1, \n                      area=[128, 35, 586, 575], \n                      pandas_options={'header': None})[0]\n  header=tabula.read_pdf(file, pages=page_number+1, \n                         area=[89, 35, 128, 575], \n                         pandas_options={'header': None})[0]\n  header=fix_headers(header)\n  tbl = tbl.rename(header, axis=1)\n  return tbl\n\ntables = [read_table_1(file, page_numbers[i]) for i,file in enumerate(files)]\n## RuntimeError: Unable to start JVM\n```\n:::\n\n\n\n:::\n\n::::\n\n\n::: learnmore\n## Other PDF Options to Explore\n\n- `parsemypdf` [@srivastavaGenieincodebottleParsemypdf2025], a collection of AI-based parsing libraries\n\n:::\n\n## References {-}\n",
    "supporting": [
      "06-pdf-tools_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}