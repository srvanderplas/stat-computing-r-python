{
  "hash": "5886512848aae0a9452bdfa610f09318",
  "result": {
    "engine": "knitr",
    "markdown": "# Joining Data {#sec-data-join}\n\n\nThe final essential data tidying and transformation skill you need to acquire is joining tables. \nIt is common for data to be organized **relationally** - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\n\n## Objectives {-}\n\n- Identify columns (keys) which can be used to join separate but related tables\n\n- Sketch/plan out join operations based on matching keys and given objectives\n\n- Implement planned join operations in R or python\n\n- Identify when join operations have not completed successfully by looking for duplicated rows, number of rows/columns in the finished object, and missing value counts.\n\n\n\n\n::: {.callout-caution collapse=\"true\"}\n## Relational Data Example: Primary School Records\n\nEach individual has certain characteristics:\n\n-   full_name\n-   gender\n-   birth date\n-   ID number\n\nEach student has specific characteristics:\n\n-   ID number\n-   parent name\n-   parent phone number\n-   medical information\n-   Class ID\n\nTeachers may also have additional information:\n\n-   ID number\n-   Class ID\n-   employment start date\n-   education level\n-   compensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\n-   ID number\n-   Student ID\n-   Class ID\n-   year\n-   term number\n-   subject\n-   grade\n-   comment\n\nAnd for teachers, there are employment records on a yearly basis\n\n-   ID number\n-   Employee ID\n-   year\n-   rating\n-   comment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\n-   location ID\n-   class ID\n-   meeting time\n-   grade level\n\nEach location might also have some logistical information attached:\n\n-   location ID\n-   room number\n-   building\n-   number of seats\n-   AV equipment\n\n![Primary School Database Schema](../images/wrangling/PrimarySchoolExample.png) <!-- <a href=\"https://dbdiagram.io/embed/5ef387179ea313663b3b048e\">Link to diagram of the database</a> -->\n\nWe could go on, but you can see that this data is hierarchical, but also relational: \n\n- each class has both a teacher and a set of students \n- each class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it can be done) because all of the teacher information would be duplicated for each student in each class; all of the student's individual info would be duplicated for each grade. \nThere would be a lot of wasted storage space and the tables would be much more confusing as well.\n\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. \nSuppose we want a list of all of the birthdays in a certain class. \nWe would need to take the following steps:\n\n-   get the Class ID\n-   get any teachers that are assigned that Class ID - specifically, get their ID number\n-   get any students that are assigned that Class ID - specifically, get their ID number\n-   append the results from teachers and students so that there is a list of all individuals in the class\n-   look through the \"individual data\" table to find any individuals with matching ID numbers, and keep those individuals' birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren't well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\n:::\n\n## Vocabulary\n\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\n\n**Keys** are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. \nA **primary key** identifies an observation in its own table. \nA **foreign key** identifies an observation in another table.\n\nThere are 3 main types of table joins:\n\n-   **Mutating joins**, which add columns from one table to matching rows in another table\\\n    Ex: adding birthday to the table of all individuals in a class\n\n-   **Filtering joins**, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don't change)\\\n    Ex: finding all teachers or students who have class ClassID\n\n-   **Set operations**, which treat observations as set elements (e.g. union, intersection, etc.)\\\n    Ex: taking the union of all student and teacher IDs to get a list of individual IDs\n\n## Illustrating Joins\n\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\n\nIf we start with two tables, x and y,\n\n![](../images/wrangling/original-dfs.png)\n\nThe next several sections will show animations demonstrating the different types of joins.\n\n### Mutating Joins\n\nWe're primarily going to focus on mutating joins, as filtering joins can be accomplished by ... filtering ... rather than by table joins.\n\n::: panel-tabset\n#### Inner Join\n\nWe can do a filtering `inner_join` to keep only rows which are in both tables (but we keep all columns)\n\n![](../images/wrangling/inner-join.gif)\n\n#### Left Join\n\nBut what if we want to keep all of the rows in x? We would do a `left_join`\n\n![](../images/wrangling/left-join.gif)\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n![](../images/wrangling/left-join-extra.gif)\n\n#### Right Join\n\nIf we wanted to keep all of the rows in y, we would do a `right_join`:\n\n![](../images/wrangling/right-join.gif)\n\n(or, we could do a left join with y and x, but... either way is fine).\n\n#### Full Join\n\nAnd finally, if we want to keep all of the rows, we'd do a `full_join`:\n\n![](../images/wrangling/full-join.gif)\n\nYou can find other animations corresponding to filtering joins and set operations [here](https://github.com/gadenbuie/tidyexplain)\n:::\n\nEvery join has a \"left side\" and a \"right side\" - so in `some_join(A, B)`, A is the left side, B is the right side.\n\nJoins are differentiated based on how they treat the rows and columns of each side. \nIn mutating joins, the columns from both sides are always kept.\n\n+-------+-----------+------------+----------+\n|       | Left Side | Right Side |          |\n+-------+-----------+------------+----------+\n|       | Join Type | Rows       | Cols     |\n+-------+-----------+------------+----------+\n| inner | matching  | all        | matching |\n+-------+-----------+------------+----------+\n| left  | all       | all        | matching |\n+-------+-----------+------------+----------+\n| right | matching  | all        | all      |\n+-------+-----------+------------+----------+\n| outer | all       | all        | all      |\n+-------+-----------+------------+----------+\n\n#### Code\n\n\n\n\n\n\n\n\n\n\n\n::: panel-tabset\n\n##### R (base)\n\nJoins in base R are accomplished with the `merge` command. \n\nSpecify the keys to join by using `by` (or `by.x` and `by.y` if the column names are different in the two tables). \nSpecify the rows to keep using `all` or `all.x` and `all.y`. \nBy default, R will merge on any variables that have the same names in each table.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerge(x, y, by = \"v1\", all = F) # inner join\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nmerge(x, y, by = \"v1\", all = T) # full join\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\nmerge(x, y, by = \"v1\", all.x = T) # left join\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 <NA>\nmerge(x, y, by = \"v1\", all.y = T) # right join\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 <NA> y4\n```\n:::\n\n\n\n\n##### R (tidy)\n`dplyr` contains functions that specifically implement mutating joins separately, primarily for code readability.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ninner_join(x, y)\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nleft_join(x, y)\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 <NA>\nright_join(x, y)\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 <NA> y4\nfull_join(x, y)\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\n```\n:::\n\n\n\n\n##### Pandas\n\nMutating joins in pandas are accomplished with the `merge` command.\nThe join type can be specified using the `how` parameter (left, right, outer, inner, cross). \nSpecify the keys to join by using `on` (or `left_on` and `right_on` if the column names are different in the two tables). \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\npd.merge(x, y) # inner join (how = 'inner' is default)\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\npd.merge(x, y, how = 'left')\n##    v1  v2   v3\n## 0   1  x1   y1\n## 1   2  x2   y2\n## 2   3  x3  NaN\npd.merge(x, y, how = 'right')\n##     v1   v2  v3\n## 0  1.0   x1  y1\n## 1  2.0   x2  y2\n## 2  4.0  NaN  y4\npd.merge(x, y, how = 'outer') # full join\n##     v1   v2   v3\n## 0  1.0   x1   y1\n## 1  2.0   x2   y2\n## 2  3.0   x3  NaN\n## 3  4.0  NaN   y4\n```\n:::\n\n\n\n:::\n\n::: callout-demo\n### Demo: Mutating Joins\n\n::: panel-tabset\n#### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble)\nlibrary(dplyr)\nt1 <- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 <- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n```\n:::\n\n\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_join(t1, t2)\n## # A tibble: 2 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n```\n:::\n\n\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don't match get filled in with NAs.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n```\n:::\n\n\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nright_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n```\n:::\n\n\n\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it's known as a `full_join`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_join(t1, t2)\n## # A tibble: 4 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n```\n:::\n\n\n\n\n#### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n```\n:::\n\n\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n```\n:::\n\n\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don't match get filled in with NAs.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n```\n:::\n\n\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n```\n:::\n\n\n\n\nAn outer join keeps everything - all rows, all columns.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  C  NaN  4.0\n## 3  D  3.0  5.0\n```\n:::\n\n\n\n:::\n:::\n\nI've included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins [here](https://r4ds.had.co.nz/relational-data.html#filtering-joins) [@r4ds].\n\n### Filtering Joins\n\n::: panel-tabset\n#### Semi Join\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n![](../images/wrangling/semi-join.gif)\n\n#### Anti Join\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n![](../images/wrangling/anti-join.gif)\n:::\n\n#### Code\n\n::: panel-tabset\n\n##### R (base)\n\nSemi and anti joins aren't available by default in base R. You have to do multiple stages of operations to get either one to work.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Semi-join\n# First, do an inner join\ninnerxy = merge(x, y, all = F)\ninnerxy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nsemixy = innerxy[,names(innerxy)%in% names(x)]\nsemixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n## Anti-join\n# First, do an outer join\nouterxy = merge(x, y, all = T)\nouterxy\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\n# Then, drop any rows with NAs\nantixy = na.omit(outerxy)\nantixy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nantixy = antixy[,names(antixy) %in% names(x)]\nantixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n```\n:::\n\n\n\n\n##### R (tidy)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nsemi_join(x, y)\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\nanti_join(x, y)\n##   v1 v2\n## 1  3 x3\n```\n:::\n\n\n\n\n##### Pandas\nIn pandas, we have to be a bit tricky to get semi and anti joins. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n# First, we merge the two data frames (inner by default)\nsemixy = pd.merge(x, y) # Semi join\nsemixy\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\n\n# Then, we drop the extra columns\nsemixy = semixy[semixy.columns.intersection(x.columns)]\nsemixy\n##    v1  v2\n## 0   1  x1\n## 1   2  x2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# This syntax keeps track of which rows are from which table\nouter = x.merge(y, how='outer', indicator=True)\nouter\n##     v1   v2   v3      _merge\n## 0  1.0   x1   y1        both\n## 1  2.0   x2   y2        both\n## 2  3.0   x3  NaN   left_only\n## 3  4.0  NaN   y4  right_only\n# Then we drop any rows that aren't 'left_only'\nantixy = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\nantixy\n##     v1  v2   v3\n## 2  3.0  x3  NaN\n# Then we drop any cols that aren't in x\nantixy = antixy[antixy.columns.intersection(x.columns)]\nantixy\n##     v1  v2\n## 2  3.0  x3\n```\n:::\n\n\n\n:::\n\n### Set Operations\n\nWhen talking about set operations, we start with two different data frames than those used above:\n\n![](../images/wrangling/original-dfs-set-ops.png)\n\n::: panel-tabset\n#### Union\n\nAll unique rows from x and y\n\n![](../images/wrangling/union.gif)\n\nOr, all unique rows from y and x.\n\n![](../images/wrangling/union-rev.gif)\n\n#### Union All\n\nAll rows from x and y, keeping duplicate rows.\n\n![](../images/wrangling/union-all.gif)\n\nThis is fundamentally the same as an `rbind` or `bind_rows` operation. \n\n#### Intersection\n\nCommon rows in x and y, keeping only unique rows.\n\n![](../images/wrangling/intersect.gif)\n\n#### Set Difference\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n![](../images/wrangling/setdiff.gif)\n\n![](../images/wrangling/setdiff-rev.gif)\n:::\n\n#### Code\n\n\n\n\n\n\n\n\n\n\n\n::: panel-tabset\n\n##### R (base)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunionxy = unique(rbind(x, y))\nunionxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 5  2  b\n\nunionallxy = rbind(x, y)\nunionallxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  1  a\n## 5  2  b\n\nintersectxy = merge(x, y, all = F)\nintersectxy\n##   v1 v2\n## 1  1  a\n```\n:::\n\n\n\n\nIt is possible to get set difference and intersection for data frames by applying the base methods `setdiff` and `intersect`, but `dplyr` does this by overriding those defaults, so it's easier to just use that.\n\n##### R (tidy)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nunion(x, y)\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  2  b\nunionall(x, y)\n## Error in unionall(x, y): could not find function \"unionall\"\nsetdiff(x, y)\n##   v1 v2\n## 1  1  b\n## 2  2  a\nsetdiff(y, x)\n##   v1 v2\n## 1  2  b\nintersect(x, y)\n##   v1 v2\n## 1  1  a\n```\n:::\n\n\n\n\n##### Pandas\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# Union\npd.concat([x, y]).drop_duplicates(keep = False)\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n## 1  2.0  b\n\n# Union all\npd.concat([x, y])\n##     v1 v2\n## 0  1.0  a\n## 1  1.0  b\n## 2  2.0  a\n## 0  1.0  a\n## 1  2.0  b\n\n# Intersection\nintersect = x.merge(y, how='inner')\nintersect\n##     v1 v2\n## 0  1.0  a\n\n# Set Difference\nsetdiffxy = x.merge(y, how='outer', indicator=True)\nsetdiffxy = setdiffxy[(setdiffxy._merge=='left_only')].drop('_merge', axis = 1)\nsetdiffxy\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n\nsetdiffyx = x.merge(y, how='outer', indicator=True)\nsetdiffyx = setdiffyx[(setdiffyx._merge=='right_only')].drop('_merge', axis = 1)\nsetdiffyx\n##     v1 v2\n## 3  2.0  b\n```\n:::\n\n\n\n:::\n\n## Example: NYC Flights\n\nWe'll use the `nycflights13` package in R. \nUnfortunately, the data in this package are too big for me to reasonably store on github (you'll recall, I had to use a small sample the last time we played with this data...). \nSo before we can work with this data, we have to load the tables into Python.\n\n::: callout-caution\n### Loading Data\n::: panel-tabset\n#### R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"../data/\")\n## <SQLiteConnection>\n##   Path: /home/susan/Projects/Class/stat-computing-r-python/data/nycflights13.sqlite\n##   Extensions: TRUE\n```\n:::\n\n\n\n\n#### Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sqlite3\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\ncur = con.cursor()\n```\n:::\n\n\n\n:::\n\n:::\n\n[I am not going to cover SQLITE commands here - I'm just going to use the bare minimum, but you can find a very nice [introduction to python and SQLITE at datacarpentry](https://datacarpentry.org/python-ecology-lesson/09-working-with-sql.html) [@thecarpentriesAccessingSQLiteDatabases2022], and an [introduction to the dbplyr package](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html) for a nice R-SQLITE interface.]{.aside}\n\n::: callout-tip\n### Try it out: Understanding Relational Data\n\n::: panel-tabset\n#### Problem\n\nSketch a diagram of which fields in each table match fields in other tables. \nUse the [data documentation](https://nycflights13.tidyverse.org/reference/index.html) to help you with your sketch.\n\n#### Solution\n\n![The nycflights database schema](https://d33wubrfki0l68.cloudfront.net/245292d1ea724f6c3fd8a92063dcd7bfb9758d02/5751b/diagrams/relational-nycflights.png) [here](https://r4ds.had.co.nz/relational-data.html#nycflights13-relational) (scroll down a bit).\n:::\n:::\n\n::: callout-caution\n### Example: Mutating Joins\n\nThese functions may become a bit more interesting once we try them out on real-world data. \nUsing the flights data, let's determine whether there's a relationship between the age of a plane and its delays.\n\n::: panel-tabset\n#### R\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age <- planes %>%\n  mutate(age = 2013 - year) %>% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane <- flights %>%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres <- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n```\n\n::: {.cell-output-display}\n![](06-data-join_files/figure-html/flights-delay-age-R-1.png){width=45%}\n:::\n\n::: {.cell-output-display}\n![](06-data-join_files/figure-html/flights-delay-age-R-2.png){width=45%}\n:::\n:::\n\n\n\n\nIt doesn't look like there's much of a relationship to me. \nIf anything, older planes are more likely to be early, but I suspect there aren't enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n#### Python\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## <plotnine.ggplot.ggplot object at 0x7f615c297cd0>\n\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## <plotnine.ggplot.ggplot object at 0x7f615c297710>\n```\n:::\n\n\n\n:::\n:::\n\n\n## Example: Gas Prices Data {#sec-gas-price-ex2}\n\nLet's return to the gas price data introduced in @sec-gas-price-ex. \nI've repeated the setup chunks here for you to read in the data appropriately.\n\n::: callout-demo\n### Setup: Gas Price Data Cleaning\n\nFor the next example, we'll read the data in from the HTML table online and work to make it something we could e.g. plot. \nBefore we can start cleaning, we have to read in the data:\n\n::: panel-tabset\n#### R\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n#### Python\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n:::\n:::\n\n::: callout-tip\n### Try it out: Formatting using merge + pivot\n\n::: panel-tabset\n#### Problem\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\n\nYou can start with the `gas_prices_raw`\n\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\n\nHow do your steps compare to the steps you used for the manual approach?\n\n#### Sketch\n\n![](../images/wrangling/gas-prices-steps2.png) \n\n#### R solution\n\nWe'll use the same data cleaning function as before:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n## Error in set_names(., fix_gas_names(names(.))): could not find function \"set_names\"\n\nhead(gas_prices_raw)\n## Error: object 'gas_prices_raw' not found\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_dates <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\n## Error: object 'gas_prices_raw' not found\ngas_prices_values <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n## Error: object 'gas_prices_raw' not found\n\nhead(gas_prices_dates)\n## Error: object 'gas_prices_dates' not found\nhead(gas_prices_values)\n## Error: object 'gas_prices_values' not found\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_dates_long <- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\n## Error: object 'gas_prices_dates' not found\ngas_prices_values_long <- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n## Error: object 'gas_prices_values' not found\n\nhead(gas_prices_dates_long)\n## Error: object 'gas_prices_dates_long' not found\nhead(gas_prices_values_long)\n## Error: object 'gas_prices_values_long' not found\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean <- gas_prices_dates_long %>%\n  filter(month_day != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %>% \n           ymd())\n## Error: object 'gas_prices_dates_long' not found\n\ngas_prices_values_long_clean <- gas_prices_values_long %>%\n  filter(price_per_gallon != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n## Error: object 'gas_prices_values_long' not found\n\nhead(gas_prices_dates_long_clean)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices_values_long_clean)\n## Error: object 'gas_prices_values_long_clean' not found\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices <- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %>%\n  select(Date, price_per_gallon)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices)\n## Error: object 'gas_prices' not found\n```\n:::\n\n\n\n\n#### Python solution\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_raw = gas_prices_html.copy()\n## NameError: name 'gas_prices_html' is not defined\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n## NameError: name 'gas_prices_raw' is not defined\n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## NameError: name 'fix_gas_names' is not defined\ncolnames\n## NameError: name 'colnames' is not defined\n\n# Set new column names\ngas_prices_raw.columns = colnames\n## NameError: name 'colnames' is not defined\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_raw.head()\n## NameError: name 'gas_prices_raw' is not defined\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_dates.head()\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values.head()\n## NameError: name 'gas_prices_values' is not defined\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n## NameError: name 'gas_prices_values' is not defined\n\ngas_prices_dates_long.head()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_values_long.head()\n## NameError: name 'gas_prices_values_long' is not defined\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna().copy()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n## NameError: name 'gas_prices_dates_long_clean' is not defined\n\n\ngas_prices_values_long_clean = gas_prices_values_long.dropna().copy()\n## NameError: name 'gas_prices_values_long' is not defined\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## NameError: name 'gas_prices_values_long_clean' is not defined\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n## NameError: name 'gas_prices_values_long_clean' is not defined\n\ngas_prices_dates_long_clean.head()\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_values_long_clean.head()\n## NameError: name 'gas_prices_values_long_clean' is not defined\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices.head()\n## NameError: name 'gas_prices' is not defined\n```\n:::\n\n\n\n:::\n:::\n\n## References {#sec-data-join-refs}\n",
    "supporting": [
      "06-data-join_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}