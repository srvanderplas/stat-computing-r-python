# Exploratory Data Analysis {#sec-eda}

## Objectives {-}

- Understand the main goals of exploratory data analysis
- Generate and answer questions about a new dataset using charts, tables, and numerical summaries

## Package Installation 

You will need the `seaborn` and `matplotlib` (python) and `ggplot2` (R) packages for this section. 

```{r}
#| eval: !expr F
install.packages("ggplot2")
```

To install python graphing libraries, pick one of the following methods (you can read more about them and decide which is appropriate for you in @sec-py-pkg-install)

::: panel-tabset
### System Terminal

```{bash}
#| eval: !expr F
pip3 install matplotlib seaborn
```

### R Terminal

This package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don't try to install packages this way).

```{r}
#| eval: !expr F
reticulate::py_install(c("matplotlib", "seaborn"))
```

### Python Terminal

In a python chunk (or the python terminal), you can run the following command. This depends on something called "IPython magic" commands, so if it doesn't work for you, try the System Terminal method instead.

```{python}
#| eval: !expr F
%pip install matplotlib seaborn
```

Once you have run this command, please comment it out so that you don't reinstall the same packages every time.

:::


::: {.callout-learnmore}
## Extra Reading {.unnumbered}

[The EDA chapter in R for Data Science](https://r4ds.had.co.nz/exploratory-data-analysis.html) [@r4ds] is very good at
explaining what the goals of EDA are, and what types of questions you
will typically need to answer in
EDA. Much of the
material in this chapter is based at least in part on the R4DS chapter.
:::


![(Image from https://www.mrdbourke.com) 
The EDA lifecycle starts with data collection and is
primarily a cycle between checking data types, assessing distributions,
feature engineering, and model iteration. These tasks are supported by
summary statistics, visualization, and
modeling.](../images/wrangling/an-EDA-lifecycle.png){fig-align="center"
width="75%"}
<!-- https://www.mrdbourke.com/content/images/size/w1000/2019/09/an-EDA-lifecycle.png -->

Major components of Exploratory Data Analysis (EDA):

-   generating questions about your data
-   look for answers to the questions (visualization, transformation,
    modeling)
-   use answers to refine the questions and generate new questions

EDA is an iterative process. It is like brainstorming - you start with an idea or question you might have about the data, investigate, and then generate new ideas. 
EDA is useful even when you are relatively familiar with the type of data you're working with: in any dataset, it is good to make sure that you know the quality of the data as well as the relationships between the variables in the dataset.

EDA is important because it helps us to know what challenges a particular data set might bring, what we might do with it. 
Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence.

While in many classes you'll be given mostly clean data, you do need to know how to clean your own data up so that you can use more interesting data sets for projects (and for fun!). 
EDA is an important component to learning how to work with messy data.

::: .callout-note
In this section, I will mostly be using the plot commands that come with
base R/python and require no extra packages. The R for Data Science book
[@r4ds] shows plot commands which use the `ggplot2` library. I'll show
you some plots from ggplot here as well, but you don't have to
understand how to generate them yet. We will learn more about ggplot2
later, though if you want to start using it now, you may.
:::

## A Note on Language Philosophies 

It is usually relatively easy to get summary statistics from a dataset, but the "flow" of EDA is somewhat different depending on the language patterns.

> You must realize that R is written by experts in statistics and
> statistical computing who, despite popular opinion, do not believe
> that everything in SAS and SPSS is worth copying. Some things done in
> such packages, which trace their roots back to the days of punched
> cards and magnetic tape when fitting a single linear model may take
> several days because your first 5 attempts failed due to syntax errors
> in the JCL or the SAS code, still reflect the approach of "give me
> every possible statistic that could be calculated from this model,
> whether or not it makes sense". The approach taken in R is different.
> The underlying assumption is that the useR is thinking about the
> analysis while doing it. -- Douglas Bates

I provide this as a historical artifact, but it does explain the difference between the approach to EDA and model output in R and Python, and the approach in SAS, which you may see in your other statistics classes. 
This is not (at least, in my opinion) a criticism -- the SAS philosophy dates back to the mainframe and punch card days, and the syntax and output still bear evidence of that -- but it is worth noting.

In R and in Python, you will have to specify each piece of output you want, but in SAS you will get more than you ever wanted with a single command. Neither approach is wrong, but sometimes one is preferable over the other for a given problem.

## Generating EDA Questions

I very much like the two quotes in the @r4ds section on EDA Questions:

> There are no routine statistical questions, only questionable
> statistical routines. --- Sir David Cox

> Far better an approximate answer to the right question, which is often
> vague, than an exact answer to the wrong question, which can always be
> made precise. --- John Tukey

As statisticians, we are concerned with variability by default. 
This is also true during EDA: we are interested in variability (or sometimes, lack thereof) in the variables in our dataset, including the co-variability between multiple variables.

We may assess variability using pictures or numerical summaries:

-   histograms or density plots (continuous variables)
-   column plots (categorical variables)
-   boxplots
-   5 number summaries (min, 25%, mean, 75%, max)
-   tabular data summaries (for categorical variables)

In many cases, this gives us a picture of both variability and the
"typical" value of our variable.

Sometimes we may also be interested in identifying unusual values: outliers, data entry errors, and other points which don't conform to our expectations. 
These unusual values may show up when we generate pictures and the axis limits are much larger than expected.

We also are usually concerned with missing values - in many cases, not all observations are complete, and this missingness can interfere with statistical analyses. 
It can be helpful to keep track of how much missingness there is in any particular variable and any patterns of missingness that would impact the eventual data analysis[^exploratory-data-analysis-1].

[^exploratory-data-analysis-1]: One package for this process in R is
    `naniar` [@tierneyNaniarDataStructures2021].

If you are having trouble getting started on EDA, @danielbourkeGentleIntroductionExploratory2019 provides a nice checklist to get you thinking:

> 1.  What question(s) are you trying to solve (or prove wrong)?
> 2.  What kind of data do you have and how do you treat different
>     types?
> 3.  What's missing from the data and how do you deal with it?
> 4.  Where are the outliers and why should you care about them?
> 5.  How can you add, change or remove features to get more out of your
>     data?

## Useful EDA Techniques

::: column-margin
[![Nintendo, Creatures, Game Freak, The Pok√©mon Company, Public domain, via Wikimedia Commons](../images/wrangling/International_Pokemon_logo.png){fig-alt="Pokemon logo"}](https://commons.wikimedia.org/wiki/File:International_Pok%C3%A9mon_logo.svg")
:::

::: callout-caution

### Example: Generations of Pokemon

Suppose we want to explore Pokemon. There's not just the original 150 (gotta catch 'em all!) - now there are over 1000! 
Let's start out by looking at the proportion of Pokemon added in each of the 9 generations.

::: panel-tabset

#### R setup {-}
```{r}
#| label: poke-read-data-r
#| message: false
library(readr)
library(dplyr)
library(tidyr)
library(stringr)

# Setup the data
poke <- read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv", na = '.') %>%
  mutate(generation = factor(gen))
```

#### Python setup {-}
```{python}
#| label: poke-read-data-py
import pandas as pd
poke = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv")
poke['generation'] = pd.Categorical(poke.gen)
```
:::
:::

This data has several categorical and continuous variables that should allow for a reasonable demonstration of a number of techniques for exploring data.

### Numerical Summary Statistics

::: panel-tabset
#### R: summary {.unnumbered}

The first, and most basic EDA command in R is `summary()`.

For numeric variables, `summary` provides 5-number summaries plus the mean. 
For categorical variables, `summary` provides the length of the variable and the Class and Mode. For factors, `summary` provides a table of the most common values, as well as a catch-all "other" category.

```{r}
#| label: readr-eda
# Make types into factors to demonstrate the difference
poke <- tidyr::separate(poke, type, into = c("type_1", "type_2"), sep = ",")
poke$type_1 <- factor(poke$type_1)
poke$type_2 <- factor(poke$type_2)

summary(poke)
```

One common question in EDA is whether there are missing values or other inconsistencies that need to be handled. 
`summary()` provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis.
We can see in this summary that 673 pokemon don't have a second type.

We also look for extreme values. 
There is at least one pokemon who appears to have a weight of 999.90 kg. 
Let's investigate further:

```{r}
#| label: poke-weight
poke[poke$weight_kg > 999,] 
# Show any rows where weight_kg is extreme
```

This is the last row of our data frame, and this pokemon appears to have
many missing values.

#### Python: describe {.unnumbered}

The most basic EDA command in pandas is `df.describe()` (which operates
on a DataFrame named `df`). Like `summary()` in R, `describe()` provides
a 5-number summary for numeric variables. For categorical variables,
`describe()` provides the number of unique values, the most common
value, and the frequency of that common value.

```{python}
#| label: poke
# Split types into two columns
poke[['type_1', 'type_2']] = poke.type.str.split(",", expand = True)
# Make each one categorical
poke['type_1'] = pd.Categorical(poke.type_1)
poke['type_2'] = pd.Categorical(poke.type_2)

poke.iloc[:,:].describe() # describe only shows numeric variables by default

# You can get categorical variables too if that's all you give it to show
poke['type_1'].describe()
poke['type_2'].describe()

```

#### R: skimr {.unnumbered}

An R package that is incredibly useful for this type of dataset
exploration is `skimr`.

```{r}
#| label: skimr-poke-demo
library(skimr)
skim(poke)
```

`skim` provides a beautiful table of summary statistics along with a
sparklines-style histogram of values, giving you a sneak peek at the
distribution.

#### python: skimpy {.unnumbered}

There is a similar package to `skimr` in R called `skimpy` in Python.

```{python}
#| label: skimpy-poke-demo
from skimpy import skim
skim(poke)
```
:::

### Assessing Distributions

We are often also interested in the distribution of values.

#### Categorical Variables

One useful way to assess the distribution of values is to generate a
cross-tabular view of the data. This is mostly important for variables
with a relatively low number of categories - otherwise, it is usually
easier to use a graphical summary method.

##### Tabular Summaries {.unnumbered}

::: panel-tabset
###### R {.unnumbered}

We can generate cross-tabs for variables that we know are discrete (such
as generation, which will always be a whole number). We can even
generate cross-tabular views for a combination of two variables (or
theoretically more, but this gets hard to read and track).

```{r}
#| label: poke-distribution
table(poke$generation)

table(poke$type_1, poke$type_2)
```

###### Python {.unnumbered}

```{python}
#| label: poke-distribution-py
import numpy as np
# For only one factor, use .groupby('colname')['colname'].count()
poke.groupby(['generation'])['generation'].count()

# for two or more factors, use pd.crosstab
pd.crosstab(index = poke['type_1'], columns = poke['type_2'])
```
:::

##### Frequency Plots {.unnumbered}

::: panel-tabset
###### Base R {.unnumbered}

```{r}
#| label: poke-distribution-plots
plot(table(poke$generation)) # bar plot
```

###### R: ggplot2 {.unnumbered}

We generate a bar chart using `geom_bar`. It helps to tell R that
generation (despite appearances) is categorical by declaring it a factor
variable. This ensures that we get a break on the x-axis at each
generation.

```{r}
#| label: poke-dist-plots-ggplot2
library(ggplot2)

ggplot(poke, aes(x = factor(generation))) +
  geom_bar() +
  xlab("Generation") + ylab("# Pokemon")
```


###### Python: matplotlib {.unnumbered}

We generate a bar chart using the contingency table we generated earlier
combined with matplotlib's `plt.bar()`.

```{python}
#| label: matplotlib-categorical-chart
import matplotlib.pyplot as plt

tab = poke.groupby(['generation'])['generation'].count()

plt.bar(tab.keys(), tab.values, color = 'grey')
plt.xlabel("Generation")
plt.ylabel("# Pokemon")
plt.show()
```


:::

#### Quantitative Variables

We covered some numerical summary statistics in the numerical summary
statistic section above. In this section, we will primarily focus on
visualization methods for assessing the distribution of quantitative
variables.

::: callout-note
##### Note: R pipe {.unnumbered}

The code in this section uses the R pipe, `%>%`. The left side of the
pipe is passed as an argument to the right side. This makes code easier
to read because it becomes a step-wise "recipe" instead of a nested mess
of functions and parentheses.

![In each step, the left hand side of the pipe is put into the first
argument of the function. Source: [Arthur Welle
(Github)](https://github.com/arthurwelle/VIS/blob/master/Pipe_Cake/)](../images/wrangling/Pipe_baking_magrittr_backAssign.gif){width="50%"}
:::

We can generate histograms[^exploratory-data-analysis-2] or kernel
density plots (a continuous version of the histogram) to show us the
distribution of a continuous variable.

[^exploratory-data-analysis-2]: A histogram is a chart which breaks up a
    continuous variable into ranges, where the height of the bar is
    proportional to the number of items in the range. A bar chart is
    similar, but shows the number of occurrences of a discrete variable.

::: panel-tabset
##### Base R {.unnumbered}

By default, R uses ranges of $(a, b]$ in histograms, so we specify which
breaks will give us a desirable result. If we do not specify breaks, R
will pick them for us.

```{r}
#| label: poke-hp-out
#| out-width: 48%
#| fig-show: hold
hist(poke$hp)
```

For continuous variables, we can use histograms, or we can examine
kernel density plots.

```{r}
#| label: poke-graphs-base-cts
#| collapse: !expr T
#| fig-show: hold
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 2
#| fig-cap: Histogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
#| fig-subcap: 
#|   - Histogram of Pokemon Height (m)
#|   - Histogram of Pokemon Height (m, log 10)
#|   - Density of Pokemon Height (m)
#|   - Density of Pokemon Height (m, log 10)

library(magrittr) # This provides the pipe command, %>%

hist(poke$weight_kg)

poke$weight_kg %>%
  log10() %>% # Take the log - will transformation be useful w/ modeling?
  hist(main = "Histogram of Log10 Weight (Kg)") # create a histogram

poke$weight_kg %>%
  density(na.rm = T) %>% # First, we compute the kernel density
  # (na.rm = T says to ignore NA values)
  plot(main = "Density of Weight (Kg)") # Then, we plot the result


poke$weight_kg %>%
  log10() %>% # Transform the variable
  density(na.rm = T) %>% # Compute the density ignoring missing values
  plot(main = "Density of Log10 pokemon weight in Kg") # Plot the result,
    # changing the title of the plot to a meaningful value
```

##### Python: matplotlib {.unnumbered}

```{python}
#| include: !expr F
plt.clf()
```

```{python}
#| label: poke-graphs-matplotlib-cts
#| fig-width: 6
#| fig-height: 6
#| fig-cap: Histogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
import matplotlib.pyplot as plt

# Create a 2x2 grid of plots with separate axes
# This uses python multi-assignment to assign figures, axes
# variables all in one go
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)

poke.weight_kg.plot.hist(ax = ax1) # first plot
ax1.set_title("Histogram of Weight (kg)")


np.log10(poke.weight_kg).plot.hist(ax = ax2)
ax2.set_title("Histogram of Log10 Weight (kg)")

poke.weight_kg.plot.density(ax = ax3)
ax3.set_title("Density of Weight (kg)")

np.log10(poke.weight_kg).plot.density(ax = ax4)
ax4.set_title("Density of Log10 Weight (kg)")

plt.tight_layout()
plt.show()
```

##### R: ggplot2

```{r}
#| label: poke-ggplot2-cts-r
#| fig-show: hold
#| collapse: true
#| fig-align: center
#| fig-cap:  Histogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
#| fig-subcap: 
#|   - Histogram of Pokemon Height (m)
#|   - Histogram of Pokemon Height (m, log 10)
#|   - Density of Pokemon Height (m)
#|   - Density of Pokemon Height (m, log 10)
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 2

library(ggplot2)
ggplot(poke, aes(x = height_m)) +
  geom_histogram(bins = 30)
ggplot(poke, aes(x = height_m)) +
  geom_histogram(bins = 30) +
  scale_x_log10()
ggplot(poke, aes(x = height_m)) +
  geom_density()
ggplot(poke, aes(x = height_m)) +
  geom_density() +
  scale_x_log10()
```

Notice that in ggplot2, we transform the axes instead of the
data. This means that the units on the axis are true to the original,
unlike in base R and matplotlib.

##### Python: seaborn

```{python}
#| label: poke-seaborn-cts-py
import seaborn as sns
import matplotlib.pyplot as plt

# Create a 2x2 grid of plots with separate axes
# This uses python multi-assignment to assign figures, axes
# variables all in one go
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)

sns.histplot(poke, x = "height_m", bins = 30, ax = ax1)
sns.histplot(poke, x = "height_m", bins = 30, log_scale = True, ax = ax2)
sns.kdeplot(poke, x = "height_m", bw_adjust = 1, ax = ax3)
sns.kdeplot(poke, x = "height_m", bw_adjust = 1, log_scale = True, ax = ax4)

plt.show()
```

:::

### Relationships Between Variables
#### Categorical - Categorical Relationships

::: panel-tabset

##### R: ggplot2 {.unnumbered}

We can generate a (simple) mosaic plot (the equivalent of a 2-dimensional
cross-tabular view) using `geom_bar` with `position = 'fill'`, which
scales each bar so that it ends at 1. I've flipped the axes using
`coord_flip` so that you can read the labels more easily.

```{r}
#| label: poke-dist-plots-ggplot2-mosaic
library(ggplot2)

ggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +
  geom_bar(color = "black", position = "fill") +
  xlab("Type 1") + ylab("Proportion of Pokemon w/ Type 2") +
  coord_flip()
```

Another way to look at this data is to bin it in x and y and shade the
resulting bins by the number of data points in each bin. We can even add
in labels so that this is at least as clear as the tabular view!

```{r}
#| label: poke-dist-plots-ggplot2-tile
ggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +
  # Shade tiles according to the number of things in the bin
  geom_tile(aes(fill = after_stat(count)), stat = "bin2d") +
  # Add the number of things in the bin to the top of the tile as text
  geom_text(aes(label = after_stat(count)), stat = 'bin2d') +
  # Scale the tile fill
  scale_fill_gradient2(limits = c(0, 100), low = "white", high = "blue", na.value = "white") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

##### Base R {.unnumbered}

Base R mosaic plots aren't nearly as pretty as the ggplot version, but I will at least show you how to create them. 

```{r}
#| label: mosaic-base-r
plot(table(poke$type_1, poke$type_2)) 
# mosaic plot - hard to read b/c too many categories
```

##### Python: matplotlib {.unnumbered}

To get a mosaicplot, we need an additional library, called
`statsmodels`, which we install with `pip install statsmodels` in the
terminal.

```{python}
#| label: mosaic-plot-poke-statsmodels
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic

mosaic(poke, ['type_1', 'type_2'], title = "Pokemon Types")
plt.show()
```

This obviously needs a bit of cleaning up to remove extra labels, but
it's easy to get to and relatively functional. Notice that it does not,
by default, show NA values.

##### Python: seaborn {.unnumbered}

Seaborn doesn't appear to have built-in mosaic plots; the [example I found](https://www.kaggle.com/code/aswathrao/seaborn#Mosaic-Plot) used the statsmodels function shown in the `maptlotlib` example and used seaborn to create facets. 
As the point here isn't to display facets, borrowing the code from that example doesn't add any value here. 

:::

#### Categorical - Continuous Relationships

::: panel-tabset
##### Base R {.unnumbered}

In R, most models are specified as `y ~ x1 + x2 + x3`, where the
information on the left side of the tilde is the dependent variable, and
the information on the right side are any explanatory variables.
Interactions are specified using `x1*x2` to get all combinations of x1
and x2 (x1, x2, x1\*x2); single interaction terms are specified as e.g.
`x1:x2` and do not include any component terms.

To examine the relationship between a categorical variable and a
continuous variable, we might look at box plots:

```{r}
#| label: boxplot-graphs
#| out-width: 100%
#| fig-align: center
#| fig-show: hold
#| collapse: true
par(mfrow = c(1, 2)) # put figures in same row
boxplot(log10(height_m) ~ type_1, data = poke)
boxplot(total ~ generation, data = poke)
```

In the second box plot, there are far too many categories to be able to
resolve the relationship clearly, but the plot is still effective in
that we can identify that there are one or two species which have a much
higher point range than other species. EDA isn't usually about creating
pretty plots (or we'd be using `ggplot` right now) but rather about
identifying things which may come up in the analysis later.

##### R: ggplot2 {.unnumbered}

```{r}
#| label: boxplot-ggplot2
#| out-width: 100%
#| fig-align: center
#| fig-show: hold
#| collapse: true

ggplot(data = poke, aes(x = type_1, y = height_m)) + 
  geom_boxplot() + 
  scale_y_log10()

ggplot(data = poke, aes(x = factor(generation), y = total)) + 
  geom_boxplot()
```

##### Python: matplotlib {.unnumbered}

```{python}
#| label: boxplot-matplotlib
#| fig-width: 10
#| fig-height: 4
import matplotlib.pyplot as plt
import numpy as np

plt.figure()

# Create a list of vectors of height_m by type_1
poke['height_m_log'] = np.log(poke.height_m)
height_by_type = poke.groupby('type_1', group_keys = True).height_m_log.apply(list)

# Plot each object in the list
plt.boxplot(height_by_type, labels = height_by_type.index)

plt.show()
```


```{python}
#| label: boxplot-matplotlib2

plt.figure()

# Create a list of vectors of total by generation
total_by_gen = poke.groupby('generation', group_keys = True).total.apply(list)

# Plot each object in the list
plt.boxplot(total_by_gen, labels = total_by_gen.index)

plt.show()
```
##### Python: seaborn  {.unnumbered}

```{python}
#| label: boxplot-seaborn
import seaborn as sns
plt.figure()
sns.boxplot(poke, x = "generation", y = "height_m", log_scale = True)
plt.show()
```
As a higher-level graphics library, `seaborn` allows you to transform the scale shown on the axes instead of having to manually transform the data. 
This is a more natural presentation, as the values on the scale are (at least in theory) a bit easier to read. 
In practice, I'd still probably customize the labels on the y-axis if I were hoping to use this for publication. 

```{python}
#| label: boxplot-seaborn2
import seaborn as sns
plt.figure()
sns.boxplot(poke, x = "generation", y = "total")
plt.show()
```
:::

You can find more on boxplots and ways to customize boxplots in the [Graphics](02b-graphics.qmd) chapter.

#### Continuous - Continuous Relationships

::: panel-tabset
##### Base R {.unnumbered}

To look at the relationship between numeric variables, we could compute
a numeric correlation, but a plot may be more useful, because it allows
us to see outliers as well.

```{r}
#| label: base-scatter-cor
#| out-width: 48%
#| fig-align: center
#| collapse: true
plot(defense ~ attack, data = poke, type = "p")

cor(poke$defense, poke$attack)
```

Sometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. 
In other cases, like in the plot below, we may discover an interesting correlation that sticks out - the identity line $y=x$ seems to stand out from the cloud here.

```{r}
#| label: base-scatter-quantized
plot(x = poke$sp_attack, y = poke$attack, type = "p")
```

A scatterplot matrix can also be a useful way to visualize relationships
between several variables.

```{r}
#| label: base-scatter-matrix
#| fig-cap: "A scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon."
#| fig-width: 8
#| fig-height: 8
#| out.width: "100%"
pairs(poke[,c("hp", "attack", "defense", "sp_attack", "sp_defense")]) # hp - sp_defense columns
```

::: callout-note
[There's more information on how to customize base R scatterplot
matrices
here](http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs).
:::

##### R: ggplot2 {.unnumbered}

To look at the relationship between numeric variables, we could compute
a numeric correlation, but a plot may be more useful, because it allows
us to see outliers as well.

```{r}
#| label: ggplot-scatter-cor
library(ggplot2)
ggplot(poke, aes(x = attack, y = defense)) + geom_point()
```

Sometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. 
When this happens, it can be a good idea to use `geom_jitter` to provide some "wiggle" in the data so that you can still see the point density. 
Changing the point transparency (alpha = .5) can also help with overplotting.

In other cases, we might find that there is a prominent feature of a scatterplot (in this case, the line $y=x$ seems to stand out a bit from the overall point cloud). We can highlight this feature by adding a line at $y=x$ in red behind the points.

```{r}
#| label: ggplot-scatter-quantized
#| out-width: "50%"
#| fig-width: 4
#| fig-height: 4
ggplot(poke, aes(x = attack, y = sp_attack)) + geom_point()

ggplot(poke, aes(x = attack, y = sp_attack)) + 
  geom_abline(slope = 1, color = "red") + 
  geom_jitter(alpha = 0.5)
```

```{r}
#| label: ggplot-scatter-matrix
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "A scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon."
library(GGally) # an extension to ggplot2
ggpairs(poke[,c("hp", "attack", "defense", "sp_attack", "sp_defense")], 
        # hp - sp_defense columns
        lower = list(continuous = wrap("points", alpha = .15)),
        progress = F) 
```

`ggpairs` can also handle continuous variables, if you want to explore the options available. 

##### Python: pandas {.unnumbered}

Believe it or not, you don't have to go to `matplotlib` to get plots in python - you can get some plots from `pandas` directly, even if you are still using `matplotlib` under the hood (this is why you have to run `plt.show()` to get the plot to appear if you're working in markdown).

```{python}
#| label: pandas-scatterplot
import matplotlib.pyplot as plt

poke.plot.scatter(x = 'attack', y = 'defense')
plt.show()
```

Pandas also includes a nice scatterplot matrix method. 

```{python}
#| label: pandas-scatterplot-matrix
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt

vars = [6, 7,14,15]
scatter_matrix(poke.iloc[:,vars], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')
plt.show()
```

```{python}
#| label: seaborn-scatterplot-matrix

vars = [6, 7,14,15]

plt.figure()
sns.set_theme(style="ticks")
sns.pairplot(data = poke.iloc[:,vars])
plt.show()
```


:::

If you want summary statistics by group, you can get that using the
`dplyr` package functions `select` and `group_by`, which we will learn
more about in the next section. (I'm cheating a bit by mentioning it
now, but it's just so useful!)

```{python}
#| include: !expr F
# Clear python plot framework
plt.clf()
```

::: callout-tip
## Try it out: EDA {#EDA-housing-data}

::: panel-tabset
### Problem {.unnumbered}

Explore the variables present in [the Lancaster County Assessor Housing Sales Data](https://github.com/srvanderplas/datasets/blob/main/raw/Lancaster%20County,%20NE%20-%20Assessor.xlsx?raw=true) [Documentation](https://github.com/srvanderplas/datasets/blob/main/code/lincoln-housing-sales.md).

Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. 
As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.

```{r}
#| label: housing-tryitout-r-read-data
if (!"readxl" %in% installed.packages()) install.packages("readxl")
library(readxl)
download.file("https://github.com/srvanderplas/datasets/blob/main/raw/Lancaster%20County,%20NE%20-%20Assessor.xlsx?raw=true", destfile = "../data/lancaster-housing.xlsx")
housing_lincoln <- read_xlsx("../data/lancaster-housing.xlsx", sheet = 1, guess_max = 7000)
```

```{python}
#| label: housing-tryitout-py-read-data
import pandas as pd
housing_lincoln = pd.read_excel("../data/lancaster-housing.xlsx")
```

### R solution {.unnumbered}

```{r}
#| label: housing-tryitout-R
housing_lincoln$TLA <- readr::parse_number(housing_lincoln$`TLA (Sqft)`)
housing_lincoln$Assd_Value <- readr::parse_number(housing_lincoln$Assd_Value)

skim(housing_lincoln)
```

Let's examine the numeric variables first:

```{r}
#| label: housing-data-numeric-date
hist(housing_lincoln$Assd_Value)

hist(housing_lincoln$Yr_Blt)
```

Let's look at the years the houses were built and the Imp\_Types.
We can find more data on what the Improvement Types mean [here](https://www.lincoln.ne.gov/files/sharedassets/intranet/county/clerk/protest-public/appraisal_card_code_sheet.pdf), where the various abbreviations are defined. 

```{r}
#| label: housing-data-year
housing_lincoln$decade <- 10*floor(housing_lincoln$Yr_Blt/10)

table(housing_lincoln$decade, useNA = 'ifany')
table(housing_lincoln$Imp_Type)
table(housing_lincoln$Imp_Type, housing_lincoln$decade)

plot(table(housing_lincoln$decade, housing_lincoln$Imp_Type),
     main = "Year Built and Improvement Type")
```

We can also look at the square footage for each improvement type:

```{r}
housing_lincoln %>%
  subset(Imp_Type %in% c("BN", "R1", "R2", "RA")) %>%
  boxplot(TLA ~ Imp_Type, data = .)
```

This makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house. 

### Python solution {.unnumbered}

```{python}
#| label: housing-tryitout-py
housing_lincoln["TLA"] = housing_lincoln["TLA (Sqft)"].str.replace("[,\$]", "", regex = True)
# For some reason, things without a comma just get NaN'd, so fix that
housing_lincoln.loc[housing_lincoln["TLA"].isna(), "TLA"] = housing_lincoln.loc[housing_lincoln["TLA"].isna(), "TLA (Sqft)"]
housing_lincoln["TLA"] = pd.to_numeric(housing_lincoln["TLA"], errors = 'coerce')

housing_lincoln["Assessed"] = housing_lincoln["Assd_Value"].str.replace("[,\$]", "", regex = True)
# For some reason, things without a comma just get NaN'd, so fix that
housing_lincoln.loc[housing_lincoln["Assessed"].isna(), "Assessed"] = housing_lincoln.loc[housing_lincoln["Assessed"].isna(), "Assd_Value"]
housing_lincoln["Assessed"] = pd.to_numeric(housing_lincoln["Assessed"], errors = 'coerce')

housing_lincoln = housing_lincoln.drop(["TLA (Sqft)", "Assd_Value"], axis = 1)

# housing_lincoln.describe()
skim(housing_lincoln)
```

Let's examine the numeric and date variables first:

```{python}
#| label: housing-numeric-py
housing_lincoln["TLA"].plot.hist()
plt.show()

housing_lincoln["Yr_Blt"].plot.hist()
plt.show()
```


Let's look at the years the houses were built and the Imp\_Types.
We can find more data on what the Improvement Types mean [here](https://www.lincoln.ne.gov/files/sharedassets/intranet/county/clerk/protest-public/appraisal_card_code_sheet.pdf), where the various abbreviations are defined. 

```{python}
#| label: housing-categorical-py
import numpy as np
housing_lincoln['decade'] = 10*np.floor(housing_lincoln.Yr_Blt/10)

housing_lincoln["decade"].groupby(housing_lincoln["decade"]).count()
housing_lincoln["Imp_Type"].groupby(housing_lincoln["Imp_Type"]).count()

pd.crosstab(index = housing_lincoln["decade"], columns = housing_lincoln["Imp_Type"])

import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic

mosaic(housing_lincoln, ["decade", "Imp_Type"], title = "Housing Built by Type, Decade")
plt.show()
```

We can also look at the square footage for each improvement type:

```{python}
#| label: housing-cat-cts-py
housing_subcat = ["BN", "R1", "RA", "R2"]

housing_sub = housing_lincoln.loc[housing_lincoln["Imp_Type"].isin(housing_subcat)]
housing_sub = housing_sub.assign(Imp_cat = pd.Categorical(housing_sub["Imp_Type"], categories = housing_subcat))

housing_sub.boxplot("TLA", by = "Imp_cat")
plt.show()
```
This makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house; we would expect an increase in square footage with each additional floor of the house (broadly speaking).

:::
:::


::: callout-note
## Learn More: Janitor R package

The janitor package [@janitorpkg] has some very convenient functions for cleaning up messy data. 
One of its best features is the `clean_names()` function, which creates names based on a capitalization/separation scheme of your choosing.

![janitor and clean_names() by Allison
Horst](../images/wrangling/janitor_clean_names.png){fig-alt="A cartoon beaver putting shapes with long, messy column names (pulled from a bin labeled ‚ÄúMESS‚Äù and ‚Äúnot so awesome column names‚Äù) into a contraption that converts them to lower snake case. The output has stylized text reading ‚ÄúWay more deal-withable column names.‚Äù Title text reads ‚Äújanitor::clean_names(): convert all column names to *_case!‚Äù"}
:::

## References {#sec-eda-refs}
