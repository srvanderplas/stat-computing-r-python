# Data Cleaning {#sec-data-cleaning}

## Objectives {-}

- Identify required sequence of steps for data cleaning
- Describe step-by-step data cleaning process in lay terms appropriately
- Apply data manipulation verbs to prepare data for analysis
- Understand the consequences of data cleaning steps for statistical analysis
- Create summaries of data appropriate for analysis or display using data manipulation techniques


## Introduction

In this section, we're going start learning how to work with data. 
Generally speaking, data doesn't come in a form suitable for analysis^[See [this twitter thread](https://twitter.com/JennyBryan/status/722954354198597632) for some horror stories. 
[This tweet](https://twitter.com/jengolbeck/status/1153064308483510272?s=20) is also pretty good at showing one type of messiness.] - you have to clean it up, create the variables you care about, get rid of those you don't care about, and so on. 

::: aside
![Data wrangling (by Allison Horst)](../images/wrangling/data_cowboy.png){fig-alt="Cartoon of a fuzzy monster with a cowboy hat and lasso, riding another fuzzy monster labeled “dplyr”, lassoing a group of angry / unruly looking creatures labeled “data.”"}
:::

Some people call the process of cleaning and organizing your data "data wrangling", which is a fantastic way to think about chasing down all of the issues in the data. 


In R, we'll be using the `tidyverse` for this. 
It's a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy^[The philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.] and interface (basically, the commands will use predictable argument names and structure). 
You've already been introduced to parts of the tidyverse - specifically, `readr` and `ggplot2`. 

`dplyr` (one of the packages in the tidyverse) creates a "grammar of data manipulation" to make it easier to describe different operations. 
I find the `dplyr` grammar to be extremely useful when talking about data operations, so I'm going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework). 

Each `dplyr` verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. 
In all tidyverse functions, **data comes first** -- literally, as it's the first argument to any function. 
In addition, you don't use df$variable to access a variable - you refer to the variable by its name alone ("bare" names). 
This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy. 


In Python, most data manipulation tasks are handled using `pandas`[@pandasIndexingSelectingData2022]. 
In the interests of using a single consistent "language" for describing data manipulation tasks, I'll use the tidyverse "verbs" to describe operations in both languages. 
The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.

There is also the `datar` python package[@pwwangDatarGrammarData2022], which attempts to port the dplyr grammar of data wrangling into python. 
While pandas tends to be fairly similar to base R in basic operation, `datar` may be more useful if you prefer the `dplyr` way of handling things using a data-first API. 

::: column-margin
I haven't had the chance to add the `datar` package to this book, but it looks promising and may be worth your time to figure out. 
It's a bit too new for me to teach right now - I want packages that will be maintained long-term if I'm going to teach them to others.
:::


::: {.callout-note}

[There is an excellent dplyr cheatsheet available from RStudio](https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf). You may want to print it out to have a copy to reference as you work through this chapter.

[Here is a data wrangling with pandas cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) that is formatted similarly to the dplyr cheat sheet.

:::


## Tidy Data

There are infinitely many ways to configure "messy" data, but data that is "tidy" has 3 attributes:

1. Each variable has its own column
2. Each observation has its own row
3. Each value has its own cell

These attributes aren't sufficient to define "clean" data, but they work to define "tidy" data (in the same way that you can have a "tidy" room because all of your clothes are folded, but they aren't clean just because they're folded; you could have folded a pile of dirty clothes). 

We'll get more into how to work with different "messy" data configurations in @sec-data-reshape and @sec-data-join, but it's worth keeping rules 1 and 3 in mind while working through this module. 



## Filter: Subset rows

Filter allows us to work with a subset of a larger data frame, keeping only the rows we're interested in. 
We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from `filter()`. 
Note that unless we store the result from `filter()` in the original object, we don't change the original. 

![dplyr filter() by Allison Horst](../images/wrangling/dplyr_filter.jpg){fig-alt="Cartoon showing three fuzzy monsters either selecting or crossing out rows of a data table. If the type of animal in the table is “otter” and the site is “bay”, a monster is drawing a purple rectangle around the row. If those conditions are not met, another monster is putting a line through the column indicating it will be excluded. Stylized text reads “dplyr::filter() - keep rows that satisfy your conditions.”"}


::: {.callout-caution}

### Example: starwars

Let's explore how it works, using the `starwars` dataset, which contains a comprehensive list of the characters in the Star Wars movies. 

In the interests of demonstrating the process on the same data, I've exported the starwars data to a CSV file using the `readr` package. I had to remove the list-columns (films, vehicles, starships) because that format isn't supported by CSV files. You can access the csv data [here](https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv). 

::: panel-tabset

#### R {-}

This data set is included in the `dplyr` package, so we load that package and then use the `data()` function to load dataset into memory. 
The loading isn't complete until we actually use the dataset though... so let's print the first few rows. 

```{r}
#| label: starwars-data-explore
library(dplyr)
data(starwars)
starwars
```

#### Python {-}

We have to use the exported CSV data in python.

```{python}
import pandas as pd
starwars = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv")
starwars

from skimpy import skim
skim(starwars)
```

:::


Once the data is set up, filtering the data (selecting certain **rows**) is actually very simple. 
Of course, we've talked about how to use logical indexing before in @sec-indexing, but here we'll focus on using specific functions to perform the same operation. 

::: panel-tabset

#### R: `dplyr`

The dplyr verb for selecting rows is `filter`. 
`filter` takes a set of one or more logical conditions, using bare column names and logical operators. 
Each provided condition is combined using AND.

```{r}
#| label: r-filter-data
# Get only the people
filter(starwars, species == "Human")

# Get only the people who come from Tatooine
filter(starwars, species == "Human", homeworld == "Tatooine")
```


#### Python

```{python}
#| label: py-filter-data
# Get only the people
starwars.query("species == 'Human'")

# Get only the people who come from Tattoine
starwars.query("species == 'Human' & homeworld == 'Tatooine'")

# This is another option if you prefer to keep the queries separate
# starwars.query("species == 'Human'").query("homeworld == 'Tatooine'")
```

#### Base R

In base R, you would perform a filtering operation using `subset`

```{r}
#| label: r-filter-data2
# Get only the people
subset(starwars, species == "Human")

# Get only the people who come from Tatooine
subset(starwars, species == "Human" & homeworld == "Tatooine")
```

Notice that with `subset`, you have to use `&` to join two logical statements; it does not by default take multiple successive arguments. 

:::

:::


### Common Row Selection Tasks

In `dplyr`, there are a few helper functions which may be useful when constructing filter statements.
In base R or python, these tasks are still important, and so I'll do my best to show you easy ways to handle each task in each language.

::: {.callout-demo collapse="true"}

#### Filtering by row number

::: panel-tabset

##### R: `dplyr` {-}

`row_number()` is a helper function that is only used inside of another dplyr function (e.g. filter). 
You might want to keep only even rows, or only the first 10 rows in a table. 

```{r}
#| label: filter-poke
library(readr)
library(dplyr)

poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")
filter(poke, (row_number() %% 2 == 0)) 
# There are several pokemon who have multiple entries in the table,
# so the pokedex_number doesn't line up with the row number.
```

##### Python {-}

In python, the easiest way to accomplish filtering by row number is by using `.iloc`. 
But, up until now, we've only talked about how Python creates slices using `start:(end+1)` notation. 
There is an additional option with slicing - `start:(end+1):by`. So if we want to get only even rows, we can use the index `[::2]`, which will give us row 0, 2, 4, 6, ... through the end of the dataset, because we didn't specify the start and end portions of the slice. 

Because Python is 0-indexed, using `::2` will give us the opposite set of rows from that returned in R, which is 1-indexed.

```{python}
#| label: filter-poke-py
import pandas as pd

poke = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")
poke.iloc[0::2]
```
If we want to get only odd rows, we can use the index `[1::2]`, which will start at row 1 and give us 1, 3, 5, ...

##### Base R {-}

In base R, we'd use `seq()` to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. 
Alternately, we can use `subset`, which requires a logical condition, and use `1:nrow(poke)` to create an index which we then use for deciding whether each row is even or odd.

```{r}
#| label: base-r-filter
poke[seq(1, nrow(poke), 2),]

subset(poke, 1:nrow(poke) %% 2 == 0)
```

This is less fun than using `dplyr` because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you're going. 
The real power of `dplyr` is in the collection of the full set of verbs with a consistent user interface; nothing done in `dplyr` is so special that it can't be done in base R as well.

:::

:::

::: {.callout-demo collapse="true"}
#### Sorting rows by variable values

Another common operation is to sort your data frame by the values of one or more variables.

::: panel-tabset

##### R: `dplyr` {-}

`arrange()` is a dplyr verb for sort rows in the table by one or more variables. 
It is often used with a helper function, `desc()`, which reverses the order of a variable, sorting it in descending order. 
Multiple arguments can be passed to `arrange` to sort the data frame by multiple columns hierarchically; each column can be modified with `desc()` separately.

```{r}
#| label: arrange-poke-dplyr
arrange(poke, desc(total))
```

##### Python {-}

In pandas, we use the `sort_values` function, which has an argument `ascending`. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.

```{python}
#| label: arrange-poke-py
poke.sort_values(['total'], ascending = False)
```


##### Base R {-}

The `sort()` function in R can be used to sort a vector, but when sorting a data frame we usually want to use the `order()` function instead. 
This is because `sort()` orders the values of the argument directly, where `order()` returns a sorted index.

```{r}
#| label: base-r-sort
x <- c(32, 25, 98, 45, 31, 19, 5)
sort(x)
order(x)
```

When working with a data frame, we want to sort the entire data frame's rows by the variables we choose; it is easiest to do this using an index to reorder the rows.

```{r}
#| label: arrange-poke-baser
poke[order(poke$total, decreasing = T),]
```
:::

:::

::: {.callout-demo collapse="true"}
#### Keep the top $n$ values of a variable

::: panel-tabset

##### R: `dplyr` {-}

`slice_max()` will keep the top values of a specified variable. 
This is like a filter statement, but it's a shortcut built to handle a common task. 
You could write a filter statement that would do this, but it would take a few more lines of code.

```{r}
#| label: slice-poke-arrange
slice_max(poke, order_by = total, n = 5)
```

By default, `slice_max()` returns values tied with the nth value as well, which is why our result has 6 rows.

```{r}
#| label: slice-poke-arrange-2
slice_max(poke, order_by = total, n = 5, with_ties = F) 
```

Of course, there is a similar `slice_min()` function as well:

```{r}
#| label: slice-poke-arrange-3
slice_min(poke, order_by = total, n = 5)
```

`slice_max` and `slice_min` also take a `prop` argument that gives you a certain proportion of the values:

```{r}
#| label: slice-poke-arrange-4
slice_max(poke, order_by = total, prop = .01)
```

##### Python {-}

In Python, `nlargest` and `nsmallest` work roughly the same as `dplyr`'s `slice_max` and `slice_min` for integer counts.

```{python}
#| label: slice-poke-nlargest
poke.nlargest(5, 'total')
poke.nsmallest(5, 'total')
```

To get proportions, though, we have to do some math:

```{python}
#| label: slice-poke-nlargest-2
poke.nlargest(int(len(poke)*0.01), 'total')
poke.nsmallest(int(len(poke)*0.01), 'total')
```

##### Base R {-}

The simplest way to do this type of task with base R is to combine the order() function and indexing.
In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.

```{r}
poke[order(poke$total, decreasing = T)[1:5],]
poke[order(poke$total, decreasing = T)[1:round(nrow(poke)*.01)],]
```

:::

:::



::: {.callout-tip}

### Try it out: Filtering

::: panel-tabset

#### Problem {-}

Use the [Pokemon data](https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv) to accomplish the following:

- create a new data frame that has only water type Pokemon
- write a filter statement that looks for any Pokemon which has water type for either type1 or type2


#### R: `dplyr` {-}

```{r}
#| label: poke-tryitout
#| message: false
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

filter(poke, str_detect(type, "Water"))[,c('gen', 'type', 'weight_kg')] 

```
`str_detect` looks for Water in the type entry -- Pokemon can have one or two types, and in type, these are separated by a comma. Instead of splitting the types apart (which we could do), it's easier to just check to see if water exists in the entire string.

#### Python
```{python}
#| label: poke-tryitout-py
import pandas as pd
poke = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

poke.query("type.str.contains('Water')")[['gen', 'type', 'weight_kg']]
```

#### Base R

```{r}
#| label: poke-tryitout2
#| message: !expr F
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

subset(poke, grepl("Water", type))
```

`grepl` is a function that searches a string (`grep`) and returns a logical value (`grep+l`). 
It's very useful for subsetting. 

:::

:::


## Select: Pick columns

Sometimes, we don't want to work with a set of 50 variables when we're only interested in 5.
When that happens, we might be able to pick the variables we want by index (e.g. `df[, c(1, 3, 5)]`), but this can get tedious.

::: panel-tabset

### R: `dplyr` {-}

In `dplyr`, the function to pick a few columns is `select()`. The syntax from the help file (`?select`) looks deceptively simple.

> select(.data, ...)

So as with just about every other tidyverse function, the first argument in a select statement is the data. 
After that, though, you can put just about anything that R can interpret. `...` means something along the lines of "put in any additional arguments that make sense in context or might be passed on to other functions".

So what can go in there?

::: {.callout-note collapse=true}

#### Ways to select variables in `dplyr`

First, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).^[It accomplishes this through the magic of quasiquotation, which we will not cover in this course because it's basically witchcraft.]  
Most `dplyr` commands work with "bare" variable names - you don't need to put the variable name in quotes to reference it. 
There are a few exceptions to this rule, but they're very explicitly exceptions.

- `var3:var5`: `select(df, var3:var5)` will give you a data frame with columns var3, anything between var3 and var 5, and var5

- `!(<set of variables>)` will give you any columns that aren't in the set of variables in parentheses
    - `(<set of vars 1>) & (<set of vars 2>)` will give you any variables that are in both set 1 and set 2. `(<set of vars 1>) | (<set of vars 2>)` will give you any variables that are in either set 1 or set 2.
    - `c()` combines sets of variables.

`dplyr` also defines a lot of variable selection "helpers" that can be used inside `select()` statements. 
These statements work with bare column names (so you don't have to put quotes around the column names when you use them).

- `everything()` matches all variables
- `last_col()` matches the last variable. `last_col(offset = n)` selects the n-th to last variable.
- `starts_with("xyz")` will match any columns with names that start with xyz. Similarly, `ends_with()` does exactly what you'd expect as well.
- `contains("xyz")` will match any columns with names containing the literal string "xyz". Note, `contains` does not work with regular expressions (you don't need to know what that means right now).
- `matches(regex)` takes a regular expression as an argument and returns all columns matching that expression.
- `num_range(prefix, range)` selects any columns that start with prefix and have numbers matching the provided numerical range.

There are also selectors that deal with character vectors. 
These can be useful if you have a list of important variables and want to just keep those variables.

- `all_of(char)` matches all variable names in the character vector `char`. If one of the variables doesn't exist, this will return an error.
- `any_of(char)` matches the contents of the character vector `char`, but does not throw an error if the variable doesn't exist in the data set.

There's one final selector -

- `where()` applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.

:::

Let's try these selector functions out and see what we can accomplish!

```{r}
#| label: nycflightsinstall
library(nycflights13)
data(flights)
str(flights)
```

We'll start out with the `nycflights13` package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.

::: {.callout-tip}

You might want to try out your EDA (Exploratory Data Analysis) skills to see what you can find out about the dataset, before seeing how `select()` works.

:::


We could get a data frame of departure information for each flight:

```{r}
#| label: select-departure
select(flights, flight, year:day, tailnum, origin, matches("dep"))
```

Perhaps we want the plane and flight ID information to be the first columns:

```{r}
#| label: select-rearrange
flights %>%
  select(carrier:dest, everything())
```

Note that `everything()` won't duplicate columns you've already added.

Exploring the difference between bare name selection and `all_of()`/`any_of()`

```{r}
#| label: select-bare-vs-names
#| error: !expr T
flights %>%
  select(carrier, flight, tailnum, matches("time"))

varlist <- c("carrier", "flight", "tailnum",
             "dep_time", "sched_dep_time", "arr_time", "sched_arr_time",
             "air_time")

flights %>%
  select(all_of(varlist))

varlist <- c(varlist, "whoops")

flights %>%
  select(all_of(varlist)) # this errors out b/c whoops doesn't exist

flights %>%
select(any_of(varlist)) # this runs just fine
```


So for now, at least in R, you know how to cut your data down to size rowwise (with `filter`) and column-wise (with `select`).



### Python {-}

First, let's install the nycflights13 package[@chowNycflights13DataPackage2020] in python by typing the following into your **system terminal**.

```{bash}
pip install nycflights13
```

Then, we can load the `flights` data from the `nycflights13` package.

```{python}
from nycflights13 import flights
```

Select operations are not as easy in python as they are when using `dplyr::select()` with helpers, but of course you can accomplish the same tasks.

```{python}
#| label: select-rearrange-py
cols = flights.columns

# Rearrange column order by manual indexing
x = cols[9:13].append(cols[0:9])
x = x.append(cols[13:19])

# Then use the index to rearrange the columns
flights.loc[:,x]
```

#### List Comprehensions {-}

In Python, there are certain shorthands called "list comprehensions" [@pythonfoundationDataStructures2022] that can perform similar functions to e.g. the `matches()` function in dplyr. 

Suppose we want to get all columns containing the word 'time'. 
We could iterate through the list of columns (`flights.columns`) and add the column name any time we detect the word 'time' within. 
That is essentially what the following code does: 

```{python}
#| label: select-cols-helpers-py
# This gets all columns that contain time
timecols = [col for col in flights.columns if 'time' in col]
timecols
```

Explaining the code:

- `for col in flights.columns` iterates through the list of columns, storing each column name in the variable `col`
- `if 'time' in col` detects the presence of the word 'time' in the column name stored in `col`
- the `col` out front adds the column name in the variable `col` to the array of columns to keep

#### Selecting columns in Python {-}

```{python}
#| label: select-bare-vs-names-py
#| error: !expr T
# This gets all columns that contain time
timecols = [col for col in flights.columns if 'time' in col]
# Other columns
selcols = ["carrier", "flight", "tailnum"]
# Combine the two lists
selcols.extend(timecols)

# Subset the data frame
flights.loc[:,selcols]

selcols.extend(["whoops"])
selcols

# Subset the data frame
flights.loc[:,selcols]

# Error-tolerance - use list comprehension to check if 
# variable names are in the data frame
selcols_fixed = [x for x in selcols if x in flights.columns]
flights.loc[:,selcols_fixed]
```

### Base R {-}

In base R, we typically select columns by name or index directly. 
This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. `matches` in `dplyr`.

`grepl` is a shorthand function for `grep`, which searches for a pattern in a vector of strings.
`grepl` returns a logical vector indicating whether the pattern (`"dep"`, in this case) was found in the vector (`names(flights)`, in this case). 

```{r}
#| label: select-departure-base-r
depcols <- names(flights)[grepl("dep", names(flights))]
collist <- c("flight", "year", "month", "day", "tailnum", "origin", depcols)

flights[,collist]
```

Perhaps we want the plane and flight ID information to be the first columns:

```{r}
#| label: select-rearrange-base-r
new_order <- names(flights)
new_order <- new_order[c(10:14, 1:9, 15:19)]

flights[,new_order]
```

This is less convenient than `dplyr::everything` in part because it depends on us to get the column indexes right. 

:::

<!-- End Select-pick-columns tabset -->

::: {.callout-demo collapse="true"}

### Rearranging Columns

::: panel-tabset

#### `dplyr::relocate`

Another handy `dplyr` function is `relocate`; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. 
But, I'm covering relocate here mostly because it also comes with this amazing cartoon illustration.

![relocate lets you rearrange columns (by Allison Horst)](../images/wrangling/dplyr_relocate.png){fig-alt="Cartoon of fuzzy monsters moving columns around in fork lifts, while one supervises. Stylized text reads “dplyr::relocate() - move columns around! Default: move to FRONT , or move to .before or .after a specified column."}

```{r}
#| label: relocate-dplyr
# Move flight specific info to the front
data(flights, package = "nycflights13")
relocate(flights, carrier:dest, everything())

# move numeric variables to the front
flights %>% relocate(where(is.numeric))
```

#### Python

There are similar ways to rearrange columns in pandas, but they are a bit harder to work with - you have to specify the column names (in some way) and then perform the operation yourself.

```{python}
#| label: relocate-pandas
import numpy as np
cols = list(flights.columns.values) # get column names

# Move flight specific info to the front
flightcols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']
flights[flightcols + list(flights.drop(flightcols, axis = 1))]

# move numeric variables to the front
numcols = list(flights.select_dtypes(include = np.number).columns.values)
flights[numcols + list(flights.drop(numcols, axis = 1))]
```

:::

:::

## Mutate: Add and transform variables

Up to this point, we've been primarily focusing on how to decrease the dimensionality of our dataset in various ways. 
But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. 
The tidyverse verb for this is `mutate`, but in base R and python, we'll simply use assignment to add columns to our data frames.

::: aside

![Mutate (by Allison Horst)](../images/wrangling/dplyr_mutate.png){fig-alt="Cartoon of cute fuzzy monsters dressed up as different X-men characters, working together to add a new column to an existing data frame. Stylized title text reads “dplyr::mutate - add columns, keep existing.”"}

:::

We'll use the Pokemon data to demonstrate. 
Some Pokemon have a single "type", which is usually elemental, such as Water, Ice, Fire, etc., but others have two. 
Let's add a column that indicates how many types a pokemon has.

::: panel-tabset

### Base R {-}

```{r}
#| label: mutate-add-variables
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

# This splits type_1,type_2 into two separate variables. 
# Don't worry about the string processing (gsub) just now
# Focus on how variables are defined.
poke$type_1 <- gsub(",.*$", "", poke$type) # Replace anything after comma with ''
poke$type_2 <- gsub("^.*,", "", poke$type) # Use the 2nd type
poke$type_2[poke$type_1 == poke$type_2] <- NA # Type 2 only exists if not same as Type 1

poke$no_types <- 1 # set a default value
poke$no_types[grepl(",", poke$type)] <- 2 # set the value if there's not a comma in type

# This is a bit faster
poke$no_types <- ifelse(grepl(",", poke$type), 2, 1)

# Sanity check
# This checks number of types vs. value of type_2
# If type 2 is NA, then number of types should be 1
t(table(poke$type_2, poke$no_types, useNA = 'ifany'))
```

Notice that we had to type the name of the dataset at least 3 times to perform the operation we were looking for.
I could reduce that to 2x with the `ifelse` function, but it's still a lot of typing.

### R: `dplyr` {-}

```{r}
#| label: mutate-dplyr
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

poke <- poke %>%
  # This splits type into type_1,type_2 : two separate variables. 
  # Don't worry about the string processing (str_extract) just now
  # Focus on how variables are defined: 
  #   we use a function on the type column
  #   within the mutate statement.
  mutate(type_1 = str_extract(type, "^(.*),", group = 1),
         type_2 = str_extract(type, "(.*),(.*)", group = 2)) %>%
  mutate(no_types = if_else(is.na(type_2), 1, 2))

select(poke, type_2, no_types) %>% table(useNA = 'ifany') %>% t()
```
The last 2 rows are just to organize the output - we keep only the two variables we're working with, and get a crosstab.


### Python {-}

In python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with). 

```{python}
#| label: mutate-python
import pandas as pd
poke = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

# This splits type into two columns, type_1 and type_2, based on ","
poke[["type_1", "type_2"]] = poke["type"].apply(lambda x: pd.Series(str(x).split(",")))

# This defines number of types
poke["no_types"] = 1 # default value
poke.loc[~poke.type_2.isna(), "no_types"] = 2 # change those with a defined type 2


poke.groupby(["no_types", "type_2"], dropna=False).size()
# When type_2 is NaN, no_types is 1
# When type_2 is defined, no_types is 2
```

Another function that may be useful is the `assign` function, which can be used to create new variables if you don't want to use the `["new_col"]` notation. 
In some circumstances, `.assign(var = ...)` is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I'd like to [not get into right now](https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy) for simplicity's sake). 

:::

The learning curve here isn't actually knowing how to assign new variables (though that's important). 
The challenge comes when you want to do something *new* and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. 


:::{.callout-note collapse=true}

### Mutate and new challenges

I'm not going to be able to teach you how to handle every mutate statement task you'll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to _read documentation_, _google things intelligently_, and to _understand what you're reading_ enough to actually implement it. 
This is something that comes with practice (and lots of googling, stack overflow searches, etc.).

Google and StackOverflow are very common and important programming skills!

![[Source](https://twitter.com/madsbrodt/status/1339127984670773251)](../images/wrangling/twitter-google-stackoverflow.png){fig-alt="A screenshot of a tweet from \@madsbrodt. Tweet text: Googling and StackOverflow'ing is a natural part of programming. Don't think of it as cheating. Knowing what to search for, and which results will fit your given situation is an important skill (flame emoji)."}

![[Source](https://twitter.com/cszhu/status/1230954186520461312)](../images/wrangling/twitter-happy-debugging.png){fig-alt="A screenshot of a tweet from \@cszhu. Tweet text: if you're not happy single, you won't be happy in a relationship. true happiness comes from closing 100 chrome tabs after solving an obscure programming bug, not from someone else."}


In this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line. 

Part of the goal of this textbook is to help you **learn how to read through a package description and evaluate whether the package will do what you want**. 
We're going to try to build some of those skills starting now. 
It would be relatively easy to teach you how to do a set list of tasks, but you'll be better statisticians and programmers if you learn the skills to solve niche problems on your own.

![Apologies for the noninclusive language, but the sentiment is real. [Source](https://twitter.com/abt_programming/status/459414524303785984)](../images/wrangling/twitter-teach-program.png){fig-alt="A screenshot of a tweet from \@abt_programming. Tweet text: Give a man a program, frustrate him for a day. Teach a man to program, frustrate him for a lifetime - Muhammad Waseem."}

:::



::: callout-note

Here is a quick table of places to look in R and python to solve some of the more common problems. 

Problem | R | Python
--- | ----- | -----
Dates and Times | `lubridate` package (esp. `ymd_hms()` and variants, `decimal_date()`, and other convenience functions) | `pandas` has some date time support by default; see the [`datetime` module](https://docs.python.org/3/library/datetime.html) for more functionality. |
String manipulation | `stringr` package | Quick Tips [@chinguyenTipsStringManipulation2021], Whirlwind Tour of Python chapter [@jacobvanderplasStringManipulationRegular2016]

:::

## Summarize

The next verb is one that we've already implicitly seen in action: `summarize` takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. 
You have used it to get single-row summaries of vectorized data in R, and we've used e.g. `group_by` + `count` in Python to perform certain tasks as well. 

Here (in a trivial example), I compute the overall average HP of a Pokemon in each generation, as well as the average number of characters in their name. 
Admittedly, that last computation is a bit silly, but it's mostly for demonstration purposes.

::: panel-tabset

### R: `dplyr`{-}

```{r}
#| label: summarize
#| cache: !expr F
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")
poke %>%
  mutate(name_chr = nchar(name)) %>%
  summarize(n = max(pokedex_no), hp = mean(hp), name_chr = mean(name_chr))
```

### Python {-}

In python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as `mean`. 
You can also build custom summary functions [@whortonApplyingCustomFunctions2021], or use the `agg()` function to define multiple summary variables.
`agg()` will even let you use different summary functions for each variable, just like `summarize`.

```{python}
#| label: summarize-py
import pandas as pd

poke = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

poke = poke.assign(name_length = poke.name.str.len())
poke[['hp', 'name_length']].mean()
poke[['hp', 'name_length']].agg(['mean', 'min'])
poke[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})
```

:::

The real power of summarize, though, is in combination with Group By. 
We'll see more summarize examples, but it's easier to make good examples when you have all the tools - it's hard to demonstrate how to use a hammer if you don't also have a nail. 

## Group By + (?) = Power!

Frequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to

1. split my dataset into smaller datasets - one for each day
2. compute summary values for each smaller dataset
3. put my summarized data back together into a single dataset

This is known as the `split-apply-combine` [@wickhamSplitapplycombineStrategyData2011,@GroupSplitapplycombine2022] or sometimes, `map-reduce` [@deanMapReduceSimplifiedData2008] strategy (though map-reduce is usually on specifically large datasets and performed in parallel). 

In tidy parlance, `group_by` is the verb that accomplishes the first task. `summarize` accomplishes the second task and implicitly accomplishes the third as well.

::: aside

![The ungroup() command is just as important as the group_by() command! (by Allison Horst)](../images/wrangling/ungroup_blank.png){fig-alt="Two fuzzy monsters in the foreground in birthday party hats celebrate together because one has opened a gift with group_by() inside. Text above those two reads “Other really important parts sold separately!” In the background, a very hopeful little monster stands along holding a package labeled “ungroup”, waiting to be invited."}

:::

::: panel-tabset

### R: `dplyr`{-}

```{r}
#| label: summarize2
#| cache: !expr F
poke <- read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")
poke %>%
  mutate(name_chr = nchar(name)) %>%
  group_by(gen) %>%
  summarize(n = length(unique(pokedex_no)), hp = mean(hp), name_chr = mean(name_chr))
```

### Python {-}

```{python}
#| label: summarize2-py
import pandas as pd

poke = pd.read_csv("https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv")

poke = poke.assign(name_length = poke.name.str.len())
poke.groupby('gen')[['hp', 'name_length']].mean()
poke.groupby('gen')[['hp', 'name_length']].agg(['mean', 'min'])
poke.groupby('gen')[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})
```

:::


When you `group_by` a variable, your result carries this grouping with it. In R, `summarize` will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the `ungroup()` command. In Python, you should consider using `reset_index` or `grouped_thing.obj()` to access the original information[@danchoAnswerThereUngroup2021].


::: callout-caution

### Storms Example 

Let's try a non-trivial example, using the `storms` dataset that is part of the `dplyr` package.

::: panel-tabset

#### R {-}

```{r}
#| label: read-data-storms
library(dplyr)
library(lubridate) # for the make_datetime() function
data(storms)
storms

storms <- storms %>%
  # Construct a time variable that behaves like a number but is formatted as a date
  mutate(time = make_datetime(year, month, day, hour))
```

#### Python {-}

```{python}
#| label: read-data-storms-py
import pandas as pd
import numpy as np
storms = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv", on_bad_lines='skip')

# Construct a time variable that behaves like a number but is formatted as a date
storms = storms.assign(time = pd.to_datetime(storms[["year", "month", "day", "hour"]]))

# Remove month/day/hour 
# (keep year for ID purposes, names are reused)
storms = storms.drop(["month", "day", "hour"], axis = 1)
```

:::

We have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes). 

One thing we might want to know is at what point each storm was the strongest. Let's define strongest in the following way: 

1. The points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause). 
2. If there's a tie, we might want to know when the maximum wind speed occurred. 
3. If that still doesn't get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred. 

Let's start by translating these criteria into basic operations. I'll use dplyr function names here, but I'll also specify what I mean when there's a conflict (e.g. filter in dplyr means something different than filter in python). 

Initial attempt:

1. **For each storm** (`group_by`), 
2. we need the point where the storm has lowest atmospheric pressure. (`filter` - pick the row with the lowest pressure). 

Then we read the next part: "If there is a tie, pick the maximum wind speed."

1. `group_by`
2. `arrange` by ascending pressure and descending wind speed
3. `filter` - pick the row(s) which have the lowest pressure and highest wind speed

Then, we read the final condition: if there is still a tie, pick the status and category and compute the average time.

1. `group_by`
2. `arrange` by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)
3. `filter` - pick the row(s) which have the lowest pressure and highest wind speed
4. `summarize` - compute the average time and category (if there are multiple rows)

Let's write the code, now that we have the order of operations straight!

::: panel-tabset

#### R {-}

```{r}
#| label: max-power-storm
max_power_storm <- storms %>%
  # Storm names can be reused, so we need to have year to be sure it's the same instance
  group_by(name, year) %>%
  filter(pressure == min(pressure, na.rm = T)) %>%
  filter(wind == max(wind, na.rm = T)) %>%
  summarize(pressure = mean(pressure), 
            wind = mean(wind), 
            category = unique(category), 
            status = unique(status), 
            time = mean(time)) %>%
  arrange(time) %>%
  ungroup()
max_power_storm
```

#### Python {-}

```{python}
#| label: max-power-storm-py
grouped_storms = storms.groupby(["name", "year"])

grouped_storm_sum = grouped_storms.agg({
  "pressure": lambda x: x.min()
}).reindex()

# This gets all the information from storms
# corresponding to name/year/max pressure
max_power_storm = grouped_storm_sum.merge(storms, on = ["name", "year", "pressure"])

max_power_storm = max_power_storm.groupby(["name", "year"]).agg({
  "pressure": "min",
  "wind": "max",
  "category": "mean",
  "status": "unique",
  "time": "mean"
})
```

:::


If we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm. 

::: panel-tabset

#### R {-}

```{r}
#| label: ggplot-storms
library(ggplot2)
ggplot(max_power_storm, aes(x = pressure)) + geom_histogram()
```

#### Python {-}
```{python}
#| label: ggplot-storms-py
from plotnine import *

ggplot(max_power_storm, aes(x = "pressure")) + geom_histogram(bins=30)
```
:::

We could also look to see whether there has been any change over time in pressure.

::: panel-tabset

#### R {-}
```{r}
#| label: ggplot-power-pressure-r
ggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()
```

#### Python {-}
```{python}
#| label: ggplot-power-pressure-py
ggplot(max_power_storm, aes(x = "time", y = "pressure")) + geom_point()
```

:::

It seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic.

::: column-margin
![Radar coverage map from 1995, from @massPacificNorthwestHas2006](../images/wrangling/coastal_radar_1995.png)
:::

Another interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?

::: panel-tabset

#### R {-}

```{r}
#| label: duration-storm
storm_strength_duration <- storms %>%
  group_by(name, year) %>%
  summarize(duration = difftime(max(time), min(time), units = "days"), 
            max_strength = max(category)) %>%
  ungroup() %>%
  arrange(desc(max_strength))

storm_strength_duration %>%
  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()
```

#### Python {-}
```{python}
#| label: duration-storm-py
storm_strength_duration = storms.groupby(["name", "year"]).agg(duration = ("time", lambda x: max(x) - min(x)),max_strength = ("category", "max"))

ggplot(aes(x = "factor(max_strength)", y = "duration"), data = storm_strength_duration) + geom_boxplot()
```

:::

You don't need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with. 

We could also look to see how a storm's diameter evolves over time, from when the storm is first identified (group_by + mutate) 

Diameter measurements don't exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (`?storms` documents the dataset and its variables). 


::: panel-tabset

#### R {-}

Note the use of `as.numeric(as.character(max(category)))` to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted. 

```{r}
#| label: storm-evo
storm_evolution <- storms %>%
  filter(!is.na(hurricane_force_diameter)) %>%
  group_by(name, year) %>%
  mutate(time_since_start = difftime(time, min(time), units = "days")) %>%
  ungroup()

ggplot(storm_evolution, 
       aes(x = time_since_start, y = hurricane_force_diameter, 
           group = name)) + geom_line(alpha = .2) + 
  facet_wrap(~year, scales = "free_y")
```

#### Python {-}

```{python}
#| label: storm-evo-py
storm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]

storm_evolution = storm_evolution.assign(age = storm_evolution.groupby(["name", "year"], group_keys = False).apply(lambda x: x.time - x.time.min()))

(ggplot(storm_evolution, 
       aes(x = "age", y = "hurricane_force_diameter", 
           group = "name")) + geom_line(alpha = .2) + 
  facet_wrap("year", scales = "free_y"))
```

:::


For this plot, I've added `facet_wrap(~year)` to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms. 

It seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.

::: panel-tabset

#### R {-}

```{r}
#| label: storm-evo-year
storm_evolution %>%
  filter(year == 2008) %>%
  arrange(desc(time_since_start))

storm_evolution %>% filter(name == "Ike") %>%
  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()
```

#### Python {-}


```{python}
#| label: storm-evo-year-py
storm_evolution.query("year==2008").sort_values(['age'], ascending = False).head()

(ggplot(
  storm_evolution.query("year==2008 & name=='Ike'"),
  aes(x = "time", y = "hurricane_force_diameter", color = "category")) +
  geom_point())
```

:::

:::



## Summarizing Across Multiple Variables

Suppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). 
We don't want to write out all of the summarize statements individually, so we use `across()` instead (in `dplyr`). 

::: panel-tabset
#### R {-}

The dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. `across()` is particularly useful - it allows you to make a modification to several columns at the same time.

![dplyr's across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)](../images/wrangling/dplyr_across.png)

```{r}
#| label: storms-dplyr-other-functions
library(lubridate) # for the make_datetime() function
data(storms)

storms <- storms %>%
  # Construct a time variable that behaves like a number but is formatted as a date
  mutate(time = make_datetime(year, month, day, hour))

# Use across to get average of all numeric variables
avg_hurricane_intensity <- storms %>%
  filter(status == "hurricane") %>%
  group_by(name) %>%
  summarize(across(where(is.numeric), mean, na.rm = T), .groups = "drop") 

avg_hurricane_intensity %>%
  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %>%
  arrange(desc(wind)) %>% 
  # get top 10
  filter(row_number() <= 10) %>%
  knitr::kable() # Make into a pretty table
```

#### Python {-}

[Stackoverflow reference](https://stackoverflow.com/questions/63200530/python-pandas-equivalent-to-dplyr-1-0-0-summarizeacross)

We can use python's list comprehensions in combination with `.agg` to accomplish the same task as `dplyr`'s `across` function.

```{python}
#| label: storms-dplyr-other-functions-py
import pandas as pd
import numpy as np
storms = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv")

# Construct a time variable that behaves like a number but is formatted as a date
storms = storms.assign(time = pd.to_datetime(storms[["year", "month", "day", "hour"]]))

# Remove year/month/day/hour
storms = storms.drop(["year", "month", "day", "hour"], axis = 1)

# Remove non-hurricane points
storms = storms.query("status == 'hurricane'")

# Get list of all remaining numeric variables
cols = storms.select_dtypes(include =[np.number]).columns.values
(storms.
set_index("name").
filter(cols).
groupby('name').
agg({col: 'mean' for col in cols}))
```

By default, pandas skips NaN values. 
If we want to be more clear, or want to pass another argument into the function, we can use what is called a **lambda function** - basically, a "dummy" function that has some arguments but not all of the arguments. 
Here, our lambda function is a function of `x`, and we calculate `x.mean(skipna=True)` for each x passed in (so, for each column). 


```{python}
#| label: storms-dplyr-other-functions-py2
# Get list of all remaining numeric variables
cols = storms.select_dtypes(include =[np.number]).columns.values
(storms.
set_index("name").
filter(cols).
groupby('name').
agg({col: lambda x: x.mean(skipna=True) for col in cols}))
```
:::



## Try it out - Data Cleaning

You can [read about the gapminder project here](https://www.gapminder.org/data/documentation/). 

The gapminder data used for this set of problems contains data from 142 countries on 5 continents. 
The filtered data in `gapminder` (in R) contain data about every 5 year period between 1952 and 2007, the country's life expectancy at birth, population, and per capita GDP (in US \$, inflation adjusted). 
In the `gapminder_unfiltered` table, however, things are a bit different. 
Some countries have yearly data, observations are missing, and some countries don't have complete data. 
The `gapminder` package in python (install with `pip install gapminder`) is a port of the R package, but doesn't contain the unfiltered data, so we'll instead use a [CSV export](https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv). 


::: {.callout-tip collapse="true"}
### Read in the Data
::: panel-tabset

#### R {-}
```{r}
#| label: tryitout-gapminder
if (!"gapminder" %in% installed.packages()) install.packages("gapminder")
library(gapminder)
gapminder_unfiltered
```

#### Python {-}

```{python}
#| label: tryitout-gapminder-py
import pandas as pd

gapminder_unfiltered = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv")
```
:::

:::


::: {.callout-tip collapse="true"}
### Task 1: How Bad is It?

::: panel-tabset

#### Problem {-}
Using your EDA skills, determine *how bad* the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information. 

#### R {-}

```{r}
#| label: gapminder-tryitout-1
gapminder_unfiltered %>% 
  group_by(country) %>% 
  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), 
            missingpop = sum(is.na(pop)),
            missingGDP = sum(is.na(gdpPercap))) %>%
  filter(n != length(seq(1952, 2007, by = 5)))
```

In order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. 

#### Python {-}

```{python}
#| label: gapminder-tryitout-1-py
(
  gapminder_unfiltered.
  set_index("country").
  filter(["lifeExp", "pop", "gdpPercap"]).
  groupby("country").
  agg(lambda x: x.notnull().sum()).
  query("lifeExp != 12 | pop != 12 | gdpPercap != 12")
  )
```

In order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. 

:::
:::


::: {.callout-tip collapse="true"}
### Task 2: Exclude any data which isn't at 5-year increments
Start in 1952 (so 1952, 1957, 1962, ..., 2007). 

::: panel-tabset

#### R {-}

```{r}
#| label: gapminder-tryitout-5y-increments
gapminder_unfiltered %>%
  filter(year %in% seq(1952, 2007, by = 5))
```

#### Python {-}

[Reminder about python list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)

[Explanation of the query @ statement](https://stackoverflow.com/questions/62914335/python-pandas-query-for-values-in-list)

```{python}
#| label: gapminder-tryitout-5y-increments-py
years_to_keep = [i for i in range(1952, 2008, 5)]
gapminder_unfiltered.query("year in @years_to_keep")
```

:::
:::


::: {.callout-tip collapse="true"}
### Task 3: Exclude any countries that don't have a full set of observations

::: panel-tabset

#### R {-}

```{r}
#| label: gapminder-tryitout-countries-filter
gapminder_unfiltered %>%
  filter(year %in% seq(1952, 2007, by = 5)) %>%
  group_by(country) %>%
  mutate(nobs = n()) %>% # Use mutate instead of summarize so that all rows stay
  filter(nobs == 12) %>%
  select(-nobs)
```

#### Python {-}

```{python}
#| label: gapminder-tryitout-countries-filter-py

years_to_keep = [i for i in range(1952, 2008, 5)]

(
  gapminder_unfiltered.
  # Remove extra years
  query("year in @years_to_keep").
  groupby("country").
  # Calculate number of observations (should be exactly 12)
  # This is the equivalent of mutate on a grouped data set
  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).
  # Keep rows with 12 observations
  query("nobs == 12").
  # remove nobs column
  drop("nobs", axis = 1)
  )
```


:::
:::

::: {.callout-learnmore collapse="true"}

## Additional Resources 
- [Introduction to dplyr](https://stat545.com/dplyr-intro.html) and [Single Table dplyr functions](https://stat545.com/dplyr-single.html)

- R for Data Science: [Data Transformations](https://r4ds.had.co.nz/transform.html)

- Additional practice exercises: [Intro to the tidyverse](https://stat579-at-isu.github.io/materials/03_tidyverse/01_dplyr.html#19), [group_by + summarize examples](https://stat579-at-isu.github.io/materials//03_tidyverse/02_dplyr-examples.html), [group_by + mutate examples](https://stat579-at-isu.github.io/materials//03_tidyverse/03_dplyr-examples.html#1) (from a similar class at Iowa State)

- [Base R data manipulation](https://vknight.org/SAS-R/Content/R-Chapter-03/)

- [Videos of analysis of new data from Tidy Tuesday](https://www.youtube.com/playlist?list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8) - may include use of other packages, but almost definitely includes use of dplyr as well. 
  - [TidyTuesday Python github repo](https://github.com/waiyanps/TidyTuesday-Python) - replicating Tidy Tuesday analyses in Python with Pandas

:::

## References

