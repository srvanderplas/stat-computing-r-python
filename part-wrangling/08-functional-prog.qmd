# Functional Programming {#sec-lists}

In this section, we're going to change focus slightly from learning specific functions to learning programming **patterns**. 
We're going to start this process by talking about functional programming and its connection to lists. 
While this topic may be a bit advanced if you're just starting to learn how to program, it may help to skim through the deeper explanation so that you can at least recognize some of these words if you encounter them later.

## Objectives {-}

- Use functional programming to replace for loops
- Articulate *why* functional programming can be preferable to using for loops
- Use functional programming to clean data, model data subsets, and assemble hierarchical data.

## Programming Philosophies

::: callout-advanced
This section is intended for everyone, but I do not expect that people who are just learning to program for the first time will fully absorb everything in this section. 
Get what you can out of this, use it to improve how you write code, and come back to it later if it's too confusing.
:::

Just as spoken languages fall into families, like Indo-European or Sino-Tibetan, programming languages also have broad classifications. 
Here are a few "families" or classifications of programming languages [@kuchlingFunctionalProgrammingHOWTO2022]:

- Many languages are **procedural**: a program provides a list of instructions that tell the computer what to do with provided input. C, Pascal, Fortran, and UNIX shells are naturally procedural. JavaScript is also a fairly natural procedural language. Many R analysis scripts are also naturally written in a procedural style; SAS code is almost always procedural.
- **Declarative languages** use code to describe the problem that needs to be solved, and the language figures out how to solve it. SQL is the most common declarative language you'll encounter for data-related tasks.
- **Object oriented** languages (sometimes abbreviated OOP, for object-oriented programming) manipulate collections of objects or classes. Data is stored in classes that have associated functions, which are often called methods. Java is explicitly object-oriented; C++ and Python support object-oriented programming but don't force you to use those features.
- **Functional** programming languages describe a problem using a set of functions, which only take inputs and produce outputs. Functions don't have any internal tracking of **state** - purely functional languages move from input to output without storing variables or even printing output to the command line, but it is common to adopt a functional approach to programming without requiring strict adherence to all principles of a fully functional approach. Haskell and Rust are fairly standard functional programming languages.


::: {.youtube-video-container .aside}
Hadley's talk on The Joy of Functional Programming for Data Science

<iframe width="640" height="360" src="https://www.youtube.com/embed/bzUmK0Y07ck" title="&quot;The Joy of Functional Programming (for Data Science)&quot; with Hadley Wickham" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
:::


Functional programming languages have a goal of writing **pure functions** - functions that do not change the global state (stuff stored in objects, memory, parameters, or files) of the program and thus have no side effects. 
The return value of a pure function is based solely on the inputs to the function.
Not all functions can be pure functions - for instance, there's no pure way to do file IO operations. 
But it is a nice goal to be able to move parameters into functions and have the correct object returned from that function, so that you can pipe multiple operations together into a pipeline.

Most general-purpose languages like C++ and Python and even some domain languages like R support multiple different programming paradigms. 
While preparing to write this chapter, I saw functional programming books with examples in Java [@urmaModernJavaAction2018], JavaScript [@FPJS], and C# [@buonannoFunctionalProgrammingHow2017] - all languages that I would associate with OOP or procedural styles.
I also found books teaching object oriented programming using Fortran 90-95 [@OOPfortran], which is something I wouldn't have considered possible.

All of this is to say that while certain languages are built around principles like OOP or functional programming, almost every language has users who rely more heavily on one approach than the other. 
There are very few "pure" programming languages, which reminds me of one of my favorite quotes about English: 

> "The problem with defending the purity of the English language is that English is about as pure as a cribhouse whore. We don't just borrow words; on occasion, English has pursued other languages down alleyways to beat them unconscious and rifle their pockets for new vocabulary." ― [James D. Nicoll](https://www.goodreads.com/quotes/694108-the-problem-with-defending-the-purity-of-the-english-language)

::: callout-demo
### Object Oriented Philosophy in R and Python
::: panel-tabset
#### Python

When you call `df.size()` in Python, you are calling the `size` method that is part of the `df` object, which is a `DataFrame`. 
This suggests that Pandas, at least, is programmed using an object-oriented paradigm. 

#### R

<!-- R is a bit of a trickier case.  -->
<!-- R has several different types of object-oriented structures that we usually avoid talking about until you start looking at developing your own R packages.  -->
<!-- R also was designed by people who enjoyed using Lisp - another multi-paradigm approach that accommodates functional programming, procedural programming, and object-oriented programming.  -->
An easy example of R's object oriented nature is that when you fit different models or perform different tests, the default output is different. 

```{r}
#| label: model-output
data(mtcars)

r1 <- t.test(mtcars$mpg~mtcars$vs)
print(r1)

r2 <- lm(mtcars$mpg ~ mtcars$vs)
print(r2)
```

The output is different because each test/regression model object has a different print method, which allows R to create different output for each type of object. 

:::

:::

Functional programming allows us to write programs that are more modular, and thus, are easier to test and debug.
In addition, functional programming encourages you to think about data **pipelines**: a sequence of steps that are reproducible and reusable for different data sets and analyses.
Functional programming is convenient for another (more esoteric, but important) reason - it allows you to prove that a function or series of functions is actually correct (rather than just testing input/output combinations). 

![A pipeline of functional data analysis. Each function (station) modifies the data in some way and the returned result is passed into the next function (station) as input. This allows a sequence of functions to format data, visualize it, model it, and then package the results. While this paradigm doesn't require a functional approach, functional programming does make it simpler. Modified from [Allison Horst's work](https://github.com/allisonhorst/stats-illustrations)](../images/wrangling/functional-pipeline.png){fig-alt="Cute fuzzy monsters putting rectangular data tables onto a conveyor belt. Along the conveyor belt line are different automated “stations” that update the data. A monster at the end of the conveyor belt is carrying away a table that reads “Complete analysis.”"}

If you have been using the R pipe (`|>` or `%>%`), you didn't realize it, but you were already using functional programming. Piping results from one function to another in a chain is a prime example of the "pure function" idea - it allows us to chain each step of a sequence together to create a sequence that is modular and testable.

::: callout-demo

### A simple Functional Example

A **functional** is a function that takes another function as input and returns a vector as output.

One simple example of a functional that is found in both R and Python is the `apply` function (or variants in R like `lapply`, `sapply`, `tapply`). 
In Python, `.apply` is a method in Pandas, but we can find an even more low-level equivalent in the ideas of **list comprehensions** and **map** functions. 

One additional concept that is helpful before we start is the idea of a **lambda** function - a small anonymous function (that is, a function that is not  named or stored in a variable).
Lambda functions are great for filling in default arguments, but they have many other uses as well.

Can you identify the lambda functions in each of the following examples?

::: panel-tabset

#### R

This code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.

```{r}
lapply(1:5, function(x) rnorm(5, mean = x, sd = 1))
```

Or, if you have R 4.1.0 or above, you can use a shorthand version:

```{r}
lapply(1:5, \(x) rnorm(5, mean = x))
```

The `\(x)` is shorthand for `function(x)` and allows you to quickly and easily define anonymous functions in R.


#### Python

This code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.

```{python}
import numpy as np

# List comprehension approach
r1 = [np.random.normal(i, size = 5) for i in range(1, 6)]
print(r1) 

# Functional approach
# Defining a lambda function allows us to fill in non-default options
r2 = map(lambda a: np.random.normal(a, size = 5), range(1, 6))

# This is what map spits out by default
print(r2)
# get your results back out with list()
r2b = list(r2) 
print(r2b)
```

:::

:::

## Replacing Loops with Functional Programming

One really convenient application of functional programming is to replace loops. 
As Hadley Wickham says in [-@advr], 

> the real downside of for loops is that they’re very flexible: a loop conveys that you’re iterating, but not what should be done with the results

That is, in many cases when programming with data, what we want is to iterate over a vector and return a vector of results. 
This is a *perfect* use case for functional programming, since we're specifying both that we're iterating AND more explicitly collecting the results into a form that makes sense.

If we work with this definition of functional programming, then python list comprehensions are also a functional approach: they specify how the results are collected (usually by putting `[]` around the statement) and how the iteration will occur @rituraj_jainNestedListComprehensions2018.

::: aside
There is an excellent vignette comparing Base R functional programming approaches to the `purrr` package that is worth a look if you've used one and want to try the other [@hadleywickhamPurrrBase2022].
:::

Let's look at a few examples.

::: callout-demo
Suppose we want to look at the Lego data and create a decade variable that describes the decade a set was first released.


::: panel-tabset

### base R

```{r}
lego <- read.csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv")

lego$decade <- sapply(lego$year, \(x) floor(x/10)*10)
head(lego[,c("set_num", "name", "year", "decade")])
```


Strictly speaking, this use of `sapply` isn't necessary - because R is vectorized by default, we could also have used `lego$decade <- floor(lego$year/10)*10`.
However, there are functions in R that are not fully vectorized, and it is useful to know this approach for those use-cases as well, and it's easier to demonstrate this approach with a relatively simple use case.

### R: `purrr`

In purrr, you can create anonymous functions using `~` with `.` as a placeholder. If you need more parameters, you can use `.x`, `.y` and `map2` (for now) or `.1`, `.2`, `.3`, `...` with `pmap`. 

```{r}
library(purrr)
library(readr)
library(dplyr)

lego <- read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv")

lego <- lego |> # Either pipe will work here
  mutate(decade = purrr::map_int(year, ~floor(./10)*10))

lego |> 
  select(set_num, name, year, decade) |> 
  head()
```


### Python

```{python}
import pandas as pd
import math

lego = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv")
lego['decade'] = [math.floor(i/10)*10 for i in lego.year]
lego[['set_num', 'name', 'year', 'decade']].head()
```


:::
:::

For a more interesting example, though, let's consider fitting a different linear regression for each generation of Pokemon, describing the relationship between HP (hit points) and CP (combat power, aka `total` in this dataset). 

::: aside
I am sure that the python code I've written here is a bit kludgy, so if you are more fluent in python/pandas than I am, please feel free to submit a pull request if you know a better or more "pretty" way to do this.
:::


::: callout-caution

### Example: Pokemon modeling


::: panel-tabset

#### base R
```{r}
#| label: base-split-regression
poke <- read.csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv")
# Get rid of mega pokemon - they're a different thing
poke <- subset(poke, !grepl("Mega", poke$variant)) # step 1

# Split into a list of data frames from each gen
poke_gens <- split(poke, poke$gen) # step 2

# Fit linear regressions for each generation of pokemon
models <- lapply(poke_gens, \(df) lm(total ~ hp, data = df)) # step 3

# Pull out coefficients and r-squared values
results <- lapply(models, \(res) data.frame(coef1 = coef(res)[1], coef2 = coef(res)[2], rsq = summary(res)$r.squared))  # step 4

# Join the results back into a data.frame
results <- do.call("rbind", results) # step 5

results
```

1. Data in data frame
2. Data split into a list of data frames
3. Models in a list corresponding to data in step 2
4. Results in a list of data frames corresponding to models in step 3
5. Bind results in step 4 back into a data frame

In each step, we specify not only what the iterative action should be, but also what form the results will take.

#### Tidy R
In the tidyverse, we use `tidyr::nest()` to accomplish a similar thing to `split` in base R.

This approach is designed to work entirely within a single data frame, which keeps the environment relatively clean and ensures that each step's results are stored in a convenient, easy-to-find place.

```{r}
#| label: tidyr-nest-regression
library(purrr)
library(readr)
library(dplyr)
library(stringr)
library(tidyr)

res <- read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv") %>%
  # str_detect doesn't play nice with NAs, so replace NA with ""
  mutate(variant = replace_na(variant, "")) %>%
  # Remove mega pokemon
  filter(str_detect(variant, "Mega", negate = T)) %>% # step 1
  # Sub-data-frames
  nest(.by = gen) %>% # step 2
  # Fit model
  mutate(model = map(data, ~lm(total ~ hp, data = .))) %>% # step 3
  # Extract coefficients
  mutate(res = map(model, ~data.frame(coef1 = coef(.)[1], 
                                      coef2 = coef(.)[2], 
                                      rsq = summary(.)$r.squared))) %>% # step 4
  # Bind together
  unnest(c(res)) # step 5
res
```

Our data takes the form:

1. An ungrouped data frame
2. A data frame with 9 rows, one for each generation, with a list-column `data` that contains the full data for each generation
3. We fit our model and store the model results into another list-column named `model` that contains the fitted model object
4. We define some summary information and store it into a list-column containing each 1-row data frame
5. We "unnest" the summary information, which is equivalent to bringing the columns we defined up to the primary level and binding the rows together.

At each step, we're specifying the form of the results along with the contents.


#### Python

This construct of storing everything inside a single data frame isn't as common in Python, but we can make it work with only a little extra effort. 

You will need to `pip install statsmodels` to get the `statsmodels` [@seabold2010statsmodels] package that implements many basic statistical models. 
The `scikit-learn` package [@scikitlearn] is another commonly used package [@menonLinearRegressionLines2018], but it does not have the easy accessor functions to pull out e.g. coefficients and r-squared values, so we'll use `statsmodels` here. 

```{python}
#| label: groupby-regression
import pandas as pd
from statsmodels.formula.api import ols

# Create a function to fit a linear regression
# There is probably a better way to do this flexibly,
# but this approach is simple and useful for illustrative purposes
def pokereg(data):
  x = data[["hp"]].values
  y = data[["total"]].values
  model = ols('total ~ hp', data)
  results = model.fit()
  return results

res = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv")

# Replace NAs with ""
res["variant"] = ["" if pd.isna(i) else i for i in res.variant]
# Remove mega pokemon
res = res.query('~(variant.str.contains("(?:^[mM]ega)"))')

# Group data frames and apply regression function to each group
res_reg = (
  res # step 1
    .groupby("gen") # step 2
    .apply(pokereg) # step 3
)

# Make results into a dataframe and rename the column as 'results'
res_reg = pd.DataFrame(res_reg).rename(columns = {0:'results'}) # step 4

# Get values of interest and store in new columns # step 5
res_reg = res_reg.reset_index() # store gen in its own column
res_reg['coef1'] = res_reg.results.map(lambda x: x.params[0])
res_reg['coef2'] = res_reg.results.map(lambda x: x.params[1])
res_reg['rsq'] = res_reg.results.map(lambda x: x.rsquared)

res_reg[['gen', 'coef1', 'coef2', 'rsq']]
```

While this doesn't store our data in the same DataFrame as the model results, we do have a key that links the two: the `gen` variable is present in both `res` and `res_reg` and can be used to join the data to the regression results, if necessary.

1. Data in an ungrouped data frame
2. We group by `gen` (generation)
3. We apply the function `pokereg` to fit a linear regression, and the results are stored in an indexed Series where the index corresponds to `gen`.
4. We make the results into a DataFrame so that we can add extra columns, and rename the automatically created Series to `results` to be more descriptive
5. We create summary information and store the summaries in columns in the data frame.

While the grouping and binding operations are in a different order in Python than in R, the basic specification of the structure of the output each time we iterate is similar.

:::

:::


::: {.youtube-video-container .aside}
Here's another demonstration of the use of the `tidymodels` package and `purrr` to fit multiple regression models to data subsets.

<iframe width="640" height="360" src="https://www.youtube.com/embed/KdDyRt9XV_g" title="Nested data frames and multiple models in R with Equitable Equations" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
:::


## Complex Data Structures

Not all datasets are strictly tabular. 
One of the most common situations where we get data that can't be made into a completely tabular structure is when we're dealing with hierarchical data: tree structures, (network) graph structures, and even most webpages contain data that isn't strictly tabular in nature.
Sometimes, we can get that data into a tabular structure, but it generally depends on the data itself.

One of the most common structures for storing data on the web is [JSON: JavaScript Object Notation](https://www.json.org/json-en.html)[@wikipediacontributorsJSON2023]. 

::: aside
(JSON is pronounced "Jason", like the person's name).
:::

```{r}
#| label: get-tmdb-data
#| include: !expr F
#| eval: !expr F
# https://developers.themoviedb.org/3/search/search-movies
# https://developers.themoviedb.org/3/discover/movie-discover

library(httr)

resp = GET(sprintf("https://api.themoviedb.org/3/authentication/token/new?api_key=%s", tmdb_key))
request_token = httr::content(resp, "parsed")$request_token

person = "Patrick%20Stewart"
personsearch = GET(sprintf("https://api.themoviedb.org/3/search/person?api_key=%s&language=en-US&page=1&include_adult=false&query=%s", tmdb_key, person))
# content(personsearch, "parsed")
patrick_stewart <- content(personsearch, "parsed")
ps_id <- patrick_

mysearch = GET(sprintf("https://api.themoviedb.org/3/person/%d/movie_credits?api_key=%s", ps_id, tmdb_key))

writeLines(content(mysearch, "text"), con = "../data/Patrick_Stewart.json")


query <- sprintf("https://api.themoviedb.org/3/search/movie?api_key=%s&query=%s", tmdb_key, "Star%20Trek")
mysearch = GET(query)
total_pgs = content(mysearch, "parsed")$total_pages

query <- paste0(query, "&page=", 1:6)
res <- map(query, GET)
res_content <- map(res, content, "text")
res_json <- map(res_content, fromJSON)
res_tbl <- map_df(res_json, as_tibble) %>%
  unnest(results) %>%
  nest(results = -c(page, total_pages, total_results))

writeLines(toJSON(res_tbl, pretty = TRUE), con = "../data/Star_Trek.json")
```

![The Movie Database](../images/wrangling/tmdb.svg){fig-alt="The Movie Database logo" width="50%"}
In this section we'll work with some data gathered from TMDB (the movie database). 
I submitted a query for all movies that Patrick Stewart was involved with, and you can find the resulting JSON file [here](https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json). 

::: callout-demo

### JSON File Parsing

::: panel-tabset

#### R

We'll use the `jsonlite` package to read the data in, but invariably this package still requires us to do some post-processing ourselves.

```{r}
library(jsonlite)
data_url <- "https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json"

ps_json <- fromJSON(data_url)
```


<details><summary>Exploring the output structure</summary>
```{r}
# head(ps_json) # This output is too long
ps_json |> str(max.level = 1)
map(ps_json, head) # show the first 6 rows of each element in the list
```
</details>

By default, fromJSON does a LOT of heavy lifting for us: 

1. Identifying the structure of the top-level data: cast, crew, and id information
2. Parses cast information into a data frame with list-columns
3. Parses crew information into a data frame with list-columns

It's hard to explain how *nice* this is to someone who hasn't had to parse this type of information by hand before... so let's briefly explore that process.

```{r}
library(jsonlite)

ps_messy <- fromJSON(data_url, simplifyVector = T, simplifyDataFrame = F)
```


<details><summary>Exploring the output structure (long version)</summary>
```{r}
# Top-level objects (show the first object in the list)
ps_messy$cast[[1]]
ps_messy$crew[[1]]
ps_messy$id
```
</details>

Let's start with the cast list. Most objects seem to be single entries; the only thing that isn't is the `genre_ids` field. So let's see whether we can just convert each list entry to a data frame, and then deal with the `genre_ids` column afterwards.

```{r}
#| error: !expr T
cast_list <- ps_messy$cast
```


<details><summary>Data frame conversion</summary>
```{r}
as.data.frame(cast_list[[1]])
```
</details>

```{r}
map(cast_list, as.data.frame)
```

Well, that didn't work, but the error message at least tells us what index is causing the problem: 6. Let's look at that data:

<details><summary>Data frame conversion errors</summary>
```{r}
cast_list[[6]][1:5]
```
</details>

Ok, so `backdrop_path` is `NULL`, and `as.data.frame` can't handle the fact that some fields are defined (length 1) and others are NULL (length 0). We could possibly replace the NULL with NA first?

```{r}
fix_nulls <- function(x) {
  lapply(x, \(y) if (is.null(y)) NA else y)
}

cast_list_fix <- map(cast_list, fix_nulls)

cast_list_fix[[6]][1:5]

map(cast_list_fix, as.data.frame)

cast_list_fix[[8]][1:5]
```

Ok, well, this time, we have an issue with position 8, and we have an empty list of genre_ids.

An empty list and NULL both have length 0, so let's alter our `fix_nulls` function to test for things of length 0 instead of testing for nulls. 
That should fix both problems using the same code, and we're trying to directly test for the issue which was causing problems, which is perhaps a better approach anyways.

```{r}
fix_nulls <- function(x) {
  lapply(x, \(y) if (length(y) == 0) NA else y)
}

cast_list_fix <- map(cast_list, fix_nulls)
cast_list_fix[[8]][1:5]

cast_list_df <- map_df(cast_list_fix, as.data.frame)
cast_list_df[1:10, 1:5]
```

We still have too many rows for each entry because of the multiple `genre_ids`. 
But we can fix that with the `nest` command.

```{r}
cast_list <- nest(cast_list_df, genre_ids = genre_ids )
cast_list[1:10,c(1:4, 17)]
```

Then, we'd have to apply this whole process to the crew list as well. 
Let's see how robust our process actually is!

```{r}
crew_list <- ps_messy$crew
crew_list_fix <- map(crew_list, fix_nulls)
crew_list_df <- map_df(crew_list_fix, as.data.frame)
crew_list <- nest(crew_list_df, genre_ids = genre_ids )
crew_list[1:5,c(1:4, 17)]
```

Ok, so that actually worked, but only because the structure of the crew data is the same as the structure of the cast data. 

It's good to see what we'd have to do manually if `fromJSON()` failed on us. 
It's also an excellent example of functional programming in a practical setting.

Let's finish this up by converting our cast and crew data frames into a single data frame with a variable indicating which source DF is relevant.

```{r}
patrick_stewart_movies <- bind_rows(
  mutate(cast_list, role = "cast"),
  mutate(crew_list, role = "crew")
)
patrick_stewart_movies %>%
  arrange(id)
```

We could theoretically clean this up so that movies where Patrick Stewart was in both the cast and crew are on a single row, but I think this is "good enough" for now. 

#### Python

Pandas includes a `read_json` function, so let's try that and see if it works as well as `fromJSON()` did in R:

```{python}
#| error: !expr T
import pandas as pd

data_url = "https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json"

pd.read_json(data_url)
```

If we read the [documentation for read_json](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html), we can see that we have a few different options - maybe playing around with some of those options will help? Our top-level structure is a list with 3 values: cast, crew, and id. So let's see if we can read things in as a series instead of a DataFrame first, and hopefully we can use that to get some traction on the situation.

```{python}
patrick_stewart = pd.read_json(data_url, typ='series', orient = 'records')

# List the objects
patrick_stewart.index

# First item in the cast list
patrick_stewart.cast[0]
```

So now how do we get our data into a proper form? If we read the documentation a bit further, we can see a "See also" section that has a `json_normalize` function which promises to "Normalize semi-structured JSON data into a flat table". That sounds pretty good, let's try it!

```{python}
ps_cast = pd.json_normalize(patrick_stewart.cast)
ps_cast.head()
```

Huh, that actually worked! (I'm not used to this type of thing working on the first try).

```{python}
ps_crew = pd.json_normalize(patrick_stewart.crew)
ps_crew.head()
```

We can combine these as we did in R into a single data frame, and sort by movie ID to simplify the list.

```{python}
ps_cast['role'] = 'cast'
ps_crew['role'] = 'crew'
ps_movies = pd.concat([ps_cast, ps_crew])
ps_movies[['id', 'original_title', 'character', 'job']].sort_values(['id'])
```

:::

:::


::: callout-tip

### Try It Out: JSON File Parsing
![The Movie Database](../images/wrangling/tmdb.svg){fig-alt="The Movie Database logo" width="50%"}

I used TMDB to find all movies resulting from the query "Star Trek" and stored the resulting JSON file [here](https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json). 


::: panel-tabset

#### Problem

Create a data frame using the Star Trek [query results](https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json). 
Because there were 6 pages of query results, the JSON file looks a bit different than the format used in the example above. 
Can you create a plot of the release date and rating of each movie?


#### R solution

```{r}
library(jsonlite)
library(tidyr)
library(dplyr)

file_loc <- "https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json"

startrek <- fromJSON(file_loc) |>
  unnest(results)

startrek |>
  select(title, release_date, popularity, vote_average, vote_count) |>
  head()

# convert release_date to datetime
library(lubridate)
startrek <- startrek |>
  mutate(rel_date = ymd(release_date))

startrek |>
  arrange(rel_date) |>
  select(title, rel_date, popularity, vote_average, vote_count) |>
  head()

library(ggplot2)
ggplot(startrek, aes(x = rel_date, y = popularity)) + geom_point() + 
  xlab("Release Date")

ggplot(startrek, aes(x = rel_date, y = vote_average)) + geom_point() + 
  xlab("Release Date")
```



#### Python solution

```{python}
import pandas as pd
import numpy as np
file_loc = "https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json"
trek = pd.read_json(file_loc)

trek['results2'] = trek.results.map(pd.json_normalize)

# This doesn't actually keep the page info but I don't think we need that
trek_tidy = pd.concat(trek.results2.to_list())
trek_tidy['rel_date'] = pd.to_datetime(trek_tidy.release_date)

import matplotlib.pyplot as plt
p1 = trek_tidy.plot.scatter('rel_date', 'popularity')
plt.show()

p2 = trek_tidy.plot.scatter('rel_date', 'vote_average')
plt.show()
```


:::

:::

## Assembling Hierarchical Data

Another common situation we find ourselves in as analysts is to have multiple levels of data. 

Let's start with a totally absurd hypothetical situation: Suppose I watched the documentary ["Chicken People"](https://web.archive.org/web/20240803202824/https://www.imdb.com/title/tt4819510/) and became interested in the different breeds of chicken. 
As a data scientist, I want to assemble a dataset on chicken breeds that I might use to decide what breed(s) to raise.

A site such as [Cackle Hatchery](https://www.cacklehatchery.com/chicken-breeds/) has an overall summary table as well as pages for each individual breed. 
I'm not going to show you how to web scrape here - it's not relevant to this chapter - but we can at least outline the process:

- Acquire the overall table
- Use the links to each breed in the overall table to get more specific information for each breed
    - This will require a function to scrape that individual data
    - We can use map to apply that function to acquire individual data from each breed

I've used this approach to generate two files: 

- [chicken-breeds.csv](https://github.com/srvanderplas/datasets/blob/main/raw/chicken-breeds.csv) - the original table of breed information
- [chicken-breed-details.json](https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json), which is a JSON file assembled by scraping information off of each breed's individual page. 

::: callout-tip
### Try It Out: Chicken Breed Data Assembly

::: panel-tabset

#### Problem

Can you create a nested data frame that has all of the information from both the CSV and JSON file in a single tabular structure?

#### R solution

```{r}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(jsonlite)
library(stringr)

overall <- read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv")
details <- fromJSON("https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json")

head(overall)
head(details)

breed_details <- details |>
unnest(c("name", "description")) |>
  mutate(name = str_remove_all(name, " Chicken") |>
           str_remove_all("[^\\x00-\\x7F]+") |> 
           str_remove_all("Standard|Game") |>
           str_replace_all("D’", "d") |>
           str_to_title() |>
           str_squish())

overall <- overall |>
  mutate(name = `Chicken Breed Name` |>
           str_remove_all("[^\\x00-\\x7F]+") |>
           str_remove_all("Standard|Game") |>
           str_replace_all("D’", "d") |>
           str_to_title() |>
           str_squish())

# Names aren't exactly the same, but close enough after some minor string manipulation
anti_join(overall, breed_details)
anti_join(breed_details, overall)

chickens <- full_join(overall, breed_details)
head(chickens)
```

#### Python

```{python}
import pandas as pd
overall = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv")
details = pd.read_json("https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json")

overall['name'] = overall['Chicken Breed Name'].str.replace("[^\x00-\x7F]|Standard|Game|Chicken", '', regex=True).str.replace("D’", "d").str.replace("\s{1,}", " ", regex = True).str.title().str.strip()

details = details.explode(['name', 'description'])

details['name'] = details['name'].str.replace("[^\x00-\x7F]|Standard|Game|Chicken", '', regex=True).str.replace("D’", "d").str.replace("\s{1,}", " ", regex = True).str.title().str.strip()
  
chickens = overall.merge(details, how='outer', on = 'name',  indicator=True)
chickens[(chickens._merge!='both')][['name', '_merge']]

chickens.head()
```

:::

:::


::: callout-tip
### Try It Out: Cleaning Chicken Data

::: panel-tabset

#### Problem

Unnest the chicken breed facts data, cleaning the responses. 
Which jobs are most suitable for a functional programming approach?

#### R solution

```{r}
# Column names in breed_facts are too different
# chickens_exp <- chickens |> unnest('breed_facts', names_sep='facts')

fix_names <- function(df) {
  if (!is.null(df)) {
    names(df) <- names(df) |>
      str_to_title() |>
      str_remove_all("[^A-z]") |> # Remove anything that isn't A-z, including spaces.
      str_replace_all(c("CountryOfOrigin?" = "Origin", "Weights" = "Weight", "Tlc" = "TLC", "Albc" = "ALBC", "Apa" = "APA", "BroodyS" = "Broody", "Temperment" = "Temperament", "Broody" = "Broody_facts", "Purpose" = "Purpose_facts")) |>
      str_remove_all("Shell|FarmSource|SourceFarm|Small|PoultryShow") |>
      str_replace_all("^$", "xxx") # replace blank names with xxx
    df
  } else {
    return(NULL) 
  }
}
chickens_fix <- chickens |> 
  mutate(breed_facts = map(breed_facts, fix_names))

# Test names
chickens_fix$breed_facts %>% map(names) |> unlist() |> unique()
```

We've fixed some of the misspellings and duplications. Rooster, Pullet, and Cockerel are all likely to be parsing issues stemming from Weight, but that's the reality of working with data that is gathered from the internet.

```{r}
chickens_exp <- chickens_fix |> unnest("breed_facts")

head(chickens_exp[,c(1, 16:37)])
```

There's still quite a bit of cleaning left to do to get this data to be "pretty". 

```{r}
tidy_col <- function(x, text = "(?:\\(estimates only, see FAQ\\))|(?:^APA)|(?:^TLC)|EggSize|(?:Fertility Percentage)|(?:Purpose and Type)") {
  str_remove_all(x, "[\u0600-\u06FF]") |> # Remove non-ascii characters
    str_remove_all("[Â®â¢Ââ]") |>
    str_remove_all(text) |>
    str_remove_all("[:\\.\\?!\\*]") |>
    str_replace_all("\u0094", "-") |>
    str_replace_all("-{1,}", "-") |>
    str_squish()
}

tmp <- mutate(chickens_exp, across(Class:Purpose_facts, tidy_col))

head(select(tmp, 1, Class:Purpose_facts))
```

If we consider the use of `across()` as a functional programming technique (which it is), then it is much easier to create a generic `tidy_col` function than to tidy each column individually. There are probably a few things we've missed, but the data looks decent for the amount of time we put in.

#### Python

```{python}
import pandas as pd

```

XXX TODO

:::

:::



## References {#sec-functional-prog-refs}
