[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing using R and Python",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#content-overload",
    "href": "index.html#content-overload",
    "title": "Statistical Computing using R and Python",
    "section": "Content Overload!",
    "text": "Content Overload!\nThis book is designed to explain and demonstrate statistical programming concepts and techniques. I started this project in Summer 2020 as a less-tedious way to learn programming compared to hours and hours of video lectures. I’ve always found that watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding, but it’s hard to start from nothing, too.\nBecause this book was begun as an alternative to recorded lectures with slides, I have included all of the comics, snark, and gifs that I would normally have put in lecture slides. I’ve also supplemented this with other things that you can’t usually put in slide presentations: YouTube videos, extra resources, links to other textbooks that are more specific than this one. My goal is to make this a collection of the best information I can find on data science and statistical programming.\n\nThere is a downside to this approach: in most cases, this book includes way more information than you need. Everyone starts with a different level of computing experience, so I’ve attempted to make this book comprehensive. Unfortunately, that means some sections will seem like they are stating the obvious, and some sections will have more detail than you ever wanted to know. Use this book in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-format-guide",
    "href": "index.html#book-format-guide",
    "title": "Statistical Computing using R and Python",
    "section": "Book Format Guide",
    "text": "Book Format Guide\nI’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\nButtons/Links\nThe book contains a number of features which should help you navigate, use, improve, and respond to the textbook.\n\n\n\n\nTextbook features, menus, and interactive options (click to enlarge)\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThese sections contain instructions for installing and configuring things. They may be collapsed by default, but that doesn’t make them any less important – it’s just useful to condense them after you’ve completed the setup so that you can focus on more important things.\n\n\n\nOperating System Instructions\nSome instructions depend on your operating system. Where it’s shorter, I will use tabs to provide you with OS specific instructions. Here are the icons I will use:\n\n\n Windows\n Mac\n Linux\n\n\n\nWindows-specific instructions\n\n\nMac specific instructions\n\n\nLinux specific instructions. I will usually try to make this generic, but if it’s gui based, my instructions will usually be for KDE.\n\n\n\n\n\n\n\n\n\nWarnings\n\n\n\nThese sections contain things you may want to look out for: common errors, mistakes, and unfortunate situations that may arise when programming.\n\n\n\n\n\n\n\n\nDemos\n\n\n\nThese sections contain basic demonstrations of how functions or concepts work. They are slightly less interactive than examples.\n\n\n\n\n\n\n\n\nExamples\n\n\n\nExamples are intended to be interactive - you should attempt them before looking at the code solutions. Often, examples will have tabs which provide the problem, a sketch (sometimes), and then solutions in R and python.\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe problem will be in the first tab for you to start with\n\n\nA solution will be provided in R, potentially with an explanation.\n\n\nA solution will be provided in Python as well, with an explanation of that code.\n\n\n\nIn some cases, the problem will be more open-ended and may not adhere to this format, but most example sections in this book will have solutions provided. I highly recommend that you attempt to solve the problem yourself before you look at the solutions - this is the best way to learn. Passively reading code does not result in information retention.\n\n\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\n\n\nThese sections will direct you to additional resources that may be helpful to consult as you learn about a topic. You do not have to use these sections unless you are 1) bored, or 2) hopelessly lost. They’re provided to help but are not expected reading (Unlike the essential reading sections in red).\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\n\n\nThese sections are intended to apply to more advanced courses. If you are taking an introductory course, feel free to skip that content for now.\n\n\n\nExpandable Sections\nThese are expandable sections, with additional information when you click on the line\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\n\n\n\nAnother type of expandable note\n\n\n\n\n\nAnswers or punchlines may be hidden in this type of expandable section as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#analytics",
    "href": "index.html#analytics",
    "title": "Statistical Computing using R and Python",
    "section": "Analytics",
    "text": "Analytics\nI have enabled Google Analytics on this site for the purposes of measuring this work’s impact and use both in my own classes and elsewhere. I’m not using the individual tracking/ad-targeting settings (to the best of my knowledge) - my only purpose in using Google Analytics is to assess how often this site is used, and where its’ users are located at a rough (state/regional) level.\nIf you are using this site and aren’t affiliated with the University of Nebraska Lincoln, or have found it useful, please let me know by making a comment in Giscus (below) or sending me an email! These affirmations help me make a case that spending time on this resource is actually a good investment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Computing using R and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe cover of this book is an amalgam of different images by the lovely @allison_horst, which are released under the cc-by 4.0 license. I have modified them to remove most of the R package references and arrange them to represent the topics covered in this book.\nLaptop icon used in the tab/logo created by Good Ware - Flaticon\nThroughout this book, I have borrowed liberally from other online tutorials, published books, and blog posts. I have tried to ensure that I link to the source material throughout the book and provide appropriate credit to anyone whose examples I have used, modified, or repurposed. Special thanks to the tutorials provided by Posit/RStudio and the tidyverse project.\nI don’t have official editors, but thank you to those who make use of the giscus comment box to let me know about issues and typos. So far, you’ve helped me fix at least 3 issues so far!\n\n\n\n\n\n\nTools\n\n\n\n\n\nThis book was built with the following parameters/settings/library versions:\n\nimport os\nimport sys\n\nitemlist = [\"PWD\", \"SHELL\", \"USER\", \"PYTHONIOENCODING\", \"VIRTUAL_ENV\", \"RETICULATE_PYTHON\", \"R_HOME\", \"R_PLATFORM\", \"LD_LIBRARY_PATH\", \"R_LIBS_USER\", \"R_LIBS_SITE\",\"RENV_PROJECT\", \"RSTUDIO_PANDOC\", \"RMARKDOWN_MATHJAX_PATH\", \"R_SESSION_INITIALIZED\", \"PYTHONPATH\"]\n\nitemlist = list(set(itemlist) & set(os.environ))\nitemlist.sort()\n\nfor item in itemlist:\n    print(f'{item}{\" : \"}{os.environ[item]}')\n## LD_LIBRARY_PATH : /home/susan/.virtualenvs/book/lib:/usr/lib:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server\n## PWD : /home/susan/Projects/Class/stat-computing-r-python\n## PYTHONIOENCODING : utf-8\n## PYTHONPATH : /usr/lib/python311.zip:/usr/lib/python3.11:/usr/lib/python3.11/lib-dynload:/home/susan/.virtualenvs/book/lib/python3.11/site-packages:/home/susan/.cache/R/renv/cache/v5/linux-debian-bookworm/R-4.5/x86_64-pc-linux-gnu/reticulate/1.43.0/0b3db378d9940f6846a626b24352530a/reticulate/python\n## RENV_PROJECT : /home/susan/Projects/Class/stat-computing-r-python\n## RMARKDOWN_MATHJAX_PATH : /usr/lib/rstudio/resources/app/resources/mathjax-27\n## RSTUDIO_PANDOC : /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64\n## R_HOME : /usr/lib/R\n## R_LIBS_SITE : /usr/local/lib/R/site-library/:/usr/local/lib/R/site-library/:/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library:/usr/lib/R/library\n## R_LIBS_USER : /home/susan/Projects/Class/stat-computing-r-python/renv/library/linux-debian-bookworm/R-4.5/x86_64-pc-linux-gnu\n## R_PLATFORM : x86_64-pc-linux-gnu\n## R_SESSION_INITIALIZED : PID=17650:NAME=\"reticulate\"\n## SHELL : /bin/bash\n## USER : susan\n## VIRTUAL_ENV : /home/susan/.virtualenvs/book\n\nprint(sys.path)\n## ['', '/usr/bin', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/home/susan/.virtualenvs/book/lib/python3.11/site-packages', '/home/susan/.cache/R/renv/cache/v5/linux-debian-bookworm/R-4.5/x86_64-pc-linux-gnu/reticulate/1.43.0/0b3db378d9940f6846a626b24352530a/reticulate/python', '/home/susan/.virtualenvs/book/lib/python311.zip', '/home/susan/.virtualenvs/book/lib/python3.11', '/home/susan/.virtualenvs/book/lib/python3.11/lib-dynload']\n\n\nlibrary(devtools)\ndevtools::session_info()\n## ─ Session info ───────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.5.1 (2025-06-13)\n##  os       Debian GNU/Linux 12 (bookworm)\n##  system   x86_64, linux-gnu\n##  ui       X11\n##  language (EN)\n##  collate  en_US.UTF-8\n##  ctype    en_US.UTF-8\n##  tz       America/Chicago\n##  date     2025-09-09\n##  pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n##  quarto   1.6.39 @ /usr/local/bin/quarto\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────\n##  ! package     * version date (UTC) lib source\n##  P cachem        1.1.0   2024-05-16 [?] CRAN (R 4.5.1)\n##  P cli           3.6.5   2025-04-23 [?] CRAN (R 4.5.1)\n##  P devtools    * 2.4.5   2022-10-11 [?] CRAN (R 4.5.1)\n##  P digest        0.6.37  2024-08-19 [?] CRAN (R 4.5.1)\n##  P ellipsis      0.3.2   2021-04-29 [?] CRAN (R 4.5.1)\n##    evaluate      1.0.5   2025-08-27 [1] CRAN (R 4.5.1)\n##  P fastmap       1.2.0   2024-05-15 [?] CRAN (R 4.5.1)\n##  P fontawesome * 0.5.3   2024-11-16 [?] CRAN (R 4.5.1)\n##  P fs            1.6.6   2025-04-12 [?] CRAN (R 4.5.1)\n##  P glue          1.8.0   2024-09-30 [?] CRAN (R 4.5.1)\n##  P htmltools     0.5.8.1 2024-04-04 [?] CRAN (R 4.5.1)\n##  P htmlwidgets   1.6.4   2023-12-06 [?] CRAN (R 4.5.1)\n##  P httpuv        1.6.16  2025-04-16 [?] CRAN (R 4.5.1)\n##  P jsonlite      2.0.0   2025-03-27 [?] CRAN (R 4.5.1)\n##  P knitr         1.50    2025-03-16 [?] CRAN (R 4.5.1)\n##  P later         1.4.4   2025-08-27 [?] CRAN (R 4.5.1)\n##  P lattice       0.22-7  2025-04-02 [?] CRAN (R 4.5.1)\n##  P lifecycle     1.0.4   2023-11-07 [?] CRAN (R 4.5.1)\n##  P magrittr      2.0.3   2022-03-30 [?] CRAN (R 4.5.1)\n##  P Matrix        1.7-4   2025-08-28 [?] CRAN (R 4.5.1)\n##  P memoise       2.0.1   2021-11-26 [?] CRAN (R 4.5.1)\n##  P mime          0.13    2025-03-17 [?] CRAN (R 4.5.1)\n##  P miniUI        0.1.2   2025-04-17 [?] CRAN (R 4.5.1)\n##  P pkgbuild      1.4.8   2025-05-26 [?] CRAN (R 4.5.1)\n##  P pkgload       1.4.0   2024-06-28 [?] CRAN (R 4.5.1)\n##  P png           0.1-8   2022-11-29 [?] CRAN (R 4.5.1)\n##  P profvis       0.4.0   2024-09-20 [?] CRAN (R 4.5.1)\n##  P promises      1.3.3   2025-05-29 [?] CRAN (R 4.5.1)\n##  P purrr         1.1.0   2025-07-10 [?] CRAN (R 4.5.1)\n##  P R6            2.6.1   2025-02-15 [?] CRAN (R 4.5.1)\n##  P Rcpp          1.1.0   2025-07-02 [?] CRAN (R 4.5.1)\n##  P remotes       2.5.0   2024-03-17 [?] CRAN (R 4.5.1)\n##  P renv          1.1.5   2025-07-24 [?] CRAN (R 4.5.1)\n##  P reticulate    1.43.0  2025-07-21 [?] CRAN (R 4.5.1)\n##  P rlang         1.1.6   2025-04-11 [?] CRAN (R 4.5.1)\n##  P rmarkdown     2.29    2024-11-04 [?] CRAN (R 4.5.1)\n##  P rstudioapi    0.17.1  2024-10-22 [?] CRAN (R 4.5.1)\n##  P sessioninfo   1.2.3   2025-02-05 [?] CRAN (R 4.5.1)\n##  P shiny         1.11.1  2025-07-03 [?] CRAN (R 4.5.1)\n##  P urlchecker    1.0.1   2021-11-30 [?] CRAN (R 4.5.1)\n##  P usethis     * 3.2.0   2025-08-28 [?] CRAN (R 4.5.1)\n##  P vctrs         0.6.5   2023-12-01 [?] CRAN (R 4.5.1)\n##  P xfun          0.53    2025-08-19 [?] CRAN (R 4.5.1)\n##  P xtable        1.8-4   2019-04-21 [?] CRAN (R 4.5.1)\n##  P yaml          2.3.10  2024-07-26 [?] CRAN (R 4.5.1)\n## \n##  [1] /home/susan/Projects/Class/stat-computing-r-python/renv/library/linux-debian-bookworm/R-4.5/x86_64-pc-linux-gnu\n##  [2] /home/susan/.cache/R/renv/sandbox/linux-debian-bookworm/R-4.5/x86_64-pc-linux-gnu/9a444a72\n## \n##  * ── Packages attached to the search path.\n##  P ── Loaded and on-disk path mismatch.\n## \n## ─ Python configuration ───────────────────────────────────────────────────────\n##  python:         /home/susan/.virtualenvs/book/bin/python\n##  libpython:      /usr/lib/python3.11/config-3.11-x86_64-linux-gnu/libpython3.11.so\n##  pythonhome:     /home/susan/.virtualenvs/book:/home/susan/.virtualenvs/book\n##  version:        3.11.2 (main, Apr 28 2025, 14:11:48) [GCC 12.2.0]\n##  numpy:          /home/susan/.virtualenvs/book/lib/python3.11/site-packages/numpy\n##  numpy_version:  2.3.2\n##  \n##  NOTE: Python version was forced by VIRTUAL_ENV\n## \n## ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part-tools/00-tools-intro.html",
    "href": "part-tools/00-tools-intro.html",
    "title": "Part I: Tools",
    "section": "",
    "text": "This part of the textbook provides an overview of the different tools we will be using: terminals and consoles, R, python, quarto, markdown, pandoc, and git. It can be a bit confusing at first, especially if you’re not familiar with how your computer works, where files are stored, and different ways to tell your computer what to do.\n1  Computer Basics gives you some important background material about how a computer functions.\n2  Setting Up Your Computer tells you exactly what software you need to install for the rest of this textbook.\n4  Scripts and Notebooks and Terminals discusses the different ways we can talk to R and python, and the pros and cons of each.\n5  Version Control with Git discusses how to use git (setup is covered in 2  Setting Up Your Computer) and how to solve common git problems.\n6  Reproducibility and Professional Communication discusses some solutions for dynamic document creation and literate programming (storing code, writeup, and documentation in the same file) using knitr, quarto, rmarkdown, and more.",
    "crumbs": [
      "Part I: Tools"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html",
    "href": "part-tools/01-computer-basics.html",
    "title": "1  Computer Basics",
    "section": "",
    "text": "Objectives\nBefore you learn how to program a computer, it can be helpful to learn a few basic things about how computers work. Modern computing environments hide most of the details about where and how files are stored from the user, but when you write computer programs, these details suddenly become important.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#objectives",
    "href": "part-tools/01-computer-basics.html#objectives",
    "title": "1  Computer Basics",
    "section": "",
    "text": "Know the meaning of computer hardware and operating system terms such as hard drive, memory, CPU, OS/operating system, file system, directory, and system paths\nUnderstand the basics of how the above concepts relate to each other and contribute to how a computer works\nUnderstand the file system mental model for computers enough to identify where your files are stored\nLocate and follow directions for software installation based on your computer’s hardware and operating system.\n\n\n\n\n\n\n\nDo It Yourself\n\n\n\nIn this section, you will be identifying your computer’s specifications.\nYou will want to have a notebook or text file that you can reference later to record this information. Go ahead and determine where you will save this information now.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#hardware",
    "href": "part-tools/01-computer-basics.html#hardware",
    "title": "1  Computer Basics",
    "section": "\n1.1 Hardware",
    "text": "1.1 Hardware\nThe components that make up the physical computer are the hardware. This 3-minute video is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\nThe important distinction for hardware is between Random Access Memoroy (RAM, or ‘memory’) and disk storage (hard drives). You can usually store much more on disk than you can have available in RAM, but when working with “big” data1, we must use different approaches than when working with data that can fit in memory.\nWe also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). For now, you are unlikely to challenge a modern processor when you first start learning R and Python, but as you acquire new skills, you may want to learn a bit about parallel processing (sending tasks to multiple processors). Most of the other details aren’t critical to programming with data just yet – graphics cards are important for some applications, but if you’re just learning R and python, you have a ways to go before you get there.\n\n\n\n\n\n\nDo It Yourself\n\n\n\nExamine the hardware on your computer using one of the following methods:\n\nWindows: Ctrl+Shift+Escape &gt; Task Manager &gt; More Options &gt; Performance tab\nMac: Apple menu &gt; System Settings &gt; General (sidebar) &gt; About &gt; System Report\nLinux: The inxi command will give you most of this on the command line, and hwinfo --short will give you a considerably more detailed printout.\n\nFind out:\n\nWhat processor do you have?\nThis most likely will start with ARM, Intel, AMD, or Apple M1\nHow much RAM do you have? (most likely between 8 and 64 GB)\nHow much hard drive space do you have?\nWhat graphics device do you have? (this might be slightly harder to find – it’s also less critical)\n\n\n\n\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#operating-systems",
    "href": "part-tools/01-computer-basics.html#operating-systems",
    "title": "1  Computer Basics",
    "section": "\n1.2 Operating Systems",
    "text": "1.2 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\nChances are, you can’t imagine doing computing without an operating system of some sort (and they’ve been ubiquitous on computers since the late 1980s). Even some appliances now have enough computing functions to require an operating system and an internet connection! Technically, you can use some Arduino and Raspberry Pi boards without an operating system2, but anything more complicated is almost guaranteed to have some minimal operating system available.\nYou should be able to identify your operating system (OS for short) and follow instructions based on that information. You will typically need to know not only the class of operating system (Windows/Mac/Linux) but also the version (e.g. Windows 11, Mac OSX Sierra, Debian 12, RedHat 7).\n\n\n\n\n\n\nDo It Yourself\n\n\n\nLocate your operating system and version information. When was your system last updated?\nIf your system hasn’t been updated in a while, consider updating it now – system updates may break software you’ll install to work with R and python packages in Chapter 23.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#file-systems",
    "href": "part-tools/01-computer-basics.html#file-systems",
    "title": "1  Computer Basics",
    "section": "\n1.3 File Systems",
    "text": "1.3 File Systems\nFile systems are, unsurprisingly, places you save files. They are modeled after physical file cabinets – individual documents are kept in a hierarchical sequence of folders. Ultimately, a collection of folders is stored on a drive.\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore, and new apps don’t show the user where on the computer their files are stored, forcing users to rely on the search feature instead of understanding file folders and paths. Dan Robitzski provided an interesting discussion of the problem, making the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized closet and dresser (or file cabinet) [1].\n\n\n\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\n1.3.1 Local and Network File Systems\nIt is important to distinguish between two primary types of file systems.\n\nlocal file storage: files are stored on a physical disk contained within the machine you are actively using. A local file might be found at an address like C:/Users/username/\\ Documents/unnamed.txt or /home/users/username/Documents/unnamed.txt or /Users/username/Documents/unnamed.txt.\nnetwork file storage, where files are stored “in the cloud” and you may have a link or a copy on your local machine.\nExamples of network storage are Google Drive, Dropbox, Microsoft OneDrive, and iCloud. Organizations may have privately-hosted network file storage, but these services are still dependent on access to the internet and thus fall under network file storage.\n\nIf you have used primarily mobile devices or Chromebook-style laptops, then you have likely dealt primarily with network storage. When programming, it is essential to know where your files are being stored. You cannot conduct a file search to find your data and code (this is an interactive process). Instead, you will need to keep all of the files you need for a project together in a folder, and then keep track of where the project folder is stored.\nSome operating systems (Windows, Mac OS) prefer to save files in network storage services that may (or may not) be also stored on your physical hard drive. Over time, it has become harder to ensure that you are working on a local machine, but working “in the cloud” can cause odd errors when programming and in particular when working with version control systems4\n\n1.3.2 Allowed File Names\nDifferent operating systems (and file system formats) have different rules for how file names are handled within the file system.\n\nFile Naming Rules. All length limits assume UTF-8 characters – limits may be shorter when using multibyte characters.\n\n\nWindows \n\nMac OSX \n\nLinux \n\n\n\n\nDisallowed Characters\n\n&lt;, &gt;, \", /, \\, |, ?, *\n\n\n:, some programs will restrict use of / . Avoid names that start with . unless the file should be hidden.\nNULL character, / . Can’t name files . or .. . Avoid \\, \", ', *, ;, ?, [, ], (, ), ~, !, $, &lt;, &gt;, #, @, &, |, spaces, tabs, and newlines. Avoid names that start with . unless the file should be hidden.\n\n\nCase Sensitive\nNo. A.jpg is the same as a.JPG\n\n\nIt’s complicated. Act as if it’s case sensitive to be safe.\nYes. A.jpg is different from a.jpg and A.JPG\n\n\n\nName Length\nEntire file path should be &lt;256 characters5.\n(For HFS+ systems) File names &lt; 255 characters. File paths can be longer.\nFile names &lt; 255 characters, File paths &lt; 4096 characters (most file system options, including ext4)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSetup\nAnswer\n\n\n\nA Windows user saves a picture as my-pup.png and references the picture in a file as ![Puppy picture](My-pup.PNG). The picture link works fine when compiled on the Windows machine, but causes an error when the folder is copied to a Linux server and compiled.\nWhat do you think the error might look like?\nWhat went wrong?\nHow can the user ensure that the picture link works on every operating system?\n\n\nOn the Linux machine the user will get a file not found error.\nWindows is a case-insensitive operating system, so my-pup.png and My-pup.PNG will both point to the same file. Thus, when referencing the picture My-pup.PNG, the system finds my-pup.png and concludes they are the same file.\nLinux is a case-sensitive operating system, which means that my-pup.png and My-pup.PNG point to different files. On Linux, the file reference is to My-pup.PNG, and the only file in the directory is my-pup.png, which doesn’t match the specified file name. Thus, Linux will raise a file not found error because the file My-pup.PNG does not exist on the system.\nThe user should reference my-pup.png instead of My-pup.PNG. This file name will work across all major operating systems.\n\n\n\n\n\n\n1.3.3 File Paths\n\n\n\nWhen you write a program, you may have to reference external files - data stored in data.csv, a diagram or picture, or a link to additional documentation.\nTo reference a file, you have to tell the computer where to look – that is, you have to give it a file path. File paths come in two basic types:\n\nglobal file path: Starts at the file system location (e.g. C:\\ or /home or /Users) and describes how to navigate to the file.\nlocal file path: Starts at the program’s current location (the working directory) and navigates to the file from that point.\n\nWhen you work on a project that may need to exist on some other machine, it’s important to use local file paths – the global path will likely not be the same, but you can usually set the local project-specific structure up to be the same across machines.\nIn fact, there’s a very common shortcut that programmers take – they set up a project-specific folder that is self contained. That is, all of the data and code necessary for that project is provided within the folder. Then, the code within the folder can use local paths and will work when the project folder is copied to a new machine.\nTo help with organization, it’s not uncommon to use a project structure like this:\n- main-folder\n    - raw-data\n        - design.csv\n        - observations.csv\n        - other-vars.csv\n    - processed-data\n    - code\n        - 01-read-clean.xxx\n        - 02-analysis.xxx\n        - 03-simulation.xxx\n    - writeup.qmd\n    - README\n    - project-file.xxx\nThe README file contains a basic overview of the project’s contents. Files are added to the processed-data subfolder after code is run. Files in raw-data are set to read-only to prevent the data from being accidentally overwritten. A project-file.xxx file tells the program you’re using (RStudio, VSCode, Positron, etc) what the specific settings are, and also that this directory should be treated as the project root – that is, local file paths will start from this directory. When working on code, we will typically assume that the working directory (where the program looks for files) is main-folder.\n[2] discusses several common layouts used for research projects.\n\n1.3.3.1 Using Paths to Navigate\nFile paths can be a bit tricky to construct, but there are a couple of shorthands that help a lot:\n\n\n. represents the working directory\n\n.. is the directory that is above the working directory in the file system.\nYou enter a folder with / (Linux, Mac) or \\ (Windows)\n(in R, \\ is an escape character. To represent a literal \\, you have to actually type \\\\ is the directory separator)\n\nSo, the path ./dir1/dir2/my-project.Rproj assumes the following file structure:\n. - current directory\n    - dir1 \n        - dir2 \n            - my-project.Rproj\nIt’s a bit more challenging to think about paths that use the .. shorthand. Let’s think about a path like ../../../other-dir/other-data.csv:\n- my-dir\n    - dir1\n        - dir2\n            -my-project.Rproj\n- other-dir\n    - other-data.csv\n\n\n\n\n\n\n\n\nA local path from my-dir to my-project.Rproj needs to go through dir1 and dir2.\n\n\n\n\n\nA local path from my-project.Rproj to other-data.csv requires using the ../ to go to the parent folder.\n\n\n\n\n\n1.3.3.2 Constructing File Paths\nOn Windows, file paths are constructed as follows: C:\\Folder 1\\Folder_2\\file.R. Paths are generally not case sensitive, so you can reference the same file path as c:\\folder 1\\folder_2\\file.R. Usually, paths are encased in \"\" because spaces make interpreting file paths complicated and Windows paths have lots of spaces.\nOn Unix systems, file paths are constructed as follows: /home/user/folder1/folder2/file.R. Paths are case sensitive, so you cannot reach /home/user/folder1/folder2/file.R if you use /home/user/folder1/folder2/file.r. On Unix systems, spaces in file paths must be escaped with \\, so any space character in a terminal should be typed \\ instead.\n\n\n\n\n\n\nIt’s easier to just not use spaces in file paths, which you quickly find out when you’ve had to type paths into the terminal a few times, or if you ever use LaTeX [3].\n\n\n\nThis quickly gets complicated and annoying when working on code that is meant for multiple operating systems. These complexities are why when you’re constructing a file path in R or python, you should use commands like file.path(\"folder1\", \"folder2\", \"file.r\") or os.path.join(\"folder1\", \"folder2\", \"file.py\"), so that your code will work on Windows, Mac, and Linux by default.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#system-paths",
    "href": "part-tools/01-computer-basics.html#system-paths",
    "title": "1  Computer Basics",
    "section": "\n1.4 System Paths",
    "text": "1.4 System Paths\nWhen you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don’t need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu that point to the right location.\nUnfortunately, that isn’t sufficient when you’re programming, because you may need to know where a program is in order to reference that program – for instance, if you need to pop open a browser window as part of your program, you’re (most likely) going to have to tell your computer where that browser executable file lives.\nTo simplify this process, operating systems have what’s known as a “system path” or “user path” - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available.\n\n\n\n\n\n\nError Messages That Indicate Path Problems\n\n\n\nIf you run across an error like this:\n\ncould not locate xxx.exe\nThe system cannot find the path specified\nCommand Not Found\n\nYou might start thinking about whether your system path is set correctly for what you’re trying to do.\n\n\n\n\n\n\n\n\nDemonstration: Path Errors\n\n\n\nLet’s see what path errors look like using different tools you might encounter.\n\n\nR\nPython\nBash\n\n\n\n\ntmp &lt;- read.csv(\"lego_sets.csv\") # Wrong Path\n## Error in file(file, \"rt\"): cannot open the connection\n\ntmp &lt;- read.csv(\"../data/lego_sets.csv\") # Right Path\n\n\n\n\nimport pandas as pd\n\ntmp = pd.read_csv(\"lego_sets.csv\") # Wrong Path\n## FileNotFoundError: [Errno 2] No such file or directory: 'lego_sets.csv'\n\ntmp = pd.read_csv(\"../data/lego_sets.csv\") # Right Path\n\n\n\n\nhead -n5 lego_sets.csv\n## head: cannot open 'lego_sets.csv' for reading: No such file or directory\n\n\n\n\n\n\nIf you want to locate where an executable is found (in this example, we’ll use git), you can run where git on windows, or which git on OSX/Linux.\nSome programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools &gt; Global Options &gt; Git/SVN, you can set the path to git.\n\n\n\n\n\n\nModifying Your System Path\n\n\n\n\n\nHow to set system paths (general)\nOperating-system specific instructions cobbled together from a variety of different sources:\n\n\n On Windows\n\n\n On Mac\n\n\n On Linux\n\n\n\n\n\n\n\n\n\n\n\nShell Commands\n\n\n\n\n\nCheck out Section 41.1.1.2 for some basic shell commands in each operating system that will help you navigate your computer.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "href": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "title": "1  Computer Basics",
    "section": "\n1.5 References",
    "text": "1.5 References\n\n\n\n\n[1] \nD. Robitzski, “Gen z kids apparently don’t understand how file systems work. Futurism,” Sep. 24, 2021. [Online]. Available: https://futurism.com/the-byte/gen-z-kids-file-systems. [Accessed: Jan. 09, 2023]\n\n\n[2] \nS. Picardi, “Project organization,” in Reproducible data science, Utah State University: Github.com, 2024 [Online]. Available: https://ecorepsci.github.io/reproducible-science/project-organization.html. [Accessed: Aug. 11, 2025]\n\n\n[3] \nY. Xie, “Don’t use spaces or underscores in file paths; use dashes instead. Blog,” Mar. 15, 2018. [Online]. Available: https://yihui.org/en/2018/03/space-pain/. [Accessed: Aug. 20, 2025]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#footnotes",
    "href": "part-tools/01-computer-basics.html#footnotes",
    "title": "1  Computer Basics",
    "section": "",
    "text": "How big “big” is changes every couple of years – it used to be several GB circa 2010, and now it’s TB of data.↩︎\nChips and boards used without an operating system are often called “embedded systems”.↩︎\nIf you are using an operating system that is older, know that some of the installation instructions may require modification (but there are likely others online who have attempted something similar, so you can usually Google for how to adjust things when they don’t work).↩︎\nTo disable OneDrive sync for certain windows folders, use this guide. On Mac, see “Turn off Desktop and Documents” to stop iCloud sync of your Desktop and Documents folders (you can still manually copy things into iCloud for backup).↩︎\nLonger paths can be enabled via registry edits if you’re brave/foolish.↩︎",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html",
    "href": "part-tools/02-setting-up-computer.html",
    "title": "2  Setting Up Your Computer",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#objectives",
    "href": "part-tools/02-setting-up-computer.html#objectives",
    "title": "2  Setting Up Your Computer",
    "section": "",
    "text": "Set up RStudio, R, Quarto, and python\nBe able to run demo code in R and python",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#what-are-you-asking-me-to-install",
    "href": "part-tools/02-setting-up-computer.html#what-are-you-asking-me-to-install",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.1 What are you asking me to install?",
    "text": "2.1 What are you asking me to install?\nIt’s a good idea to be skeptical when someone is telling you to install things. 🤨 Here’s a very broad overview of what each of these programs or services does and why I’m asking you to install or sign up for them.\n\nWhat each program does, in general terms\n\n\n\n\n\n\nProgram\nLogo\nPurpose\n\n\n\nR\n\nA statistical programming language built around working with data\n\n\nPython\n\nA general-purpose programming language that is popular for machine learning tasks.\n\n\nRStudio IDE\n\nAn integrated desktop environment created to make it easy to work with R, Python, and other data-science programming tools.\n\n\nQuarto\n\nA document creation system based on pandoc. Quarto allows you to include code, results, and pictures generated from data within a document so that they automatically update when the document is recompiled.\n\n\nGit\n\nA version control system used to track changes to files.\n\n\nGitHub\n\nAn online collaboration platform based on git that makes it easy to back up, share, and collaborate on programming projects.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#why-do-i-need-to-install-this-stuff",
    "href": "part-tools/02-setting-up-computer.html#why-do-i-need-to-install-this-stuff",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.2 Why do I need to install this stuff?",
    "text": "2.2 Why do I need to install this stuff?\n\nR is good for data wrangling, statistical programming, modeling, and data visualization\nPython is good for machine learning, deep learning, image processing, and web scraping\nQuarto allows you to write R and python code, keeping code and documentation in the same document\ngit is helpful for tracking changes to code and data, and for collaborating\n\nWhile some parts of this book are fairly language-agnostic (R and python are both good languages for working with data), this setup is opinionated - the book focuses on a set of programs which are useful for doing statistical programming, including writing reports, keeping track of code, visualizing data, cleaning data, and getting set up for modeling data and producing results.\nIf you’re just trying to learn R, perhaps you don’t need to install python or quarto. If you’re not working with other people, maybe you don’t need to install git. You are welcome to make those decisions for yourself, but if you’re not sure, you might just want to install the whole toolbox - you’ll hopefully learn how to use all of the tools along the way, and it’ll be less confusing later if you already have access to all of the tools and don’t need to go back and get something else when you need it.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#installation-process",
    "href": "part-tools/02-setting-up-computer.html#installation-process",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.3 Installation Process",
    "text": "2.3 Installation Process\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class message board or contact me for help.\n\n\n\n\n\n\n2.3.1 Install R (and associated tools)\n\n\n\n\n\n\n\n Windows\n Mac\n Linux\n\n\n\n\nGet the R installer from CRAN: https://cran.rstudio.com/bin/windows/base/\nInstall the Rtools4 package that matches the R version you installed: https://cran.rstudio.com/bin/windows/Rtools/\nFor example, if you installed R 4.4.3, you should download RTools 4.4. If you installed R 4.0.1, you should install RTools 4.0.\n\n\n\n\nGet the R installer from CRAN: https://cran.rstudio.com/bin/macosx/\nInstall XCode - a set of developer tools - so that you can install R packages more easily. There are two ways to do this:\n\n\nGet XCode from the App store directly (simple, but you may have to do other things later)\nInstall Homebrew and then use homebrew to install XCode.1\n\n\nI personally suggest option b, because Homebrew is used for a lot of different software-development related things, and having it will make life easier later.\n\nInstalling Homebrew + XCode\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install mas # Search the apple store\nmas search xcode # find xcode\nmas install 497799835 # install the program by ID\n\n\n\nIf you’re using a distribution like Debian, Fedora, RedHat, or Ubuntu, or a distribution derived from one of these distributions, follow the instructions below.\nIf you don’t know your distribution (or if you’re working on a derivative distribution, like Mint), you can figure out which set of instructions to follow by executing lsb_release -a in your system terminal.\nIf you’re using Arch or another distribution that isn’t derived from something listed above, google for instructions specific to your distribution - these instructions cover the most common, user-friendly distributions, but if you’ve chosen something more niche, it is assumed you can figure out how to install R. (If that isn’t the case, leave a comment below with your distribution, so I know how best to update this book.)\n\nInstallation Instructions\n\nGet the R installer from CRAN: https://cran.rstudio.com/bin/linux/ - pick your distribution, and follow the distribution-specific instructions from there.\n\nYou could install R using your package manager, but often this version is out of date. The instructions for each version will get you a more up-to-date version that will be easier to work with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Install Python\n\n\n\n\n\nDownload and install the latest version of python 3\nExtra Instructions to Setup Python on  Windows\ncheck the box that asks if you want to add Python to the system path.\nThis will save you a lot of time and frustration. If you didn’t do this, you can follow these instructions to fix the issue (you’ll need to restart your machine).\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\n\n\n\nIf you’re interested in python, you should install Jupyter using the instructions here (I would just do pip3 install jupyterlab)\nWe will not use jupyter much in this book - I prefer quarto - but the python community has decided to distribute code primarily in jupyter notebooks, so having it on your machine may be useful so that you can run other people’s code.\nAdditional instructions for installing Python 3 from Python for Everybody if you have trouble.\n\n\n\n\n\n\n\n\n\n\n2.3.3 Install RStudio\n\n\n\n\n\nDownload and install the latest version of RStudio for your operating system. RStudio is a integrated development environment (IDE) for R, created by Posit. It contains a set of tools designed to make writing R, python, javascript, and other data-related code easier.\n\n\n\n\n\n\n\n\n\n2.3.4 Install quarto\n\n\n\n\n\nDownload and install the latest version of Quarto for your operating system. Quarto is a command-line tool released by Posit that allows you to create documents using R or python, combining code, results, and written text.\nThe following steps may be helpful or required for your class. If you want to be safe, go ahead and complete these steps as well.\n\n\n\n\n\n\n\n\n\n2.3.5 Install git\n\n\n\n\n\n\nInstall git using the instructions here.\n\nConsult the troubleshooting guide if you have issues.\n\n\n\n\n\n\n\n Mac Warning\n\n\n\nWith each OS version upgrade, you may find that git breaks. To fix it, you will have to re-install Mac command line tools. Once you do this, git will start working again. See [1] for more information.\n\n\n\n2.3.5.1 Optional: Install a git client\nInstructions\nI don’t personally use a git client other than RStudio, but you may prefer to have a client that allows you to use a point-and-click interface. Some of the features are built into RStudio, but I’m sure there are other features that aren’t and I just don’t use them frequently.\n\n2.3.5.2 GitHub: Git on the Web\n\n\n\n\n\n\nSet up a GitHub Account Now\n\n\n\nInstructions for setting up a GitHub account.\nBe sure you remember your signup email, username, and password - you will need them later.\n\n\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\n\n\n\n\n\n\nGit and Github: Slightly crude (but memorable) analogy\n\n\n\n\n\nGit is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories.\n\n\n\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.2\n\n2.3.5.3 Introduce yourself to git\nYou need to tell git what your name and email address are, because every “commit” you make will be signed. This needs to be done once on each computer you’re using.\nFollow the instructions here, or run the lines below:\n\n\n\n\n\n\nNote\n\n\n\nThe lines of code below use interactive prompts. Click the copy button in the upper right corner of the box below, and then paste the whole thing into the R console. You will see a line that says “Your full name:” - type your name into the console. Similarly, the next line will ask you for an email address.)\n\n\n\nuser_name &lt;- readline(prompt = \"Your full name: \")\nuser_email &lt;- readline(prompt = \"The address associated w your github account: \")\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = user_name, user.email = user_email, scope = \"user\")\n\n# Tell git to ignore all files that are OS-dependent and don't have useful data.\ngit_vaccinate() \n\n\n2.3.5.4 Set up GitHub Authentication\nThe next step is to tell GitHub how to recognize information from your computer, and to associate that computer with your user. To make this process as smooth as possible, we have to configure either HTTPS or SSH authentication.\nSSH authentication is slightly more secure (but the reasons are very technical) and can be used for additional ID verification, while HTTPS authentication can be used over some networks which block SSH traffic. GitHub’s recommendation has apparently changed several times, but as of 2022, it seems that HTTPS is easiest to configure across platforms [2].\n\n\nSSH\nHTTPS\n\n\n\n\n# Create a ssh key if one doesn't already exist\nif (!file.exists(git2r::ssh_path(\"id_rsa.pub\"))) {\n  # Create an ssh key (with no password - less secure, but simpler)\n  system(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ''\") \n  # Find the ssh-agent that will keep track of the password\n  system(\"eval $(ssh-agent -s)\")\n  # Add the key\n  system(\"ssh-add ~/.ssh/id_rsa\")\n} \n\nThen, in RStudio, go to Tools &gt; Global Options &gt; Git/SVN. View your public key, and copy it to the clipboard.\nThen, proceed to github. Make sure you’re signed in.\n\nClick on your profile pic in upper right corner -&gt; Settings -&gt; SSH and GPG keys.\nClick “New SSH key”.\nPaste your public key in the “Key” box. Give it an informative title. For example, you might use 2025-laptop to record the year and computer.\nClick “Add SSH key”.\n\nYou can double-check that everything worked by opening a terminal and running ssh -T git@github.com, as described in the GitHub SSH Authentication documentation.\n\n\n\nIn the R console, run:\n\n\nusethis::create_github_token()\n\n\nClick “Generate token” in the github window that appears.\nCopy the generated PAT to your clipboard\nIn the R console, run:\n\n\ngitcreds::gitcreds_set()\n\nPaste your PAT in response to the console prompt.\n\n\n\nWhichever option (HTTPS or SSH) you chose will determine which address you should select when cloning a GitHub repository.\n\n\nSSH\nHTTPS\n\n\n\n\n\nWhen you clone a github repository, make sure to select the SSH protocol before copying the web address.\n\n\n\n\n\nWhen you clone a github repository, make sure to select the HTTPS protocol before copying the web address.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.6 Install Document Processing Software\n\n\n\n\n\n\nLaunch R, and type the following commands into the console:\n\n\ninstall.packages(c(\"tinytex\", \"knitr\", \"rmarkdown\", \"quarto\"))\nlibrary(tinytex)\ninstall_tinytex()\n\nIf you have an existing installation of LaTeX, using tinytex or another installer, I highly recommend that you uninstall LaTeX before re-installing the most recent version.\n\nConfigure RStudio to use the tinytex installation of LaTeX:\n\n\nGo to Tools -&gt; Global Options -&gt; Sweave\n\n\n\n\n\n\n\n\n\n\n\nTry it out! Exploring (and configuring) RStudio\n\n\n\nOpen RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console to set up some basic packages:\n\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\"))\n\nRun the following code in the R console to set up Python:\n\ninstall.packages(\"reticulate\")\nreticulate::virtualenv_create(\"stat-python\") # Create a python environment\nreticulate::virtualenv_install(\"stat-python\", c(\"numpy\", \"matplotlib\", \"pandas\"))\n\nConfigure RStudio to use the python virtual environment you created:\n\nGo to Tools -&gt; Global Options -&gt; Python\nClick the Python interpreter Select button\nClick Virtual Environments\nChoose stat-python and hit “Select”\nRestart RStudio when prompted to ensure the setting takes effect.\n\n\nTest out Python:\n\nGo to the Console tab, and click on the R logo. A dropdown should be present with an option to select Python.\nWait for the Python prompt to appear (it will look like this: &gt;&gt;&gt;)\nType 2 + 2, hit enter, and make sure the result is 4.\n\n\nCan you find the text editor?\n\nCreate a new quarto document (File -&gt; New File -&gt; Quarto Document).\nSelect a blank quarto document (bottom of the pop-up window)\nChange the text editor to Source mode (it will default to visual mode). This makes it possible to paste a quarto document without Visual mode trying to “fix” the markup characters.\nPaste in the contents of this document.\nCompile the document (Ctrl/Cmd + Shift + K) and use the Viewer pane to see the result.\n\n\nIf this all worked, you have RStudio, Quarto, R, and Python set up correctly on your machine.\n\n\n\n\n\n\n\n\n\nCustomizing RStudio\n\n\n\n\n\nIf you prefer dark mode (or other color schemes), you can customize RStudio with different themes. Github user max-alletsee maintains a repository of a ton of different RStudio themes, including some that are colorblind friendly or designed for people with different visual processing constraints.\nThere are also keyboard shortcuts and you can set custom shortcuts as well.\nSome people like fonts that use ligatures (multi-block characters). Ligatures can cause e.g. the assignment operator &lt;- to look like \\(\\leftarrow\\) in your text editor. If you like this idea, then you can enable font ligatures by installing Fira Code or another ligature-friendly font and allowing RStudio to use ligatures.\n\n\n\n\n\n\n\n[1] \ndustbuster, “Answer to \"git is not working after macOS update (xcrun: Error: Invalid active developer path (/library/developer/CommandLineTools)\". Stack overflow,” Sep. 26, 2018. [Online]. Available: https://stackoverflow.com/a/52522566/2859168. [Accessed: Jan. 13, 2023]\n\n\n[2] \nk107, “Answer to \"git clone with HTTPS or SSH remote?\". Stack overflow,” Jun. 15, 2022. [Online]. Available: https://stackoverflow.com/a/11041782. [Accessed: Jan. 31, 2025]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#footnotes",
    "href": "part-tools/02-setting-up-computer.html#footnotes",
    "title": "2  Setting Up Your Computer",
    "section": "",
    "text": "If you prefer a different package manager, that’s fine - Homebrew is widely used, but there are other options. Ultimately, you just need to have XCode so that you can compile R packages.↩︎\nYes, I’m aware that this sounds paranoid. It’s been a very rare occasion that I’ve needed to restore something from another backup. You don’t want to take chances. I knew a guy who had to retype his entire masters thesis from the printed out version the night before it was due because he had stored it on a network drive that was decommissioned. You don’t want to be that person.↩︎",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html",
    "href": "part-tools/03-Rstudio-interface.html",
    "title": "3  RStudio’s Interface",
    "section": "",
    "text": "Objectives\nHere, I’ll provide a set of annotated screenshots highlighting many of the features of RStudio’s IDE (Integrated Development Environment) which will be useful as you work through this textbook. You shouldn’t expect to remember all of this right now, but I’m providing it in the hopes that you’ll be able to come back to it when future instructions like “Click the render button at the top of the text editor window” don’t make sense or match what you’re seeing.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#objectives",
    "href": "part-tools/03-Rstudio-interface.html#objectives",
    "title": "3  RStudio’s Interface",
    "section": "",
    "text": "Locate different panes of RStudio\nUse cues such as buttons and icons to identify what type of file is open and what language is being interpreted",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#overview",
    "href": "part-tools/03-Rstudio-interface.html#overview",
    "title": "3  RStudio’s Interface",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n An RStudio window is by default divided into 4 panes, each of which may contain several tabs. You can reconfigure the locations of these tabs based on your preferences by selecting the toolbar button with 4 squares (just left of the Addins dropdown menu).\nIn the default configuration, - The top left is the editor pane, where you will write code and other content. - The bottom left is the console pane, which contains your R/python interactive consoles as well as a system terminal and location for checking the status of background jobs. - The top right contains the environment and history tabs (among others) - The top left contains the files and help tabs (among others)\nYou do not need to know what all of these tabs do right now. For the moment, it’s enough to get a sense of the basics - where to write code (top left), where to look for results (bottom left), where to get help (bottom right), and where to monitor what R/python are doing (top right).",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-editorfile-pane-top-left",
    "href": "part-tools/03-Rstudio-interface.html#the-editorfile-pane-top-left",
    "title": "3  RStudio’s Interface",
    "section": "3.2 The Editor/File Pane (Top Left)",
    "text": "3.2 The Editor/File Pane (Top Left)\nThe buttons and layout within this pane change based on the type of file you have open.\n\nR scriptPython scriptQuarto markdownText file\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a python file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a quarto markdown file is open, there is a render button at the top which allows you to compile the file to see its “pretty”, non-markup form. In the same toolbar, there are buttons to add a code chunk as well as to run a selcted line of code or chunk of code. You can toggle between source (shown) and visual mode to see a more word-like rendering of the quarto markdown file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the text file indicates the file type. When a text file (or other unknown file extension) is open, there are very few buttons in the editor window. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-console-pane-bottom-left",
    "href": "part-tools/03-Rstudio-interface.html#the-console-pane-bottom-left",
    "title": "3  RStudio’s Interface",
    "section": "3.3 The Console Pane (Bottom Left)",
    "text": "3.3 The Console Pane (Bottom Left)\nLet’s compare what the console pane looks like when we run a line of R code compared to a line of python code. The differences will help you figure out whether you need to exit out of Python to run R code and may help you debug some errors.\n\nPythonR\n\n\n\n\n\nWhen running python code from a script file, the console will show you that you are running in python by the logo at the top of the console pane. You will initially see lines indicating that you’re running R, and then you’ll see the lines highlighted in red which show R running the code in python – this is what converts the console from R to python. The command you ran will appear after &gt;&gt;&gt;, and the results will appear immediately below. A &gt;&gt;&gt; waits for a new command - to get back to R, you will need to type exit (as instructed by the red text). In the environment pane, you can see another indicator that you’re viewing the python environment, with an object named ‘r’ that will allow you to move data back and forth between the two languages if you want to do so.\n\n\n\n\n\n\n\nWhen running R code from a script file, the console will show you that you are running in R by the logo at the top of the console pane. You will initially see lines indicating that you’re running R (they’re missing here because this isn’t the first command I ran in this session). The command you ran will appear after &gt;, and the results will appear immediately below, with boxed numbers in front of each sequential line. A &gt; waits for a new command . In the environment pane, you may see a new value pop up named .Last.value - this is part of user settings and you can stop it from appearing if you want to.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-top-right-pane",
    "href": "part-tools/03-Rstudio-interface.html#the-top-right-pane",
    "title": "3  RStudio’s Interface",
    "section": "3.4 The Top Right Pane",
    "text": "3.4 The Top Right Pane\nThis pane contains a set of tabs that change based on your project and what you have enabled. If you’re using git with an Rstudio project, then this tab will show your git repository. If you’re working with an Rstudio project that has multiple files, such as a book or a website, then the pane will also have a Build tab that will build all of your project files.\nFor now, though, let’s assume you’re not in an Rstudio project and you just want to know what the heck an Environment pane (or any of the other tabs in here by default) is. We’re going to focus on two of the tabs that are the most relevant to you right now: Environment, and History.\n\n3.4.1 Environment tab\nThe Environment tab shows you any objects which are defined in memory in whatever language you’re currently using (as long as it’s R or python). You’ll see headers like “Data”, “Values”, and “Functions” within this table, and two columns - the name of the thing, and the value of the thing (if it’s a complicated object, you’ll see what type of object it is and possibly how long it is).\n\n\n\nThe environment tab shows you all of the objects in memory that the language you’re working in knows about.\n\n\nIf you’re working in both R and python, you can toggle which language’s environment you’re looking at using the language drop down button on the far left side.\n\n\n3.4.2 History tab\nAnother useful tab in this pane is the History tab, which shows you a running list of every command you’ve ever run. While I strongly encourage you to write your code in a text file in the editor pane, sometimes you deleted a line of code accidentally and want to get it back… and the history tab has you covered (unless you’ve cleared the history out).\n\n\n\nThe history tab shows you a list of all commands you’ve run and allows you to send them to the console or to source (the text editor).",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-bottom-right-pane",
    "href": "part-tools/03-Rstudio-interface.html#the-bottom-right-pane",
    "title": "3  RStudio’s Interface",
    "section": "3.5 The Bottom Right Pane",
    "text": "3.5 The Bottom Right Pane\nThis pane also contains a mishmash of tabs that have various uses. Here, we’ll focus on 3: Files, Packages, and Help. But first, to quickly summarize the remaining tabs, the Plots tab shows any plots you’ve generated (which we haven’t done yet), and the Viewer/Presentation tabs show you compiled documents (markdown), interactive graphics, and presentations.\n\n3.5.1 Files tab\n\n\n\nThe files tab shows you the files in your current working directory (by default), though you can navigate through it and find other files as necessary. If you want to return to your working directory, there’s a button for that in the “More” menu. One of the most important pieces of information in this pane is your path - you can construct the file path by using ~/ for home, and then for each folder, adding a slash between. The path to the folder we’re looking at here is thus ~/Projects/Class/stat-computing-r-python/.\n\n\n\n\n3.5.2 Packages tab\nThe packages tab isn’t quite relevant yet, but it will be soon. R and python both work off of packages - extensions to the default language that make it easier to accomplish certain tasks, like reading data from Excel files or drawing pretty charts. This tab shows all of the R packages you have installed on your machine, and which ones are currently loaded.\n\n\n\nYou can get important information from the packages tab, like what packages are loaded, easy access to documentation for each package, and what version of the package is installed.\n\n\nUnfortunately, the packages tab doesn’t cover python packages yet.\n\n\n3.5.3 Help tab\nThe help tab is a wonderful way to get help with how to use an R or python function.\n\n\n\nThe help tab makes it easy to get access to function documentation within Rstudio, so you don’t have to switch windows.\n\n\nBy default, you can search for an R function name in the search window, and documentation for matching functions will appear in the main part of the pane. To get help with python functions, you need to (in the python console) use ?&lt;function name, so I would type in at the &gt;&gt;&gt; prompt ?print to get the equivalent python help file.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html",
    "href": "part-tools/04-scripts-notebooks.html",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "",
    "text": "4.1 Objectives\nA better title for this chapter might be “where to put code” or “how to talk to R and Python”.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#objectives",
    "href": "part-tools/04-scripts-notebooks.html#objectives",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "",
    "text": "Understand the different ways you can interact with a programming language\nIdentify which interface (terminal, interactive, script, notebook) and language are being used based on the appearance of the interface\nSelect the appropriate way of interacting with a computer for a given task given considerations such as target audience, human intervention, and need to repeat the analysis.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "href": "part-tools/04-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "\n4.2 A Short History of Talking to Computers",
    "text": "4.2 A Short History of Talking to Computers\nThe fundamental goal of this chapter is to learn how to talk to R and Python. Before we get there, though, it’s helpful to learn a bit about how we talk to computers more generally. This will not only give you context for R and Python - it will also help you identify things that need to be done before you can complete a task.\n\n\n\n\n\n\n4.2.1 In the beginning…\n\n\n\n\n\nIn the very beginning, people told computers what to do using punch cards [1]. Actually, punch cards somewhat predate computers - they were used to tabulate census results long before modern machines that we would consider actual computers.\nPunch cards required that you have every step of your program and data planned out in advance - you’d submit your punch cards to the computer, and then come back 24-72 hours later to find out you’d gotten two cards out of order1. Dropping a tray of punch cards was … problematic.\n![Punch cards for a computer program. The red diagonal line on the top is a way to ensure the program is properly sorted. Image from Wikimedia, By ArnoldReinhold - Own work, CC BY-SA 3.0, ]](../images/tools/punch-cards-wikimedia.jpg)\nThankfully, we’re mostly free of the days where being a bit clumsy could erase a semester of hard work. As things grew more evolved, engineers developed visual displays (monitors). This enabled a new mode of interacting with computers: directly typing commands in, and receiving a response as soon as the task was completed. The primary way this interactivity happened, at least at first, was using interfaces called terminals or command prompts.\n\n\n\n\n4.2.2 Terminals\nA terminal is a text interface where you give instructions to a computer that tell the computer what to do. A command prompt is another, similar, term (often used interchangeably) which refers to the character used to indicate that the computer is waiting for a command. On different systems, this character might be &gt;&gt;&gt;, &gt;, $, %, or something you can customize yourself. Most systems allow you to tinker with the configuration of the terminal - what is shown on your machine may not be similar to what is shown in the images below, but the goal of looking at multiple prompts is to help you recognize the common components across operating systems enough to make sense of your machine.\n\n\nWhen I first learned how to use a computer, circa 1992, terminal interfaces (DOS) were the primary way you used a computer. My parents set up a custom menu that would allow me to launch computer games by typing different numbers. Windows 3.1 was an absolute revelation – we had to buy a mouse, and you could actually move things around a screen! And the screen had more than 4 colors 🌈!\nMost modern computer users don’t engage with terminal interfaces very often (or at all). As you learn to program, you will become more comfortable with the terminal for completing basic tasks like moving files around, running programs, and obtaining diagnostic information. Usually, you pick up this information over time and when you’re frustrated trying to do a task some other way.\nFor now, it’s enough to know what a terminal is and to recognize it when you see it.\nLet’s look at a terminal window for each operating system for a minute and examine the important parts.\n\n\n Windows\n Mac\n Linux\n\n\n\nThe default terminal on Windows is cmd.exe.\n\n\nWindows terminal. The location on the computer is shown first, and the prompt character is &gt;.\n\nMany people dislike cmd.exe and prefer to install PowerShell, which is a more fully featured terminal program. \n\n\n\n\nIn a Mac terminal, you can see the username, computer name, location, and prompt character ‘%’. (base) in this image is printed because this user has a conda environment loaded – this is something that may happen depending on how you install python. Macs use Zsh, which is a command line program (Z-shell) that interprets user input.\n\n\n\n\n\nThis is an example of a KDE-based terminal on Linux. The Username, computer name, location, and prompt character ‘$’ are all present. This linux terminal uses BASH, which is a slightly different command line program than the Z-shell used on Macs. In Linux, it is common to choose the terminal program you most prefer, and there are many options.\n\n\n\n\nSystem terminals have their own languages, and they’re not consistent across operating systems. By default, terminals come with a set of commands described as  Batch (Windows),  Zsh (Mac), or  Bash (default in most Linux systems).\nThe most important things to know how to do in a system terminal are:\n\nLaunch a program like python or R\nChange your location/working directory (dir &lt;path\\to\\folder&gt; on Windows, cd &lt;path/to/folder&gt; on Linux/Mac)\n\nThere are lots of other things you can do, but those are the two big ones.\nLet’s try launching R and python from a system terminal, and then see that we can get the same windows within RStudio. Keeping all of the windows you need for programming in one place is one of the most important features of an integrated development environment (IDE) like RStudio.\n\n\n\n\n\n\n4.2.3 Try it out! Interactive Command Prompts in R and Python\n\n\n\n\n\nR - System Terminal\nPython - System Terminal\nRStudio - R\nRStudio - Python\n\n\n\nOpen your system terminal and type R. Hit enter. You can issue commands directly to R by typing something in at the &gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nLaunching R from the system terminal\n\n\n\nOpen your system terminal and type python (on some systems, like mine, you may have to type python3 instead). Hit enter. You can issue commands directly to python by typing something in at the &gt;&gt;&gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nLaunching python from the system terminal\n\n\n\nOpen RStudio and navigate to the Console tab. You can issue commands directly to R by typing something in at the &gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nThe Console tab is the left-most tab in RStudio - you can see that R’s welcome message shows up first, along with version information. The last component visible is the &gt;, which indicates that R is waiting for you to tell it to do something. Hitting enter will submit the command to calculate 2+2. Hopefully, you get 4.\n\n\n\nOpen RStudio and navigate to the Terminal tab. This is a “system terminal” - that is, where you tell the computer what to do.\nWe tell the computer we want to work in python by typing in python3 or python (depending on how your computer is set up). This will launch an interactive python session (ipython).\nYou should get a prompt that looks like this: &gt;&gt;&gt;\n Type in 2+2 and hit enter.\n\n\n\n\n\nUsing the console interactively can be useful for quick things, like performing a simple arithmetic calculation, but imagine doing a complicated analysis that requires typing in 10, 15, or 20 commands! You’d have to re-type the same commands any time you wanted to bring up the results, which very quickly gets tedious.\n\n4.2.4 Scripts\nAt one point, I wanted to keep a record of the temperature around my house so that I could examine how my heating bill changed with the temperature and determine if an upgrade to a heat pump was cost effective. I decided to record the outside temperature every 6 hours, writing that information to a file along with the date and time. This only required a few commands, but I wasn’t willing to commit to being at the computer every 6 hours for the rest of my life, and I wanted the data to be complete.\nEnter scripts.\nIf you need to repeat the same analysis, or even just remember what commands you used, typing each command in each time is not ideal. A script is a text file which records a series of commands so that they can be run together.\nInteractive mode is useful for quick, one-off analyses, but if you need to repeat an analysis (or remember what you did), interactive mode is just awful. Once you close the program, the commands (and results) are gone. This is particularly inconvenient when you need to run the same task multiple times.\nTo somewhat address this issue, most computing languages allow you to provide a sequence of commands in a text file, or a script. In many languages, scripts are intended to run on their own, from start to finish. We often call this executing a script, and this is typically done from a terminal prompt.\n\n\n\n\n\n\n4.2.4.1 Try it out! Scripts and Terminals\n\n\n\nLet’s take a minute and see how someone might call or run a script from the terminal.\n\nDownload scripts.zip and unzip the file.\nOpen a system terminal in the directory where you unzipped the files.\nFollow the directions below exactly to ensure that you have the terminal open in the correct location.\n\n\n\n Windows\n Mac\n Linux\n\n\n\nOpen the folder. Type cmd into the location bar at the top of the window and hit enter. The command prompt will open in the desired location.\n\n\nOpen a finder window and navigate to the folder you want to use. If you don’t have a path bar at the bottom of the finder window, choose View &gt; Show Path Bar. Control-click the folder in the path bar and choose Open in Terminal.\n\n\nOpen the folder in your file browser. Select the path to the folder in the path bar and copy it to the clipboard. Launch a terminal and type cd, and then paste the copied path. Hit enter. (There may be more efficient ways to do this, but these instructions work for most window managers).\n\n\n\n\nNow, let’s try out running a script from the terminal in R and Python!\n\n\n\nR\nPython\n\n\n\nThis assumes that the R binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type Rscript words.R dickens-oliver-twist.txt\nYou should get some output that looks like this:\nuser@computer:~/scripts$ Rscript words.R dickens-oliver-twist.txt \ntext\n the  and        to   of    a  his   in   he  was \n8854 4902 4558 3767 3763 3569 2272 2224 1931 1684\n\n\nThis assumes that the python binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type python3 words.py and hit Enter. You will be prompted for the file name. Enter dickens-oliver-twist.txt and hit Enter again.\nYou should get some output that looks like this:\nuser@computer:~/scripts$ python3 words.py \nEnter file:dickens-oliver-twist.txt\nthe 8854\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information about how to use system terminals, see Section 41.1.1.\n\n\nScripts, and compiled programs generated from scripts, are responsible for much of what you interact with on a computer or cell phone day-to-day. When the goal is to process a file or complete a task in exactly the same way each time, a script is the right choice for the job.\n\n4.2.4.2 Using Scripts Interactively\nScripts are also used interactively in some languages, like R and python, when doing data analysis. Because data analysis depends on the data, and data isn’t ever exactly the same or 100% what you expect it to be, people programming with data often use scripts and run the code in the script interactively. About 70% of my day-to-day computing is done using R or python scripts that are run interactively.\nWhen a script is ready for “production” - that is, ready to be used without interactive human supervision, data scientists may use it that way.\nBut many data scientists and statisticians never move beyond using scripts interactively – and that is ok!\nWhen you look for help online, though, it’s important to be able to distinguish between help that assumes you’re calling or executing a script (running it from the command line) and help that assumes you’re working with a script and using it interactively.\n\n4.2.4.3 Sourcing Scripts\nTo make this even more confusing, it’s possible to run an entire script, or a chunk of a script, within RStudio. When we talk about running an entire script file within RStudio, we will often say we’re sourcing the file. This is because in R, to include a file of commands within another script, you run the command source(\"path/to/file.R\").\nThe difference between sourcing, running, and executing a script is fairly nuanced and the vocabulary is often used interchangeably, which doesn’t help you as you’re learning!\n\n\n\n\n\n\n4.2.4.4 Try it out! Sourcing a Script\n\n\n\nIf you haven’t already, download scripts.zip and unzip the file.\nOpen RStudio and use RStudio to complete the following tasks.\n\n\nR\nPython\n\n\n\n\nUse RStudio to open the words-noinput.R file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"&lt;paste path here&gt;\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.R file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? Why do you think that is?\nClick on the last line of the file and hit Run (or Ctrl/Cmd + Enter). Do you get the output now?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What could you learn from doing this?\n\n\n\n\nUse RStudio or your preferred python editor to open the words-noinput.py file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"&lt;paste path here&gt;\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.py file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? What changes?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What do you learn from doing this?\n\n\n\n\n\n\nUsing scripts interactively allows us to see what is happening in the code step-by-step, and to examine the results during the program’s evaluation. This can be beneficial when applying a script to a new dataset, because it allows us to change things on the fly while still keeping the same basic order of operations.\nYou can run single lines of code within a script file by clicking on the line and using the “Run” button in RStudio, or by typing Ctrl/Cmd + Enter, which will run the selected line(s) of code in the Console.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#writing-code-for-people",
    "href": "part-tools/04-scripts-notebooks.html#writing-code-for-people",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "\n4.3 Writing Code for People",
    "text": "4.3 Writing Code for People\nOne problem with scripts and interactive modes of using programming languages is that we’re spending most of our time writing code for computers to read – which doesn’t necessarily imply that our code is easy for humans to read. As you become more proficient at programming, you will realize quickly that writing readable code is more challenging than writing working code. It is hard to remember what a specific block of code did when you come back to it months after you wrote it!\nThere are two solutions to this problem, and I encourage you to make liberal use of both of them (together).\n\n4.3.1 Code Comments\nA comment is a part of computer code which is intended only for people to read. It is not evaluated or run by the computing language.\nTo “comment out” a single line of code in R or python, put a # (pound sign/hashtag) just before the part of the code you do not want to be evaluated. This works in both R and Python. Other languages also have so-called inline comments, but may use a different character to indicate that something is a comment. // Comment text and &lt;!-- Comment text --&gt; are in-line comments for JavaScript and HTML, respectively.\n\n4.3.1.1 Adding Comments to Code\n\n\nR\nPython\n\n\n\n\n2 + 2 + 3\n\n[1] 7\n\n2 + 2 # + 3\n\n[1] 4\n\n# This line is entirely commented out\n\n\n\n\n2 + 2 + 3\n\n7\n\n2 + 2 # + 3\n\n4\n\n# This line is entirely commented out\n\n\n\n\nMany computing languages, such as Java, C/C++, and JavaScript have mechanisms to comment out an entire paragraph.\nNeither R nor Python has so-called “block comments” - instead, you can use keyboard shortcuts in RStudio to comment out an entire chunk of code (or text) using Ctrl/Cmd-Shift-C.\n\n4.3.2 Literate Programming - Notebooks and more!\nWhile code comments add human-readable text to code, scripts with comments are still primarily formatted for the computer’s convenience. However, most of the time spent on any given document is spent by people, not by computers. Some groups write parallel documents - code, user manuals, internal wikis, tutorials, etc. which explain the purpose of code and how to use it, but this can get clumsy over time, and requires updating multiple documents (sometimes in multiple places), which can lead to the documentation getting out-of-sync from the code.\nTo solve this problem, Donald Knuth invented the concept of literate programming: interspersing text and code in the same document using structured text to indicate which lines are code and which lines are intended for human consumption.\nThis textbook is written using a literate format - quarto markdown - which allows me to include code chunks in R, python, and other languages, alongside the text, pictures, and other formatting necessary to create a textbook.\nOne major side effect of literate programming is that it is easy to include the results of a block of code inside the primary document. This means that you can generate plots, tables, and other information using code, and include them into a document, without having to copy and paste or insert figures within the text. While this advantage may not seem worth it to you at this point, once you have a ton of plots in a document and the data changes just slightly, the advantage becomes crystal clear.\nWriting documents using literate programming saved me so much time in as an industry data scientist that I really only had about 8 hours of work in a week - I’d automated the rest away by building re-usable reports that updated when new data arrived.\n\n4.3.2.1 Quarto\nOne type of literate programming document is a quarto markdown document.\nWe will use quarto markdown documents for most of the components of this class because they allow you to answer assignment questions, write reports with figures and tables generated from data, and provide code all in the same file.\nWhile literate documents aren’t ideal for jobs where a computer is doing things unobserved (such as pulling data from a web page every hour), they are extremely useful in situations where it is desireable to have both code and an explanation of what the code is doing and what the results of that code are in the same document.\n\n\n\n\n\n\n4.3.2.2 Try It Out! Words in quarto\n\n\n\nIn the scripts.zip file you downloaded earlier, there is a words.qmd file as well as a words.html file.\nOpen words.html in your browser, and open words.qmd in RStudio. words.html was generated from words.qmd.\n\nRead through words.html and find the corresponding sections of words.qmd.\n\n\n\nQuarto text (left) and rendered HTML (right)\n\n\nIn RStudio, click the “Render” button. The HTML file should appear at the bottom left in the “Viewer” tab.\n\n\n\nThe render button is located at the top of the text editor/script pane (usually, the top left panel).\n\nCongratulations, you’ve just compiled your first Quarto document!\n\n\n\n\n\n\n\n\n4.3.2.3 Try It Out! Quarto Markdown\n\n\n\nIn RStudio, create a new quarto markdown document: File &gt; New File &gt; Quarto Document. Give your document a title and an author, and select HTML as the output.\nCopy the following text into your document and hit the “Render” button at the top of the file.\nThis defines an R code chunk. The results will be included in the compiled HTML file.\n\n```{r}\n2 + 2 \n```\n\nThis defines a python code chunk. The results will be included in the compiled HTML file.\n\n```{python}\n2 + 2\n```\n\n# This is a header\n\n## This is a subheader\n\nI can add paragraphs of text, as well as other structured text such as lists:\n\n1. First thing\n2. Second thing\n  - nested list\n  - nested list item 2\n3. Third thing\n\nI can even include images and [links](https://www.oldest.org/entertainment/memes/)\n\n![Godwin's law is almost as old as the internet itself.](https://www.oldest.org/wp-content/uploads/2017/10/Godwins-Law.jpg)\n\n\nMarkdown is a format designed to be readable and to allow document creators to focus on content rather than style.\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nYou can read more about pandoc markdown (and quarto markdown, which is a specific type of pandoc markdown) here [2].\nMarkdown documents are compiled into their final form (usually, HTML, PDF, Docx) in multiple stages:\n\nAll code chunks are run and the results are saved and inserted into the markdown document.\nRmd/qmd -&gt; md\nThe markdown document is converted into its final format using pandoc, a program that is designed to ensure you can generate almost any document format. This may involve conversion to an intermediate file (e.g. .tex files for PDF documents).\n\nAn error in your code will likely cause a failure at stage 1 of the process. An error in the formatting of your document, or missing pictures, and miscellaneous other problems may cause errors in stage 2.\n\n\n\n\n\n\nHistory\n\n\n\nQuarto markdown is the newest version of a long history of literate document writing in R. A previous version, Rmarkdown, had to be compiled using R; quarto can be compiled using R or python or the terminal directly.\nPrior to Rmarkdown, the R community used knitr and Sweave to integrate R code with LaTeX documents (another type of markup document that has a steep learning curve and is harder to read).\n\n\n\n4.3.2.4 Jupyter\nWhere quarto comes primarily out of the R community and those who are agnostic whether R or Python is preferable for data science related computing, Jupyter is an essentially equivalent notebook that comes from the python side of the world.\nQuarto supports using the jupyter engine for chunk compilation, but jupyter notebooks have some (rather technical) features that make them less desirable for an introductory computing class [3]. As a result, this book makes an opinionated decision to prefer quarto over jupyter.\n\n\n\n\n\n\n4.3.2.5 Learn More about Notebooks\n\n\n\n\n\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\n\nYihui Xie is the person responsible for knitr and Rmarkdown and was involved in the development of quarto.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#part-tools-03-refs",
    "href": "part-tools/04-scripts-notebooks.html#part-tools-03-refs",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "\n4.4 References",
    "text": "4.4 References\n\n\n\n\n[1] \n\n“Punched card input/output.” Jan. 08, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Punched_card_input/output&oldid=1132250858\n\n\n\n[2] \nPosit PBC, “Quarto - markdown basics,” 2023. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html. [Accessed: Jan. 09, 2023]\n\n\n[3] \nY. Xie, “The first notebook war,” Sep. 10, 2018. [Online]. Available: https://yihui.org/en/2018/09/notebook-war/. [Accessed: Jan. 09, 2023]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#footnotes",
    "href": "part-tools/04-scripts-notebooks.html#footnotes",
    "title": "4  Scripts and Notebooks and Terminals",
    "section": "",
    "text": "The cards command in SAS that precedes data input is a relic of this era. You can also use datalines now, but cards requires less typing so many people still use it.↩︎",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scripts and Notebooks and Terminals</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html",
    "href": "part-tools/05-git-and-github.html",
    "title": "5  Version Control with Git",
    "section": "",
    "text": "Objectives\nThere is an entire textbook on how to use git and GitHub with R, Happy Git and Github for the UseR [1]. This chapter will liberally use chunks of that textbook, and rather than reproduce them here, I will simply link to the relevant sections.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#objectives",
    "href": "part-tools/05-git-and-github.html#objectives",
    "title": "5  Version Control with Git",
    "section": "",
    "text": "Install git\nCreate a github account\nUnderstand why version control is useful and what problems it can solve\nUnderstand the distinction between git and github, and what each is used for\nUse version control to track changes to a document (git add, commit, push, pull)",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#what-is-version-control",
    "href": "part-tools/05-git-and-github.html#what-is-version-control",
    "title": "5  Version Control with Git",
    "section": "\n5.1 What is Version Control ?",
    "text": "5.1 What is Version Control ?\n\n\n\n\n\n\nNote\n\n\n\nMost of this section is either heavily inspired by Happy Git and Github for the UseR [1] or directly links to that book. There’s no sense trying to repeat something that’s pretty close to perfect.\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is a good solution to the file naming problem. Image Source “Piled Higher and Deeper” by Jorge Cham www.phdcomics.com\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful, because the entire version history is (easily1) retrievable2.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R or _production.py to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this book is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with a service like GitHub (or self-hosted options, like GitLab or Gogs) - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but services like GitHub which enable collaboration are indispensable for modern statistical computing and open-source software development.\n\n5.1.1 Git Basics\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything. Image by Randall Munroe (XKCD) CC-A-NC-2.5.\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out (if you need to). You also don’t have to rename files with different version numbers - you can confidently save over your old files, so long as you remember to commit frequently. There is even a way to “tag” certain commits with versions, so that you can keep track of which version of the paper was e.g. submitted to the journal, and can revisit that when you make revisions to show what revisions were made.\n\n\n\n\n\n\nEssential Reading: Git\n\n\n\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, if you haven’t already.\n\n\nNow that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "href": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "title": "5  Version Control with Git",
    "section": "\n5.2 Using Version Control (with RStudio)",
    "text": "5.2 Using Version Control (with RStudio)\nThe first skill you need to practice is using version control. By using version control from the very beginning, you will learn better habits for programming, but you’ll also get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\n\n\n\n\n\n\n\n\n\n\n\nSo, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is based off of a relatively high-level understanding of git - I do not have any idea how it works under the hood, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion.\n\n\n\n\n\n\nMagic?\n\n\n\nThe MOST IMPORTANT thing to know about git, other than what it does, is that most people who use it have no idea how it works (and that’s ok)! So if this all seems like arcane magic to you, you’re in good company.\n\n\n\n5.2.1 Introduce yourself to git and Authenticate\nMake sure you’ve completed the steps in Section 2.3.5.2 before you proceed.\n\n5.2.2 Create a Repository\nRepositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. If you combine a git repository with an RStudio project, you get a very powerful combination that will make your life much easier, allowing you to focus on writing code instead of figuring out where all of your files are for each different project you start.\nTo create a repository, you can start with your local computer first, or you can start with the online repository first. Both methods are relatively simple, but the options you choose depend on which method you’re using, so be careful to pick one approach (and remember which one you picked!) for each new project.\n\n\nLocal repository first\nGitHub repository first\n\n\n\nLet’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it.\nIf you want, you can use the following R code to set this up:\n\ndir &lt;- \"./hello-world-1\"\nif (!dir.exists(dir)) {\n  dir.create(dir)\n}\nfile &lt;- file.path(dir, \"README\")\nif (!file.exists(file)) {\n  writeLines(\"hello world\", con = file)\n}\n\nTo create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in\ngit init\nAlternately, if you prefer a GUI (graphical user interface) approach, that will work too:\n\nOpen Rstudio\nProject (upper right corner) -&gt; New Project -&gt; Existing Directory. Navigate to the directory.\n(In your new project) Tools -&gt; Project options -&gt; Git/SVN -&gt; select git from the dropdown, initialize new repository. RStudio will need to restart.\nNavigate to your new Git tab on the top right.\n\n\n\n\n\nThe next step is to add our file to the repository.\nUsing the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\".\nUsing the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button.\n\n\n\n\nThe final step is to create a corresponding repository on GitHub.\n\nNavigate to your GitHub profile and make sure you’re logged in.\nCreate a new repository using the “New” button.\nName your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, this will quickly become much harder).\n\nYou’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go.\n\n\n\n\n\n\nTip\n\n\n\nRemember to use the method (HTTPS/SSH) that matches the method you set up for authentication.\n\n\n\n\n\n\n\n\nIn the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally).\nGUI method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage (remember to use the correct protocol - HTTPS or SSH - depending on the authentication method you set up earlier)\nOpen RStudio -&gt; Project -&gt; New Project -&gt; From version control. Paste your repository URL into the box. Hit enter.\nMake a change to the README file\nClick commit, then push your changes\nCheck that the remote repository (Github) updated\n\n\n\n\n\nCommand line method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nNavigate to the location you want your repository to live on your machine.\nClone the repository by using the git shell or terminal: git clone &lt;your repo url here&gt;. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git\n\nMake a change to your README file and save the change\nCommit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking).\nPush your changes to the remote (GitHub) repository and check that the repo has updated: git push\n\n\n\n\n\n\n\n\n\n\n5.2.3 Adding files\ngit add tells git that you want it to track a particular file.\n\n\ngit add diagram: add tells git to add the file to the index of files git monitors.\n\nYou don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about”, and what it thinks belongs in each commit.\nIf you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (approximately) equivalent to clicking the check box3.\n\n5.2.3.1 What files should I add to git?\nGit is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB4. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something GitHub provides for free.\nYou should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions5.\nYou should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good. This is why I recommend that you run usethis::git_vaccinate(), which tells git to ignore these files for every repository on a machine.\nWhile you are learning, you should only add and commit files which you created manually and consciously want to track. You do not need to commit both a quarto .qmd file and the .html file it generates – keeping only the quarto file is enough. You do need to make sure to commit any pictures or files referenced in the files you create, though (as long as they’re relatively small in size and not confidential), because otherwise, your project won’t be self-contained.\n\n5.2.4 Staging your changes\nIn RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, the shell command git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file.\nMore formally, staging is saying “I’d like these changes to be added to the current version, I think”. Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out.\nIndividually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do).\n\n5.2.5 Committing your changes\nA git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache.\n\n\nHere, we commit the red line as a change to our file.\n\nIn general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things:\n\nYou’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing)\nWhen you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred.\n\n5.2.6 Pushing and Pulling\nWhen you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document.\n\n\n\n\n\n\nCaution\n\n\n\nIf you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document.\nFor this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible. In both quarto and LaTeX, a single line break isn’t seen as a new paragraph, so this convention doesn’t affect the rendered document at all, and it makes dealing with version control much easier.\n\n\nPulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up).\nPushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine.\n\n\n\n\n\n\ngit push copies the version of the project on your computer to GitHub\n\n\n\n\n\ngit pull copies the version of the project on GitHub to your computer\n\n\n\n\n\nFigure 5.1: Git push and git pull are used to sync your computer with the remote repository (usually hosted on GitHub)\n\n\nIn general, your workflow will be\n\nClone the project or create a new repository\nMake some changes\nStage the changes with git add\nCommit the changes with git commit\nPull any changes from the remote repository\nResolve any merge conflicts\nPush the changes (and merged files) with git push\n\nIf you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#references",
    "href": "part-tools/05-git-and-github.html#references",
    "title": "5  Version Control with Git",
    "section": "\n5.3 References",
    "text": "5.3 References\n\n\n\n\n[1] \nJ. Bryan, J. Hester, and {The Stat 545 TAs}, Happy git and GitHub for the useR. 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 09, 2022]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#footnotes",
    "href": "part-tools/05-git-and-github.html#footnotes",
    "title": "5  Version Control with Git",
    "section": "",
    "text": "relatively speaking↩︎\nWith exceptions – there are ways to suppress the ability to see every commit ever made to a git repository using tools like git squash, and these tools are useful in cases where you want to simplify the repository’s structure.↩︎\nTechnically, the check box is referred to as ‘staging’ your files, however, to accomplish the same thing at the command line, I usually use git add. From a user-level perspective, it’s equivalent, though I’m sure there’s probably a difference somewhere under the hood.↩︎\nYes, I’m seriously pushing it with this book; several of the datasets are ~30 MB↩︎\nThere are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository↩︎",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html",
    "href": "part-tools/06-documents.html",
    "title": "6  Reproducibility and Professional Communication",
    "section": "",
    "text": "Objectives\nThis chapter will be shorter in length than many of the rest, but you should not devote less time to it. Instead, you should spend the time playing with the different options presented here and deciding which one of each is your favorite. Rather than detailing all of the customization options in each package, I think you’ll have an easier time looking at examples, trying to customize them yourself to get the effect you want, and figuring out how to do that by reading the documentation, stackoverflow posts, and other help files – those are the skills you’ll need when you try to put this knowledge into action.\nAt the end of this chapter there are a few extras – for instance, how to use GitHub to host your documents, how to create a blog with blogdown, and more. You should feel free to investigate, but as long as you are able to create presentation slides, posters, and a CV, you’re good to go.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#objectives",
    "href": "part-tools/06-documents.html#objectives",
    "title": "6  Reproducibility and Professional Communication",
    "section": "",
    "text": "Create professional documents (slides, posters, CVs) using LaTeX and/or markdown\n\n\n\n\n\n\n\nReproducibility with Rmarkdown (by Allison Horst)",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "href": "part-tools/06-documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.1 Literate Programming, knitr, rmarkdown, and quarto\n",
    "text": "6.1 Literate Programming, knitr, rmarkdown, and quarto\n\nLiterate programming is a programming method where you explain the code in natural language (e.g. English) in roughly the same space that you write the code (in a programming language). This solves two problems: code isn’t always clear as to what its goals are, and natural language descriptions of algorithms aren’t always clear enough to contain the details of how something is actually implemented.\nThe knitr, Rmarkdown, and quarto packages are all implementations of literate programming. The packages tend to overlap a bit, because knitr and Rmarkdown were written by the same author, Yihui Xie, and quarto is the next generation of Rmarkdown that incorporates more options for using other data-science related programming languages.\n\n\nknitr is primarily focused on the creation of Rnw (r no weave) files, which are essentially LaTeX files with R code inside. Rnw files are compiled into pdfs.\n\nrmarkdown uses Rmd or Rmarkdown files, which can then be compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nquarto uses qmd files, which are compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nAll of these programs work essentially the same way: code chunks are run in the specified language, figures are saved, tables are created, and the results are added to the intermediate file (.tex or .md). Then, another program (LaTeX or Pandoc) compiles the intermediate file into the final result. Understanding this process is key to being able to debug any errors you may encounter, because you need to identify which program is having the error - the code chunk? adding the results to the intermediate file? compiling from the intermediate file to the end result?\n\n\nknitr\nrmarkdown\nquarto\n\n\n\n\n\nKnitr uses R to produce a tex (.tex) file, which is then compiled to PDF using LaTeX.\n\n\n\n\n\nrmarkdown uses R to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\n\nquarto uses R or python to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\nOne major advantage of literate programming packages from a practical perspective is that it largely removes the need to keep track of graphs and charts when you’re writing a paper, making a presentation, etc. The charts and tables based on your method automatically update when the document is recompiled.\nYou’ve probably been using quarto to submit your homework throughout the semester. In this chapter, we’re going to explore some other applications of literate programming: creating slides, posters, and more.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#review-quarto-formatting",
    "href": "part-tools/06-documents.html#review-quarto-formatting",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.2 Review: Quarto Formatting",
    "text": "6.2 Review: Quarto Formatting\nThis section’s material is stolen copied directly from the Quarto documentation [1].\nText Formatting\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n*italics* and **bold**\n\nitalics and bold\n\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\nHeadings\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n# Header 1\nHeader 1\n\n\n## Header 2\nHeader 2\n\n\n### Header 3\nHeader 3\n\n\n#### Header 4\nHeader 4\n\n\n##### Header 5\nHeader 5\n\n\n###### Header 6\nHeader 6\n\n\n\nLinks & Images\n\n\nMarkdown Syntax\nOutput\n\n\n\n&lt;https://quarto.org&gt;\nhttps://quarto.org\n\n\n[Quarto](https://quarto.org)\nQuarto\n\n\n![Caption](elephant.png)\n\n\nCaption\n\n\n\n[![Caption](elephant.png)](https://quarto.org)\n\n\n\n[![Caption](elephant.png)](https://quarto.org \"An elephant\")\n\n\n\n[![](elephant.png){fig-alt=\"Alt text\"}](https://quarto.org)\n\n\n\nLists\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n\n\nunordered list\n\nsub-item 1\n\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n\n\n*   item 2\n\n    Continued (indent 4 spaces)\n\n\nitem 2\nContinued (indent 4 spaces)\n\n\n\n\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\nordered list\n\nitem 2\n\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\n(@)  A list whose numbering\n\ncontinues after\n\n(@)  an interruption\n\n\nA list whose numbering\n\ncontinues after\n\nan interruption\n\n\n\n\nterm\n: definition\n\nterm\n\ndefinition\n\n\n\n\nTables\n\n\nMarkdown Syntax\nOutput\n\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nLearn more in the article on Tables.\nEquations\nUse $ delimiters for inline math and $$ delimiters for display math. For example:\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\ninline math: $E = mc^{2}$\ninline math: \\(E=mc^{2}\\)\n\n\n\ndisplay math:\n\n$$E = mc^{2}$$\ndisplay math:\\[E = mc^{2}\\]\n\n\n\n\nIf you want to define custom TeX macros, include them within $$ delimiters enclosed in a .hidden block. For example:\n::: {.hidden}\n$$\n \\def\\RR{{\\bf R}}\n \\def\\bold#1{{\\bf #1}}\n$$\n:::\nFor HTML math processed using MathJax (the default) you can use the \\def, \\newcommand, \\renewcommand, \\newenvironment, \\renewenvironment, and \\let commands to create your own macros and environments.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#a-very-brief-introduction-to-latex",
    "href": "part-tools/06-documents.html#a-very-brief-introduction-to-latex",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.3 A Very Brief Introduction to LaTeX",
    "text": "6.3 A Very Brief Introduction to LaTeX\nLaTeX is a document preparation utility that attempts to take the focus off of layout (so you don’t have to spend 30 minutes trying to get the page break in the right place in e.g. Word) and bibliographic details.\n\n\nI’m not convinced LaTeX succeeds at freeing you from layout concerns, but it’s certainly true that it is much more powerful than Word for layout purposes.\nThe philosophy of LaTeX is that presentation shouldn’t get in the way of content: you should be able to change the presentation formatting systematically, without having to mess with the content. This (theoretically) allows you to switch templates easily, make document-wide changes in a single command, and more.\n\n\n\n\n\n\n6.3.1 Try it out\n\n\n\nIn Rstudio, copy the text in the document below, paste it into a text file in the editor window, and name it test.tex. You should see a Compile PDF button show up at the top of the document. Click that button to compile the document.\n\\documentclass{article} % this tells LaTeX what type of document to make\n% Note, comments are prefaced by a % sign. If you need to type the actual symbol\n% you will have to escape it with \\%.\n\n\\begin{document}\nHello \\LaTeX!\n\\end{document}\n\n\nMost commonly, you’ll use the article document class for papers, and beamer for presentations and posters. Other useful classes include moderncv (for CVs) and book.\n\n\nThere is a LaTeX class maintained by the UNL math department for thesis formatting. You can easily add R code chunks to a LaTeX file by changing the extension of any .tex file to .Rnw.\nThe Statistics graduate students maintain a bookdown (rmarkdown) version of the UNL thesis class on github here. At some point, hopefully someone will port this to quarto.\nThere are several types of latex commands:\n\n\nDeclarations: statements like \\documentclass, \\usepackage or \\small, which are stated once and take effect until further notice.\n\nEnvironments: statements with matching \\begin{xxx} and \\end{xxx} clauses that define a block of the document which is treated differently. Common environments include figures and tables.\n\nSpecial characters: another type of command that don’t define formatting or structure, but may print special characters, e.g. \\% to print a literal % character.\n\nBoth declarations and environments may come with both optional and required arguments. Required arguments are placed in {...} brackets, while optional arguments are placed in [...] brackets. You can, for instance, start your document with \\documentclass[12pt]{article} to specify the base font size.\nOne of the most useful features in LaTeX is math mode, which you can enter by enclosing text in $ ... $ (for inline statements), $$ ... $$ or \\[ ... \\] (for statements on their own line), or using other environments like \\begin{array} ... \\end{array} that come in math-specific packages. Once in math mode, you can use math symbol commands to get characters like \\(\\theta, \\pi, \\sum, \\int, \\infty\\), and more.\n\n\n\n\n\n\n6.3.2 Try it out\n\n\n\nWith any document creation software, the easiest way to learn how to do it is to find a sample document, tinker with it, see if you can make things the way you want them to be, and then google the errors when you inevitably screw something up.\n\n\nProblem\nSolution\n\n\n\nTake the sample document up above and see if you can do the following tasks: (I’ve linked to documentation that may be useful)\n\nAdd an image\nAdd the quadratic formula and the PDF of a normal distribution to the document\nIn extremely large text, print LaTeX using the \\LaTeX command\nIn extremely small, italic text, print your name\n\n\n\n\\documentclass{article} % this tells LaTeX what type of document to make\n\n% Add the graphicx package so that we can include images\n\\usepackage{graphicx}\n\n\\begin{document}\nHello \\LaTeX!\n\n% Include a figure\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=.5\\textwidth]{../../images/gen-prog/ms-frizzle.png}\n\\caption{Ms. Frizzle, amazing teacher and driver of the Magic School Bus.}\n\\end{figure}\n\n% Add the quadratic formula and the normal PDF to the document\n$y = ax^2 + bx + c$ can be solved to get $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\nThe PDF of a normal distribution is $$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n\n% In extremely large text, print \\LaTeX\n\n\\Huge\\LaTeX\n\n% In extremely small italic text, print your name\n\n\\tiny\\emph{Your name}\n\n\\end{document}\nYou can see the compiled pdf here.\n\n\n\n\n\n\n6.3.3 Knitr\nA LaTeX document has the file extension .tex, but it’s very easy to convert a LaTeX document into a .Rnw (R-no-weave) document: change the file extension. Then, you can add R code chunks, and the .Rnw document will be compiled to a .tex document in R, and then the .tex document will be compiled to .pdf using LaTeX.\nR code chunks are embedded in LaTeX documents using:\n% start of chunk\n&lt;&lt;chunk-name, ...options...&gt;&gt;=\n\n@\n% end of chunk\nYou can embed numerical results inline using \\Sexpr{...} where your R code goes in the ....\nYou could in theory use python within knitr via the reticulate package [2], but it will be easier by far to use quarto. Pick the tool that does the job well.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#slides",
    "href": "part-tools/06-documents.html#slides",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.4 Slides",
    "text": "6.4 Slides\n\n6.4.1 Beamer (LaTeX) and knitr\nBeamer is a powerful LaTeX class which allows you to create slides. The only change necessary to turn a beamer slide deck into a knitr slide deck is to add fragile as an option to any slide with verbatim content.\nYou can also create Beamer slides with Rmarkdown. Example presentation. Standard trade-offs (formatting details vs. document complexity) apply.\n\n\nCheck out the UNL-themed Beamer quarto template\n\n\n\n\n\n\n6.4.1.1 Try it out\n\n\n\nDownload and compile beamer-demo.Rnw.\nCan you change the theme of the presentation?\nAdd another slide, and on that slide, show an appropriate style ggplot2 graph of the distribution of board game ratings, reading in the board game ratings using the following code:\n\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-12/board_games.csv\")\n\n\n\n\n\nKarl Broman has a set of slides that show how to use beamer + knitr to make reproducible slides with notes.\nYou can also create Beamer slides using Rmarkdown or quarto, if you want, but you may have more control over the fine details if you go straight to the Rnw file without going through markdown first. It’s a trade-off – the file will probably be simpler in markdown, but you won’t have nearly as much control.\n\n6.4.2 HTML slides\nRStudio has a host of other options for html slide presentations. There are some definite advantages to HTML presentations: they’re easy to share (via URL), you can add gifs, emojis, and interactive graphics, and you can set up github to host the presentations as well.\n\n\nI have a repository for all of the presentations I’ve given, and I use github pages to render the html presentations. Very easy, convenient, and I never have to carry a flash drive around at a conference or mess with the conference computers.\nThe downside to HTML slides is that there are approximately 100000 different javascript libraries that create HTML slides, and all of them have different capabilities. Many of these libraries have extensions that will let you create markdown slides, but they each have slightly different markdown syntax and capabilities.\n\n\nRmarkdown slide options available by default in RStudio\n\nYou can get the full details of any fully supported slide class in Rmarkdown by looking at the Rmarkdown book [3], which is freely available online. These guidelines will give you specifics about how to customize slides, add incremental information, change transitions, print your slides to PDF, and include speaker notes.\nQuarto has simplified the slide options available to you - for HTML slides, you have one option, which is to use reveal.js. While this may sound limiting, it’s really not - RStudio/Posit (the company behind quarto) has done a ton of work to make quarto a lovely experience, and that extends to the slides. I have almost entirely switched to using quarto for everything because it’s so much easier to arrange figures, add alt-text, and style presentations. See the quarto presentation documentation here [4]. If you have collaborators who are stuck on MS Office, quarto allows you to compile to a PowerPoint presentation.\nRather than repeat the documentation for each slide package in this document, I think it is probably easier just to link you to the documentation and a sample presentation for each option.\nQuarto:\n\n\nreveal.js Example presentation\n\n\nRmarkdown:\n\n\nreveal.js Example presentation Example with UNL CSS Theme\n\n\nioslides Example presentation\n\n\nslidy Example presentation\n\n\nxaringan Example presentation, Example presentation 2 using UNL CSS theme\n\n\nIf you’re familiar with CSS (or happier tinkering to get the look of something exactly right) then xaringan and reveal.js are excellent full-featured options.\n\n\nI relied heavily on 2D slide layouts available in reveal.js during my PhD prelim and defense.\nA nice feature of reveal.js presentations is support for 2D slide layouts, so you can have multiple sections in your presentation, and move vertically through each section, or horizontally between sections. That is useful for presentations where you may not plan on covering everything, but where you want to have all of the information available if necessary.\n\n\nUNL themed HTML presentations:\n\n\nxaringan (zip of all required files)\n\nquarto reveal.js (zip of all required files)\n\n\n\n\n\n\n\n6.4.3 Try it out\n\n\n\nTake a few minutes and try each of them out to see what feels right to you. Each one has a slightly different “flavor” of markdown, so read through the example to get a sense for what is different.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#posters",
    "href": "part-tools/06-documents.html#posters",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.5 Posters",
    "text": "6.5 Posters\nPosters are another common vehicle for presenting academic project results. Because posters are typically printed on paper or fabric, the standard file format is still PDF. As some venues move to digital posters, it is becoming more realistic to use HTML poster layouts that contain interactive elements.\n\n6.5.1 LaTeX\nOverleaf has a fantastic gallery of posters made in LaTeX.\nThere are several LaTeX options for making scientific posters: baposter, beamerposter, tikzposter are among the most common. We’ll focus on beamerposter here, but you are free to explore the other poster classes at will. As with beamer, you can easily integrate knitr code chunks into a document, so that you are generating your images reproducibly.\nBasic code for a poster in beamer (along with the necessary style files) that I’ve minimally customized to meet UNL branding requirements can be found here.\n\n\n\n\n\n\n6.5.1.1 Try it out\n\n\n\nDownload the beamer template and do the following:\n\nChange the 3-column span box to a 2-column span box.\nMake the “Block Colors” box purple\nMove the References block up to fill the 4th column.\n\n\n\n\n6.5.2 Markdown\nWhile most posters are still put together in PDF form, there is growing support for HTML posters, and many conferences have digital poster options for display. This may allow you to use interactive graphics and other features in a poster that would not translate well to PDF. Here is a list of Rmarkdown poster options; some even have PDF export capabilities so that you can have the interactive version plus a static version.\n\n6.5.2.1 Posterdown\nTo start, install posterdown with install.packages(\"posterdown\").\n\n\nUse the RStudio menu to create a posterdown presentation file – with a prefilled template\n\n\n\nUNL-themed posterdown template\nYou can also find additional customization options here. As with other markdown items, you can customize things even more using CSS. The nice thing about HTML posters, though, is that you can directly link to them if they’re hosted on a site.\nYou can also print a poster to PDF by running the following command: pagedown::chrome_print(\"myfile.Rmd\").\n\n6.5.2.2 Pagedown\nThe pagedown package also has a couple of poster templates, including poster-relaxed and poster-jacobs.\nThere are also templates for letters, business cards, and more in pagedown, if you’re feeling ambitious.\n\n\n\n\n\n\n6.5.2.3 Try it out\n\n\n\nDownload the pagedown template and do the following:\n\nChange the 3-column layout to 4 columns. Adjust the breaks ({.mybreak}) accordingly to make the poster look good.\nMake the 2nd-level headers #249ab5 (cerulean)\nMove the References block to the 4th column.\nPrint your poster to a PDF",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#resumecv",
    "href": "part-tools/06-documents.html#resumecv",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.6 Resume/CV",
    "text": "6.6 Resume/CV\nYou can also create resumes and CVs in markdown and LaTeX. There is no real substitute for playing around with these classes, but I really like moderncv in LaTeX.\n\n\nYou can see my highly customized CV here, with timelines and numbered publications. It has to be compiled multiple times to get everything right.\nPagedown also comes with a html resume template (Use the menu -&gt; Rmarkdown -&gt; From Template -&gt; HTML Resume) that can be printed to html and pdf simultaneously. There is also the vitae package, which has even more templates, integration with other packages/sites, and more.\n\n\nAt this point, the biggest reason I haven’t switched to HTML is that I really like my timeline CV and I don’t have enough time to fiddle with it more.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#hosting-content-with-github-pages",
    "href": "part-tools/06-documents.html#hosting-content-with-github-pages",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.7 Hosting Content with Github Pages",
    "text": "6.7 Hosting Content with Github Pages\nGithub will host HTML content for you using Github pages (case in point: this textbook). This means you can version control your content (for instance, presentations or your CV) and have GitHub do the hosting (so you don’t have to find a webserver, buy a domain name, etc).\n\n\n\n\n\n\nSetting up Github Pages\n\n\n\n\nCreate a new repository named username.github.io on your personal github site (not the unl-stat850 classroom group)\nClone your repository\nModify your README.md file and push your changes\nGo to https://username.github.io and see your README.md file rendered as HTML.\n\n\n\n\n\n\n\nGithub will render any README.md file as actual HTML; it will also allow you to host plain HTML pages. By default, the README file is rendered first, but in subsequent directories, a file named index.html will be rendered as the “home page” for the subdirectory, if you have such a file. Otherwise you’ll have to know the file name.\nI tend to separate things out into separate repositories, but you can host HTML content on other repositories too, by enabling github pages in the repository settings. On my personal page, I have repositories for my CV, Presentations, etc. Each repository that has pages enabled can be accessed via https://srvanderplas.github.io/\\&lt;repository name\\&gt;/\\&lt;repository file path\\&gt;. So, to see my stat-computing-r-python repository, you’d go to https://srvanderplas.github.io/stat-computing-r-python/ (Oh, wait, you’re likely already there!).\n\n\nI’ve been putting my presentations on Github since 2014, so it has a pretty good record of every set of slides I’ve created. I highly recommend this strategy - storing everything online makes it easy to share your work with others, reference later, and more importantly, easy for you to find in 3 years. One thing I learned, though, was that it’s helpful to create a presentation repository by year – I eventually hit the maximum repository limit (which was irritating), and it’s also nice to be able to find things quickly and remember when you gave that particular presentation.\nThis mechanism provides a very convenient way to showcase your work, share information with collaborators, and more - instead of sending files, you can send a URL and no one has to download anything overtly.\n\n\n\n\n\n\nSetting up Github Pages in an existing repository\n\n\n\n\n\n\n\n\n\nIf you want to track your quarto/rmarkdown code and then render the output to a separate folder, you can use the docs/ folder. Github has this as an option as well – where we selected “main” branch above, we would select “docs/” instead (it’s grayed out b/c there isn’t a docs folder in the repo). That is how this book is hosted - the book compiles to the docs/ folder, and that way the book is rendered in final form and you don’t have to see all of the other crud that is in the repository.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#additional-resources-to-explore",
    "href": "part-tools/06-documents.html#additional-resources-to-explore",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.8 Additional Resources to Explore",
    "text": "6.8 Additional Resources to Explore\nThere are many other XXXdown packages made for Rmarkdown. Quarto is more multi-functional and contains blog, book, and website capabilities in a single package. However, most of the things which worked in Rmarkdown also work in Quarto, and Quarto has clearly been built off of the success of the ___down packages for Rmarkdown.\n\nblogdown\nbookdown (what I used to make this book in the SAS + R era)\npkgdown (to easily build documentation websites for R packages)\nROpenSci tutorial: How to set up hosting on github\nliftr - use Docker to make persistently reproducible documents\n\nIn addition, @mcanouil maintains a list of Quarto talks, topics, tools, and examples that is worth a look.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#references",
    "href": "part-tools/06-documents.html#references",
    "title": "6  Reproducibility and Professional Communication",
    "section": "\n6.9 References",
    "text": "6.9 References\n\n\n\n\n[1] \nPosit, “Markdown Basics,” Quarto. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html#text-formatting. [Accessed: Oct. 17, 2022]\n\n\n[2] \nK. Ushey, J. Allaire, and Y. Tang, Reticulate: Interface to ’python’. 2022 [Online]. Available: https://CRAN.R-project.org/package=reticulate\n\n\n\n[3] \nY. Xie, J. J. Allaire, and G. Grolemund, “Chapter 4 Presentations,” in R Markdown: The Definitive Guide, 1st ed., CRC Press, 2018 [Online]. Available: https://bookdown.org/yihui/rmarkdown/presentations.html. [Accessed: Sep. 28, 2022]\n\n\n[4] \nPosit, “Quarto - Presentations,” Quarto. [Online]. Available: https://quarto.org/docs/presentations/. [Accessed: Sep. 28, 2022]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-gen-prog.html",
    "href": "part-gen-prog/00-gen-prog.html",
    "title": "Part II: General Programming",
    "section": "",
    "text": "In this portion of the textbook, we’ll talk about the basics of programming in a general sense (that is, we’re not yet focusing on programming with data).\nBefore we start in on the hard stuff, we’ll quickly go through what programming is and what the vocabulary of a programming language looks like in 7  Introduction to Programming.\n8  Variables and Basic Data Types will discuss the basics: variable types, how to assign variables, and how to convert between simple variable types.\n10  Functions, Packages, and Environments will discuss how to use built-in and package functions to make R and python more powerful. After this section, you should be able to use R or Python as a calculator.\n11  Data Structures will discuss the use of vectors and matrices in R and Python. Along the way, you’ll get a quick refresher in mathematical logic - the use of And, Or, and Not.\nIf you’ve had linear algebra, 12  Matrix Calculations will tell you how to use R and python to perform matrix calculations. If you haven’t had linear algebra yet, skip this section and move on to 13  Control Structures.\n13  Control Structures will discuss control structures - ways to change the flow of a program based on variable values and operating condition. This will include discussions of if-statements and different types of loops.\n14  Writing Functions will discuss writing your own functions.\nOnce we’ve covered these topics, we should be ready to focus on programming with, for, and on data.",
    "crumbs": [
      "Part II: General Programming"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html",
    "href": "part-gen-prog/00-intro.html",
    "title": "7  Introduction to Programming",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#objectives",
    "href": "part-gen-prog/00-intro.html#objectives",
    "title": "7  Introduction to Programming",
    "section": "",
    "text": "Define programming and provide examples of programming\nIdentify reserved words in R and python\nKnow how to get help when using R and python",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#what-is-programming",
    "href": "part-gen-prog/00-intro.html#what-is-programming",
    "title": "7  Introduction to Programming",
    "section": "7.2 What is Programming?",
    "text": "7.2 What is Programming?\n\nProgramming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\n\nbreak a problem down into smaller, easily solvable problems\nsolve the small problems\ncommunicate the solution to a computer using a programming language\n\nIn this book, we’ll be using both R and Python, and we’ll be using these languages to solve problems that are related to working with data. At first, we’ll start with smaller, simpler problems that don’t involve data, but by the end, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical “sentences”. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they’ll last you a lifetime.\nJust as you wouldn’t expect to learn French or Mandarin fluently after taking a single class, you cannot expect to be fluent in R or python once you’ve worked through this book. Fluency takes years of work and practice, and lots of mistakes along the way. You cannot learn a language (programming or otherwise) if you’re worried about making mistakes.\n\n\nTake a minute and put those concerns away, take a deep breath, and remember the Magic School Bus Motto:\n\n\n\nFor those who don’t know, the Magic School Bus is a PBS series that aired in the 1990s and was brought back by Netflix in 2017. It taught kids about different principles of science and the natural world.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#strategies-for-programming",
    "href": "part-gen-prog/00-intro.html#strategies-for-programming",
    "title": "7  Introduction to Programming",
    "section": "7.3 Strategies for Programming",
    "text": "7.3 Strategies for Programming\nAs you start to program, you will need to develop some skill at reading and interpreting error messages, breaking problems down into smaller parts, and thinking through what code instructs the computer to do. It’s useful to at least mention some strategies here, though Chapter 15 goes into much more detail.\n\nGoogle the error message (or put the code and the error into AI and ask it to explain why there is an error).\nMake a list of steps or a flowchart illustrating what the code does, breaking the code down into smaller pieces each time.\n\nKeep an eye out for how these strategies are used in Chapter 8 and Chapter 10.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "href": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "title": "7  Introduction to Programming",
    "section": "7.4 Programming Vocabulary: Hello World",
    "text": "7.4 Programming Vocabulary: Hello World\nI particularly like the way that Python for Everybody [1] explains vocabulary (my changes in parentheses):\n\nUnlike human languages, the (R and) Python vocabulary is actually pretty small. We call this “vocabulary” the “reserved words”. These are words that have very special meaning to (R and) Python. When (R) Python sees these words in a (R) Python program, they have one and only one meaning to (R) Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of (R or) Python’s reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like “sit”, “stay”, and “fetch”. When you talk to a dog and don’t use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, “I wish more people would walk to improve their overall health”, what most dogs likely hear is, “blah blah blah walk blah blah blah blah.” That is because “walk” is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nPythonR\n\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while\n\n\n\nThe reserved words in the language where humans talk to R include the following:\n\nif          else     repeat      while\nfor         in       next        break\nTRUE        FALSE    NULL        Inf\nNA_integer_ NA_real_ NA_complex_ NA_character_\nNaN         NA       function    ...\n\n\n\n\nThat is it, and unlike a dog, (R) Python is already completely trained. When you say ‘try’, (R) Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the (R) Python equivalent of “speak” (in human-to-dog language). The nice thing about telling (R and) Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nPythonR\n\n\n\nprint('Hello world!')\n## Hello world!\n\n\n\n\nprint('Hello world!')\n## [1] \"Hello world!\"\n\n\n\n\nThe “Hello World” program looks exactly the same in R as it does in python!\n\nAnd we have even written our first syntactically correct (R and) Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history [2], because it is also similar to Lisp, but both languages are still very similar to C and often run C or C++ code in the background.\n\n\n\n\n\n\nReserved Word Errors\n\n\n\nWhat happens when we try to assign a new value to a reserved word? We’ll learn more about assignment in the next chapter, but for now, know it is equivalent to setting the value of a variable. x = 3 or x &lt;- 3 defines the variable \\(x\\) as having the value 3.\n\nRPython\n\n\n\n1for &lt;- 3\n## Error in parse(text = input): &lt;text&gt;:1:5: unexpected assignment\n## 1: for &lt;-\n##         ^\n\n\n1\n\nWe just tried to assign the value 3 to the word for. for is a reserved word and cannot have a different value assigned to it, so we get an unexpected assignment error.\n\n\n\n\nWhen we assign the value of a non-reserved word, things work just fine:\n\n1x &lt;- 3\n\n\n1\n\nx is not a reserved word, so we can safely assign x the value 3 without getting any errors.\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: RStudio alerts us to the syntax error in the left column of the editor window and even helps identify the error via mouseover text.\n\n\n\n\n\n\n1for = 3\n## invalid syntax (&lt;string&gt;, line 1)\n\n\n1\n\nWe just tried to assign the value 3 to the word for. for is a reserved word and cannot have a different value assigned to it, so we get an invalid syntax error located at the =.\n\n\n\n\nWhen we assign the value of a non-reserved word, things work just fine:\n\n1x = 3\n\n\n1\n\nx is not a reserved word, so we can safely assign x the value 3 without getting any errors.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#getting-help",
    "href": "part-gen-prog/00-intro.html#getting-help",
    "title": "7  Introduction to Programming",
    "section": "7.5 Getting help",
    "text": "7.5 Getting help\nIn both R and python, you can access help with a ? via the terminal/console.\n\n\n\n\n\n\nDemo: Getting Help\n\n\n\n\nRPython\n\n\n\n?list # Get help with the list function\n\n\n\nSometimes, it is easier to start a python terminal outside of RStudio (or in the RStudio terminal window), as the reticulate R package that allows us to run python code doesn’t always react the same as the normal python terminal.\nHelp is one particular case where this seems to be true\n\nhelp(list)\n## Help on class list in module builtins:\n## \n## class list(object)\n##  |  list(iterable=(), /)\n##  |  \n##  |  Built-in mutable sequence.\n##  |  \n##  |  If no argument is given, the constructor creates a new empty list.\n##  |  The argument must be an iterable if specified.\n##  |  \n##  |  Methods defined here:\n##  |  \n##  |  __add__(self, value, /)\n##  |      Return self+value.\n##  |  \n##  |  __contains__(self, key, /)\n##  |      Return key in self.\n##  |  \n##  |  __delitem__(self, key, /)\n##  |      Delete self[key].\n##  |  \n##  |  __eq__(self, value, /)\n##  |      Return self==value.\n##  |  \n##  |  __ge__(self, value, /)\n##  |      Return self&gt;=value.\n##  |  \n##  |  __getattribute__(self, name, /)\n##  |      Return getattr(self, name).\n##  |  \n##  |  __getitem__(...)\n##  |      x.__getitem__(y) &lt;==&gt; x[y]\n##  |  \n##  |  __gt__(self, value, /)\n##  |      Return self&gt;value.\n##  |  \n##  |  __iadd__(self, value, /)\n##  |      Implement self+=value.\n##  |  \n##  |  __imul__(self, value, /)\n##  |      Implement self*=value.\n##  |  \n##  |  __init__(self, /, *args, **kwargs)\n##  |      Initialize self.  See help(type(self)) for accurate signature.\n##  |  \n##  |  __iter__(self, /)\n##  |      Implement iter(self).\n##  |  \n##  |  __le__(self, value, /)\n##  |      Return self&lt;=value.\n##  |  \n##  |  __len__(self, /)\n##  |      Return len(self).\n##  |  \n##  |  __lt__(self, value, /)\n##  |      Return self&lt;value.\n##  |  \n##  |  __mul__(self, value, /)\n##  |      Return self*value.\n##  |  \n##  |  __ne__(self, value, /)\n##  |      Return self!=value.\n##  |  \n##  |  __repr__(self, /)\n##  |      Return repr(self).\n##  |  \n##  |  __reversed__(self, /)\n##  |      Return a reverse iterator over the list.\n##  |  \n##  |  __rmul__(self, value, /)\n##  |      Return value*self.\n##  |  \n##  |  __setitem__(self, key, value, /)\n##  |      Set self[key] to value.\n##  |  \n##  |  __sizeof__(self, /)\n##  |      Return the size of the list in memory, in bytes.\n##  |  \n##  |  append(self, object, /)\n##  |      Append object to the end of the list.\n##  |  \n##  |  clear(self, /)\n##  |      Remove all items from list.\n##  |  \n##  |  copy(self, /)\n##  |      Return a shallow copy of the list.\n##  |  \n##  |  count(self, value, /)\n##  |      Return number of occurrences of value.\n##  |  \n##  |  extend(self, iterable, /)\n##  |      Extend list by appending elements from the iterable.\n##  |  \n##  |  index(self, value, start=0, stop=9223372036854775807, /)\n##  |      Return first index of value.\n##  |      \n##  |      Raises ValueError if the value is not present.\n##  |  \n##  |  insert(self, index, object, /)\n##  |      Insert object before index.\n##  |  \n##  |  pop(self, index=-1, /)\n##  |      Remove and return item at index (default last).\n##  |      \n##  |      Raises IndexError if list is empty or index is out of range.\n##  |  \n##  |  remove(self, value, /)\n##  |      Remove first occurrence of value.\n##  |      \n##  |      Raises ValueError if the value is not present.\n##  |  \n##  |  reverse(self, /)\n##  |      Reverse *IN PLACE*.\n##  |  \n##  |  sort(self, /, *, key=None, reverse=False)\n##  |      Sort the list in ascending order and return None.\n##  |      \n##  |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n##  |      order of two equal elements is maintained).\n##  |      \n##  |      If a key function is given, apply it once to each list item and sort them,\n##  |      ascending or descending, according to their function values.\n##  |      \n##  |      The reverse flag can be set to sort in descending order.\n##  |  \n##  |  ----------------------------------------------------------------------\n##  |  Class methods defined here:\n##  |  \n##  |  __class_getitem__(...) from builtins.type\n##  |      See PEP 585\n##  |  \n##  |  ----------------------------------------------------------------------\n##  |  Static methods defined here:\n##  |  \n##  |  __new__(*args, **kwargs) from builtins.type\n##  |      Create and return a new object.  See help(type) for accurate signature.\n##  |  \n##  |  ----------------------------------------------------------------------\n##  |  Data and other attributes defined here:\n##  |  \n##  |  __hash__ = None\n\nHelp on class list in module builtins:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n\n\n\n\n\n\n\n\n\n\nExample: Debugging - Getting help with for loops\n\n\n\nSuppose we want to get help on a for loop in either language.\n\nRPython\n\n\nTo get help in R, we put a ? in front of a function name to pull up the help file on that function. However, when we try this with a for loop by typing ?for into the console, we get a + sign.\n&gt; ?for\n+ \nThat isn’t what we expected! The + sign indicates that R is still waiting for some input - we haven’t given it a complete statement. But why is the statement not complete? Googling for “?for” in R gets us something – a help page about Getting Help with R.\nWe get a tiny hint here:\n\nStandard names in R consist of upper- and lower-case letters, numerals (0-9), underscores (_), and periods (.), and must begin with a letter or a period. To obtain help for an object with a non-standard name (such as the help operator ?), the name must be quoted: for example, help(‘?’) or ?“?”.\n\nIs it possible that for is a non-standard name? Perhaps because it’s a reserved word? We can try it out: ?\"for\" (but first, we need to hit Escape while the console pane is selected, to get out of the incomplete command ?for). And, that works! We get a complete statement (a &gt; on the next line in the console waiting for more input), and the help page titled “Control Flow” pops up.\n\n?\"for\"\n?`for`\n\nBecause for is a reserved word in R, we have to use quotes or backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name. The backtick trick also works for functions that don’t start with letters, like +.\n\n\nIn python, we try to use ?for to access the same information (this works for many python functions).\n\n?for # help printed in the help pane\n\n(You will have to run this in interactive mode for it to work in either language)\nWe can also just type in help in the python console. The prompt will change to help&gt;.\nWe then type for at the help prompt.\nhelp&gt; for\nThis gets us:\nThe \"for\" statement\n*******************\n\nThe \"for\" statement is used to iterate over the elements of a sequence\n(such as a string, tuple or list) or other iterable object:\n\n   for_stmt ::= \"for\" target_list \"in\" starred_list \":\" suite\n                [\"else\" \":\" suite]\n\nThe \"starred_list\" expression is evaluated once; it should yield an\n*iterable* object.  An *iterator* is created for that iterable. The\nfirst item provided by the iterator is then assigned to the target\nlist using the standard rules for assignments (see Assignment\nstatements), and the suite is executed.  This repeats for each item\nprovided by the iterator.  When the iterator is exhausted, the suite\nin the \"else\" clause, if present, is executed, and the loop\nterminates.\n\nA \"break\" statement executed in the first suite terminates the loop\nwithout executing the \"else\" clause’s suite.  A \"continue\" statement\nexecuted in the first suite skips the rest of the suite and continues\nwith the next item, or with the \"else\" clause if there is no next\nitem.\n\nThe for-loop makes assignments to the variables in the target list.\nThis overwrites all previous assignments to those variables including\nthose made in the suite of the for-loop:\n\n   for i in range(10):\n       print(i)\n       i = 5             # this will not affect the for-loop\n                         # because i will be overwritten with the next\n                         # index in the range\n\nNames in the target list are not deleted when the loop is finished,\nbut if the sequence is empty, they will not have been assigned to at\nall by the loop.  Hint: the built-in type \"range()\" represents\nimmutable arithmetic sequences of integers. For instance, iterating\n\"range(3)\" successively yields 0, 1, and then 2.\n\nChanged in version 3.11: Starred elements are now allowed in the\nexpression list.\n\nRelated help topics: break, continue, while\nOf course, we can also google “python for loop” and get documentation that way. When you are just learning how to program, it is often easier to read web documentation that provides lots of examples than the highly technical “manual” or “man” pages for commands that are shown by default1. This is completely normal – manual pages are written for reference while programming (which assumes you know how to program), and tutorials online are written for people learning how to program. Reading manual pages is a skill that you develop over time.\nw3schools has an excellent python help page that may be useful as well. Searching for help using google also works well, particularly if you know what sites are likely to be helpful, like w3schools and stackoverflow. A similar set of pages exists for R help on basic functions\n\n\n\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nA nice explanation of the difference between an interpreter and a compiler. Both Python and R are interpreted languages that are compiled from lower-level languages like C.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "href": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "title": "7  Introduction to Programming",
    "section": "7.6 References",
    "text": "7.6 References\n\n\n\n\n[1] D. C. R. Severance, Python for Everybody: Exploring Data in Python 3. Ann Arbor, MI: CreateSpace Independent Publishing Platform, 2016 [Online]. Available: https://www.py4e.com/html3/\n\n\n[2] R. Ihaka, “R : Past and future history,” 1998 [Online]. Available: https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#footnotes",
    "href": "part-gen-prog/00-intro.html#footnotes",
    "title": "7  Introduction to Programming",
    "section": "",
    "text": "In Linux, you access manual pages for commands by running man &lt;command-name&gt;. There is an old joke about running the command man woman, only to get the response No manual entry for woman, because there is no manual page that will help men understand women. Of course, in reality, there is also no command named woman…↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html",
    "href": "part-gen-prog/01-basic-var-types.html",
    "title": "8  Variables and Basic Data Types",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#objectives",
    "href": "part-gen-prog/01-basic-var-types.html#objectives",
    "title": "8  Variables and Basic Data Types",
    "section": "",
    "text": "Know the basic data types and what their restrictions are\nKnow how to test to see if a variable is a given data type\nUnderstand the basics of implicit and explicit type conversion\nWrite code that assigns values to variables",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "href": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.1 Basic Definitions",
    "text": "8.1 Basic Definitions\nFor a general overview, [1] is an excellent introduction to data types:\n\n\n\n\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn most programming languages (including R and python), there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float - decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (“double precision”).1\n\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\n\n\n\n\n\n\nCapitalization matters!\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#variables",
    "href": "part-gen-prog/01-basic-var-types.html#variables",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.2 Variables",
    "text": "8.2 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\n\n8.2.1 Assignment\nWe assign variables values using the syntax object_name &lt;- value (R) or object_name = value (python). You can read this as “object name gets value” in your head.\n\n\nDataCamp Introduction to R Chapter 1: Intro to basics\nDataCamp Introduction to Python for Data Science Chapter 1: Python Basics\n\n\nIn R, &lt;- is used for assigning a value to a variable. So x &lt;- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’”. Technically, you can also use = to assign things to variables in R, but most style guides consider this to be poor programming practice, so seriously consider defaulting to &lt;-.\nIn Python, = is used for assigning a value to a variable. This tends to be much easier to say out loud, but lacks any indication of directionality.\n\n8.2.1.1 Demo: Assignment\n\n\nR\nPython\n\n\n\n\nmessage &lt;- \"So long and thanks for all the fish\"\nyear &lt;- 2025\nthe_answer &lt;- 42L\nearth_demolished &lt;- FALSE\n\n\n\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in R, we assign variables values using the &lt;- operator, where in Python, we assign variables values using the = operator. Technically, = will work for assignment in both languages, but &lt;- is more common than = in R by convention.\n\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n8.2.2 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R. In Python, object names must start with a letter and can consist of letters, numbers, and _ (that is, . is not a valid character in a Python variable name). While it is technically fine to use uppercase variable names in Python, it’s recommended that you use lowercase names for variables (you’ll see why later).\nWhat happens if we try to create a variable name that isn’t valid?\nIn both languages, starting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol” in R and “invalid syntax” in python.\n\n8.2.2.1 Invalid Names\n\n\nR\nPython\n\n\n\n\n1st_thing &lt;- \"check your variable names!\"\n## Error in parse(text = input): &lt;text&gt;:1:2: unexpected symbol\n## 1: 1st_thing\n##      ^\n\n\n\n\n1st_thing &lt;- \"check your variable names!\"\n\nNote: Run the above chunk in your python window - the book won’t compile if I set it to evaluate 😥. It generates an error of SyntaxError: invalid syntax (&lt;string&gt;, line 1)\n\nsecond.thing &lt;- \"this isn't valid\"\n## NameError: name 'second' is not defined\n\nIn python, trying to have a . in a variable name gets a more interesting error: “ is not defined”. This is because in python, some objects have components and methods that can be accessed with .. We’ll get into this more later, but there is a good reason for python’s restriction about not using . in variable names.\n\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n8.2.2.2 Learn More\nWhy is naming things hard? - Blog post by Neil Kakkar\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\n\nsome.people.use.periods (in R, obviously this doesn’t work in python)\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated 👿\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#types",
    "href": "part-gen-prog/01-basic-var-types.html#types",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.3 Types",
    "text": "8.3 Types\n\n8.3.1 Testing Types\nYou can use different functions to test whether a variable has a specific type.\n\n\nR\nPython\n\n\n\n\nis.logical(FALSE)\nis.integer(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nis.integer(2)\nis.numeric(2)\nis.character(\"Hello, programmer!\")\nis.function(print)\n## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] TRUE\n## [1] TRUE\n## [1] TRUE\n\nIn R, you use is.xxx functions, where xxx is the name of the type in question.\n\n\n\nisinstance(False, bool)\nisinstance(2, int)\nisinstance(2, (int, float)) # Test for one of multiple types\nisinstance(3.1415, float)\nisinstance(\"This is python code\", str)\n## True\n## True\n## True\n## True\n## True\n\nIn python, test for types using the isinstance function with an argument containing one or more data types in a tuple ((int, float) is an example of a tuple - a static set of multiple values).\nIf we want to test for whether something is callable (can be used like a function), we have to get slightly more complicated:\n\ncallable(print)\n## True\n\nThis is glossing over some much more technical information about differences between functions and classes (that we haven’t covered) [2].\n\n\n\n\n\n\n\n\n\nExample: Assignment and Testing Types\n\n\n\n\n\nCharacter\nLogical\nInteger\nDouble\nNumeric\n\n\n\n\nx &lt;- \"R is awesome\"\ntypeof(x)\n## [1] \"character\"\nis.character(x)\n## [1] TRUE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\n\nx = \"python is awesome\"\ntype(x)\n## &lt;class 'str'&gt;\nisinstance(x, str)\n## True\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- FALSE\ntypeof(x)\n## [1] \"logical\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] TRUE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\nIn R, is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false. 2\n\nx = False\ntype(x)\n## &lt;class 'bool'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## True\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\nNote that in python, boolean variables are also integers. If your goal is to test whether something is a T/F value, you may want to e.g. test whether its value is one of 0 or 1, rather than testing whether it is a boolean variable directly, since integers can also function directly as bools in Python.\n\n\n\nx &lt;- 2\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\n\nWait, 2 is an integer, right?\n2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically.\n\nx &lt;- 2L # The L immediately after the 2 indicates that it is an integer.\ntypeof(x)\n## [1] \"integer\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] TRUE\nis.double(x)\n## [1] FALSE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2\ntype(x)\n## &lt;class 'int'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- 2.45\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2.45\ntype(x)\n## &lt;class 'float'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## True\n\n\n\nA fifth common “type”3, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing.\nThe numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work.\nThe general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "href": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.4 Type Conversions",
    "text": "8.4 Type Conversions\nProgramming languages will generally work hard to seamlessly convert variables to different types. This is called implicit type casting - the computer implicitly changes the variable type to avoid a conflict.\n\n8.4.1 Implicit Type Conversion\n\n\nR\nPython\n\n\n\n\nTRUE + 2\n## [1] 3\n\n2L + 3.1415\n## [1] 5.1415\n\n\"abcd\" + 3\n## Error in \"abcd\" + 3: non-numeric argument to binary operator\n\n\n\n\nTrue + 2\n## 3\n\nint(2) + 3.1415\n## 5.141500000000001\n\n\"abcd\" + 3\n## TypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nThis conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R or python will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops.\nWhen you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be:\n\n8.4.2 Explicit Type Conversion\n\n\nR\nPython\n\n\n\n\nas.double(TRUE) + 2\n## [1] 3\n\nas.double(2L) + 3.1415\n## [1] 5.1415\n\nas.numeric(\"abcd\") + 3\n## [1] NA\n\n\n\n\nint(True) + 2\n## 3\n\nfloat(2) + 3.1415\n## 5.141500000000001\n\nfloat(\"abcd\") + 3\n## ValueError: could not convert string to float: 'abcd'\n\nimport pandas as pd # Load pandas library\npd.to_numeric(\"abcd\", errors = 'coerce') + 3\n## np.float64(nan)\n\n\n\n\nWhen we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value - in R. In Python, we get a more descriptive error by default, but we can use the pandas library (which adds some statistical functionality) to get a similar result to the result we get in R.\nThere’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "href": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.5 What Type is it?",
    "text": "8.5 What Type is it?\nIf you don’t know what type a value is, both R and python have functions to help you with that.\n\n8.5.1 Determining Variable Types\n\n\nR\nPython\n\n\n\nIf you are unsure what the type of a variable is, use the typeof() function to find out.\n\nw &lt;- \"a string\"\nx &lt;- 3L\ny &lt;- 3.1415\nz &lt;- FALSE\n\ntypeof(w)\n## [1] \"character\"\ntypeof(x)\n## [1] \"integer\"\ntypeof(y)\n## [1] \"double\"\ntypeof(z)\n## [1] \"logical\"\n\n\n\nIf you are unsure what the type of a variable is, use the type() function to find out.\n\nw = \"a string\"\nx = 3\ny = 3.1415\nz = False\n\ntype(w)\n## &lt;class 'str'&gt;\ntype(x)\n## &lt;class 'int'&gt;\ntype(y)\n## &lt;class 'float'&gt;\ntype(z)\n## &lt;class 'bool'&gt;\n\n\n\n\n\n\n\n\n\n\nTry It Out: Variables and Types\n\n\n\n\n\nR\nPython\nR Solution\nPython Solution\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring &lt;- \ninteger &lt;- \ndecimal &lt;- \nlogical &lt;- \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring = \ninteger = \ndecimal = \nlogical = \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nstring &lt;- \"hi, I'm a string\"\ninteger &lt;- 4L\ndecimal &lt;- 5.412\nlogical &lt;- TRUE\n\nlogical + decimal\n## [1] 6.412\ninteger + decimal\n## [1] 9.412\nas.numeric(string) + integer\n## [1] NA\n\n\"abcd\" + \"efgh\"\n## Error in \"abcd\" + \"efgh\": non-numeric argument to binary operator\nTRUE + TRUE\n## [1] 2\n\nIn R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value.\nTo concatenate strings in R (like the default behavior in python), we would use the paste0 function: paste0(\"abcd\", \"efgh\"), which returns abcdefgh.\n\n\n\nimport pandas as pd\n\nstring = \"hi, I'm a string\"\ninteger = 4\ndecimal = 5.412\nlogical = True\n\nlogical + decimal\n## 6.412\ninteger + decimal\n## 9.411999999999999\npd.to_numeric(string, errors='coerce') + integer\n## np.float64(nan)\n\n\"abcd\" + \"efgh\"\n## 'abcdefgh'\nTrue + True\n## 2\n\nIn Python, when a string is added to another string, the two strings are concatenated. This differs from the result in R, which is a “non-numeric argument to binary operator” error.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "href": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "title": "8  Variables and Basic Data Types",
    "section": "\n8.6 References",
    "text": "8.6 References\n\n\n\n\n[1] \n\nWhy TRUE + TRUE = 2: Data Types. (Feb. 03, 2020) [Online]. Available: https://www.youtube.com/watch?v=6otW6OXjR8c. [Accessed: May 18, 2022]\n\n\n[2] \nRyan, “Answer to \"how do i detect whether a variable is a function?\". Stack overflow,” Mar. 09, 2009. [Online]. Available: https://stackoverflow.com/a/624948/2859168. [Accessed: Jan. 10, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#footnotes",
    "href": "part-gen-prog/01-basic-var-types.html#footnotes",
    "title": "8  Variables and Basic Data Types",
    "section": "",
    "text": "This means that doubles take up more memory but can store more decimal places. You don’t need to worry about this much in R, and only a little in Python, but in older and more precise languages such as C/C++/Java, the difference between floats and doubles can be important.↩︎\nThere is also an R package dedicated to pure evil that will set F and T randomly on startup. Use this information wisely.↩︎\nnumeric is not really a type, it’s a mode. Run ?mode for more information.↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html",
    "href": "part-gen-prog/01a-math-logical-operators.html",
    "title": "9  Mathematical and Logical Operators",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#objectives",
    "href": "part-gen-prog/01a-math-logical-operators.html#objectives",
    "title": "9  Mathematical and Logical Operators",
    "section": "",
    "text": "Understand and use mathematical operators for mathematical computation\nUnderstand and use logical operators to evaluate complex conditions\nUnderstand and use basic string operations (repetition and concatenation)\nEvaluate expressions using order of operations to predict how the computer will execute code\nTranslate mathematical concepts into a series of computational operations",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#mathematical-operators",
    "href": "part-gen-prog/01a-math-logical-operators.html#mathematical-operators",
    "title": "9  Mathematical and Logical Operators",
    "section": "9.1 Mathematical Operators",
    "text": "9.1 Mathematical Operators\nLet’s first start with a special class of functions that you’re probably familiar with from your math classes - mathematical operators.\nHere are a few of the most important ones:\n\n\n\nTable 9.1: Mathematical operators in R and Python\n\n\n\n\n\nOperation\nR symbol\nPython symbol\n\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\n\n\n\nThese operands are all for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated (and different between R and python).\n\n\n\n\n\n\nExample: Integer and Modular Division\n\n\n\nInteger division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nLet’s demonstrate with the problem 14/3, which evaluates to 4.6666667 when division is used, but has integer part 4 and remainder 2.\n\nRPython\n\n\n14 %/% 3 in R would be 4, and 14 %% 3 in R would be 2.\n\n14 %/% 3\n## [1] 4\n14 %% 3\n## [1] 2\n\n\n\n\n14 // 3\n## 4\n14 % 3\n## 2",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#order-of-operations",
    "href": "part-gen-prog/01a-math-logical-operators.html#order-of-operations",
    "title": "9  Mathematical and Logical Operators",
    "section": "9.2 Order of Operations",
    "text": "9.2 Order of Operations\nBoth R and Python operate under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\nRPython\n\n\n\n1(1+1)^(5-2)\n21 + 2^3 * 4\n33*1^3\n## [1] 8\n## [1] 33\n## [1] 3\n\n\n1\n\nThe items in parentheses are evaluated first, so the expression becomes (2)^(3). Then, exponentiation is performed, yielding 8.\n\n2\n\nExponentiation is performed first (PEMDAS), so the expression becomes 1 + 8*4. Then multiplication is performed, yielding 1 + 32. Addition is the final step, so we get 33.\n\n3\n\nExponentiation is performed first, so this becomes 3*1. Then multiplication is performed, producing a final result of 3.\n\n\n\n\n\n\n\n1(1+1)**(5-2)\n21 + 2**3*4\n33*1**3\n## 8\n## 33\n## 3\n\n\n1\n\nThe items in parentheses are evaluated first, so the expression becomes (2)^(3). Then, exponentiation is performed, yielding 8.\n\n2\n\nExponentiation is performed first (PEMDAS), so the expression becomes 1 + 8*4. Then multiplication is performed, yielding 1 + 32. Addition is the final step, so we get 33.\n\n3\n\nExponentiation is performed first, so this becomes 3*1. Then multiplication is performed, producing a final result of 3.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#simple-string-operations",
    "href": "part-gen-prog/01a-math-logical-operators.html#simple-string-operations",
    "title": "9  Mathematical and Logical Operators",
    "section": "9.3 Simple String Operations",
    "text": "9.3 Simple String Operations\nPython has some additional operators that work on strings. In R, you will have to use functions to perform these operations, as R does not have string operators.\n\n9.3.1 String Operations in R and Python\n\nPythonR\n\n\nIn Python, + will concatenate (stick together) two strings. Multiplying a string by an integer will repeat the string the specified number of times.\n\n\"first \" + \"second\"\n## 'first second'\n\"hello \" * 3\n## 'hello hello hello '\n\n\n\nIn R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\npaste(\"first\", \"second\", collapse = \" \")\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works w/ 2 objects passed in\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n## [1] \"first second\"\n## [1] \"first second\"\n## [1] \"first\"  \"second\"\n## [1] \"first second\"\n## [1] \"a-first b-second c-third d-fourth\"\n## [1] \"a\" \"b\" \"c\"\n## [1] \"abc\"\n\nYou don’t need to understand the details of this code at this point in the class, but it is useful to know how to combine strings in both languages.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#sec-logical-ops",
    "href": "part-gen-prog/01a-math-logical-operators.html#sec-logical-ops",
    "title": "9  Mathematical and Logical Operators",
    "section": "9.4 Logical Operators",
    "text": "9.4 Logical Operators\nLogical variables can be combined through the use of logical operators in much the same way that numerical variables are combined through mathematical operators.\nThere are specific logical operators which are used to aggregate and combine multiple logical variables: the primary logical operators are and, or, and not 1.\nIn pseudocode, which is human-readable logic structured like computer code but without the syntax, we usually write these out in all caps.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n(X XOR Y) requires that one (and only one) of X or Y is true. Sometimes called exclusive or.\n\n\n\n\n\n\n\nTruth Tables: Useful Tools for Understanding Logical Expressions\n\n\n\n\n\nWhen constructing a logical expression that combines Boolean variables, it can be helpful to build a truth table that lists all possible inputs on the left and the output of the operator on the right. A truth table demonstrating the logical operators and, or, not and xor is provided in Table 9.2.\n\n\n\nTable 9.2: Truth table for each of the common logical operators.\n\n\n\n\n\na\nb\na and b\na or b\nnot a\nnot b\na xor b\n\n\n\n\nT\nT\nT\nT\nF\nF\nF\n\n\nT\nF\nF\nT\nF\nT\nT\n\n\nF\nT\nF\nT\nT\nF\nT\n\n\nF\nF\nF\nF\nT\nT\nF\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.3: Logical operators in R and Python. These operators are intended for single values; evaluation of vectors may require different operators. Note the use of ^ for xor in Python!\n\n\n\n\n\nOperation\nR\nPython\n\n\n\n\nand\n&\n& or and\n\n\nor\n|\n| or or\n\n\nnot\n!\nnot\n\n\nxor\nxor()\n^\n\n\n\n\n\n\nWhen writing code, we use the logical operators in R and Python shown in Table 9.3.\n\n\n\n\n\n\nExploring Logical Operators with R and Python\n\n\n\n\n\nWe can generate each entry in the truth table using the relevant logical operators in R and python.\n\nANDORNOTXOR\n\n\nIn R, and comparisons use & as the operator.\n\n\nTRUE & TRUE\n## [1] TRUE\nTRUE & FALSE\n## [1] FALSE\nFALSE & TRUE\n## [1] FALSE\nFALSE & FALSE\n## [1] FALSE\n\nIn Python, and expressions use & as the operator.\n\nTrue & True\n## True\nTrue & False\n## False\nFalse & True\n## False\nFalse & False\n## False\n\nAlternately, in Python, you can also spell out the whole word and use and explicitly.\n\nTrue and True\n## True\nTrue and False\n## False\nFalse and True\n## False\nFalse and False\n## False\n\n\n\nIn R, or is denoted with | (the vertical bar, shift + the button above the enter key on most keyboards).\n\nTRUE | TRUE\n## [1] TRUE\nTRUE | FALSE\n## [1] TRUE\nFALSE | TRUE\n## [1] TRUE\nFALSE | FALSE\n## [1] FALSE\n\nIn Python, or expressions use | as the operator.\n\nTrue | True\n## True\nTrue | False\n## True\nFalse | True\n## True\nFalse | False\n## False\n\nAlternately, in Python, you can also spell out the whole word and use or explicitly.\n\nTrue or True\n## True\nTrue or False\n## True\nFalse or True\n## True\nFalse or False\n## False\n\n\n\nIn R, negation occurs using the ! operator.\n\n!TRUE\n## [1] FALSE\n!FALSE\n## [1] TRUE\n\nIn Python, negation occurs using the not operator.\n\nnot True\n## False\nnot False\n## True\n\n\n\nIn R, exclusive or uses the xor() function.\n\nxor(TRUE, TRUE)\n## [1] FALSE\nxor(TRUE, FALSE)\n## [1] TRUE\nxor(FALSE, TRUE)\n## [1] TRUE\nxor(FALSE, FALSE)\n## [1] FALSE\n\nIn Python, exclusive or uses the ^ operator.\n\nTrue ^ True\n## False\nTrue ^ False\n## True\nFalse ^ True\n## True\nFalse ^ False\n## False\n\nNote that this is why the exponentiation operator in Python is ** instead of ^. Using ^ may produce errors (TypeError: unsupported operand type(s)) or may produce unexpected results with no warning at all, as in the following example.\n\n3^4\n## 7\n\n\n\n\n\n\n\n\n9.4.1 Order of Operations\nJust as with mathematical operators, there is an order of operations to logical operators.\nPrecedence order: (top is evaluated first)\n\nNOT\nAND\nOR\n\n\nRPython\n\n\n\na1 &lt;- TRUE\nb1 &lt;- FALSE\nc1 &lt;- FALSE\n\na1 | b1 & c1 # AND takes precedence\n## [1] TRUE\na1 | (b1 & c1) # same as above, with parentheses\n## [1] TRUE\n(a1 | b1) & c1 # force OR to be first using parentheses\n## [1] FALSE\n\n\n\n\na1 = True\nb1 = False\nc1 = False\n\na1 or b1 and c1 # AND takes precedence\n## True\na1 or (b1 and c1) # same as above, with parentheses\n## True\n(a1 or b1) and c1 # force OR to be first using parentheses\n## False\n\n\n\n\n\n\n9.4.2 De Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements, similar to distributive laws in numerical operations. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\n\n\n\n\nVisual Representation of DeMorgan’s Laws\n\n\n\n\n\nWe can also represent De Morgan’s Laws visually using Venn Diagrams.\n\n\n\nVenn Diagram of Set A and Set B\n\n\nWe will use the convention that .\n\n\n\n\n\n\nDeMorgan’s First Law\n\n\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\nRPython\n\n\n\n!(TRUE | TRUE)\n## [1] FALSE\n!(TRUE | FALSE)\n## [1] FALSE\n!(FALSE | TRUE)\n## [1] FALSE\n!(FALSE | FALSE)\n## [1] TRUE\n\n!TRUE & !TRUE\n## [1] FALSE\n!TRUE & !FALSE\n## [1] FALSE\n!FALSE & !TRUE\n## [1] FALSE\n!FALSE & !FALSE\n## [1] TRUE\n\n\n\n\nnot(True or True)\n## False\nnot(True or False)\n## False\nnot(False or True)\n## False\nnot(False or False)\n## True\n\nnot(True) and not(True)\n## False\nnot(True) and not(False)\n## False\nnot(False) and not(True)\n## False\nnot(False) and not(False)\n## True\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeMorgan’s Second Law\n\n\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\nRPython\n\n\n\n!(TRUE & TRUE)\n## [1] FALSE\n!(TRUE & FALSE)\n## [1] TRUE\n!(FALSE & TRUE)\n## [1] TRUE\n!(FALSE & FALSE)\n## [1] TRUE\n\n!TRUE | !TRUE\n## [1] FALSE\n!TRUE | !FALSE\n## [1] TRUE\n!FALSE | !TRUE\n## [1] TRUE\n!FALSE | !FALSE\n## [1] TRUE\n\n\n\n\nnot(True and True)\n## False\nnot(True and False)\n## True\nnot(False and True)\n## True\nnot(False and False)\n## True\n\nnot(True) or not(True)\n## False\nnot(True) or not(False)\n## True\nnot(False) or not(True)\n## True\nnot(False) or not(False)\n## True",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01a-math-logical-operators.html#footnotes",
    "href": "part-gen-prog/01a-math-logical-operators.html#footnotes",
    "title": "9  Mathematical and Logical Operators",
    "section": "",
    "text": "A fourth commonly used logical operator is exclusive or (xor). xor is True if only one of the two conditions is True, but False if both are True. xor is not a basic boolean operator, as it can be written as a combination of other operators: A xor B = (A or B) and not(A and B).↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mathematical and Logical Operators</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html",
    "href": "part-gen-prog/02-prog-functions.html",
    "title": "10  Functions, Packages, and Environments",
    "section": "",
    "text": "Objectives\nIn addition to variables, functions are extremely important in programming. Functions allow you to repeat a series of steps using different information and get the result. In a way, a function is to a variable as a verb is to a noun - functions are a concise way of performing an action.\nPackages contain groups of functions to accomplish tasks. In order to use functions from a package, you must install the package, and load it.\nEnvironments are important for managing the set of installed packages that is available to use in a project. Environment management is different in R than it is in Python, and for a beginner, the R approach requires a bit less thought. For now, you will pick one Python environment management option and stick with it – later, you may develop opinions on which approach is useful for different tasks.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#objectives",
    "href": "part-gen-prog/02-prog-functions.html#objectives",
    "title": "10  Functions, Packages, and Environments",
    "section": "",
    "text": "Use pre-written functions to perform operations\nUse python environments to manage packages\nInstall and load packages in R and Python\n\nUse pipes to write readable code\n\n\n\n\n\n\n\nTLDR\n\n\n\n\nFunctions are discrete blocks of code that execute a sequence of steps.\nFunctions take arguments as parameters and return values\nPackages contain functions that are connected by a common goal or task sequence.\nPackages must be installed and loaded in order for the functions in the package to be used.\nEnvironments manage the available packages and functions that can be loaded and used.\nIn R, a pipe |&gt; can be used to chain functions together in a more readable way.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#using-functions",
    "href": "part-gen-prog/02-prog-functions.html#using-functions",
    "title": "10  Functions, Packages, and Environments",
    "section": "10.1 Using Functions",
    "text": "10.1 Using Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, mathematical operators (like those above) are a special type of functions.\nWe’re not going to talk about how to create our own functions just yet. Instead, in this chapter, let’s figure out how to use functions.\n\n\n\n\n\n\nCheat Sheets!\n\n\n\nIt may be helpful at this point to print out the R reference card1 and the Python reference card.2 These cheat sheets contain useful functions for a variety of tasks in each language.\n\n\n\n10.1.1 Function Vocabulary\nSuppose I have a function called add(x, y) which takes two numbers and adds them together.\nIn this example, add is the function name, and x and y are parameters: placeholder names for information to be passed into the function. Not all functions have named parameters, but it is common for named parameters to provide some indication of what information is supposed to go in that spot.\nWhen I call the function – that is, I use it to add two numbers together, I have to pass in arguments. Arguments are values which are assigned to parameters in the function and affect the result.\nThis is technical and a bit nit-picky, but it’s good to see information multiple times. We’ll revisit functions in Chapter 14, when we discuss how to write our own functions.\nThe function call is add(x = 3, y = 2), where 3 and 2 are the arguments. The function call is evaluated and returns 5 as the answer (assuming that add does what it says it does).\n\n\n\n\n\n\nFunction Vocabulary in Help Files\n\n\n\nLet’s see these words in a more concrete setting.\n\n\n\nSmall excerpt from the R reference card, showing the which.max and which.min functions\n\n\n\nwhich.max and which.min are the function names\nx is the parameter\n\nWhen I type which.max(x = c(2:10)) into the R console and hit Enter,\n\nc(2:10) = c(2, 3, 4, 5, 6, 7, 8, 9, 10) is the argument\ninside which.max, this argument has the name x (mostly helpful for debugging)\nwhich.max will return 9, which is the index of x with the largest value (10)\n\n\n\nMethods are a special type of function that operate on a specific data type. In Python, methods are applied using the syntax variable.method_name(). So, you can get the length of a string variable my_string using my_string.length().\nR has methods too, but they are invoked differently. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\n\n\n\n\nYour Turn: Cheat Sheet Exploration and Using Functions\n\n\n\n\nProblemR SolutionPython Solution\n\n\nTry out some of the functions mentioned on the R and Python cheat sheets.\nCan you figure out how to define a list or vector of numbers? If so, can you use a function to calculate the maximum value?\nCan you find the R functions that will allow you to repeat a string variable multiple times or concatenate two strings?\nCan you do this task in Python?\n\n\n\n# Define a vector of numbers\nx &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the maximum\nmax(x)\n## [1] 5\n\n# function to repeat a variable multiple times\nrep(\"test\", 3)\n## [1] \"test\" \"test\" \"test\"\n# Concatenate strings, using \"ing... \" as the separator\npaste(rep(\"test\", 3), collapse = \"ing... \")\n## [1] \"testing... testing... test\"\n\n\n\n\n# Define a list of numbers\nx = [1, 2, 3, 4, 5]\n\n# Calculate the maximum\nmax(x)\n## 5\n\n# Repeat a string multiple times\nx = (\"test\", )*3 # String multiplication \n                 # have to use a tuple () to get separate items\n# Then use 'yyy'.join(x) to paste items of x together with yyy as separators\n'ing... '.join(x)\n## 'testing... testing... test'\n\n\n\n\n\n\n\n\n10.1.2 Using R and Python as Overpowered Calculators\nNow that you’re familiar with how to use functions, if not how to define them, you are capable of using R or python as a very fancy calculator. Obviously, both languages can do many more interesting things, which we’ll get to, but let’s see if we can make R and Python do some very basic stuff that hopefully isn’t too foreign to you.\n\n\n\n\n\n\nDemonstration: Triangle Side Length\n\n\n\n\n\n\nA right triangle with sides a, b, and hypotenuse c labeled.\n\n\nConsider this triangle. I’ve measured the sides in an image editor and determined that \\(a = 212\\) pixels, \\(b = 345\\) pixels, and \\(c = 406\\) pixels. I suspect, however, that my measurements aren’t quite right - for one thing, I tried to measure in the center of the line, but it wasn’t as easy to do that well on the diagonal.\nLet’s assume that my measurements for \\(a\\) and \\(b\\) are accurate and calculate how far off my estimate was for side \\(c\\). We will use the sqrt function to accomplish this task – in R, sqrt(), and in Python, math.sqrt(). In Python we need to run import math first to load the math library before we can use the math.sqrt function.\n\nRPython\n\n\n\n1a &lt;- 212\nb &lt;- 345\nc_meas &lt;- 406\n\n2c_actual &lt;- sqrt(a^2 + b^2)\n\n3pct_error &lt;- (c_meas - c_actual)/c_actual * 100\npct_error\n## [1] 0.2640307\n\n\n1\n\nDefine variables for the 3 sides of the triangle\n\n2\n\nCalculate what side \\(c\\) should be, according to the Pythagorean Theorem\n\n3\n\nCalculate the percent error between the measured and actual value\n\n\n\n\n\n\n\n1import math\n\n2a = 212\nb = 345\nc_meas = 406\n\n3c_actual = math.sqrt(a**2 + b**2)\n\n4pct_error = (c_meas - c_actual)/c_actual * 100\npct_error\n## 0.264030681414134\n\n\n1\n\nImport the math package to make the sqrt function available\n\n2\n\nDefine variables for the 3 sides of the triangle\n\n3\n\nCalculate what side \\(c\\) should be, according to the Pythagorean Theorem\n\n4\n\nCalculate the percent error between the measured and actual value\n\n\n\n\n\n\n\nInteresting, I wasn’t as inaccurate as I thought!\n\n\n\n\n\n\n\n\nYour Turn: Defining Variables and Using Functions\n\n\n\nOf course, if you remember trigonometry, we don’t have to work with right triangles. Let’s see if we can use trigonometric functions to do the same task with an oblique triangle.\n\nProblemR solutionPython solution\n\n\nJust in case you’ve forgotten your Trig, the Law of Cosines says that \\[c^2 = a^2 + b^2 - 2 a b \\cos(C),\\] where \\(C\\) is the angle between sides \\(a\\) and \\(b\\).\n\n\n\nAn oblique triangle with sides labeled a, b, and c, and angles labeled as A, B, C with capital letter opposite the lowercase side.\n\n\nI measure side \\(a = 291\\) pixels, side \\(b = 414\\) pixels, and the angle between \\(a\\) and \\(b\\) to be \\(67.6^\\circ\\). What will I likely get for the length of side \\(c\\) in pixels?\nRemember to check whether R and python compute trig functions using radians or degrees! As a reminder, \\(\\pi\\) radians = \\(180^\\circ\\).\n\n\n\n# Define variables for the 3 sides of the triangle\na &lt;- 291\nb &lt;- 414\nc_angle &lt;- 67.6\nc_actual &lt;- sqrt(a^2 + b^2 - 2*a*b*cos(c_angle/180*pi))\nc_actual\n## [1] 405.2886\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n# To get the sqrt and cos functions, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 291\nb = 414\nc_angle = 67.6\nc_actual = math.sqrt(a**2 + b**2 - 2*a*b*math.cos(c_angle/180*math.pi))\nc_actual\n## 405.28860699402117\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n\n\nCongratulations, if you used a TI-84 in high school to do this sort of stuff, you’re now just about as proficient with R and python as you were with that!",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#environments",
    "href": "part-gen-prog/02-prog-functions.html#environments",
    "title": "10  Functions, Packages, and Environments",
    "section": "10.2 Environments",
    "text": "10.2 Environments\nYou may have noticed in the Python example above, we had to import math before we used the math.sqrt() and math.cos() functions. The math package is a built-in package in Python, so we don’t have to install the package in order to use it (installing Python installs math). But, before we can use math.sqrt(), we have to import or load the math package into our working space. In Python, this working space is called the object space; in R, it’s called the global environment. The terminology here differs between R and Python (which is confusing), but conceptually, it’s important to distinguish between the set of things that are available to use when you are writing a program and the set of things that are available to load.\n\n\n\n\n\n\n10.2.1 Advanced: Why environments?\n\n\n\n\n\nImagine that you’re an accomplished programmer, and you are juggling multiple different projects. Each project uses some of the same packages, but some different packages as well. You open up a project that you haven’t run in a year, and you find out that one of the packages you’ve updated more recently breaks a bunch of code you wrote a year ago, because the functions in the package have been renamed.\nWhat could prevent this from happening?\nOne way to solve this problem is to store the packages used in each project inside the project directory, in what we might call a project environment or virtual environment. This will keep each project isolated from the others, so that if you update a package in one project, it doesn’t affect any other project.\nHowever, this approach results in a lot of duplication: for one thing, you have copies of each package hanging around in every folder on your computer. That’s not storage efficient, but it does keep your code from breaking as frequently.\nPython programmers prefer the project-specific approach, while R programmers default to installing packages at the user or system level.\n\n\n\n\n10.2.2 Vocabulary by Analogy: Functions, Packages, Environments, and Repositories\nThink of a package as a book, with each page of the book containing a specific function.\nThe package repository (CRAN, PyPi, etc.) is a set of packages, roughly corresponding to a physical library or a bookstore - you can access the packages and install them (take them home). Unlike a physical library, though, usually you don’t have to return the packages you’ve checked out!\nThe set of packages you have installed corresponds to the books you have at home. You can use any of the functions (pages of those books) when you want to program (or access information).\n\n\n\n\n\n\n\n\nPackage Repository. Image Source\n\n\n\n\n\n\n\nLocally Installed Packages (my bookshelf)\n\n\n\n\n\n\n\nOne Package with many functions (pages)\n\n\n\n\n\n\n\nOne function (single page) from a package.\n\n\n\n\n\n\nFigure 10.1: A package repository, like PyPi or CRAN, contains a set of possible packages to install (but, of course, there are packages that you can install from other places, as well). Once you have installed a package, you have a local library, like the books I keep near my computer. Each book has (usually) a coherent topic, and each page has a different set of information, corresponding in this analogy to a different function.\n\n\n\nHow the packages are organized can reasonably differ based on how you prefer your house to be arranged, just as package organization differs significantly in R and Python.\n\nPython virtual environments arrange packages by project location, so that the package versions needed for each project are stored together, and other projects may have different versions.\nAnalogy: Books in my house are sorted by task and audience (programming books at my desk, fiction books near the couch, kid books in their bedrooms).\nR packages, by default, are installed in user libraries or system wide, and the same package version is used for all projects.\nAnalogy: All books in the house are kept in a single wall-to-wall set of bookshelves in the living room.\n\nThe global environment (R) or object space (Python) is the collection of objects (functions, variables, etc.) that are immediately available to the user.\nRegardless of how packages are managed (virtual environments or centrally), to access the package’s functions, I have to go get the book and open it. This step corresponds most closely to loading a package into your global environment/object environment.\nAgain, R and Python prefer to manage this step differently.\n\nIn R, functions from all loaded packages are available to the user directly using the function name. When multiple packages with lots of functions are loaded, this can be … messy, as in Figure 10.2 (a).\nIn Python, the recommended way to load packages is to import the package and possibly give it a shorter alias; the functions must still be referenced as pkg.functionName or alias.functionName instead of just functionName. This more closely matches Figure 10.2 (b), where pages are still contained in notebooks, and thus require an extra step to access.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) R environment mangement Image source\n\n\n\n\n\n\n\n\n\n\n\n(b) Python environment management Image source\n\n\n\n\n\n\n\nFigure 10.2: R and python conventions for environments are different – in R, all package functions are loaded into the environment as single objects, which can get messy if there are a lot of functions and/or a lot of loaded packages. In Python, package functions are typically loaded within their packages, so the chaos is more contained, but you have an additional step of having to find the function within the specific package containing that function. This is more verbose, but also ensures that you don’t have conflicts between two functions in different packages with the same name.\n\n\n\n\n\n\n\n\n\n\nAdvanced: Python-like R environments\n\n\n\n\n\nSome R programmers have adopted the python philosophy of project-specific package management, using an R package called renv [1].\nrenv documentation can be found here if you wish to try it out. I find that it is most useful for projects where package updates may break things - e.g. projects which run on shared systems or which are intended to work for a long period of time without maintenance.\nIf you want to use renv, you can do that by following these steps:\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n# Activate renv for a project\nrenv::activate()\n\n# this will install from github or CRAN\nrenv::install(c(\"pkg1\", \"pkg2\", \"githubuser/pkg3\")) \n\nI use renv for this textbook, because if a package update breaks things, I need to systematically check all the code chunks in the textbook to make sure they all work. I don’t want to do that every time someone fixes a minor bug, so I don’t update the packages the textbook uses more than once a semester (normally).\n\n\n\n\n\n10.2.3 Python Environments\nIn Python, packages are usually managed at the project level by creating virtual environments. The different environment management options in Python are one of the things that can make starting to learn python so difficult - it can be hard to make sure you’re using the right environment. virtualenv and conda are the main options for environment management. conda is sometimes preferred for scientific computing because it handles the complex dependencies that arise from large packages like numpy and scipi and pandas a bit better than pip does alone.\nBy default, Chapter 2 just installs python at the system level.\n\n\n\n\n\n\nDecision Fatigued?\n\n\n\nIf you don’t care about the nuances of which python environment management option you should use, follow the venv instructions for Python below using the R console. venv is relatively simple and straightforward and has less overhead.\n\n\n\n\n\n\n\n\nConsistency is critical\n\n\n\nI highly recommend that you pick one of these options and use it consistently, rather than trying the advantages and disadvantages of each option in different projects. If you switch around between virtualenv and conda, you can very quickly reach the point where you have 15 different python environments on your computer and you don’t have any idea which one you should be using. Python environment management is one of the hardest things to deal with as a beginner Python programmer.\n\n\n\nPython Environment, by Randall Munroe. CC-By-NC-2.5. The Python environmental protection agency wants to seal it in a cement chamber, with pictorial messages to future civilizations warning them about the danger of using sudo to install random Python packages.\n\n\n\n\n\n10.2.3.1 venv (Virtual Environments)\nvirtualenv (venv) can be installed using either RStudio or the system terminal.\nItems within &lt; &gt; (as well as the &lt;&gt; characters) are intended to be replaced with values specific to your situation.\n\n\n\n\n\n\nvenv setup\n\n\n\n\n\n\nSystem terminalRStudio\n\n\nIn your system terminal, navigate to your project directory.\n\ncd &lt;project-directory&gt;\npip3 install virtualenv # install virtual environments\n\n# Create a virtual environment\nvirtualenv &lt;env-name&gt;\n\n# Activate your virtual environment\nsource &lt;env-name&gt;/bin/activate\n\n# Install packages\npip install &lt;pkg1&gt; &lt;pkg2&gt; &lt;pkg3&gt;\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nvirtualenv_create(envname = \"&lt;env-name&gt;\",\n                  packages = c(\"&lt;pkg1&gt;\", \"&lt;pkg2&gt;\", \"&lt;pkg3&gt;\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\n# Activate your virtual environment\nuse_virtualenv(\"&lt;env-name&gt;\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"&lt;pkg1&gt;|&lt;pkg2&gt;|&lt;pkg3&gt;\",\n     x = as.character(py_list_packages(envname = \"&lt;env-name&gt;\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\n\n\n\n\n\n\n10.2.3.2 Conda\nconda (aka Anaconda) can be installed using RStudio or the system terminal. You must have conda installed for these instructions to work. You can install conda system-wide by following these instructions, or, if you only intend to use Python within RStudio, you can install the reticulate package and then run reticulate::install_miniconda() to install miniconda to a directory where RStudio will be able to find it.\nThese steps have been generally constructed from [2].\nItems within &lt; &gt; (as well as the &lt;&gt; characters) are intended to be replaced with values specific to your situation.\n\n\n\n\n\n\nConda setup\n\n\n\n\n\n\nSystem terminalRStudio\n\n\nYou must have conda installed for these instructions to work!\n\ncd &lt;project-directory&gt;\n# Create conda environment and install specific python version and packages \nconda create --prefix ./&lt;env-name&gt; python=&lt;python-version&gt; &lt;pkg1&gt; &lt;pkg2&gt; &lt;pkg3&gt; \n\n# Activate your virtual environment\nconda activate ./&lt;env-name&gt;\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"./&lt;env-name&gt;/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nconda_create(envname = \"&lt;env-name&gt;\",\n             packages = c(\"&lt;pkg1&gt;\", \"&lt;pkg2&gt;\", \"&lt;pkg3&gt;\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\n# Activate your virtual environment\nuse_condaenv(\"&lt;env-name&gt;\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"&lt;pkg1&gt;|&lt;pkg2&gt;|&lt;pkg3&gt;\",\n     x = as.character(py_list_packages(envname = \"&lt;env-name&gt;\")$package))\n\nRestart your R session before you start trying to work in python.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#packages",
    "href": "part-gen-prog/02-prog-functions.html#packages",
    "title": "10  Functions, Packages, and Environments",
    "section": "10.3 Packages",
    "text": "10.3 Packages\nBoth R and python have a very robust system for extending the language with user-written packages. These packages will give you access to features that aren’t present in the base language, including new statistical methods, all sorts of plotting and visualization libraries, ways of interacting with data that are way more convenient than the default base language methods, and more.\n\n10.3.1 Package repositories\nBoth R and Python have package systems, though generally, R is a bit more straightforward to deal with than python (in my opinion). Python’s extra environment management systems sometimes come with additional package repositories, and it can be hard to identify the differences between them. By contrast, all R packages seem to go through the same basic installation process and are just hosted in different places. This is largely a result of the difference between R and Python’s environment management strategies.\n\n\n\n\nFormally Published\nInformally Published/Beta\n\n\n\n\nR\nCRAN, Bioconductor\ngithub and other version control. See the remotes package documentation for all of the options.\n\n\nPython\nPyPi\ngithub and other version control systems\n\n\n\n\n\n\n\n\n\nDo you NEED to use a new package? (Advanced)\n\n\n\n\n\nThere are tons of considerations to think about when using a new package, like how well it’s maintained, how many dependencies it has, and whether the developers of the package prioritize backwards-compatibility.\nWith each package you add, your project becomes more complex. On the other hand, with each package you add, you should be able to do more things, and hopefully, you’ll be able to leverage code from other developers to accomplish more complex tasks.\nThere’s a critical balance between complexity and trying not to reinvent the wheel. As you go through this book, you may want to consider the different packages presented in light of this complexity cost/benefit analysis.\n\n\n\nBefore we talk about how to install packages, we need to step back and think a little bit about the pros and cons of different ways of managing packages, because the most common R and python setups use very different approaches.\n\n\n10.3.2 Package Installation\n\n10.3.2.1 Installing packages in Python\nMany of the instructions here are modified from [3].\nWhichever method (venv, conda) you use to manage your Python environment, when you go to install a new package, you have a few different options for how to do so.\n\nSystem consoleRStudioIPython Magic\n\n\nIn python, you will typically want to install packages using a system terminal.\n\nMake sure your virtual environment/conda environment is activated\nInstallation commands:\n\nIf you are using venv, pip3 install &lt;package name&gt; should install your package.\nIf you are using conda, conda install &lt;package name&gt; is preferable, and if that doesn’t work, then try using pip3 install &lt;package name&gt;.\n\n\n\n# If you're using virtualenv\npip install &lt;pkg1&gt;\n\n# If you're using conda, try this first\nconda install &lt;pkg1&gt;\n# If that fails, try pip\n\n\n\n\nMake sure R is using the correct python installation\nIn the R terminal, run reticulate::py_install(\"package name\")\n\n\n\nThis is less elegant, but nearly foolproof because RStudio will install the package in the version of python it can find.\n\nAt the top of the chunk, write %pip install &lt;package name&gt;\nRun this code (Cmd/Ctrl + Enter)\nComment the code out, so that you aren’t reinstalling the package every time you run the chunk.\n\n\n%pip install &lt;pkg1&gt;\n\nA slightly less elegant but more robust way to do this is to use the sys package. Loading the sys package ensures that you’re using the version of python that your file will be compiled with to install the package.\n\nimport sys\n# For pip installation\n!{sys.executable} -m pip install &lt;pkg1&gt;\n\n# For conda installation\n!{sys.executable} -m conda install &lt;pkg1&gt;\n\nOnce you’ve installed the package on your machine, you can comment these lines out so that they don’t run every time - this makes it a bit easier when you try to run old code on a new machine, as you can just uncomment those lines.\n\n\n\n\n\n10.3.2.2 Installing packages in R\nPackage management in R is a bit simpler than package management in python.\nIn almost every case, you can install packages from CRAN with install.packages(\"package name\"). If your package is not on CRAN, and is instead on e.g. GitHub, you may have to use the remotes package to install it with remotes::install_github(\"user/repo\")\n\n# CRAN packages\ninstall.packages(\"&lt;pkg1&gt;\")\n\n# Github packages\nremotes::install_github(\"username/reponame\")\n\n\n\n\n\n\n\n10.3.2.3 Demo: Package Installation\n\n\n\nSuppose you want to install the lme4 package in R and the corresponding pymer4 package in python.\n\nRPython (System console)Python (RStudio)Python (Ipython magic)\n\n\n\n1install.packages(\"lme4\")\n\n2remotes::install_github(\"lme4/lme4\", dependencies=TRUE)\n\n\n1\n\nInstall the package from CRAN (best choice if you don’t know you need the development version).\n\n2\n\nInstall the development version of the package from the GitHub repository.\n\n\n\n\nYou should only run one of these lines of code, and if you don’t know which one to run, then stick with the first line.\n\n\n\npip install pymer4 # venv\n\nconda install pymer4 # conda\n\n\n\nIn the R terminal (yes, that’s weird), run this command:\n\nreticulate::py_install(\"pymer4\")\n## Using virtual environment '/home/susan/.virtualenvs/book' ...\n\n\n\n\n%pip install pymer4\n\n\n\n\n\n\n\n\n\n10.3.3 Loading Packages\nOnce you have the package installed, you need to load the package into memory so that you can use the functions and data contained within. Again, R and python differ slightly in how programmers conventionally handle this process.\n\nR: Load all of the package’s functions, overwriting already loaded functions if necessary\nPython: Load all of the package’s functions, contained within an object that is either the package name or a shortened alias.\n\nNow, both R and python can load packages in either way, so this isn’t an either/or thing - it’s about knowing what the conventions of the language are, and then deciding whether or not it is appropriate to follow those conventions in your project. Figure 10.2 contains a visual analogy for the differences between these two approaches.\n\n10.3.3.1 R’s Package Strategy: Import the whole package and all functions\n\n\n\n\n\n\nDemo\n\n\n\nTo demonstrate this approach, let’s create a simple plot with a plotting library (ggplot2 in R, seaborn in Python). We’ll first load the library (R) or libraries (Python), and then we can look at how many functions are available to the user. We can plot the number of functions loaded from each package using a bar plot.\n\nRPython\n\n\nAll of the other packages except for ggplot2 in this plot are present by default in any new R environment.\n\nlibrary(ggplot2)\n\n1pkgs &lt;- search()\n2pkgs &lt;- pkgs[grep(\"package:\",pkgs)]\n3all_fns &lt;- lapply(pkgs, function(x) as.character(lsf.str(x)))\n4pkg_fns &lt;- data.frame(pkg = rep(pkgs, sapply(all_fns, length)),\n                      fn = unlist(all_fns))\n5pkg_fns$pkg &lt;- gsub(\"package:\", \"\", pkg_fns$pkg)\n\n\n6ggplot(pkg_fns, aes(x = pkg, y = after_stat(count), fill = pkg)) +\n  geom_bar() + theme(legend.position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n\n\n1\n\nList all containers that have been loaded (packages, but also things in the global environment and things that are autoloaded)\n\n2\n\nFind only packages (discard things in the global environmente and autoloads)\n\n3\n\nGet all functions available to the user in each loaded package\n\n4\n\nCreate a data frame with package and function names\n\n5\n\nRemove “package:” from the package name\n\n6\n\nCreate the plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn python, there are built-in functions (builtins). I have then loaded the packages I typically use for plotting (seaborn, seaborn.objects, matplotlib), manipulating data (pandas, numpy), and standard math and statistics libraries to roughly attempt to match the functionality available in base R + ggplot2.\n\nimport seaborn as sns\nimport seaborn.objects as so\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport statistics\n\npkgs = pd.DataFrame({\n1  \"name\": [\n    \"builtins\",        \"math\",          \"statistics\", \n    \"seaborn\",         \"seaborn\",       \"matplotlib\", \n    \"pandas\",          \"numpy\"                           ], \n2  \"functions\": [\n    dir(\"builtins\"),   dir(math),        dir(statistics),\n    dir(sns),          dir(so),          dir(plt),\n    dir(pd),           dir(np)                           ]\n  })\n\n3pkgs = pkgs.explode('functions')\n\n4pkgs = pkgs[~pkgs.functions.str.contains(\"__\")]\n\nplt.clf()\n5plot = sns.countplot(pkgs, x = \"name\", hue = \"name\")\nplot.set_title(\"Number of Functions in Common Python Packages\")\nplot.set_xlabel(\"Package\")\nplot.set_ylabel(\"# Functions\")\nplt.show()\n\n\n1\n\nCreate a data frame with all of the packages roughly corresponding to objects available in R after ggplot2 is loaded.\n\n2\n\nFind all functions in each package using dir(x)\n\n3\n\nExpand the list of functions so that we have a simple data frame. apply created a list which was nested inside the data frame, so expanding it involves repeating the values in the un-nested rows for each item in the nested list. See Chapter 28 for more details of how this operation works.\n\n4\n\nFilter out functions that start with __ (these aren’t functions a user would typically call directly)\n\n5\n\nGenerate the plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.3.2 Python’s Package Strategy: Use functions from the package without loading everything\nIn python, you can use import package as nickname, or you can just use import package and reference the package name directly. There are some packages which have typical aliases, and it’s best to use those so that you can look things up and not get too confused.\n\nCommon Python package aliases\n\n\n\n\n\n\n\nPackage\nCommon Alias\nExplanation\n\n\n\n\npandas\npd\nshorter\n\n\nnumpy\nnp\nshorter\n\n\nseaborn\nsns\nThis is a reference to Samuel Norman Seaborn, played by Rob Lowe, in the TV show The West Wing\n\n\nplotnine\np9\n\n\n\nBeautifulSoup (bs4)\nbs\nBeautifulSoup is a reference to Alice in Wonderland. The package name in PyPi is actually bs4.\n\n\n\n\n\n\n\n\n\nDemo\n\n\n\nLet’s see what our R environment looks like if we start fresh and do it the python way.\n\nRPython\n\n\n\ntry(detach(package:ggplot2))\n\n# This code lists all the functions available to be called\npkgs &lt;- search()\npkgs &lt;- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns &lt;- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns &lt;- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg &lt;- gsub(\"package:\", \"\", pkg_fns$pkg)\n\nggplot2::ggplot(pkg_fns, ggplot2::aes(x = pkg, fill = pkg)) +\n  ggplot2::geom_bar() + \n  ggplot2::theme(legend.position = \"none\") + \n  ggplot2::xlab(\"Package\") + ggplot2::ylab(\"# Functions\")\n\n\n\n\n\n\n\n\nggplot2 does not appear in the chart because it is not loaded. However, we can still access its functions by using package::function to tell R where to find the function.\n\n\nHere, we’ll plot the functions as loaded in R, for simplicity.\n\npkg_fns = r.pkg_fns\n\nplt.clf()\nplot = sns.countplot(pkg_fns, x = \"pkg\", hue = \"pkg\")\nplot.set_title(\"Number of Functions in Common R Packages\")\nplot.set_xlabel(\"Package\")\nplot.set_ylabel(\"# Functions\")\nplt.show()",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#pipes",
    "href": "part-gen-prog/02-prog-functions.html#pipes",
    "title": "10  Functions, Packages, and Environments",
    "section": "10.4 Pipes",
    "text": "10.4 Pipes\nPipes are useful items for moving things from one place to another. In programming, and in particular, in data programming, pipes are operators that let us move data around. In R, we have two primary pipes that are similar (you may see both used if you google for code online). Any R version after 4.1 has a built-in pipe, |&gt;; the tidyverse libraries use a pipe from the magrittr package, %&gt;%.\nFor right now, it’s ok to think of the two pipes as essentially the same (but you can read about the differences [4]).\nFundamentally, a pipe allows you to take a function b() and apply it to x, like b(x), but write it as x |&gt; b() or x %&gt;% b(). This is particularly useful in cases where there are multiple sequential analysis steps, because where in regular notation you have to read the functions from the inside out to understand the sequential steps, with pipes, you have a clear step-by-step list of the order of operations.\nIn Python, there is a pipe function in the Pandas library that works using .pipe(function) notation [5]. From what I’ve seen reading code online, however, pipes are less commonly used in Python code than they are in R code. That’s ok - languages have different conventions, and it is usually best to adopt the convention of the language you’re working in so that your code can be read, run, and maintained by others more easily.\n\n\n\n\n\n\nTry it out: Pipes\n\n\n\n\nProblemR solutionPython solution\n\n\nGenerate 100 draws from a standard normal distribution and calculate the mean.\nIn R, simulate from a normal distribution with rnorm. In python, use np.random.normal - you’ll have to import numpy as np first.\nUse 3 approaches: 1. Store the data in a variable, then calculate the mean of the variable 2. Calculate the mean of the data by nesting the two functions (e.g. mean(generate_normal(100)) in pseudocode) 3. Calculate the mean of the data using the pipe (e.g. generate_normal(100) |&gt; mean())\nConsider: What are the advantages and disadvantages of each approach? Would your answer change if there were more steps/functions required to get to the right answer?\n\n\n\ndata &lt;- rnorm(100)\nmean(data)\n## [1] -0.06164479\n\nmean(rnorm(100))\n## [1] -0.09182132\n\nlibrary(magrittr) # load the pipe %&gt;%\n\nrnorm(100) %&gt;%\n  mean()\n## [1] 0.04240459\n\nrnorm(100) |&gt; mean()\n## [1] 0.07159802\n\n\n\nIn python, task 3 isn’t really possible, because of the way Python function chaining works, but task 2 is basically the equivalent.\n\nimport numpy as np\nimport pandas as pd\n\nnums = pd.Series(np.random.normal(size = 100))\nnums.mean()\n## np.float64(-0.0014549525416426247)\n\nnp.random.normal(size=100).mean()\n## np.float64(0.013160748013409534)\n\nThe conclusion here is that it’s far easier to not use the pipe in python because the .function notation that python uses mimics the step-by-step approach of pipes in R even without using the actual pipe function. When you use data frames instead of Series, you might start using the pipe, but only in some circumstances - with user-defined functions, instead of methods. Methods are functions that are attached to a data type (technically, a class) and only work if they are defined for that class - for instance, .mean() is defined for both Pandas series and numpy arrays.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#sec-using-functions-refs",
    "href": "part-gen-prog/02-prog-functions.html#sec-using-functions-refs",
    "title": "10  Functions, Packages, and Environments",
    "section": "10.5 References",
    "text": "10.5 References\n\n\n\n\n[1] K. Ushey and H. Wickham, Renv: Project environments. 2023 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n[2] D. Blackwood, “How to use python in r with reticulate and conda. Save the data,” Nov. 04, 2021. [Online]. Available: https://medium.com/save-the-data/how-to-use-python-in-r-with-reticulate-and-conda-36685534f06a. [Accessed: Jan. 23, 2023]\n\n\n[3] G. Makarov, “Use python in rstudio. RPubs,” May 02, 2022. [Online]. Available: https://rpubs.com/georgy_makarov/897844. [Accessed: Jan. 23, 2023]\n\n\n[4] S. Machlis, “Use the new r pipe built into r 4.1. InfoWorld,” Jun. 10, 2021. [Online]. Available: https://www.infoworld.com/article/3621369/use-the-new-r-pipe-built-into-r-41.html. [Accessed: Jan. 13, 2023]\n\n\n[5] shadowtalker, “Answer to \"functional pipes in python like %&gt;% from r’s magrittr\". Stack overflow,” Jun. 24, 2015. [Online]. Available: https://stackoverflow.com/a/31037901/2859168. [Accessed: Jan. 13, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#footnotes",
    "href": "part-gen-prog/02-prog-functions.html#footnotes",
    "title": "10  Functions, Packages, and Environments",
    "section": "",
    "text": "From https://cran.r-project.org/doc/contrib/Short-refcard.pdf↩︎\nFrom http://sixthresearcher.com/wp-content/uploads/2016/12/Python3_reference_cheat_sheet.pdf↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions, Packages, and Environments</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html",
    "href": "part-gen-prog/03-data-struct.html",
    "title": "11  Data Structures",
    "section": "",
    "text": "Objectives\nThis chapter introduces some of the most important structures for storing and working with data: vectors, matrices, lists, and data frames.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#objectives",
    "href": "part-gen-prog/03-data-struct.html#objectives",
    "title": "11  Data Structures",
    "section": "",
    "text": "Understand the differences between lists, vectors, data frames, matrices, and arrays in R and python\nUse location-based indexing in R or python to pull out subsets of a complex data object\nUse logical indexing in R or python to pull out subsets of a data object\nUnderstand categories of mathematical, descriptive, and set operations on vectors\nUse vector operations in R and python to perform simple calculations\n\n\n\n\n\n\n\nPython Package Installation\n\n\n\n\n\nYou will need the numpy and pandas packages for this section. Pick one of the following ways to install python packages:\n\nSystem TerminalR TerminalPython Terminal\n\n\n\npip3 install numpy pandas lxml\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don’t try to install packages this way).\n\nreticulate::py_install(c(\"numpy\", \"pandas\", \"lxml\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called “IPython magic” commands, so if it doesn’t work for you, try the System Terminal method instead.\n\n%pip3 install numpy pandas lxml",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "href": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "title": "11  Data Structures",
    "section": "11.1 Data Structures Overview",
    "text": "11.1 Data Structures Overview\nIn Chapter 8, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complex arrangements of information, but they are still (usually) created using the same data types we have previously discussed.\n\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\n\n\n\n\n\nOpinionated Structures\n\n\n\n\n\nThose of you who have taken programming classes that were more computer science focused will realize that I am leaving out a lot of information about lower-level structures like pointers. I’m making a deliberate choice to gloss over most of those details in this chapter, because it’s already hard enough to learn 2 languages worth of data structures at a time. In addition, R doesn’t have pointers No Pointers in R, [1], so leaving out this material in python streamlines teaching both two languages, at the cost of overly simplifying some python concepts. If you want to read more about the Python concepts I’m leaving out, check out [2].\n\n\n\nIn any data structure, it’s important to be able to pull smaller pieces of data out of the structure. We do this via indexing.\nThere are three main approaches to accessing information using indexes:\n\nObject Names\nIn some cases, components of a data structure are named and can be accessed using those names.\nLocation\nThink of a location index as accessing the nth item in a list, or accessing cell A5 in an Excel spreadsheet - you have strict directions as to what row/column or item to get.\nLogical Indexing\nIn a logical index, you access all items in a structure for which a condition is TRUE. This would be like making a list of family members, and then assigning bedtimes using a statement like “all of the children go to bed at 8pm” - first you decide whether a person is a child, and then you can assign the appropriate bedtime if child is true.\n\nIn both R and Python, we will primarily use square brackets to index different data types. When the data type is rectangular (has both rows and columns), we will use [row, column] syntax – that is, [1, 3] says access the first row, third column. When the data type is a vector, we will use [item] indexing.\nAnother important difference to keep in mind is that in R, items are 1-indexed – that is, the first item in a list x is x[1]. In Python, on the other hand, items are 0-indexed – the first item in a list y is y[0].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#vectors",
    "href": "part-gen-prog/03-data-struct.html#vectors",
    "title": "11  Data Structures",
    "section": "11.2 Vectors",
    "text": "11.2 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want, as illustrated using Lego in Figure 11.11.\n\n\n\n\n\n\nFigure 11.1: Vectors of different data types illustrated via Lego. Each vector has a range of different values (hues), and vectors are represented by different size bricks that roughly correspond to storage requirements.\n\n\n\n\n\n\n\n\n\nDemo: Creating Vectors\n\n\n\n\nRPython VectorsPython Series (Pandas)\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi &lt;- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n## [1] 3\ndigits_pi[2]\n## [1] 1\ndigits_pi[3]\n## [1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n## numeric(0)\ndigits_pi[11]\n## [1] 5\n\n# Print out the vector\ndigits_pi\n##  [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\n\nIn python, we create vectors using the array function in the numpy module. To add a python module, we use the syntax import &lt;name&gt; as &lt;nickname&gt;. Many modules have conventional (and very short) nicknames - for numpy, we will use np as the nickname. Any functions we reference in the numpy module will then be called using np.fun_name() so that python knows where to find them.2\n\nimport numpy as np\ndigits_list = [3,1,4,1,5,9,2,6,5,3,5]\ndigits_pi = np.array(digits_list)\n\n# Access individual entries\ndigits_pi[0]\n## np.int64(3)\ndigits_pi[1]\n## np.int64(1)\ndigits_pi[2]\n## np.int64(4)\n\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\ndigits_pi[0]\n## np.int64(3)\ndigits_pi[11] \n## IndexError: index 11 is out of bounds for axis 0 with size 11\n\n# multiplication works on the whole vector at once\ndigits_pi * 2\n## array([ 6,  2,  8,  2, 10, 18,  4, 12, 10,  6, 10])\n\n# Print out the vector\nprint(digits_pi)\n## [3 1 4 1 5 9 2 6 5 3 5]\n\n\n\nPython has multiple things that look like vectors, including the pandas library’s Series structure. A Series is a one-dimensional array-like object containing a sequence of values and an associated array of labels (called its index).\n\nimport pandas as pd\n\ndigits_pi = pd.Series([3,1,4,1,5,9,2,6,5,3,5])\n\n# Access individual entries\ndigits_pi[0]\n## np.int64(3)\ndigits_pi[1]\n## np.int64(1)\ndigits_pi[2]\n## np.int64(4)\n\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\ndigits_pi[0]\n## np.int64(3)\ndigits_pi[11] # This errors out\n## KeyError: 11\n\n# logical indexing works here too\ndigits_pi[digits_pi &gt; 3]\n## 2     4\n## 4     5\n## 5     9\n## 7     6\n## 8     5\n## 10    5\n## dtype: int64\ndigits_pi.loc[digits_pi &gt; 3]\n## 2     4\n## 4     5\n## 5     9\n## 7     6\n## 8     5\n## 10    5\n## dtype: int64\n\n# simple multiplication works in a vectorized manner\n# that is, the whole vector is multiplied at once\ndigits_pi * 2\n## 0      6\n## 1      2\n## 2      8\n## 3      2\n## 4     10\n## 5     18\n## 6      4\n## 7     12\n## 8     10\n## 9      6\n## 10    10\n## dtype: int64\n\n# Print out the series\nprint(digits_pi)\n## 0     3\n## 1     1\n## 2     4\n## 3     1\n## 4     5\n## 5     9\n## 6     2\n## 7     6\n## 8     5\n## 9     3\n## 10    5\n## dtype: int64\n\nThe Series object has a list of labels in the first printed column, and a list of values in the second. If we want, we can specify the labels manually to use as e.g. plot labels later:\n\nimport pandas as pd\nweekdays = pd.Series(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], index = ['S', 'M', 'T', 'W', 'R', 'F', 'Sat'])\n\n\n# access individual objs\nweekdays.iloc[0]\n## 'Sunday'\nweekdays.iloc[1]\n## 'Monday'\nweekdays.loc['S']\n## 'Sunday'\nweekdays.loc['Sat']\n## 'Saturday'\n\n# access the index\nweekdays.index\n## Index(['S', 'M', 'T', 'W', 'R', 'F', 'Sat'], dtype='object')\nweekdays.index[6] = 'Z' # you can't assign things to the index to change it\n## TypeError: Index does not support mutable operations\n\nweekdays\n## S         Sunday\n## M         Monday\n## T        Tuesday\n## W      Wednesday\n## R       Thursday\n## F         Friday\n## Sat     Saturday\n## dtype: object\n\n\n\n\n\n\n\n11.2.1 Indexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is.\nThe index is important because it allows us to:\n\nChange values according to location or condition (e.g. replace all even values)\nSequentially use values in the vector to complete a task (e.g. for each data point, compute some quantity)\nSelect values in the vector based on location\nSelect values in the vector based on a condition\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n1-indexed language\n0-indexed language\n\n\nCount elements as 1, 2, 3, 4, …, N\nCount elements as 0, 1, 2, 3, , …, N-1\n\n\n\n\n\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\n\n\n\n\n\nDemo: Replacing Values using Indexes\n\n\n\n\nRPython\n\n\n\nfavorite_cats &lt;- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"\n\nfavorite_cats[2] &lt;- \"Nyan Cat\"\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"\n\n\n\n\nfavorite_cats = [\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"]\n\nfavorite_cats\n## ['Grumpy', 'Garfield', 'Jorts', 'Jean']\n\nfavorite_cats[1] = \"Nyan Cat\"\n\nfavorite_cats\n## ['Grumpy', 'Nyan Cat', 'Jorts', 'Jean']\n\n\n\n\nNot familiar with these cats 3? Curiosity won’t kill you, but it might make you laugh!\n\n\n\n\n11.2.2 Indexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Logical Indexing with R 1-index numbering\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.3: Logical Indexing with python 0-index numbering\n\n\n\n\n\n\n\nLego vectors showing the data (pink/purple) and the logical index (grey/black). Each image is numbered according to the position of the vector – in R, this numbering is from 1, in python, it is from 0.\n\n\n\nDepending on what color we consider to be true, we can select different values from the original vector.\n\n\n\n\n\n\n\n\nLanguage\nBlack is True\nGrey is True\n\n\n\n\nR\n\n\n\n\nPython\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now, just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\n\n\n\n\nDemo: Logical Indexing\n\n\n\n\nRPython\n\n\n\n# Define a character vector\nweekdays &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend &lt;- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days &lt;- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days &lt;- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n## [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days &lt;- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n## [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"\n\n\n\n\nimport numpy as np\n\nanimals = np.array([\"Cat\", \"Dog\", \"Snake\", \"Lizard\", \"Tarantula\", \"Hamster\", \"Gerbil\", \"Otter\"])\n\n# Define a logical vector\ngood_pets = np.array([True, True, False, False, False, True, True, False])\nbad_pets = np.invert(good_pets) # Invert the logical vector \n                                # so True -&gt; False and False -&gt; True\n\nanimals[good_pets]\n## array(['Cat', 'Dog', 'Hamster', 'Gerbil'], dtype='&lt;U9')\nanimals[bad_pets]\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='&lt;U9')\n\nanimals[~good_pets] # equivalent to using bad_pets\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='&lt;U9')\n\n\n\n\n\n\n\n11.2.2.1 Logical Operations on Vectors\nIndexing with logical vectors is an extremely powerful technique – so much so that it is worthwhile to quickly review how logical operations can be combined. In both R and Python, we can operate on logical vectors with standard operators – AND, OR, and NOT.\n\n\n\n\n\n\nDemo: Logical Operators and Logical Indexing\n\n\n\n\nRPython\n\n\n\n1pi_str &lt;- sprintf(\"%0.50f\", pi)\npi_str\n2pi_chars &lt;- strsplit(pi_str, \"\")[[1]]\npi_chars\n3pi_num &lt;- as.numeric(pi_chars)\npi_num\n4pi_num &lt;- pi_num[!is.na(pi_num)]\npi_num\n\n5pi_num%%3==0\npi_num[pi_num%%3==0]\n\n6pi_num%%2==0\npi_num[pi_num%%2==0]\n\n# Compound conditional statement with & = AND\n7pi_num%%2==0 & pi_num%%3==0\npi_num[pi_num%%2==0 & pi_num%%3==0]\n## [1] \"3.14159265358979311599796346854418516159057617187500\"\n##  [1] \"3\" \".\" \"1\" \"4\" \"1\" \"5\" \"9\" \"2\" \"6\" \"5\" \"3\" \"5\" \"8\" \"9\" \"7\" \"9\" \"3\" \"1\" \"1\"\n## [20] \"5\" \"9\" \"9\" \"7\" \"9\" \"6\" \"3\" \"4\" \"6\" \"8\" \"5\" \"4\" \"4\" \"1\" \"8\" \"5\" \"1\" \"6\" \"1\"\n## [39] \"5\" \"9\" \"0\" \"5\" \"7\" \"6\" \"1\" \"7\" \"1\" \"8\" \"7\" \"5\" \"0\" \"0\"\n##  [1]  3 NA  1  4  1  5  9  2  6  5  3  5  8  9  7  9  3  1  1  5  9  9  7  9  6\n## [26]  3  4  6  8  5  4  4  1  8  5  1  6  1  5  9  0  5  7  6  1  7  1  8  7  5\n## [51]  0  0\n##  [1] 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 3 1 1 5 9 9 7 9 6 3 4 6 8 5 4 4 1 8 5 1 6 1 5\n## [39] 9 0 5 7 6 1 7 1 8 7 5 0 0\n##  [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n## [13]  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n## [25]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [37] FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE  TRUE  TRUE\n##  [1] 3 9 6 3 9 9 3 9 9 9 6 3 6 6 9 0 6 0 0\n##  [1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [25] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n## [37] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n## [49] FALSE  TRUE  TRUE\n##  [1] 4 2 6 8 6 4 6 8 4 4 8 6 0 6 8 0 0\n##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n## [37] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n## [49] FALSE  TRUE  TRUE\n## [1] 6 6 6 6 0 6 0 0\n\n\n1\n\nGet 50 digits of pi as a string. sprintf works from specially formatted strings, so %0.50f says we will pass in a float and we want the string to be formatted with 50 decimal places of that float value.\n\n2\n\nSplit the string into single characters. strsplit expects a vector, and returns a list, so as we’re working with only a single string, we tell it we’re only interested in the first set of values [[1]].\n\n3\n\nConvert the single-number values to numbers. We get a warning because . doesn’t convert to a number properly and R uses NA to indicate that the conversion didn’t work 100% of the time.\n\n4\n\nUse logical indexing to get only things that actually converted to numbers properly.\n\n5\n\nGet only the values in pi_num which are evenly divisible by 3 – that is, which are multiples of 3.\n\n6\n\nGet only the values in pi_num which are evenly divisible by 2 – that is, which are multiples of 2.\n\n7\n\nGet only the values in pi_num which are evenly divisible by 2 and 3 – that is, which are multiples of 6.\n\n\n\n\n\n\n\n1import math\nimport pandas as pd\n\n2pi_str = f\"{math.pi:0.50f}\"\nprint(pi_str)\n\n3pi_chars = pd.Series(list(pi_str))\nprint(pi_chars.head())\n\n4pi_num = pd.to_numeric(pi_chars, errors=\"coerce\")\nprint(pi_num.head())\n\n5pi_num = pi_num[~pd.isna(pi_num)]\nprint(pi_num.head())\n\n6pi_num = pi_num.astype(\"int\")\nprint(pi_num.head())\n\n7print(pi_num[pi_num % 3 == 0])\n8print(pi_num[pi_num % 2 == 0])\n9print(pi_num[(pi_num % 2 == 0) & (pi_num % 3 == 0)])\n## 3.14159265358979311599796346854418516159057617187500\n## 0    3\n## 1    .\n## 2    1\n## 3    4\n## 4    1\n## dtype: object\n## 0    3.0\n## 1    NaN\n## 2    1.0\n## 3    4.0\n## 4    1.0\n## dtype: float64\n## 0    3.0\n## 2    1.0\n## 3    4.0\n## 4    1.0\n## 5    5.0\n## dtype: float64\n## 0    3\n## 2    1\n## 3    4\n## 4    1\n## 5    5\n## dtype: int64\n## 0     3\n## 6     9\n## 8     6\n## 10    3\n## 13    9\n## 15    9\n## 16    3\n## 20    9\n## 21    9\n## 23    9\n## 24    6\n## 25    3\n## 27    6\n## 36    6\n## 39    9\n## 40    0\n## 43    6\n## 50    0\n## 51    0\n## dtype: int64\n## 3     4\n## 7     2\n## 8     6\n## 12    8\n## 24    6\n## 26    4\n## 27    6\n## 28    8\n## 30    4\n## 31    4\n## 33    8\n## 36    6\n## 40    0\n## 43    6\n## 47    8\n## 50    0\n## 51    0\n## dtype: int64\n## 8     6\n## 24    6\n## 27    6\n## 36    6\n## 40    0\n## 43    6\n## 50    0\n## 51    0\n## dtype: int64\n\n\n1\n\nLoad math library (for pi) and pandas (for Series/vector structure)\n\n2\n\nGet 50 digits of pi as a string. f at the front says that this is a formatted string, and inside {} we have the value of the string math.pi and the format 0.50f – note this is essentially the same in R and python. 0.50f says we will pass in a float and we want the string to be formatted with 50 decimal places of that float value.\n\n3\n\nConverting a string to a list extracts the single characters in Python. Then we convert this to a pandas Series because it will behave more like a single-type vector.\n\n4\n\nWe convert the vector to numeric, coercing anything that isn’t numeric – this will cause ‘.’ to be converted to NA without raising an error.\n\n5\n\nWe then create a logical vector pd.isna(pi_num) that is TRUE when pi_num has the value NA. Putting ~ in front of this vector negates it, causing the logical vector to be FALSE when pi_num is NA. Using this as an index keeps only the values in the vector which are NOT NA – that is, only the numbers remain.\n\n6\n\nWe convert the float values to integers, since they represent digits.\n\n7\n\nWe create a logical vector pi_num%3==0 that is TRUE when pi_num is a multiple of 3 (or evenly divisible by 3) and FALSE otherwise. We use this as an index to extract only digits of pi that are multiples of 3.\n\n8\n\nWe create a logical vector pi_num%2==0 that is TRUE when pi_num is a multiple of 2 (or evenly divisible by 2) and FALSE otherwise. We use this as an index to extract only digits of pi that are multiples of 2.\n\n9\n\nWe combine the two conditional statements to create a compound statement that is TRUE only when the digit is a multiple of 2 AND 3 (that is, when the digit is a multiple of 6).\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Math with Vectors\nIn order to talk about mathematical operations on (numerical) vectors, we first need to consider different ways we could combine vectors. If the vectors are the same length, we could perform mathematical operations on the elements (and if they’re not the same length we could come up with some convention to coerce them to be the same length).\n\\[\\begin{align}\n\\left[\\begin{array}{c}a_1\\\\a_2\\\\a_3\\end{array}\\right] + \\left[\\begin{array}{c}b_1\\\\b_2\\\\b_3\\end{array}\\right] = \\left[\\begin{array}{c}a_1+b_1\\\\a_2 + b_2\\\\a_3 + b_3\\end{array}\\right]\n\\end{align} \\tag{11.1}\\]\nEquation 11.1 shows how an element-wise sum of two vectors is defined mathematically.\nWe could also think about performing mathematical operations on all of the entries in a vector - e.g. taking the sum of all vector entries, or the mean, or the standard deviation. This reduces a vector of numbers to a single (scalar) number via a defined function. We can use standard built-in functions, or we could define our own (see Chapter 14).\nAnother option is to think of vectors as a way to specify items, and define mathematical ways to combine different sets of items. These vector operations are more akin to set operations, where the product of two vectors is the product set of all combinations of an item from vector 1 and an item from vector 2. Technically, these operations are typically also defined for lists, but they may make more sense in practice when the vector same-type constraint is imposed.\nWe can finally combine vectors using linear algebra; if you are interested in these definitions, see Chapter 12.\n\n11.2.3.1 Element-wise Operations\nWhen using numeric vectors, the element-wise operations are the same for vectors and scalars (Table 11.1 is the same exact table as Table 9.1 in Chapter 9).\n\n\n\nTable 11.1: Element-wise mathematical operators in R and Python\n\n\n\n\n\nOperation\nR symbol\nPython symbol\n\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\n\n\n\n\nRPython\n\n\n\na &lt;- c(1:5)\nb &lt;- c(6:10)\n\na + b\n## [1]  7  9 11 13 15\nb - a\n## [1] 5 5 5 5 5\na * b\n## [1]  6 14 24 36 50\nb / a\n## [1] 6.000000 3.500000 2.666667 2.250000 2.000000\nb %/% a\n## [1] 6 3 2 2 2\nb %% a\n## [1] 0 1 2 1 0\na ^ b\n## [1]       1     128    6561  262144 9765625\n\n\n\n\nimport numpy as np\n\na = np.array([1, 2, 3, 4, 5])\nb = np.array([6, 7, 8, 9, 10])\n\na + b\n## array([ 7,  9, 11, 13, 15])\nb - a\n## array([5, 5, 5, 5, 5])\na * b\n## array([ 6, 14, 24, 36, 50])\nb / a\n## array([6.        , 3.5       , 2.66666667, 2.25      , 2.        ])\nb // a\n## array([6, 3, 2, 2, 2])\nb % a\n## array([0, 1, 2, 1, 0])\na ** b \n## array([      1,     128,    6561,  262144, 9765625])\n\n\n\n\n\n\n11.2.3.2 Vector-to-Scalar Operations\nThere are a few built-in or commonly-used vector summary operations here. This table focuses on those which are most useful for statistics.\n\n\n\nFunction\nR\nPython\n\n\n\n\nlength\nlength(x)\nlen(x)\n\n\nmean\nmean(x)\nx.average()\n\n\nvariance\nvar(x)\nx.var()\n\n\nstandard deviation\nsd(x)\nx.std()\n\n\nmaximum\nmax(x)\nx.max()\n\n\nlocation of maximum\nwhich.max(x)\nx.argmax()\n\n\nminimum\nmin(x)\nx.min()\n\n\nlocation of minimum\nwhich.min(x)\nx.argmin()\n\n\n\n\n\n\n\n\n\nDemo: Vector-to-scalar operations\n\n\n\n\nRPython\n\n\n\nset.seed(30420983)\nx &lt;- sample(1:100, size = 10)\nx\n##  [1] 43 91 25 40 60 39 18 53 58 99\nlength(x)\n## [1] 10\nmean(x)\n## [1] 52.6\nvar(x)\n## [1] 678.4889\nsd(x)\n## [1] 26.04782\nmax(x)\n## [1] 99\nmin(x)\n## [1] 18\nwhich.max(x)\n## [1] 10\nwhich.min(x)\n## [1] 7\n \n# 5 number summary + mean\nsummary(x)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   18.00   39.25   48.00   52.60   59.50   99.00\n\n\n\n\nimport numpy as np\nimport random # need for setting the seed\n\nrandom.seed(30420983)\nx = random.sample(range(1, 101), 10)\nx = np.array(x)\nx\n## array([28, 61, 65, 35, 90, 26, 19, 12, 38,  9])\nlen(x)\n## 10\nx.mean()\n## np.float64(38.3)\nx.var()\n## np.float64(609.21)\nx.std() \n## np.float64(24.68217980649197)\nx.max()\n## np.int64(90)\nx.min()\n## np.int64(9)\nx.argmax()\n## np.int64(4)\nx.argmin()\n## np.int64(9)\n\nIf we want to use Pandas, we can get a summary of our vector using .describe(), but this requires converting our numpy array to a Pandas series.\n\nimport pandas as pd\nx = pd.Series(x)\nx\n## 0    28\n## 1    61\n## 2    65\n## 3    35\n## 4    90\n## 5    26\n## 6    19\n## 7    12\n## 8    38\n## 9     9\n## dtype: int64\nx.describe()\n## count    10.000000\n## mean     38.300000\n## std      26.017302\n## min       9.000000\n## 25%      20.750000\n## 50%      31.500000\n## 75%      55.250000\n## max      90.000000\n## dtype: float64\n\n\n\n\n\n\n\n\n11.2.3.3 Set Operations\nThere are 3 basic set operations: union, intersection, and set difference, as well as the Cartesian product. The Cartesian product creates a set of points in vector space (in math terms) or tuples (in python terms), which is usually represented as something other than a vector (in R, a data frame).\nGenerally speaking, all of these operations would be valid on lists as well as vectors in R, though in python, these functions are part of numpy and require 1-dimensional arrays. There are certainly ways to implement set operations in base python, but they typically do not have convenient named functions - you’d need to write your own or import another library.\n\n\n\n\n\n\n\n\n\n\nOperation\nDefinition\nSymbol (Math)\nR function\nPython function\n\n\n\n\nUnion\nAll elements in A or B\n\\(A \\cup B\\)\nunion(A, B)\nnp.union1d(A, B)\n\n\nIntersection\nElements in both A and B\n\\(A \\cap B\\)\nintersect(A, B)\nnp.intersect1d(A, B)\n\n\nSet Difference\nElements in A but not B\n\\(A \\setminus B\\)\nsetdiff(A, B)\nnp.setdiff1d(A, B)\n\n\nCartesian Product\nCombination of each element in A with each element in B\n\\(A\\times B\\)\nexpand.grid(A, B)\n… it’s complicated, see code.\n\n\n\n\n\n\n\n\n\nDemo: Set Operations\n\n\n\n\nRPython\n\n\n\nA = c(1:4)\nB = c(1:5) * 2 # evens\n\nunion(A, B)\n## [1]  1  2  3  4  6  8 10\nintersect(A, B)\n## [1] 2 4\nsetdiff(A, B)\n## [1] 1 3\nsetdiff(B, A) \n## [1]  6  8 10\nexpand.grid(A, B) # this is a data frame\n##    Var1 Var2\n## 1     1    2\n## 2     2    2\n## 3     3    2\n## 4     4    2\n## 5     1    4\n## 6     2    4\n## 7     3    4\n## 8     4    4\n## 9     1    6\n## 10    2    6\n## 11    3    6\n## 12    4    6\n## 13    1    8\n## 14    2    8\n## 15    3    8\n## 16    4    8\n## 17    1   10\n## 18    2   10\n## 19    3   10\n## 20    4   10\n\n\n\n\nimport numpy as np\n\nA = np.array(range(1, 5))\nB = np.array(range(1, 6)) * 2\n\nnp.union1d(A, B)\n## array([ 1,  2,  3,  4,  6,  8, 10])\nnp.intersect1d(A, B)\n## array([2, 4])\nnp.setdiff1d(A, B) \n## array([1, 3])\nnp.setdiff1d(B, A)\n## array([ 6,  8, 10])\n\nTo combine numpy arrays with a Cartesian product in base Python, we have to use something called list comprehension. I’ll leave this example here, but you don’t need to fully understand how it works yet, as it is using for loops in a python-centric way.\n\n[[a0, b0] for a0 in A for b0 in B] # sets of coordinates\n## [[np.int64(1), np.int64(2)], [np.int64(1), np.int64(4)], [np.int64(1), np.int64(6)], [np.int64(1), np.int64(8)], [np.int64(1), np.int64(10)], [np.int64(2), np.int64(2)], [np.int64(2), np.int64(4)], [np.int64(2), np.int64(6)], [np.int64(2), np.int64(8)], [np.int64(2), np.int64(10)], [np.int64(3), np.int64(2)], [np.int64(3), np.int64(4)], [np.int64(3), np.int64(6)], [np.int64(3), np.int64(8)], [np.int64(3), np.int64(10)], [np.int64(4), np.int64(2)], [np.int64(4), np.int64(4)], [np.int64(4), np.int64(6)], [np.int64(4), np.int64(8)], [np.int64(4), np.int64(10)]]\n[[(a0, b0) for a0 in A] for b0 in B] # vectors of coordinates\n## [[(np.int64(1), np.int64(2)), (np.int64(2), np.int64(2)), (np.int64(3), np.int64(2)), (np.int64(4), np.int64(2))], [(np.int64(1), np.int64(4)), (np.int64(2), np.int64(4)), (np.int64(3), np.int64(4)), (np.int64(4), np.int64(4))], [(np.int64(1), np.int64(6)), (np.int64(2), np.int64(6)), (np.int64(3), np.int64(6)), (np.int64(4), np.int64(6))], [(np.int64(1), np.int64(8)), (np.int64(2), np.int64(8)), (np.int64(3), np.int64(8)), (np.int64(4), np.int64(8))], [(np.int64(1), np.int64(10)), (np.int64(2), np.int64(10)), (np.int64(3), np.int64(10)), (np.int64(4), np.int64(10))]]\n\nWe can also use a library that implements the Cartesian product explicitly, and then convert the result to a data type we would prefer to work with, like an array of coordinates, or a data frame.\n\n# Using a library\nimport itertools\nnp.array(list(itertools.product(A, B))) # array of 2d coords\n## array([[ 1,  2],\n##        [ 1,  4],\n##        [ 1,  6],\n##        [ 1,  8],\n##        [ 1, 10],\n##        [ 2,  2],\n##        [ 2,  4],\n##        [ 2,  6],\n##        [ 2,  8],\n##        [ 2, 10],\n##        [ 3,  2],\n##        [ 3,  4],\n##        [ 3,  6],\n##        [ 3,  8],\n##        [ 3, 10],\n##        [ 4,  2],\n##        [ 4,  4],\n##        [ 4,  6],\n##        [ 4,  8],\n##        [ 4, 10]])\npd.DataFrame(list(itertools.product(A, B))) # data frame\n##     0   1\n## 0   1   2\n## 1   1   4\n## 2   1   6\n## 3   1   8\n## 4   1  10\n## 5   2   2\n## 6   2   4\n## 7   2   6\n## 8   2   8\n## 9   2  10\n## 10  3   2\n## 11  3   4\n## 12  3   6\n## 13  3   8\n## 14  3  10\n## 15  4   2\n## 16  4   4\n## 17  4   6\n## 18  4   8\n## 19  4  10\n\n\n\n\n\n\n\n\n\n11.2.4 Data Types and Vector Operations\n\n\n\n\n\n\nDemo: Different Types in a Vector\n\n\n\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently typed things?\n\nRPython\n\n\n\n1c(2L, FALSE, 3.1415, \"animal\")\n2c(2L, FALSE, 3.1415)\n3c(2L, FALSE)\n## [1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n## [1] 2.0000 0.0000 3.1415\n## [1] 2 0\n\n\n1\n\nall converted to strings\n\n2\n\nconverted to numerics\n\n3\n\nconverted to integers\n\n\n\n\n\n\n\nimport numpy as np\n\n1np.array([2, False, 3.1415, \"animal\"])\n2np.array([2, False, 3.1415])\n3np.array([2, False])\n## array(['2', 'False', '3.1415', 'animal'], dtype='&lt;U32')\n## array([2.    , 0.    , 3.1415])\n## array([2, 0])\n\n\n1\n\nall converted to strings\n\n2\n\nconverted to numerics\n\n3\n\nconverted to integers\n\n\n\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - R and python decide what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\n\n\n\n\n\n\nTry it Out: Logical Indexing\n\n\n\nCreate a vector of the integers from one to 30. Use logical indexing to pick out only the numbers that are multiples of 3.\n\nKey Components of the ProblemR SolutionPython Solution\n\n\n\nWe define a vector of numbers from 1 to 30 as 1:30 in R and range(1, 31) in Python.\nIn Python, we also want to convert this range into a numpy array, so we need to import numpy.\nWe can use logical indexing to select only some elements of a vector which meet our criteria - in this case, that the number is evenly divisible by 3.\nModular Division The mathematical operator that is useful for testing “evenly divisible” is the modulus operator, which provides the remainder when a number is divided by another number. So, 15 modulo 4 (or 15 mod 4) breaks down the division problem into two components - the whole number (3x4 = 12) and the fractional remainder (15 - 12 = 3). Thus, 15 mod 4 is 3.\n\nWe use modular division frequently to test whether something is a multiple of something else. Usually in these cases we’re testing whether (x mod y == 0), which corresponds to a statement “x is a multiple of y”.\nThis translates into x%%3==0 in R or x%3==0 in Python\n\nWe index vectors using [] in both R and python.\n\n\n\n\nx &lt;- 1:30\nx [ x %% 3 == 0] \n##  [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31)) # because python is 0 indexed\nx[ x % 3 == 0] \n## array([ 3,  6,  9, 12, 15, 18, 21, 24, 27, 30])\n\n\n\n\nExtra challenge: Pick out numbers which are multiples of 2 or 3, but not multiples of 6!\n\nKey ComponentsGeneral SolutionR Solution 1R Solution 2Python Solution 1Python Solution 2\n\n\n\nx mod 3 == 0 will pick out the multiples of 3, x mod 2==0 will pick out the multiples of 2, and x mod 6==0 will pick out the multiples of 6\nWe need to combine the statements using AND and OR operators, or use the XOR (exclusive or) operator, if you prefer to go that route.\nThink of each condition as a letter or set. One option is to define variables A and B that represent the initial logical vectors, and then do logical operations on those vectors to get the full condition. Another option is to work it all out with math and only substitute the modulo operators at the end.\n\n(A OR B) AND NOT (A AND B) and slowly shift to programming notation from logical/math notation.\n(A | B) & NOT (A & B) replaces the AND and OR operators\nTackle the NOT operator with (A | B) & !(A & B).\nReplace the A and B with the conditionals: (x mod 2 == 0 | x mod 3 == 0) & !(x mod 6 == 0). Here, A&B is equivalent to x mod 6 == 0 mathematically (but it would also be ok to use the full modulus statements).\n\n\n\n\nThis operation is xor, a.k.a. exclusive or. That is, X or Y, but not both X AND Y.\nWe can write xor as (X OR Y) & !(X AND Y) – or we can use a predefined function: xor() in R, ^ in python.\n\n\n\nx &lt;- 1:30\n\nxor_sol &lt;- xor(x %% 2 == 0, x %% 3 == 0) \nx[xor_sol]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\nWe can also write out the full set of conditional statements - it should work out to the same solution.\n\nfull_sol &lt;- (x %% 2 == 0 | x %% 3 == 0) & !(x %% 2 == 0 & x %% 3 == 0)\nx[full_sol]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\nThis solution uses intermediate variables to make things easier to understand and write.\n\nx &lt;- 1:30\n\na &lt;- x %% 2 == 0 # multiples of 2\nb &lt;- x %% 3 == 0 # multiples of 3\na_and_b &lt;- x %% 6 == 0 # multiples of 6\n\na_xor_b &lt;- xor(a, b) \nx[a_xor_b]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\nSimilarly, we can avoid using XOR and just use the full set of conditional statements.\n\na_b_full &lt;- (a | b) & !(a_and_b)\nx[a_b_full]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31))\n\nxor_sol = (x % 2 == 0) ^ (x % 3 == 0)\nx[xor_sol]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])\n\nWe can also use the full set of conditional statements if we would rather be explicit.\n\nfull_sol = ((x % 2 == 0) | (x % 3 == 0)) & ~(x % 6 == 0)\nx[full_sol]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])\n\n\n\nThis solution uses intermediate variables to make things easier to understand and write.\n\nimport numpy as np\nx = np.array(range(1, 31))\n\na = x % 2 == 0 # multiples of 2\nb = x % 3 == 0 # multiples of 3\na_xor_b = a ^ b\nx[a_xor_b]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])\n\nWe can also use the full set of conditional statements if we would rather be explicit.\n\na_b_full = (a|b) & ~(a & b)\nx[a_b_full]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#lists",
    "href": "part-gen-prog/03-data-struct.html#lists",
    "title": "11  Data Structures",
    "section": "11.3 Lists",
    "text": "11.3 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\n\n\n\nFigure 11.4: A lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\n\n\n\n\n\n\nCreating Lists\n\n\n\n\nRPython\n\n\n\nx &lt;- list(\"a\", 3, FALSE)\nx\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n## \n## [[3]]\n## [1] FALSE\n\n\n\n\nx = [\"a\", 3, False]\nx\n## ['a', 3, False]\n\n\n\n\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. As with vectors, we call this process indexing.\n\n11.3.1 Indexing Lists\nEvery element in a list has an index (a location, indicated by an integer position).\n\n\n\n\n\n\nDemo: Indexing with Lists\n\n\n\n\nR conceptR codePython conceptPython code\n\n\nIn R, we count from 1.\n\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\n\nIn R, list indexing with [] will return a list with the specified elements.\n\nx &lt;- list(\"a\", 3, FALSE)\n\n1x[1]\n2x[1:2]\n\n\n1\n\nThis returns a list\n\n2\n\nThis returns multiple elements in the list. Notice that in R, a:b notation gives a, a+1, …, b, including b. In Python, the same notation does not include b, so watch out.\n\n\n\n\n[[1]]\n[1] \"a\"\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\n\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\n\n1x[[1]]\n2x[[1:2]]\n\n\n1\n\nThis returns the item\n\n2\n\nThis doesn’t work - you can only use [[]] with a single index\n\n\n\n\nError in x[[1:2]]: subscript out of bounds\n\n\n[1] \"a\"\n\n\n\n\nIn Python, we count from 0.\n\n\n\nA python-indexed lego list, counting from 0 to 4\n\n\n\n\n\nx = [\"a\", 3, False]\n\n1x[0]\n2x[1]\n3x[0:2]\n\n\n1\n\nThis returns a scalar\n\n2\n\nThis returns a scalar\n\n3\n\nThis returns a list. Notice that Python indexing 0:2 is 0, 1 – that is, creating a sequential index a:b returns integers (a, …, b-1) excluding b. This is different from R, so watch out!\n\n\n\n\n'a'\n3\n['a', 3]\n\n\nIn Python, we can use single brackets to get an object or a list back out, but we have to know how slices work. Essentially, in Python, 0:2 indicates that we want objects 0 and 1, but want to stop at 2 (not including 2). If you use a slice, Python will return a list; if you use a single index, python just returns the value in that location in the list.\n\n\n\n\n\nWe’ve also talked about indexing as it relates to vectors – for the most part, the two concepts work exactly the same, subject to some minor things in R about differences between x[[idx]] and x[idx].\n\n\n11.3.2 Concatenation\nAnother important thing to know about lists is how to combine them. If I have rosters for two classes and I want to make a list of all of my students, I need to somehow merge the two lists together.\nIn addition, it is often necessary to get only one copy of each item, even if it appears across multiple lists.\n\n\n\n\n\n\nDemo: Concatenation of Vectors and Unique Elements\n\n\n\nLet’s consider some characters from two Star Trek series: DS9 and The Next Generation. One character, Miles O’Brien, appears in both series. When we combine lists, it is sometimes sensible to have only unique elements in the list (e.g. Miles O’Brien should only appear once) – but that is not always true.\n\nRPython\n\n\n\nclass1 &lt;- c(\"Benjamin Sisko\", \"Odo\", \"Julian Bashir\", \"Jadzia Dax\", \"Miles O'Brien\", \"Quark\", \"Kira Nerys\", \"Elim Garak\")\nclass2 &lt;- c(\"Jean-Luc Picard\", \"William Riker\", \"Geordi La Forge\", \"Worf\", \"Miles O'Brien\", \"Beverly Crusher\", \"Deanna Troi\", \"Data\")\n\nstudents &lt;- c(class1, class2)\nstudents\n##  [1] \"Benjamin Sisko\"  \"Odo\"             \"Julian Bashir\"   \"Jadzia Dax\"     \n##  [5] \"Miles O'Brien\"   \"Quark\"           \"Kira Nerys\"      \"Elim Garak\"     \n##  [9] \"Jean-Luc Picard\" \"William Riker\"   \"Geordi La Forge\" \"Worf\"           \n## [13] \"Miles O'Brien\"   \"Beverly Crusher\" \"Deanna Troi\"     \"Data\"\n\nunique(students) # get only unique names\n##  [1] \"Benjamin Sisko\"  \"Odo\"             \"Julian Bashir\"   \"Jadzia Dax\"     \n##  [5] \"Miles O'Brien\"   \"Quark\"           \"Kira Nerys\"      \"Elim Garak\"     \n##  [9] \"Jean-Luc Picard\" \"William Riker\"   \"Geordi La Forge\" \"Worf\"           \n## [13] \"Beverly Crusher\" \"Deanna Troi\"     \"Data\"\n\n\n\n\nclass1 = [\"Benjamin Sisko\", \"Odo\", \"Julian Bashir\", \"Jadzia Dax\", \"Miles O'Brien\", \"Quark\", \"Kira Nerys\", \"Elim Garak\"]\nclass2 = [\"Jean-Luc Picard\", \"William Riker\", \"Geordi La Forge\", \"Worf\", \"Miles O'Brien\", \"Beverly Crusher\", \"Deanna Troi\", \"Data\"]\n\nstudents = class1 + class2\nstudents\n## ['Benjamin Sisko', 'Odo', 'Julian Bashir', 'Jadzia Dax', \"Miles O'Brien\", 'Quark', 'Kira Nerys', 'Elim Garak', 'Jean-Luc Picard', 'William Riker', 'Geordi La Forge', 'Worf', \"Miles O'Brien\", 'Beverly Crusher', 'Deanna Troi', 'Data']\nlist(dict.fromkeys(students)) # get only unique names\n## ['Benjamin Sisko', 'Odo', 'Julian Bashir', 'Jadzia Dax', \"Miles O'Brien\", 'Quark', 'Kira Nerys', 'Elim Garak', 'Jean-Luc Picard', 'William Riker', 'Geordi La Forge', 'Worf', 'Beverly Crusher', 'Deanna Troi', 'Data']",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#matrices",
    "href": "part-gen-prog/03-data-struct.html#matrices",
    "title": "11  Data Structures",
    "section": "11.4 Matrices",
    "text": "11.4 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\n\n\n\n\nDemo: Creating Matrices\n\n\n\n\nMatrix (Lego)RPython\n\n\n\n\n\nA 3x4 Lego matrix of 2x2 blocks.\n\n\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want \nmatrix(1:12, nrow = 3)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    0    0    0\n## [2,]    0    1    0    0\n## [3,]    0    0    1    0\n## [4,]    0    0    0    1\n\n\n\nIn python, matrices are just a special case of a class called ndarray - n-dimensional arrays.\n\nimport numpy as np\n# Minimal ndarray in python by typing in the values in a structured format\nnp.array([[0,  1,  2],\n          [3,  4,  5],\n          [6,  7,  8],\n          [9, 10, 11]]) \n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\n# This syntax creates a list of the rows we want in our matrix\n\n# Matrix in python using a data vector and size parameters\nnp.reshape(range(0,12), (3,4))\n## array([[ 0,  1,  2,  3],\n##        [ 4,  5,  6,  7],\n##        [ 8,  9, 10, 11]])\nnp.reshape(range(0,12), (4,3)) \n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4), order = 'F')\n## array([[ 0,  3,  6,  9],\n##        [ 1,  4,  7, 10],\n##        [ 2,  5,  8, 11]])\n\nIn python, we create 2-dimensional arrays (aka matrices) either by creating a list of rows to join together or by reshaping a 1-dimensional array. The trick with reshaping the 1-dimensional array is the order argument: F stands for “Fortran-like” and C stands for “C-like”… so to go by column, you use F and to go by row, you use C. Totally intuitive, right?\n\n\n\n\n\n\n\n\n\n\n\nDon’t Panic!\n\n\n\nIf you are using this textbook as an undergraduate or non-statistician, don’t panic – you don’t need linear algebra or matrices to learn how to do statistical programming. If you have it, great!\nFor now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\nIf you’re a graduate student in statistics or data science, you should probably already have had linear algebra… otherwise, … good luck!\n\n\nFor more on matrix operations and matrix calculations, see Chapter 12.\n\n11.4.1 Indexing in Matrices\nBoth R and python use [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix in R, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\n\n\n\n\nDemo: Matrix Indexing\n\n\n\n\nRPython\n\n\n\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] &lt;- 500\n\nmy_mat\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]  500   10   11   12\n\n\n\nRemember that zero-indexing!\n\nimport numpy as np\n\nmy_mat = np.reshape(range(1, 13), (3,4))\n\nmy_mat[2,0] = 500\n\nmy_mat\n## array([[  1,   2,   3,   4],\n##        [  5,   6,   7,   8],\n##        [500,  10,  11,  12]])\n\n\n\n\n\n\n\n\n11.4.2 Matrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\ntranspose - flip the matrix across the left top -&gt; right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\nmatrix multiplication (dot product) - If you haven’t had this in Linear Algebra, here’s a preview. See [3] for a better explanation \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\n\n\n\n\n\nDemo: Matrix Operations\n\n\n\n\nRPython\n\n\n\nx &lt;- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny &lt;- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n3 * x\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n\n# Transpose\nt(x)\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nt(y)\n##      [,1] [,2]\n## [1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n##      [,1]\n## [1,]   17\n## [2,]   39\n\n\n\n\nimport numpy as np\nx = np.array([[1,2],[3,4]])\ny = np.array([[5],[6]])\n\n# scalar multiplication\nx*3\n## array([[ 3,  6],\n##        [ 9, 12]])\n3*x\n## array([[ 3,  6],\n##        [ 9, 12]])\n\n# transpose\nx.T # shorthand\n## array([[1, 3],\n##        [2, 4]])\nx.transpose() # Long form\n## array([[1, 3],\n##        [2, 4]])\n\n# Matrix multiplication (dot product)\nnp.dot(x, y)\n## array([[17],\n##        [39]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#arrays",
    "href": "part-gen-prog/03-data-struct.html#arrays",
    "title": "11  Data Structures",
    "section": "11.5 Arrays",
    "text": "11.5 Arrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\nRPython\n\n\n\narray(1:8, dim = c(2,2,2))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    5    7\n## [2,]    6    8\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement.\n\n\n\nimport numpy as np\n\nnp.array([[[1,2],[3,4]],[[5,6], [7,8]]])\n## array([[[1, 2],\n##         [3, 4]],\n## \n##        [[5, 6],\n##         [7, 8]]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#sec-data-frames",
    "href": "part-gen-prog/03-data-struct.html#sec-data-frames",
    "title": "11  Data Structures",
    "section": "11.6 Data Frames",
    "text": "11.6 Data Frames\nIn previous sections, we talked about homogeneous structures: arrangements of data, like vectors and matrices, where every entry in the larger structure has the same type. In the rest of this chapter, we’ll be talking about the root of most data science analysis projects: the data frame.\nLike an excel spreadsheet, data frames are arrangements of data in columns and rows.\nThis format has two main restrictions:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\n\n\n\n\n\n\nFigure 11.5: A lego data frame of 4 columns and 12 rows. Each column is a separate color hue (data type), with slight variations in the hue of each individual bricks.\n\n\n\nFigure 11.5 shows a data frame of 4 columns, each with a different data type (brick size/hue). The data frame has 12 rows. This picture may look similar to one that we used to show logical indexing or lists in previous sections, and that is not a coincidence. You can get everything from a data frame that you would get from a collection of 4 separate vectors… but there are advantages to keeping things in a data frame instead. Similarly, a data frame is very similar to a list of vectors, but with an additional constraint that each vector in the list has the same number of items.\n\n\n\n\n\n\nExample: Motivating Data Frames by Working with Multiple Vectors\n\n\n\nConsider for a moment https://worldpopulationreview.com/states, which lists the population of each state. You can find this dataset in CSV form here.\nIn the previous sections, we learned how to make different vectors in R, numpy, and pandas. Let’s see what happens when we work with the data above as a set of vectors/Series compared to what happens when we work with data frames.\n\nPythonR\n\n\nI’m reading in the data using functions we haven’t learned yet – focus primarily on the operations after the lines containing pd.read_csv.\n\nimport pandas as pd\n\ndata = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/population2024.csv\")\nlist(data.columns) # get names\n## ['Unnamed: 0', 'Rank', 'State', '2024 Population', 'Growth Rate', '2023 Population', '2020 Population', 'Growth Since 2020', '% of US', 'Density (/mi²)']\n\n# Create a few population series\npopulation2024 = pd.Series(data['2024 Population'].values, index = data['State'].values)\npopulation2023 = pd.Series(data['2023 Population'].values, index = data['State'].values)\npopulation2020 = pd.Series(data['2020 Population'].values, index = data['State'].values)\n\nSuppose that we want to sort each population vector by the population in that year.\n\nimport pandas as pd\n\ndata = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/population2024.csv\")\n\npopulation2024 = pd.Series(data['2024 Population'].values, index = data['State'].values).sort_values()\npopulation2023 = pd.Series(data['2023 Population'].values, index = data['State'].values).sort_values()\npopulation2020 = pd.Series(data['2020 Population'].values, index = data['State'].values).sort_values()\n\npopulation2024.head(10)\n## Wyoming           586485\n## Vermont           647818\n## Alaska            733536\n## North Dakota      788940\n## South Dakota      928767\n## Delaware         1044321\n## Rhode Island     1098082\n## Montana          1142746\n## Maine            1402106\n## New Hampshire    1405105\n## dtype: int64\npopulation2023.head(10)\n## Wyoming           584057\n## Vermont           647464\n## Alaska            733406\n## North Dakota      783926\n## South Dakota      919318\n## Delaware         1031890\n## Rhode Island     1095962\n## Montana          1132812\n## Maine            1395722\n## New Hampshire    1402054\n## dtype: int64\npopulation2020.head(10)\n## Wyoming           577664\n## Vermont           642936\n## Alaska            732964\n## North Dakota      779563\n## South Dakota      887852\n## Delaware          991862\n## Montana          1087211\n## Rhode Island     1096444\n## Maine            1364517\n## New Hampshire    1378702\n## dtype: int64\n\nThe only problem is that by doing this, we’ve now lost the ordering that matched across all 3 vectors. Pandas Series are great for showing this problem, because they use labels that allow us to reconstitute which value corresponds to which label, but in R or even in numpy arrays, vectors don’t inherently come with labels. In these situations, sorting by one value can actually destroy the connection between two vectors, in a way that you don’t even notice!\n\n\n\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/population2024.csv\")\n\n# Use vectors instead of the data frame\nstate &lt;- df$State\npop2024 &lt;- df$X2024.Population\npop2023 &lt;- df$X2023.Population\npop2020 &lt;- df$X2020.Population\n\n# Create a vector to index population in 2022 in order\norder2024 &lt;- order(pop2024)\n\n# To keep variables together, we have to do things like this:\nhead(state[order2024])\n## [1] \"Wyoming\"      \"Vermont\"      \"Alaska\"       \"North Dakota\" \"South Dakota\"\n## [6] \"Delaware\"\nhead(pop2024[order2024])\n## [1]  586485  647818  733536  788940  928767 1044321\n\n# It makes more sense just to reorder the whole data frame:\nhead(df[order2024,])\n##     X Rank        State X2024.Population Growth.Rate X2023.Population\n## 50 49   50      Wyoming           586485       0.42%           584057\n## 49 48   49      Vermont           647818       0.06%           647464\n## 48 47   48       Alaska           733536       0.02%           733406\n## 47 46   47 North Dakota           788940       0.64%           783926\n## 46 45   46 South Dakota           928767       1.03%           919318\n## 45 44   45     Delaware          1044321       1.21%          1031890\n##    X2020.Population Growth.Since.2020 X..of.US Density...mi..\n## 50           577664             1.53%    0.17%              6\n## 49           642936             0.76%    0.19%             70\n## 48           732964             0.08%    0.22%              1\n## 47           779563              1.2%    0.23%             11\n## 46           887852             4.61%    0.28%             12\n## 45           991862             5.29%    0.31%            536\n\n\n\n\n\n\nThe primary advantage to data frames is that rows of data are kept together. Since we often think of a row of data as a single observation in a sample, this is an extremely important feature that makes data frames a huge improvement on a collection of vectors of the same length: it’s much harder for observations in a single row to get shuffled around and mismatched! This connection to the physical units of observation is also why data frames are much more common in statistical programming than in other programming courses which will teach you about lists and arrays but do not have a hybrid type for data observations that accommodates different types of variables but requires the same number and order of observations.\n\n11.6.1 Libraries\nIn R, data frames are built in as type data.frame, though there are packages that provide other implementations of data frames that have additional features, such as the tibble package used in many other common packages. We will cover functions from both base R and the tibble package in this chapter.\nIn Python, we will use the pandas library, which is conventionally abbreviated pd. So before you use any data frames in python, you will need to add the following line to your code: import pandas as pd.\n\n\n11.6.2 Data Frame Structure\nWhen you first begin working with a data frame, it becomes critical to know the size of the object, the number of rows and columns, and roughly what columns are present in the data frame. Most of these considerations are structural in nature - just like exploring a wrapped present, you first size it up, then you shake it a bit, and then you unwrap it to find out what’s inside.\n\n\n\n\n\n\nDemo: Size and Structure of Data Frames\n\n\n\n\nRPython\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\ndata(mtcars) # Load the data -- included in base R\nhead(mtcars) # Look at the first 6 rows\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nstr(mtcars) # Examine the structure of the object\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nYou can change column values or add new columns easily using assignment. The summary() function can be used on specific columns to perform summary operations (a 5-number summary useful for making e.g. boxplots is provided by default).\n\nmtcars$gpm &lt;- 1/mtcars$mpg # gpm is sometimes used to assess efficiency\n\nsummary(mtcars$gpm)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615\nsummary(mtcars$mpg)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\n\nOften, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary().\n\nsummary(mtcars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb            gpm         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02950  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04386  \n##  Median :0.0000   Median :4.000   Median :2.000   Median :0.05208  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.05423  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.06483  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.09615\ndim(mtcars)\n## [1] 32 12\nnrow(mtcars)\n## [1] 32\nncol(mtcars)\n## [1] 12\n\nMissing variables in an R data frame are indicated with NA.\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The df.head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\nmtcars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmtcars.head(5)\n##             rownames   mpg  cyl   disp   hp  ...   qsec  vs  am  gear  carb\n## 0          Mazda RX4  21.0    6  160.0  110  ...  16.46   0   1     4     4\n## 1      Mazda RX4 Wag  21.0    6  160.0  110  ...  17.02   0   1     4     4\n## 2         Datsun 710  22.8    4  108.0   93  ...  18.61   1   1     4     1\n## 3     Hornet 4 Drive  21.4    6  258.0  110  ...  19.44   1   0     3     1\n## 4  Hornet Sportabout  18.7    8  360.0  175  ...  17.02   0   0     3     2\n## \n## [5 rows x 12 columns]\n\nmtcars.info()\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 32 entries, 0 to 31\n## Data columns (total 12 columns):\n##  #   Column    Non-Null Count  Dtype  \n## ---  ------    --------------  -----  \n##  0   rownames  32 non-null     object \n##  1   mpg       32 non-null     float64\n##  2   cyl       32 non-null     int64  \n##  3   disp      32 non-null     float64\n##  4   hp        32 non-null     int64  \n##  5   drat      32 non-null     float64\n##  6   wt        32 non-null     float64\n##  7   qsec      32 non-null     float64\n##  8   vs        32 non-null     int64  \n##  9   am        32 non-null     int64  \n##  10  gear      32 non-null     int64  \n##  11  carb      32 non-null     int64  \n## dtypes: float64(5), int64(6), object(1)\n## memory usage: 3.1+ KB\n\nYou can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. You can access a column named xyz using df.xyz or using df[\"xyz\"]. To create a new column, you must use df[\"xyz\"].\n\nmtcars[\"gpm\"] = 1/mtcars.mpg # gpm is sometimes used to assess efficiency\n\nmtcars.gpm.describe()\n## count    32.000000\n## mean      0.054227\n## std       0.016424\n## min       0.029499\n## 25%       0.043860\n## 50%       0.052083\n## 75%       0.064834\n## max       0.096154\n## Name: gpm, dtype: float64\nmtcars.mpg.describe()\n## count    32.000000\n## mean     20.090625\n## std       6.026948\n## min      10.400000\n## 25%      15.425000\n## 50%      19.200000\n## 75%      22.800000\n## max      33.900000\n## Name: mpg, dtype: float64\n\nOften, it is useful to know the dimensions of a data frame. The dimensions of a data frame (rows x columns) can be accessed using df.shape. There is also an easy way to get a summary of each column in the data frame, using df.describe().\n\nmtcars.describe()\n##              mpg        cyl        disp  ...       gear     carb        gpm\n## count  32.000000  32.000000   32.000000  ...  32.000000  32.0000  32.000000\n## mean   20.090625   6.187500  230.721875  ...   3.687500   2.8125   0.054227\n## std     6.026948   1.785922  123.938694  ...   0.737804   1.6152   0.016424\n## min    10.400000   4.000000   71.100000  ...   3.000000   1.0000   0.029499\n## 25%    15.425000   4.000000  120.825000  ...   3.000000   2.0000   0.043860\n## 50%    19.200000   6.000000  196.300000  ...   4.000000   2.0000   0.052083\n## 75%    22.800000   8.000000  326.000000  ...   4.000000   4.0000   0.064834\n## max    33.900000   8.000000  472.000000  ...   5.000000   8.0000   0.096154\n## \n## [8 rows x 12 columns]\nmtcars.shape\n## (32, 13)\n\nMissing variables in a pandas data frame are indicated with nan or NULL.\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Data Frame Exploration\n\n\n\nThe dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below.\n\ndata(state)\nstate_facts &lt;- data.frame(state.x77)\nstate_facts &lt;- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) \n# State names were stored as row labels\n# Store them in a variable instead, and add it to the data frame\n\nrow.names(state_facts) &lt;- NULL # get rid of row names\n\nhead(state_facts)\n##        state Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## 1    Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## 2     Alaska        365   6315        1.5    69.31   11.3    66.7   152 566432\n## 3    Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n## 4   Arkansas       2110   3378        1.9    70.66   10.1    39.9    65  51945\n## 5 California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## 6   Colorado       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n# Write data out so that we can read it in using Python\nwrite.csv(state_facts, file = \"../data/state_facts.csv\", row.names = F)\n\nWe can write out the built in R data and read it in using pd.read_csv, which creates a DataFrame in pandas.\n\nimport pandas as pd\n\nstate_facts = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/state_facts.csv\")\n\nNow that the data has been read in, we can do some basic explorations.\n\nProblemR SolutionPython Solution\n\n\n\nHow many rows and columns does it have? Can you find different ways to get that information?\nThe Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands.\nCalculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density?\n\n\n\n\n# 3 ways to get rows and columns\nstr(state_facts)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Population: num  3615 365 2212 2110 21198 ...\n##  $ Income    : num  3624 6315 4530 3378 5114 ...\n##  $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n##  $ Life.Exp  : num  69 69.3 70.5 70.7 71.7 ...\n##  $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n##  $ HS.Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n##  $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n##  $ Area      : num  50708 566432 113417 51945 156361 ...\ndim(state_facts)\n## [1] 50  9\nnrow(state_facts)\n## [1] 50\nncol(state_facts)\n## [1] 9\n\n# Illiteracy\nstate_facts$TotalNumIlliterate &lt;- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) \n\n# Population Density\nstate_facts$PopDensity &lt;- state_facts$Population * 1e3/state_facts$Area \n# in people per square mile\n\n# minimum population\nstate_facts$state[which.min(state_facts$PopDensity)]\n## [1] \"Alaska\"\n\n\n\n\n# Ways to get rows and columns\nstate_facts.shape\n## (50, 9)\nstate_facts.index.size # rows\n## 50\nstate_facts.columns.size # columns\n## 9\nstate_facts.info() # columns + rows + missing counts + data types\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 50 entries, 0 to 49\n## Data columns (total 9 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   state       50 non-null     object \n##  1   Population  50 non-null     int64  \n##  2   Income      50 non-null     int64  \n##  3   Illiteracy  50 non-null     float64\n##  4   Life.Exp    50 non-null     float64\n##  5   Murder      50 non-null     float64\n##  6   HS.Grad     50 non-null     float64\n##  7   Frost       50 non-null     int64  \n##  8   Area        50 non-null     int64  \n## dtypes: float64(4), int64(4), object(1)\n## memory usage: 3.6+ KB\n\n# Illiteracy\nstate_facts[\"TotalNumIlliterate\"] = state_facts[\"Population\"] * 1e3 * state_facts[\"Illiteracy\"]/100\n\n# Population Density\nstate_facts[\"PopDensity\"] = state_facts[\"Population\"] * 1e3/state_facts[\"Area\"] \n# in people per square mile\n\n# minimum population\nmin_dens = state_facts[\"PopDensity\"].min()\n# Get location of minimum population\nloc_min_dens = state_facts.PopDensity.isin([min_dens])\n# Pull out matching state\nstate_facts.state[loc_min_dens]\n## 1    Alaska\n## Name: state, dtype: object\n\n\n\n\n\n\n\n\n11.6.3 Creating Data Frames\nIt is possible to create data frames from scratch by building them out of simpler components, such as lists of vectors or dicts of Series. This tends to be useful for small data sets, but it is more common to read data in from e.g. CSV files, which I’ve used several times already but haven’t yet shown you how to do (see Chapter 17 for the full how-to).\n\n\n\n\n\n\nDemo: Creating Data Frames from Scratch\n\n\n\n\nRPython\n\n\n\nmath_and_lsd &lt;- data.frame(\n  lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41),\n  test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97))\nmath_and_lsd\n##   lsd_conc test_score\n## 1     1.17      78.93\n## 2     2.97      58.20\n## 3     3.26      67.47\n## 4     4.69      37.47\n## 5     5.83      45.65\n## 6     6.00      32.92\n## 7     6.41      29.97\n\n# add a column - character vector\nmath_and_lsd$subjective &lt;- c(\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\")\n\nmath_and_lsd\n##   lsd_conc test_score                             subjective\n## 1     1.17      78.93                    finally coming back\n## 2     2.97      58.20                         getting better\n## 3     3.26      67.47                    it's totally better\n## 4     4.69      37.47                    really tripping out\n## 5     5.83      45.65                            is it over?\n## 6     6.00      32.92                              whoa, man\n## 7     6.41      29.97 I can taste color, but I can't do math\n\n\n\n\nmath_and_lsd = pd.DataFrame({\n  \"lsd_conc\": [1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41],\n  \"test_score\": [78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97]})\nmath_and_lsd\n##    lsd_conc  test_score\n## 0      1.17       78.93\n## 1      2.97       58.20\n## 2      3.26       67.47\n## 3      4.69       37.47\n## 4      5.83       45.65\n## 5      6.00       32.92\n## 6      6.41       29.97\n\n# add a column - character vector\nmath_and_lsd[\"subjective\"] = [\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\"]\n\nmath_and_lsd\n##    lsd_conc  test_score                              subjective\n## 0      1.17       78.93                     finally coming back\n## 1      2.97       58.20                          getting better\n## 2      3.26       67.47                     it's totally better\n## 3      4.69       37.47                     really tripping out\n## 4      5.83       45.65                             is it over?\n## 5      6.00       32.92                               whoa, man\n## 6      6.41       29.97  I can taste color, but I can't do math\n\n\n\n\n\n\nWhile it’s not so hard to create data frames from scratch for small data sets, it’s very tedious if you have a lot of data (or if you can’t type accurately). An easier way to create a data frame (rather than typing the whole thing in) is to read in data from somewhere else - a file, a table on a webpage, etc. We’re not going to go into the finer points of this (you’ll get into that in Chapter 17), but it is useful to know how to read neatly formatted data.\nOne source of (relatively neat) data is the TidyTuesday github repository4. If you want to practice reading in data in different formats, that is an excellent place to start.\n\n\n\n\n\n\nReading in Data\n\n\n\n\nBase RreadR packagePandas\n\n\nIn Base R, we can read the data in using the read.csv function\n\nairmen &lt;- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n##                    name last_name    first_name      graduation_date\n## 1   Adams, John H., Jr.     Adams  John H., Jr. 1945-04-15T00:00:00Z\n## 2           Adams, Paul     Adams          Paul 1943-04-29T00:00:00Z\n## 3 Adkins, Rutherford H.    Adkins Rutherford H. 1944-10-16T00:00:00Z\n## 4    Adkins, Winston A.    Adkins    Winston A. 1944-02-08T00:00:00Z\n## 5 Alexander, Halbert L. Alexander    Halbert L. 1944-11-20T00:00:00Z\n## 6  Alexander, Harvey R. Alexander     Harvey R. 1944-04-15T00:00:00Z\n##   rank_at_graduation     class graduated_from    pilot_type\n## 1             2nd Lt   SE-45-B           TAAF Single engine\n## 2             2nd Lt   SE-43-D           TAAF Single engine\n## 3             2nd Lt SE-44-I-1           TAAF Single engine\n## 4             2nd Lt   TE-44-B           TAAF   Twin engine\n## 5             2nd Lt   SE-44-I           TAAF Single engine\n## 6             2nd Lt   TE-44-D           TAAF   Twin engine\n##   military_hometown_of_record state aerial_victory_credits\n## 1                 Kansas City    KS                   &lt;NA&gt;\n## 2                  Greenville    SC                   &lt;NA&gt;\n## 3                  Alexandria    VA                   &lt;NA&gt;\n## 4                     Chicago    IL                   &lt;NA&gt;\n## 5                  Georgetown    IL                   &lt;NA&gt;\n## 6                  Georgetown    IL                   &lt;NA&gt;\n##   number_of_aerial_victory_credits reported_lost reported_lost_date\n## 1                                0          &lt;NA&gt;               &lt;NA&gt;\n## 2                                0          &lt;NA&gt;               &lt;NA&gt;\n## 3                                0          &lt;NA&gt;               &lt;NA&gt;\n## 4                                0          &lt;NA&gt;               &lt;NA&gt;\n## 5                                0          &lt;NA&gt;               &lt;NA&gt;\n## 6                                0          &lt;NA&gt;               &lt;NA&gt;\n##   reported_lost_location                                   web_profile\n## 1                   &lt;NA&gt;     https://cafriseabove.org/john-h-adams-jr/\n## 2                   &lt;NA&gt;          https://cafriseabove.org/paul-adams/\n## 3                   &lt;NA&gt; https://cafriseabove.org/rutherford-h-adkins/\n## 4                   &lt;NA&gt;                                          &lt;NA&gt;\n## 5                   &lt;NA&gt; https://cafriseabove.org/halbert-l-alexander/\n## 6                   &lt;NA&gt;  https://cafriseabove.org/harvey-r-alexander/\n\n\n\nIf we want instead to create a tibble, we can use the readr package’s read_csv function, which is a bit more robust and has a few additional features.\n\nlibrary(readr)\nairmen &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n## # A tibble: 6 × 16\n##   name         last_name first_name graduation_date     rank_at_graduation class\n##   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;              &lt;chr&gt;\n## 1 Adams, John… Adams     John H., … 1945-04-15 00:00:00 2nd Lt             SE-4…\n## 2 Adams, Paul  Adams     Paul       1943-04-29 00:00:00 2nd Lt             SE-4…\n## 3 Adkins, Rut… Adkins    Rutherfor… 1944-10-16 00:00:00 2nd Lt             SE-4…\n## 4 Adkins, Win… Adkins    Winston A. 1944-02-08 00:00:00 2nd Lt             TE-4…\n## 5 Alexander, … Alexander Halbert L. 1944-11-20 00:00:00 2nd Lt             SE-4…\n## 6 Alexander, … Alexander Harvey R.  1944-04-15 00:00:00 2nd Lt             TE-4…\n## # ℹ 10 more variables: graduated_from &lt;chr&gt;, pilot_type &lt;chr&gt;,\n## #   military_hometown_of_record &lt;chr&gt;, state &lt;chr&gt;,\n## #   aerial_victory_credits &lt;chr&gt;, number_of_aerial_victory_credits &lt;dbl&gt;,\n## #   reported_lost &lt;chr&gt;, reported_lost_date &lt;dttm&gt;,\n## #   reported_lost_location &lt;chr&gt;, web_profile &lt;chr&gt;\n\n\n\nIn pandas, we can read the csv using pd.read_csv\n\nimport pandas as pd\n\nairmen = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv\")\nairmen.head()\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## 4  Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n\n11.6.4 Working with Data Frames\nOften, we want to know what a data frame contains. R and pandas both have easy summary methods for data frames.\n\n\n\n\n\n\nDemo: Summarizing Data Frames\n\n\n\nNotice that the type of summary depends on the data type.\n\nRPython\n\n\n\nsummary(airmen)\n##      name            last_name          first_name       \n##  Length:1006        Length:1006        Length:1006       \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character  \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  graduation_date               rank_at_graduation    class          \n##  Min.   :1942-03-06 00:00:00   Length:1006        Length:1006       \n##  1st Qu.:1943-10-22 00:00:00   Class :character   Class :character  \n##  Median :1944-05-23 00:00:00   Mode  :character   Mode  :character  \n##  Mean   :1944-07-02 13:18:52                                        \n##  3rd Qu.:1945-04-15 00:00:00                                        \n##  Max.   :1948-10-12 00:00:00                                        \n##  NA's   :11                                                         \n##  graduated_from      pilot_type        military_hometown_of_record\n##  Length:1006        Length:1006        Length:1006                \n##  Class :character   Class :character   Class :character           \n##  Mode  :character   Mode  :character   Mode  :character           \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     state           aerial_victory_credits number_of_aerial_victory_credits\n##  Length:1006        Length:1006            Min.   :0.0000                  \n##  Class :character   Class :character       1st Qu.:0.0000                  \n##  Mode  :character   Mode  :character       Median :0.0000                  \n##                                            Mean   :0.1118                  \n##                                            3rd Qu.:0.0000                  \n##                                            Max.   :4.0000                  \n##                                                                            \n##  reported_lost      reported_lost_date   reported_lost_location\n##  Length:1006        Min.   :1943-07-02   Length:1006           \n##  Class :character   1st Qu.:1943-07-02   Class :character      \n##  Mode  :character   Median :1943-07-02   Mode  :character      \n##                     Mean   :1943-07-02                         \n##                     3rd Qu.:1943-07-02                         \n##                     Max.   :1943-07-02                         \n##                     NA's   :1004                               \n##  web_profile       \n##  Length:1006       \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n##                    \n## \n\nlibrary(skimr) # Fancier summaries\nskim(airmen)\n\n\nData summary\n\n\nName\nairmen\n\n\nNumber of rows\n1006\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n9\n28\n0\n1003\n0\n\n\nlast_name\n0\n1.00\n3\n12\n0\n617\n0\n\n\nfirst_name\n0\n1.00\n3\n17\n0\n804\n0\n\n\nrank_at_graduation\n5\n1.00\n3\n14\n0\n7\n0\n\n\nclass\n20\n0.98\n3\n9\n0\n72\n0\n\n\ngraduated_from\n0\n1.00\n4\n23\n0\n4\n0\n\n\npilot_type\n0\n1.00\n11\n13\n0\n5\n0\n\n\nmilitary_hometown_of_record\n9\n0.99\n3\n19\n0\n366\n0\n\n\nstate\n11\n0.99\n2\n5\n0\n48\n0\n\n\naerial_victory_credits\n934\n0.07\n31\n137\n0\n50\n0\n\n\nreported_lost\n1004\n0.00\n1\n1\n0\n1\n0\n\n\nreported_lost_location\n1004\n0.00\n23\n23\n0\n1\n0\n\n\nweb_profile\n813\n0.19\n34\n95\n0\n190\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nnumber_of_aerial_victory_credits\n0\n1\n0.11\n0.46\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ngraduation_date\n11\n0.99\n1942-03-06\n1948-10-12\n1944-05-23\n52\n\n\nreported_lost_date\n1004\n0.00\n1943-07-02\n1943-07-02\n1943-07-02\n1\n\n\n\n\n\n\n\n\nimport numpy as np\n\n# All variables - strings are summarized with NaNs\nairmen.describe(include = 'all')\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## mean                   NaN  ...                                                NaN\n## std                    NaN  ...                                                NaN\n## min                    NaN  ...                                                NaN\n## 25%                    NaN  ...                                                NaN\n## 50%                    NaN  ...                                                NaN\n## 75%                    NaN  ...                                                NaN\n## max                    NaN  ...                                                NaN\n## \n## [11 rows x 16 columns]\n\n# Only summarize numeric variables\nairmen.describe(include = [np.number])\n##        number_of_aerial_victory_credits\n## count                       1006.000000\n## mean                           0.111829\n## std                            0.457844\n## min                            0.000000\n## 25%                            0.000000\n## 50%                            0.000000\n## 75%                            0.000000\n## max                            4.000000\n\n# Only summarize string variables (objects)\nairmen.describe(include = ['O'])\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## \n## [4 rows x 15 columns]\n\n# Get counts of how many NAs in each column\nairmen.info(show_counts=True)\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 1006 entries, 0 to 1005\n## Data columns (total 16 columns):\n##  #   Column                            Non-Null Count  Dtype  \n## ---  ------                            --------------  -----  \n##  0   name                              1006 non-null   object \n##  1   last_name                         1006 non-null   object \n##  2   first_name                        1006 non-null   object \n##  3   graduation_date                   995 non-null    object \n##  4   rank_at_graduation                999 non-null    object \n##  5   class                             986 non-null    object \n##  6   graduated_from                    1006 non-null   object \n##  7   pilot_type                        1006 non-null   object \n##  8   military_hometown_of_record       997 non-null    object \n##  9   state                             995 non-null    object \n##  10  aerial_victory_credits            72 non-null     object \n##  11  number_of_aerial_victory_credits  1006 non-null   float64\n##  12  reported_lost                     2 non-null      object \n##  13  reported_lost_date                2 non-null      object \n##  14  reported_lost_location            2 non-null      object \n##  15  web_profile                       193 non-null    object \n## dtypes: float64(1), object(15)\n## memory usage: 125.9+ KB\n\nIn pandas, you will typically want to separate out .describe() calls for numeric and non-numeric columns. Another handy function in pandas is .info(), which you can use to show the number of non-NA values. This is particularly useful in sparse datasets where there may be a LOT of missing values and you may want to find out which columns have useful information for more than just a few rows.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "href": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "title": "11  Data Structures",
    "section": "11.7 References",
    "text": "11.7 References\n\n\n\n\n[1] N. Matloff, The art of r programming: A tour of statistical software design. No Starch Press, 2011 [Online]. Available: https://books.google.com/books?id=o2aLBAAAQBAJ\n\n\n[2] M. Fripp, “Answer to \"python pandas dataframe, is it pass-by-value or pass-by-reference\". Stack overflow,” Aug. 12, 2016. [Online]. Available: https://stackoverflow.com/a/38925257/2859168. [Accessed: Jan. 10, 2023]\n\n\n[3] MathIsFun.com, “How to multiply matrices. Math is fun,” 2021. [Online]. Available: https://www.mathsisfun.com/algebra/matrix-multiplying.html. [Accessed: Jan. 10, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#footnotes",
    "href": "part-gen-prog/03-data-struct.html#footnotes",
    "title": "11  Data Structures",
    "section": "",
    "text": "Throughout this section (and other sections), lego pictures are rendered using https://www.mecabricks.com/en/workshop. It’s a pretty nice tool for building stuff online!↩︎\nA similar system exists in R libraries, but R doesn’t handle multiple libraries having the same function names well, which leads to all sorts of confusion. At least python is explicit about it.↩︎\nGrumpy cat, Garfield, Nyan cat. Jorts and Jean: The initial post and the update (both are worth a read because the story is hilarious). The cats also have a Twitter account where they promote labor unions and worker rights.↩︎\nTidy Tuesday is a collaborative project where the R community gets together and explores a dataset, cleaning it, visualizing it, and generally working to collectively hone R skills together. You can find some very nice YouTube livestreams, as well as lots of examples using the #tidytuesday twitter tag.↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html",
    "href": "part-gen-prog/035-matrix-calcs.html",
    "title": "12  Matrix Calculations",
    "section": "",
    "text": "Objectives\nWhile R and Python are extremely powerful statistical programming languages, the core of most programming languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in whatever language you are planning to use to work with data.\nIn this section, we will essentially be using our programming language as overgrown calculators.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#objectives",
    "href": "part-gen-prog/035-matrix-calcs.html#objectives",
    "title": "12  Matrix Calculations",
    "section": "",
    "text": "Understand how to do matrix algebra in relevant programming languages",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "href": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "title": "12  Matrix Calculations",
    "section": "\n12.1 Matrix Operations",
    "text": "12.1 Matrix Operations\n\n\nTable 12.1: Table of common mathematical and matrix operations in R and Python [1].\n\n\n\nOperation\nR\nPython\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nElementwise Multiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nModulo (Remainder)\n%%\n%\n\n\nInteger Division\n%/%\n//\n\n\nElementwise Exponentiation\n^\n**\n\n\nMatrix/Vector Multiplication\n%*%\nnp.dot()\n\n\nMatrix Exponentiation\n^\nnp.exp()\n\n\nMatrix Transpose\nt(A)\nnp.transpose(A)\n\n\nMatrix Determinant\ndet(A)\nnp.linalg.det(A)\n\n\nMatrix Diagonal\ndiag(A)\nnp.linalg.diag(A)\n\n\nMatrix Inverse\nsolve(A)\nnp.linalg.inv(A)\n\n\n\n\n\n\n\n12.1.1 Basic Mathematical Operators\n\n\nR\nPython\n\n\n\n\nx &lt;- 1:10\ny &lt;- seq(3, 30, by = 3)\n\nx + y\n##  [1]  4  8 12 16 20 24 28 32 36 40\nx - y\n##  [1]  -2  -4  -6  -8 -10 -12 -14 -16 -18 -20\nx * y\n##  [1]   3  12  27  48  75 108 147 192 243 300\nx / y\n##  [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n##  [8] 0.3333333 0.3333333 0.3333333\nx^2\n##  [1]   1   4   9  16  25  36  49  64  81 100\nt(x) %*% y\n##      [,1]\n## [1,] 1155\n\n\n\n\nimport numpy as np\n\nx = np.array(range(1, 11))\ny = np.array(range(3, 33, 3)) # python indexes are not inclusive\n\nx + y\n## array([ 4,  8, 12, 16, 20, 24, 28, 32, 36, 40])\nx - y\n## array([ -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20])\nx * y\n## array([  3,  12,  27,  48,  75, 108, 147, 192, 243, 300])\nx / y\n## array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n##        0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])\nx ** 2\n## array([  1,   4,   9,  16,  25,  36,  49,  64,  81, 100])\nnp.dot(x.T, y)\n## np.int64(1155)\n\n\n\n\n\n12.1.2 Matrix Operations\nOther matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy:\n\n\nR\nPython\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\nmat\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    6    4    5\n## [3,]    7    8    9\nt(mat) # transpose\n##      [,1] [,2] [,3]\n## [1,]    1    6    7\n## [2,]    2    4    8\n## [3,]    3    5    9\ndet(mat) # get the determinant\n## [1] 18\ndiag(mat) # get the diagonal\n## [1] 1 4 9\ndiag(diag(mat)) # get a square matrix with off-diag 0s\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    4    0\n## [3,]    0    0    9\ndiag(1:3) # diag() also will create a diagonal matrix if given a vector\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    2    0\n## [3,]    0    0    3\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nmat\n## array([[1, 2, 3],\n##        [6, 4, 5],\n##        [7, 8, 9]])\nmat.T\n## array([[1, 6, 7],\n##        [2, 4, 8],\n##        [3, 5, 9]])\nnp.linalg.det(mat) # numerical precision...\n## np.float64(18.000000000000004)\nnp.diag(mat)\n## array([1, 4, 9])\nnp.diag(np.diag(mat))\n## array([[1, 0, 0],\n##        [0, 4, 0],\n##        [0, 0, 9]])\nnp.diag(range(1, 4))\n## array([[1, 0, 0],\n##        [0, 2, 0],\n##        [0, 0, 3]])\n\n\n\n\n\n12.1.3 Matrix Inverse\nThe other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. In Python, we use the np.linalg.inv() function to invert a matrix, which may be a bit more linguistically familiar.\n\n\nR\nPython\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\n\nminv &lt;- solve(mat) # get the inverse\n\nminv\n##            [,1]       [,2]       [,3]\n## [1,] -0.2222222  0.3333333 -0.1111111\n## [2,] -1.0555556 -0.6666667  0.7222222\n## [3,]  1.1111111  0.3333333 -0.4444444\nmat %*% minv \n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nminv = np.linalg.inv(mat)\nminv\n## array([[-0.22222222,  0.33333333, -0.11111111],\n##        [-1.05555556, -0.66666667,  0.72222222],\n##        [ 1.11111111,  0.33333333, -0.44444444]])\nnp.dot(mat, minv)\n## array([[ 1.00000000e+00,  0.00000000e+00,  1.11022302e-16],\n##        [-8.88178420e-16,  1.00000000e+00, -5.55111512e-16],\n##        [ 0.00000000e+00,  2.22044605e-16,  1.00000000e+00]])\nnp.round(np.dot(mat, minv), 2)\n## array([[ 1.,  0.,  0.],\n##        [-0.,  1., -0.],\n##        [ 0.,  0.,  1.]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#references",
    "href": "part-gen-prog/035-matrix-calcs.html#references",
    "title": "12  Matrix Calculations",
    "section": "\n12.2 References",
    "text": "12.2 References\n\n\n\n\n[1] \nQuartz25, Jesdisciple, H. Röst, D. Ross, L. D’Oliveiro, and BLibrestez55, Python Programming. Wikibooks, 2016 [Online]. Available: https://en.wikibooks.org/wiki/Python_Programming. [Accessed: May 28, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html",
    "href": "part-gen-prog/04-control-struct.html",
    "title": "13  Control Structures",
    "section": "",
    "text": "Objectives\nControl structures are statements in a program that determine when code is evaluated (and how many times it might be evaluated). There are two main types of control structures: if-statements and loops.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#objectives",
    "href": "part-gen-prog/04-control-struct.html#objectives",
    "title": "13  Control Structures",
    "section": "",
    "text": "Understand how to use conditional statements\nUnderstand how conditional statements are evaluated by a program\nUse program flow diagrams to break a problem into parts and evaluate how a program will execute\nUnderstand how to use loops\nSelect the appropriate type of loop for a problem",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#mindset",
    "href": "part-gen-prog/04-control-struct.html#mindset",
    "title": "13  Control Structures",
    "section": "\n13.1 Mindset",
    "text": "13.1 Mindset\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\n\nHere’s another example:\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’ Image from Randal Munroe, xkcd.com, available under a CC-By 2.5 license.\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#conditional-statements",
    "href": "part-gen-prog/04-control-struct.html#conditional-statements",
    "title": "13  Control Structures",
    "section": "\n13.2 Conditional Statements",
    "text": "13.2 Conditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\n\n\n\n\n\n\n13.2.1 Demo: Conditional Statements\n\n\n\n\n\nR\nPython\n\n\n\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) { \n  y &lt;- 8\n} else {\n  y &lt;- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x &gt; 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) y &lt;- 8 else y &lt;- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x &gt; 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n## x = 3 ; y = 8\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x &gt; 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it “belongs”.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python… but it’s a consequence of trying to make the code more readable.\n\n\n\n13.2.2 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n13.2.3 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\nWhile different languages have different conventions for this, it’s helpful to conceptualize the if-else-if-else pattern as a series of binary choices evaluated in sequence. I like to imagine a superhero trying to make contingency plans for a battle with their nemesis - if they try this, then I’ll do this, and if that doesn’t work, then I’ll do this other thing. The important thing is to make sure that all possible outcomes are covered, because if there is an edge case that isn’t covered, it will inevitably lead to a bug that you’ll have to track down later.\n\n\n\n\n\n\nExample - Conditional Evaluation\n\n\n\nSuppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\n\nProgram Flow Map\nR\nPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as &lt;18, 18-25, 26-40, 41-55, 56-65, and &gt;65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\nThe important thing to realize when examining this program flow map is that if age &lt;= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age &lt;= 18), (age &lt;= 25), and (age &lt;= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - &lt;40 is evaluated first, and so &lt;= 25 and &lt;= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage &lt;- 40 # change this as you will to see how the code works\n\nif (age &lt; 18) {\n  bracket &lt;- \"&lt;18\"\n} else if (age &lt;= 25) {\n  bracket &lt;- \"18-25\"\n} else if (age &lt;= 40) {\n  bracket &lt;- \"26-40\"\n} else if (age &lt;= 55) {\n  bracket &lt;- \"41-55\" \n} else if (age &lt;= 65) {\n  bracket &lt;- \"56-65\"\n} else {\n  bracket &lt;- \"&gt;65\"\n}\n\nbracket\n## [1] \"26-40\"\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don’t indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age &lt; 18:\n  bracket = \"&lt;18\"\nelif age &lt;= 25:\n  bracket = \"18-25\"\nelif age &lt;= 40:\n  bracket = \"26-40\"\nelif age &lt;= 55:\n  bracket = \"41-55\"\nelif age &lt;= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \"&gt;65\"\n  \nbracket\n## '26-40'\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.3.1 Try It Out - Chained If/Else Statements\n\n\n\n\n\nProblem\nFlow Map\nR Solution\nPython Solution\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n\n\nThe control flow diagram for the tax brackets\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- 0\n\nif (x &lt;= 10275) {\n  y &lt;- x*.1 # tax paid\n  x &lt;- 0 # All money has been taxed\n} else {\n  y &lt;- y + 10275 * .1\n  x &lt;- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x &lt;= (41775 - 10275)) {\n  y &lt;- y + x * .12\n  x &lt;- 0\n} else {\n  y &lt;- y + (41775 - 10275) * .12\n  x &lt;- x - (41775 - 10275) \n}\n\nif (x &lt;= (89075 - 41775)) {\n  y &lt;- y + x * .22\n  x &lt;- 0\n} else {\n  y &lt;- y + (89075 - 41775) * .22\n  x &lt;- x - (89075 - 41775)\n}\n\nif (x &lt;= (170050 - 89075)) {\n  y &lt;- y + x * .24\n  x &lt;- 0\n} else {\n  y &lt;- y + (170050 - 89075) * .24\n  x &lt;- x - (170050 - 89075)\n}\n\nif (x &lt;= (215950 - 170050)) {\n  y &lt;- y + x * .32\n  x &lt;- 0\n} else {\n  y &lt;- y + (215950 - 170050) * .32\n  x &lt;- x - (215950 - 170050)\n}\n\nif (x &lt;= (539900 - 215950)) {\n  y &lt;- y + x * .35\n  x &lt;- 0\n} else {\n  y &lt;- y + (539900 - 215950) * .35\n  x &lt;- x - (539900 - 215950)\n}\n\nif (x &gt; 0) {\n  y &lt;- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed &lt;= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed &lt;= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed &lt;= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed &lt;= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed &lt;= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed &lt;= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed &gt; 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n## Total Tauntaxed Rate on $ 200000  in income =  22.12 %\n\nThere are better ways to represent this calculation that depend on concepts like vectors or loops – see Section 13.2.5 for details. Any time you find yourself copy-pasting code and changing values, you should consider using vectorized code, a loop, or eventually, a function instead. It’s less typing and easier to maintain.\n\n\n\n\n\n\n\n\n\n\n\n13.2.4 Compact Chained Conditionals\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe case statement is relatively new in Python (added in 3.10), so if you are using an older version of Python, you will not be able to make the code in this section work.\n\n\nThe code to implement a long series of if-else statements can get rather long and hard to follow. This complexity is particularly unnecessary when all of the if() statements are related to one variable, and it’s even more unnecessary when that variable has a fixed number of values that correspond to different actions.\nThe switch statement (implemented using match and case in Python, case_when in dplyr, and switch in base R) is a way to make long sets of conditionals clearer and more compact.\nA general switch statement looks like this:\nswitch (test-value) {\n  case (first-value):\n    &lt;do something&gt;\n  case (second-value):\n    &lt;do something else&gt;\n  default-value:\n    &lt;do the default thing&gt;\n}\nEssentially, in a switch statement, you look for one of a finite list of values, and do something based on which value is true first in the sequence. Ideally the values first-value and second-value are completely disjoint, but that doesn’t always happen – however, in a switch statement, as in chained if-statements, the first value to be TRUE causes subsequent statements not to be executed at all, which essentially means that subsequent statements are implicitly of the form second-value and NOT first-value, third-value and NOT second-value and NOT first-value, and so on.\nNote that any series of chained if statements based on a single quantitative variable can be converted to allow the use of a switch statement by assigning labels to each different group of conditions and then using a switch statement to evaluate the variable holding the labels.\nFor each implementation, we’ll look at how we might do something based on a coffee order size, using Starbucks size labels (which I’ve never gotten straight, but I don’t drink a ton of coffee).\n\n\nR (base)\nPython\ncase_match (R-dplyr)\n\n\n\nThe switch statement in base R works differently depending on whether the first argument is a character string or a number. If the first argument is a number, it’s converted to an integer. Factors (sequential integers with labels) should be manually converted to character variables or integers; a switch statement with a factor as the first argument will issue a warning. If the first argument is a character vector, then the remaining arguments should be named according to the possible values of that character vector. An additional unnamed argument should be included at the end to handle any unaccounted for cases.\n\nsize &lt;- c(\"short\", \"tall\", \"grande\", \"venti\", \"trenta\")\n\nsizefac &lt;- factor(size, levels = size, ordered = T)\n\nsize[1]\n## [1] \"short\"\nsizefac[1]\n## [1] short\n## Levels: short &lt; tall &lt; grande &lt; venti &lt; trenta\n\nswitch(\n  size[3],\n  \"short\" = 8, \n  \"tall\" = 12,\n  \"grande\" = 16,\n  \"venti\" = 20,\n  \"trenta\" = 30,\n  NA) # default value is unnamed\n## [1] 16\n\n\nswitch(\n  sizefac[3],\n  \"short\" = 8, \n  \"tall\" = 12,\n  \"grande\" = 16,\n  \"venti\" = 20,\n  \"trenta\" = 30, \n  NA) \n## [1] 16\n# Warning if you use a factor without converting\n# to a character variable\n\n\nswitch(as.numeric(sizefac[3]), 8, 12, 16, 20, 30) \n## [1] 16\n# If an integer, all arguments must be unnamed\n# No default value is possible\n\n\n\n\nimport pandas as pd\nsize = [\"short\", \"tall\", \"grande\", \"venti\", \"trenta\"]\n\nmatch size[2]:\n  case \"short\": 8\n  case \"tall\": 12\n  case \"grande\": 16\n  case \"venti\": 20\n  case \"trenta\": 30\n  case _: pd.NA\n## 16\n\n\n\nThe approach taken by the case_match function is similar to that in python, but it’s vectorized, which is convenient – we don’t have to loop over the full vector to get the size in ounces for each cup.\ncase_match also works with factors without requiring an explicit type conversion.\n\nlibrary(dplyr)\n\nsize &lt;- c(\"short\", \"tall\", \"grande\", \"venti\", \"trenta\")\n\nsizefac &lt;- factor(size, levels = size, ordered = T)\n\ncase_match(\n  size,\n  \"short\" ~ 8, \n  \"tall\" ~ 12, \n  \"grande\" ~ 16, \n  \"venti\" ~ 20, \n  \"trenta\" ~ 30, \n  .default = NA\n)\n## [1]  8 12 16 20 30\n\ncase_match(\n  sizefac,\n  \"short\" ~ 8, \n  \"tall\" ~ 12, \n  \"grande\" ~ 16, \n  \"venti\" ~ 20, \n  \"trenta\" ~ 30, \n  .default = NA\n)\n## [1]  8 12 16 20 30\n\nBut, case_match does something else that’s somewhat unique – it allows you to specify multiple labels that correspond to the same output value. Suppose our Starbucks is out of 8 and 30 oz cups, and they are thus offering free upgrades from short to tall orders, while not offering any trenta orders.\n\ncase_match(\n  sizefac,\n  c(\"short\",\"tall\") ~ 12, \n  \"grande\" ~ 16, \n  \"venti\" ~ 20, \n  .default = NA\n)\n## [1] 12 12 16 20 NA\n\nThis setup allows us to collapse categories easily while still keeping the basic syntax and options for order sizes the same.\n\n\n\n\ncase_when in dplyr\n\ndplyr also includes an additional function that is somewhat different than the canonical switch statement: case_when. case_when uses logical expressions on one side, making it much closer to a direct equivalent of an if-then-else-if… chain of expressions. The first expression to evaluate to TRUE is what determines the output.\n\n\n\n\n\n\nTry It Out: Income Taxes\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nLet’s consider our income tax example from before. Try to write the tax calculation out using a case-when statement in R. Can you come up with an equivalent formulation in Python by defining an intermediate labeled variable?\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- case_when(\n  x &lt;= 10275  ~ x*.1,\n  x &lt;= 41775  ~ 10275*.10 +         (x-10275)*.12,\n  x &lt;= 80975  ~ 10275*.10 +     (41775-10275)*.12 + \n          (x - 41775)*.22,\n  x &lt;= 170050 ~ 10275*.10 +     (41775-10275)*.12 + \n      (80975 - 41775)*.22 +       (x - 80975)*.24,\n  x &lt;= 215950 ~ 10275*.10 +     (41775-10275)*.12 + \n      (80975 - 41775)*.22 +  (170050 - 80975)*.24 +\n         (x - 170050)*.32,\n  x &lt;= 539900 ~ 10275*.10 +     (41775-10275)*.12 + \n      (80975 - 41775)*.22 +  (170050 - 80975)*.24 +\n    (215950 - 170050)*.32 +        (x-215950)*.35,\n  .default =    10275*.10 +     (41775-10275)*.12 + \n      (80975 - 41775)*.22 +  (170050 - 80975)*.24 +\n    (215950 - 170050)*.32 + (539900 - 215950)*.35 + \n       (x - 539900) * .37\n)\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.2 %\"\n\nThis is a much more concise set of statements that are still pretty clear. Note that we’ve had to refactor the calculation, so that each calculation happens separately rather than being cumulative, but that isn’t so terrible.\n\n\n\nimport numpy as np\n\n# Start with total income\nincome = 200000\n\n# x will hold income that hasn't been taxed yet\nx = income\n\nbrackets = np.array([10275, 41775, 80975, 170050, 215950, 539900])\nbrackets = brackets.astype('int32')\nbracket_labels = np.array([str(i) for i in brackets])\nbracket_labels = np.append(bracket_labels, \"Inf\")\nbrackets = np.append(brackets, np.inf)\n\nmatch bracket_labels[brackets &gt;= x][0]:\n  case \"10275\": \n    y = x*.1\n  case \"41775\":\n    y = (x-10275)*.12 + 10275*.1\n  case \"80975\":\n    y = (x-41775)*.22 + (41775-10275)*.12 + 10275*.1\n  case \"170050\":\n    y = (x-80975)*.24 + (80975-41775)*.22 + (41775-10275)*.12 + 10275*.1\n  case \"215950\":\n    y = (x-170050)*.32 + (170050-80975)*.24 + (80975-41775)*.22 + (41775-10275)*.12 + 10275*.1\n  case \"539900\":\n    y = (x-215950)*.35 + (215950-170050)*.32 + (170050-80975)*.24 + (80975-41775)*.22 + (41775-10275)*.12 + 10275*.1\n  case _:\n    y = (x-539900)*.37 + (539900-215950)*.35 + (215950-170050)*.32 + (170050-80975)*.24 + (80975-41775)*.22 + (41775-10275)*.12 + 10275*.1\n\ny\n## 44393.5\n\nprint(\"Total Tax Rate on $\" + str(income) + \" in income = \" + str(round(y/income, 4)*100) + \"%\")\n## Total Tax Rate on $200000 in income = 22.2%\n\nTo be able to use a python case statement, we first have to decide on some labels that are mutually exclusive and indicate which set of tax rates to apply. In this case, I’ve decided to use the next bracket above the total income level as the label, which tells me that I don’t have to worry about any income over the labeled level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.5 Computational Efficiency and Conditionals\n\n\n\n\n\nIn some cases, it is possible to replace conditional sequences with carefully constructed vectorized calculations. This is much more efficient (especially in R, where vectorization is one primary way to decrease evaluation time) and can be easier to read and understand as well.\n\n\n\n\n\n\nDemo: Tax Brackets without Conditionals\n\n\n\nWe can use vectorized calculations and make the tax calculations even more succinct, eliminating the need for any if-statements (or similar constructs).\n\n\nR\nPython\n\n\n\n\nbrackets &lt;- c(    0, 10275, 41775, 80975, 170050, 215950, 539900, Inf)\nrates &lt;-    c(   .1,   .12,   .22,    .24,    .32,   .35,    .37)\n\nsum(-diff(pmax(0, x-brackets)) * rates)\n## [1] 44393.5\n\n\n\n\nimport numpy as np\n\n# Start with total income\nx = 200000\n\nbrackets = np.array([    0, 10275, 41775, 80975, 170050, 215950, 539900, np.inf])\nrates    = np.array([   .1,   .12,   .22,    .24,    .32,   .35,    .37])\n\nnp.sum(-np.diff(np.maximum(0, x - brackets))*rates)\n## np.float64(44393.5)\n\n\n\n\nThis is the simplest refactoring of this problem, but it’s also less directly comprehensible. It will evaluate much more quickly than the conditional formulations in computer time, but the programmer has to go through and understand what is happening, which isn’t always easy to do, particularly when you didn’t write the code.\n\nHere is a longer sequence of code that performs the same calculations, with variables named descriptively.\n\n\nR\nPython\n\n\n\n\nbrackets &lt;- c(    0, 10275, 41775, 80975, 170050, 215950, 539900, Inf)\nrates &lt;-    c(   .1,   .12,   .22,    .24,    .32,   .35,    .37)\n\namount_above_bracket &lt;- x - brackets\namount_above_bracket\n\ncumulative_amount_subject_to_rate &lt;- pmax(0, amount_above_bracket)\ncumulative_amount_subject_to_rate\n\namount_subject_to_rate &lt;- -diff(cumulative_amount_subject_to_rate)\ntaxes_per_level &lt;- amount_subject_to_rate*rates\n\n# Display calculation in table\nrbind(amount = amount_subject_to_rate, rate = rates, tax_at_level = taxes_per_level)\n\ntotal_taxes &lt;- sum(taxes_per_level)\ntotal_taxes\n## [1]  200000  189725  158225  119025   29950  -15950 -339900    -Inf\n## [1] 200000 189725 158225 119025  29950      0      0      0\n##                 [,1]     [,2]     [,3]     [,4]     [,5] [,6] [,7]\n## amount       10275.0 31500.00 39200.00 89075.00 29950.00 0.00 0.00\n## rate             0.1     0.12     0.22     0.24     0.32 0.35 0.37\n## tax_at_level  1027.5  3780.00  8624.00 21378.00  9584.00 0.00 0.00\n## [1] 44393.5\n\n\n1\n\nbrackets defines the cutoffs for the different hierarchical tax rates, including the [539900, Inf) implicit bracket\n\n2\n\nx - brackets calculates the amount of income taxed at or above each rate\n\n3\n\npmax(0, x-brackets) does not allow the values to go below 0, so that we’re not paying negative taxes (though that would be nice, and some tax credits actually work that way)\n\n4\n\ndiff(...) subtracts value 1 from value 2, value 2 from value 3, and so on, shortening the vector by 1. This produces negative values in this case, so we have to multiply by -1 to get them back to positive values (we could also reverse the vector, diff, and then reverse again, e.g. sum(rev(diff(pmax(0, rev(x-brackets)))) * rates)).\n\n5\n\nThen, we can multiply our values by their corresponding rates (-diff(pmax(0, x-brackets))*rates).\n\n6\n\nFinally, we add up the values (sum) in the vector to get the total tax burden for someone making $200,000 per year.\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n# Start with total income\nx = 200000\n\n1brackets = np.array([    0, 10275, 41775, 80975, 170050, 215950, 539900, np.inf])\nrates    = np.array([   .1,   .12,   .22,    .24,    .32,   .35,    .37])\n\n2amount_above_bracket = x - brackets\namount_above_bracket\n\n3cumulative_amount_subject_to_rate = np.maximum(0, amount_above_bracket)\ncumulative_amount_subject_to_rate\n\n4amount_subject_to_rate = -np.diff(cumulative_amount_subject_to_rate)\n5taxes_per_level = amount_subject_to_rate*rates\n\n# Display calculation in table\npd.DataFrame({\"amount\" : amount_subject_to_rate, \"rate\" : rates, \"tax_at_level\" : taxes_per_level})\n\n6total_taxes = sum(taxes_per_level)\ntotal_taxes\n## array([ 200000.,  189725.,  158225.,  119025.,   29950.,  -15950.,\n##        -339900.,     -inf])\n## array([200000., 189725., 158225., 119025.,  29950.,      0.,      0.,\n##             0.])\n##     amount  rate  tax_at_level\n## 0  10275.0  0.10        1027.5\n## 1  31500.0  0.12        3780.0\n## 2  39200.0  0.22        8624.0\n## 3  89075.0  0.24       21378.0\n## 4  29950.0  0.32        9584.0\n## 5     -0.0  0.35          -0.0\n## 6     -0.0  0.37          -0.0\n## np.float64(44393.5)\n\n\n1\n\nbrackets defines the cutoffs for the different hierarchical tax rates, including the [539900, Inf) implicit bracket\n\n2\n\nx - brackets calculates the amount of income taxed at or above each rate\n\n3\n\npmax(0, x-brackets) does not allow the values to go below 0, so that we’re not paying negative taxes (though that would be nice, and some tax credits actually work that way)\n\n4\n\ndiff(...) subtracts value 1 from value 2, value 2 from value 3, and so on, shortening the vector by 1. This produces negative values in this case, so we have to multiply by -1 to get them back to positive values (we could also reverse the vector, diff, and then reverse again, e.g. sum(rev(diff(pmax(0, rev(x-brackets)))) * rates)).\n\n5\n\nThen, we can multiply our values by their corresponding rates (-diff(pmax(0, x-brackets))*rates).\n\n6\n\nFinally, we add up the values (sum) in the vector to get the total tax burden for someone making $200,000 per year.\n\n\n\n\n\n\n\nThis code uses slightly more memory by storing intermediate results, but it is much more readable – you can clearly associate the steps to each line of code. When the tax code changes, it will also be easier to change – instead of having to change the values of every tax bracket level in the conditional statement, we have to change the vector one time, and everything else will update accordingly.\n\n\nIt is always a trade-off to decide between a balance of computational time, computational resource requirements, programmer time (both to program and to maintain the code), and readability. The vectorized calculation is less readable, but more computationally efficient. We can make it slightly more readable by increasing storage requirements and using intermediate variables that are well named to make the calculation process more comprehensible.\n\n\n\n\nSometimes, it’s that you wrote the code years ago, and past you was invariably smarter and dumber than you are currently.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#loops",
    "href": "part-gen-prog/04-control-struct.html#loops",
    "title": "13  Control Structures",
    "section": "\n13.3 Loops",
    "text": "13.3 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n13.3.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true. The simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\n\n\n\nExample: While Loops\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing while-loop pseudocode (while x &lt;= N) { # code that changes x in some way} and the program flow map expansion where we check if x &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\n\nx &lt;- 0\n\nwhile (x &lt; 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x &lt;- x + 1\n}\n## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n\n\n\n\nx = 0\n\nwhile x &lt; 10:\n  print(x)\n  x = x + 1\n## 0\n## 1\n## 2\n## 3\n## 4\n## 5\n## 6\n## 7\n## 8\n## 9\n\n\n\n\n\n\n\n\n\n\n\n\n13.3.1.1 Try it Out - While Loops\n\n\n\n\n\nProblem\nMath Notation\nR Solution\nPython solution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk &lt;- 1\nprod &lt;- 1\nans &lt;- (exp(pi) - exp(-pi))/(2*pi)\ndelta &lt;- 0.0001\n\nwhile (abs(prod - ans) &gt;= 0.0001) {\n  prod &lt;- prod * (1 + 1/k^2)\n  k &lt;- k + 1\n}\n\nk\n## [1] 36761\nprod\n## [1] 3.675978\nans\n## [1] 3.676078\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) &gt;= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k &gt; 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n## At  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Avoid Infinite Loops\n\n\n\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\n\nR\nPython\n\n\n\nThis while loop runs until either x &lt; 10 or n &gt; 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n&gt;30 check to the loop so that we don’t tie up the computer for eternity.\n\nx &lt;- 0\nn &lt;- 0 # count the number of times the loop runs\n\nwhile (x &lt; 10) { \n  print(x)\n  x &lt;- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n &lt;- n + 1\n  if (n &gt; 30) \n    break # this stops the loop if n &gt; 30\n}\n## [1] 0\n## [1] -0.6999157\n## [1] -1.488577\n## [1] -3.416473\n## [1] -3.551984\n## [1] -4.403012\n## [1] -3.638665\n## [1] -5.867218\n## [1] -5.493864\n## [1] -4.73579\n## [1] -6.201982\n## [1] -4.831426\n## [1] -6.013207\n## [1] -6.357643\n## [1] -5.404441\n## [1] -5.857407\n## [1] -6.506772\n## [1] -6.397666\n## [1] -5.519748\n## [1] -7.008371\n## [1] -7.187883\n## [1] -7.633027\n## [1] -7.703524\n## [1] -7.734773\n## [1] -7.729125\n## [1] -6.862941\n## [1] -6.840562\n## [1] -6.381707\n## [1] -6.060932\n## [1] -5.638446\n## [1] -4.375715\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x &lt; 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n &gt; 50:\n    break # this stops the loop if n &gt; 50\n## 0\n## [0.26347533]\n## [1.89365279]\n## [2.20355313]\n## [3.28545654]\n## [4.89117171]\n## [5.50485549]\n## [5.73126195]\n## [2.84108075]\n## [3.41491495]\n## [2.82287873]\n## [1.1817676]\n## [0.82886643]\n## [1.13379805]\n## [1.41570014]\n## [1.03726872]\n## [0.48418744]\n## [0.99300931]\n## [0.99367004]\n## [2.24995568]\n## [1.16782232]\n## [0.13320904]\n## [-0.07952444]\n## [0.15374879]\n## [0.01431605]\n## [0.04275899]\n## [0.76944886]\n## [0.554636]\n## [1.03214126]\n## [1.02176661]\n## [2.07486238]\n## [2.39914868]\n## [0.45870138]\n## [1.47565132]\n## [1.42003753]\n## [1.41940179]\n## [2.04938844]\n## [4.74342591]\n## [3.16382021]\n## [2.00783176]\n## [2.03214799]\n## [2.15530727]\n## [2.91544739]\n## [1.53786484]\n## [2.12686281]\n## [2.73375327]\n## [3.47318604]\n## [4.34221329]\n## [4.34083016]\n## [5.61808865]\n## [5.95977016]\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\n\n13.3.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nA visual demonstration of for loops iterating through a vector of monsters to dress them up for a parade. Image by Allison Horst.\n\n\n\n\n\n\n\nDemo - For Loop Syntax\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\n\n\n\nfor i in range(5):\n  print(i)\n## 0\n## 1\n## 2\n## 3\n## 4\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is very commonly used to do a task for every value of a vector.\n\n\n\n\n\n\n13.3.2.1 Example: For Loops\n\n\n\n\n\nR\nPython\n\n\n\nIn R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n## [1] \"January\"\n## [1] \"February\"\n## [1] \"March\"\n## [1] \"April\"\n## [1] \"May\"\n## [1] \"June\"\n## [1] \"July\"\n## [1] \"August\"\n## [1] \"September\"\n## [1] \"October\"\n## [1] \"November\"\n## [1] \"December\"\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3\n\n\nabbr3 &lt;- rep(\"\", length(month.name))\n\nfor (i in 1:length(month.name))\n  abbr3[i] &lt;- substr(month.name[i], 1, 3)\n\ndata.frame(full_name = month.name, abbrev = abbr3)\n##    full_name abbrev\n## 1    January    Jan\n## 2   February    Feb\n## 3      March    Mar\n## 4      April    Apr\n## 5        May    May\n## 6       June    Jun\n## 7       July    Jul\n## 8     August    Aug\n## 9  September    Sep\n## 10   October    Oct\n## 11  November    Nov\n## 12  December    Dec\n\n\n1\n\nCreate new vector of the correct length\n\n2\n\nWe have to iterate along the index (1 to length) instead of the name in this case because we want to store the result in a corresponding row of a new vector\n\n3\n\nWe can combine the two vectors into a data frame so that each row corresponds to a month and there are two columns: full month name, and abbreviation\n\n\n\n\n\n\nIn python, we have to define our vector or list to start out with, but that’s easy enough:\n\nimport calendar\n1month_name = list(calendar.month_name)[1:13]\n\n2for i in month_name:\n  print(i)\n## January\n## February\n## March\n## April\n## May\n## June\n## July\n## August\n## September\n## October\n## November\n## December\n\n\n1\n\nCreate a list with month names. For some reason, by default there’s a “” as the first entry, so we’ll get rid of that\n\n2\n\nIterate along the vector, printing out each element\n\n\n\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3.\nPython handles lists best when you use pythonic expressions. The linked post has an excellent explanation of why enumerate works best here.\n\n1abbr3 = [\"\"] * len(month_name)\n\n\n2for i, val in enumerate(month_name):\n3  abbr3[i] = val[0:3:]\n  \nabbr3\n## ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n\n1\n\nCreate new vector of the correct length\n\n2\n\nWe have to iterate along the index because we want to store the result in a corresponding row of a new vector. Python allows us to iterate along both the index i and the value val at the same time, which is convenient.\n\n3\n\nStrings have indexes by character, so this gets characters 0, 1, and 2.\n\n\n\n\n\n\n\n\n\nThere are additional types of iterators and control statements for iterators available in some languages, such as the doWhile loop and recursion. If you’re curious, expand the section below, but you can always write a doWhile loop as a while loop and can usually restate a recursion as a loop, so these constructs are largely for convenience.\n\n\n\n\n\n\n13.3.3 do-while Loops\n\n\n\n\n\nThe do-while loop runs the code first and then evaluates the logical condition to determine whether the loop will be run again.\n\n\n\n\n\n\nDemo: do-while loops\n\n\n\n\n\nR\nPython\n\n\n\nIn R, do-while loops are most naturally implemented using a very primitive type of iteration: a repeat statement.\n\nrepeat {\n  # statements go here\n  if (condition)\n    break # this exits the repeat statement\n}\n\n\n\nIn python, do-while loops are most naturally implemented using a while loop with condition TRUE:\n\nwhile TRUE:\n  # statements go here\n  if condition:\n    break\n\n\n\n\n\n\n\n13.3.4 Recursion\nAn additional means of running code an indeterminate number of times is the use of recursion, which we cannot cover until we learn about functions. I have added an additional section, Section 41.2.3, to cover this topic, but it is not essential to being able to complete most basic data programming tasks. Recursion is useful when working with structures such as trees (including phylogenetic trees) and nested lists.\n\n13.3.5 Controlling Loops with Break, Next, Continue\nWhile I do not often use break, next, and continue statements, they do exist in both languages and can be useful for controlling the flow of program execution. I have moved the section on this to Section 41.2.2 for the sake of brevity and to reduce the amount of new material those without programming experience are being exposed to in this section.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "href": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "title": "13  Control Structures",
    "section": "\n13.4 References",
    "text": "13.4 References",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html",
    "href": "part-gen-prog/05-functions.html",
    "title": "14  Writing Functions",
    "section": "",
    "text": "Objectives\nA function is a set of actions that we group together and name. Throughout this course, you’ve used a bunch of different functions in R and python that are built into the language or added through packages: mean, ggplot, length, print. In this chapter, we’ll be writing our own functions.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#objectives",
    "href": "part-gen-prog/05-functions.html#objectives",
    "title": "14  Writing Functions",
    "section": "",
    "text": "Identify the parts of a function from provided source code\nPredict what the function will return when provided with input values and source code\nGiven a task, lay out the steps necessary to complete the task in pseudocode\nWrite a function which uses necessary input values to complete a task",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "href": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "title": "14  Writing Functions",
    "section": "\n14.1 When to write a function?",
    "text": "14.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more about functions\n\n\n\nThere is some extensive material on this subject in R for Data Science [1] on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\n\n\n\n\n\nExample: Turning Code into Functions\n\n\n\nThis example is modified from R for Data Science [2, Ch. 19].\nWhat does this code do? Does it work as intended?\n\n\nR\nPython\n\n\n\n\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has:\n\n\nR\nPython\n\n\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we start by rewriting it using general names:\n\n\nR\nPython\n\n\n\nIn this case, it might help to replace df$a with x.\n\nx &lt;- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n##  [1] 0.4556553 0.2010496 0.5084320 0.0000000 0.3001374 1.0000000 0.4529373\n##  [8] 0.6444310 0.3772043 0.2856359\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n## 0    1.000000\n## 1    0.307891\n## 2    0.668981\n## 3    0.942728\n## 4    0.140163\n## 5    0.000000\n## 6    0.749131\n## 7    0.332778\n## 8    0.048342\n## 9    0.499360\n## Name: a, dtype: float64\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\n\nR\nPython\n\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng &lt;- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n##  [1] 0.4556553 0.2010496 0.5084320 0.0000000 0.3001374 1.0000000 0.4529373\n##  [8] 0.6444310 0.3772043 0.2856359\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n## 0    1.000000\n## 1    0.307891\n## 2    0.668981\n## 3    0.942728\n## 4    0.140163\n## 5    0.000000\n## 6    0.749131\n## 7    0.332778\n## 8    0.048342\n## 9    0.499360\n## Name: a, dtype: float64\n\n\n\n\nFinally, we turn this code into a function:\n\n\nR\nPython\n\n\n\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n##  [1] 0.4556553 0.2010496 0.5084320 0.0000000 0.3001374 1.0000000 0.4529373\n##  [8] 0.6444310 0.3772043 0.2856359\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n## 0    1.000000\n## 1    0.307891\n## 2    0.668981\n## 3    0.942728\n## 4    0.140163\n## 5    0.000000\n## 6    0.749131\n## 7    0.332778\n## 8    0.048342\n## 9    0.499360\n## Name: a, dtype: float64\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#syntax",
    "href": "part-gen-prog/05-functions.html#syntax",
    "title": "14  Writing Functions",
    "section": "\n14.2 Syntax",
    "text": "14.2 Syntax\n\n\nR and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\nIn R, functions are defined as other variables, using &lt;-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return &lt;value&gt;, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don’t return anything; in R, this is a bit less common, for reasons we won’t get into here.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "href": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "title": "14  Writing Functions",
    "section": "\n14.3 Arguments and Parameters",
    "text": "14.3 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\n\n\n\n\n\nExample: Parts of a Function\n\n\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a dog’s name and returns “ is a good pup!”.\n\n\nR\nPython\n\n\n\n\ndog &lt;- \"Eddie\"\n\ngoodpup &lt;- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n## [1] \"Eddie is a good pup!\"\n\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n## 'Eddie is a good pup!'\n\n\n\n\nIn this example function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\n\n\n\n\n\nTry it out: Function Parts\n\n\n\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be. Also determine whether the function output is stored in memory or just printed to the command line.\n\n\nFunction 1\nAnswer\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\n\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n## Hello World\n\n\nFunction output is not stored in memory and is printed to the command line.\n\n\n\n\n\n\nFunction 2\nAnswer\n\n\n\n\nmy_mean &lt;- function(x) {\n  censor_x &lt;- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n## [1] 6\n\n\nFunction output is saved to memory (x) and printed to the command line\n\n\n\n\n\n\n\n14.3.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\n\nR\nPython\n\n\n\n\n\ndivide &lt;- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\n\nR\nPython\n\n\n\n\ndivide(3, 6)\n## [1] 0.5\n\ndivide(x = 3, y = 6)\n## [1] 0.5\n\ndivide(y = 6, x = 3)\n## [1] 0.5\n\ndivide(6, 3)\n## [1] 2\n\ndivide(x = 6, y = 3)\n## [1] 2\n\ndivide(y = 3, x = 6)\n## [1] 2\n\n\n\n\ndivide(3, 6)\n## 0.5\n\ndivide(x = 3, y = 6)\n## 0.5\n\ndivide(y = 6, x = 3)\n## 0.5\n\ndivide(6, 3)\n## 2.0\n\ndivide(x = 6, y = 3)\n## 2.0\n\ndivide(y = 3, x = 6)\n## 2.0\n\n\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters.\n\n14.3.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\n\n\n\n\n\nInput Validation Example\n\n\n\n\n\nR\nPython\n\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in x + y: non-numeric argument to binary operator\n\nadd &lt;- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in add(\"tmp\", 3): is.numeric(x) is not TRUE\nadd(3, 4)\n## [1] 7\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we’re good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n## TypeError: can only concatenate str (not \"int\") to str\n\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n## x and y must be add-able\nadd(3, 4)\n## 7\n\nYou can read more about error handling in Python here\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused [3].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#scope",
    "href": "part-gen-prog/05-functions.html#scope",
    "title": "14  Writing Functions",
    "section": "\n14.4 Scope",
    "text": "14.4 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n14.4.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nPython\n\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\nR\n\na &lt;- 10\n\nmyfun &lt;- function() {\n  a &lt;- 20\n  a\n}\n\nmyfun()\n## [1] 20\n\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n## 20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n14.4.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat does this output?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\nmyfun &lt;- function() {\n  if (!exists(\"aa\")) {\n    aa &lt;- 1\n  } else {\n    aa &lt;- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n## [1] 1\nmyfun()\n## [1] 1\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n## 1\nmyfun()\n## 1\n\nNote that the try command here is used to handle the case where a doesn’t exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used [4], Ch 17.\n\n\n\n\n\n\n14.4.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat will this code output?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\nmyfun &lt;- function() {\n  x + 1\n}\n\nx &lt;- 14\n\nmyfun()\n## [1] 15\n\nx &lt;- 20\n\nmyfun()\n## [1] 21\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n## 15\n\nx = 20\n\nmyfun()\n## 21\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Function Scope\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. From [2, Ch. 6]\n\n\nR code\nR solution\nPython code\nPython solution\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n## [1] 202\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n## 202",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#sec-functions-refs",
    "href": "part-gen-prog/05-functions.html#sec-functions-refs",
    "title": "14  Writing Functions",
    "section": "\n14.5 References",
    "text": "14.5 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[3] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[4] \nA. Martelli and D. Ascher, Python Cookbook. O’Reilly Media, 2002 [Online]. Available: https://learning.oreilly.com/library/view/python-cookbook/0596001673/ch05s24.html. [Accessed: May 31, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#footnotes",
    "href": "part-gen-prog/05-functions.html#footnotes",
    "title": "14  Writing Functions",
    "section": "",
    "text": "This is not strictly true, you can of course use pass-by-reference, but we will not be covering that in this class as we are strictly dealing with the bare minimum of learning how to write a function here.↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html",
    "href": "part-gen-prog/06-debugging.html",
    "title": "15  Debugging",
    "section": "",
    "text": "Objectives\nNow that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll become more adept at screwing things up).",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#objectives",
    "href": "part-gen-prog/06-debugging.html#objectives",
    "title": "15  Debugging",
    "section": "",
    "text": "Create reproducible examples of problems\nUse built in debugging tools to trace errors\nUse online resources to research errors\n\n\n\nThe faces of debugging (by Allison Horst)",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "href": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "title": "15  Debugging",
    "section": "\n15.1 Avoiding Errors: Defensive Programming",
    "text": "15.1 Avoiding Errors: Defensive Programming\nOne of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively [1]. By that, I mean, code in a way that you will make debugging things easier later.\n\nModularize your code. Each function should do only one task, ideally in the least-complex way possible.\nMake your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly.\nComment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you.\nDon’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex.\nReduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse [2] is notorious for this.\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constituent packages are some of the most installed R packages on CRAN.)\n\n\nAnother way to handle dependency management is to use the renv package [3], which creates a local package library with the appropriate versions of your packages stored in the same directory as your project. renv was inspired by the python concept of virtual environments, and it does also work with python if you’re using both R and python inside a project (e.g. this book uses renv). renv will at the very least help you minimize issues with code not working after unintentional package updates.\n\nAdd safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a”.)\nDon’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression.\nCollect your often-reused code in packages that you can easily load and make available to “future you”. The process of making a package often encourages you to document your code better than you would a script. A good resource for getting started making R packages is [4], and a similar python book is [5].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#working-through-errors",
    "href": "part-gen-prog/06-debugging.html#working-through-errors",
    "title": "15  Debugging",
    "section": "\n15.2 Working through Errors",
    "text": "15.2 Working through Errors\n\n15.2.1 First steps\n\n15.2.1.1 Get into the right mindset\nYou can’t debug something effectively if you’re upset. You have to be in a puzzle-solving, detective mindset to actually solve a problem. If you’re already stressed out, try to relieve that stress before you tackle the problem: take a shower, go for a walk, pet a puppy.\n\n\nA debugging manifesto [6]\n\n\n15.2.1.2 Check your spelling\nI’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation.\n\n\nTitle: user.fist_name [7]\n\n\n15.2.2 General Debugging Strategies\n\n\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nWhile defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in [8]; I’ve added some steps that are commonly overlooked and modified the context from the original package development to introductory programming. I’ve also integrated some lovely illustrations from Julia Evans (@b0rk) to lighten the mood.\n\nRealize that you have a bug\nRead the error message\n\n\n\nDebugging strategy: Reread the error message[9]\n\n\n\nGoogle! Seriously, just google the whole error message.\nIn R you can automate this with the errorist and searcher packages. Python is so commonly used that you’ll likely be able to find help for your issue if you are specific enough.\n\n\n\n\n\nDebugging strategy: Shorten your feedback loop [10]\n\n\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. \nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\n\n\nDebugging strategy: Change working code into broken code [12]\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nCome up with one question. If you’re stuck, it can be helpful to break it down a bit and ask one tiny question about the bug.\n\n\n\nDebugging strategy: Come up with one question [13]\n\n\n\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked.\n\n\n\n\n\nDebugging strategy: Write a unit test [14]\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck1. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. See [15] for a more thorough explanation.\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "href": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "title": "15  Debugging",
    "section": "\n15.3 Dividing Problems into Smaller Parts",
    "text": "15.3 Dividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solvable.\n\n\n\n\n\n\n15.3.1 Demo: Exhaustion\n\n\n\nThis example inspired by [16].\n\n\nGeneral problem\nSpecific problem\nSubproblems\nBrainstorm\nSubproblem solutions\n\n\n\n“I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\n\n\n“I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\n\nMoving through the list in the previous tab, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\n\n\n\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfullness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "href": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "title": "15  Debugging",
    "section": "\n15.4 Minimal Working (or Reproducible) Examples",
    "text": "15.4 Minimal Working (or Reproducible) Examples\n\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and really famous python developers browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\nThere are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working.\n\n\n\n\n\n\nOther Minimum Working Example/Reprex resources\n\n\n\n\nreprex package: Do’s and Don’ts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney\n\n\n\n\n\n\n\n\n\n15.4.1 Demo: Minimum Working Examples in Practice\n\n\n\n\n\nSAS markdown\nPython/Quarto\n\n\n\nNote: You don’t need to know anything about SAS to understand this example.\nA long time ago, when this book covered R and SAS, I had issues with SAS graphs rendering in black and white most of the time.\nI started debugging the issue with the following code chunk:\n```{r sas-cat-aes-map-07, engine=\"sashtml\", engine.path=\"sas\", fig.path = \"image/\"}\nlibname classdat \"sas/\";\n\nPROC SGPLOT data=classdat.fbiwide; \nSCATTER x = Population y = Assault /\n  markerattrs=(size=8pt symbol=circlefilled) \n  group = Abb; /* maps to point color by default */\nRUN;\nQUIT; \n  \nPROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */\nSCATTER x = Population y = Assault /\n  markercharattrs=(size=8) \n  markerchar = Abb /* specify marker character variable */\n    group = Abb\n  ; \nRUN;\nQUIT; \n```\nAfter running the code separately in SAS and getting a figure that looked like what I’d expected, I set out to construct a reproducible example so that I could post to the SASmarkdown github issues page and ask for help.\nThe first thing I did was strip out all of the extra stuff that didn’t need to be in the chunk - this chunk generates 2 pictures; I only need one. This chunk requires the fbiwide data from the classdata R package (that I exported to CSV); I replaced it with a dataset in the sashelp library.\nWhen I was done, the chunk looked like this:\nPROC SGPLOT data=sashelp.snacks;\nSCATTER x = date y = QtySold /\n  markerattrs=(size=8pt symbol=circlefilled)\n  group = product; /* maps to point color by default */\nRUN;\nQUIT;\nThen, I started constructing my reproducible example. I ran ?sas_enginesetup to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file).\nI copied the example from that page:\nindoc &lt;- '\n---\ntitle: \"Basic SASmarkdown Doc\"\nauthor: \"Doug Hemken\"\noutput: html_document\n---\n\n# I've deleted the intermediate chunks because they screw \n# everything up when I print this chunk out\n'\n\nknitr::knit(text=indoc, output=\"test.md\")\nrmarkdown::render(\"test.md\")\nThen, I created several chunks which would do the following: 1. Write the minimal example SAS code above to a file 2. Call that file in a SASmarkdown chunk using the %include macro, which dumps the listed file into the SAS program. This generates the plot using SASmarkdown. 3. Call the file using SAS batch mode\n(this runs the code and produces a plot outside of SASmarkdown, to prove that the issue is SASmarkdown itself)\nFinally, I included the image generated from the batch mode call manually.\nYou can see the resulting code here.\nI pasted my example into the issues page, and then included some additional information:\n\nA screenshot of the rendered page\nThe image files themselves\nA description of what happened\nMy suspicions (some obvious option I’m missing?)\nAn additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks.\n\nThis process took me about 45 minutes, but that was still much shorter than the time I’d spent rerunning code trying to get it to work with no success.\nIn less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. I had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do).\nThen, I had to tinker with the book a bit to figure out if there were easier ways to get the same result. The end result, though, was that I got what I wanted - color figures throughout the book!\n\n\nWhile converting the book from Rmarkdown to quarto, I ran into an issue setting up GitHub Actions (basically, when I push changes, GitHub rebuilds the book from scratch automatically).\nI found an issue describing the same segfault issue I had been getting, and so I made a post there with a new github repository containing a minimal working example that I set up to test the problem.\nWithin 24h, I had gotten replies from people working at RStudio/Posit, and one of them had diagnosed the problem. After I asked a few more questions, one of them submitted a pull request to my repository with a solution.\nI didn’t know enough python or enough about GitHub Actions to diagnose the problem myself, but because I managed to create a reproducible example, I got the answers I needed from people with more experience.\n\n\n\n\n\n\n\n\n\n\n\n15.4.2 Example: Debugging Exercises\n\n\n\nUse this list of StackOverflow posts to try out your new debugging techniques. Can you figure out what’s wrong? What information would you need from the poster in order to come up with a solution? How much time did you spend trying to figure out what the poster was actually asking?",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#debugging-tools",
    "href": "part-gen-prog/06-debugging.html#debugging-tools",
    "title": "15  Debugging",
    "section": "\n15.5 Debugging Tools",
    "text": "15.5 Debugging Tools\nNow that we’ve discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging.\n\n15.5.1 Low tech debugging with print() and other tools\nSometimes called “tracing” techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. When the code is executed, you get a window into what the variables look like during execution.\nThis is called print debugging and it is an incredibly useful tool.\n\n\n\n\n\n\n15.5.1.1 Demo: Nested Functions\n\n\n\n\n\nR\nPython\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n      z + y\n    }\n    cc(3) + 2\n  }\n  x + bb(4)\n}\n\naa(5)\n## [1] 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  print(paste(\"Entering aa(). x = \", x))\n  bb &lt;- function(y) {\n    print(paste(\"Entering bb(). x = \", x, \"y = \", y))\n    cc &lt;- function(z) {\n      print(paste(\"Entering cc(). x = \", x, \"y = \", y, \"z = \", z))\n      cres &lt;- z + y\n      print(paste(\"Returning\", cres, \"from cc()\"))\n      cres\n    }\n    bres &lt;- cc(3) + 2\n    print(paste(\"Returning\", bres, \"from bb()\"))\n    bres\n  }\n  ares &lt;- x + bb(4)\n  print(paste(\"Returning\",ares, \"from aa()\"))\n  ares\n}\n\naa(5)\n## [1] \"Entering aa(). x =  5\"\n## [1] \"Entering bb(). x =  5 y =  4\"\n## [1] \"Entering cc(). x =  5 y =  4 z =  3\"\n## [1] \"Returning 7 from cc()\"\n## [1] \"Returning 9 from bb()\"\n## [1] \"Returning 14 from aa()\"\n## [1] 14\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      return z + y\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  print(\"Entering aa(). x = \" + str(x))\n  def bb(y):\n    print(\"Entering bb(). x = \" + str(x) + \", y = \" + str(y))\n    def cc(z):\n      print(\"Entering cc(). x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))\n      cres = z + y\n      print(\"Returning \" + str(cres) + \" from cc()\")\n      return cres\n    bres = cc(3) + 2\n    print(\"Returning \" + str(bres) + \" from bb()\")\n    return bres\n  ares = x + bb(4)\n  print(\"Returning \" + str(ares) + \" from aa()\")\n  return ares\n\naa(5)\n## Entering aa(). x = 5\n## Entering bb(). x = 5, y = 4\n## Entering cc(). x = 5, y = 4, z = 3\n## Returning 7 from cc()\n## Returning 9 from bb()\n## Returning 14 from aa()\n## 14\n\n\n\n\n\n\nFor more complex data structures, it can be useful to add str(), head(), or summary() functions.\n\n\n\n\n\n\n15.5.1.2 Real world demo: Web Scraping\n\n\n\nIn fall 2020, I wrote a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I used print debugging to show the sequence of tags on the page.\nI was assuming that the order of the parents would be “html”, “body”, “div”, “table”, “tbody”, “tr” - descending from outer to inner (if you know anything about HTML/XML structure).\nTo prevent the site from changing on me (as websites tend to do…), I’ve saved the HTML file here.\n\n\nR\nPython\n\n\n\n\nlibrary(xml2) # read html\n\nsearch_for_parent &lt;- function(node, tag) {\n  # Get all of the parent nodes \n  parents &lt;- xml2::xml_parents(node)\n  # Get the tags of every parent node\n  tags &lt;- purrr::map_chr(parents, rvest::html_name)\n  print(tags)\n  \n  # Find matching tags\n  matches &lt;- which(tags == tag)\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match &lt;- min(matches)\n  if (length(matches) == 1) return(parents[min_match]) else return(NULL)\n}\n\npage &lt;- read_html(\"https://srvanderplas.github.io/stat-computing-r-python/files/realclearpolitics_frag.html\")\n# find all poll results in any table\npoll_results &lt;- xml_find_all(page, \"//td[@class='lp-results']\") \n# find the table that contains it\nsearch_for_parent(poll_results[1], \"table\") \n## [1] \"tr\"    \"tbody\" \"table\" \"div\"   \"body\"  \"html\" \n## [1] 3\n## {xml_nodeset (1)}\n## [1] &lt;table cellpadding=\"2\" cellspacing=\"0\" class=\"sortable\"&gt;\\n&lt;thead&gt;&lt;tr clas ...\n\n\n\nYou may need to pip install lxml requests bs4 to run this code.\n\n# %pip install lxml requests bs4\nfrom bs4 import BeautifulSoup\nimport requests as req\nimport numpy as np\n\n\ndef search_for_parent(node, tag):\n  # Get all of the parent nodes\n  parents = node.find_parents()\n  # get tag type for each parent node\n  tags = [x.name for x in parents]\n  print(tags)\n  \n  # Find matching tags\n  matches = np.array([i for i, val in enumerate(tags) if val == tag])\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match = np.min(matches)\n  if matches.size == 1:\n    ret = parents[min_match]\n  \n  return ret\n\n\nhtml_file = open('shorturl.at/jkS59', 'r')\n## FileNotFoundError: [Errno 2] No such file or directory: 'shorturl.at/jkS59'\npage = html_file.read() \n## NameError: name 'html_file' is not defined\n# Read the page as HTML\nsoup = BeautifulSoup(page, 'html')\n## NameError: name 'page' is not defined\n# Find all poll results in any table\npoll_results = soup.findAll('td', {'class': 'lp-results'})\n## NameError: name 'soup' is not defined\n# Find the table that contains the first poll result\nsearch_for_parent(poll_results[0], 'table')\n## NameError: name 'poll_results' is not defined\n\n\n\n\nBy printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node.\n\n\n\n\n\n\n\n\n15.5.1.3 Example: Hurricanes in R\n\n\n\nNot all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully.\n\n\nSetup\nBuggy code\nSolution 1: Identification\nSolution 2: Fixing\nSolution 3: Verifying\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(maps)\nlibrary(ggthemes)\nworldmap &lt;- map_data(\"world\")\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\n\n\nThe code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nfor (i in 1:5) {\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, lets split the setup from the loop.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nprint(basemap) # make sure the basemap is fine\n\n\n\n\n\n\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\nstr(storms) # make sure the data exists and is formatted as expected\n## tibble [19,537 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ name                        : chr [1:19537] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n##  $ year                        : num [1:19537] 1975 1975 1975 1975 1975 ...\n##  $ month                       : num [1:19537] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day                         : int [1:19537] 27 27 27 27 28 28 28 28 29 29 ...\n##  $ hour                        : num [1:19537] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat                         : num [1:19537] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n##  $ long                        : num [1:19537] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n##  $ status                      : Factor w/ 9 levels \"disturbance\",..: 7 7 7 7 7 7 7 7 8 8 ...\n##  $ category                    : num [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n##  $ wind                        : int [1:19537] 25 25 25 25 25 25 25 30 35 40 ...\n##  $ pressure                    : int [1:19537] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n##  $ tropicalstorm_force_diameter: int [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n##  $ hurricane_force_diameter    : int [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n\nEverything looks ok in the setup chunk…\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, so from this we can see that something is going wrong with our filter statement - we have no rows of data.\n\n\n\nhead(storms)\n## # A tibble: 6 × 13\n##   name   year month   day  hour   lat  long status       category  wind pressure\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n## 1 Amy    1975     6    27     0  27.5 -79   tropical de…       NA    25     1013\n## 2 Amy    1975     6    27     6  28.5 -79   tropical de…       NA    25     1013\n## 3 Amy    1975     6    27    12  29.5 -79   tropical de…       NA    25     1013\n## 4 Amy    1975     6    27    18  30.5 -79   tropical de…       NA    25     1013\n## 5 Amy    1975     6    28     0  31.5 -78.8 tropical de…       NA    25     1012\n## 6 Amy    1975     6    28     6  32.4 -78.7 tropical de…       NA    25     1012\n## # ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;\n\nWhoops. I meant “category” when I typed “status”.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 13 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, that’s something, at least. We now have some data for category 1 storms…\n\nfilter(storms, year == 2013) %&gt;%\n  # Get max category for each named storm\n  group_by(name) %&gt;%\n  filter(category == max(category)) %&gt;%\n  ungroup() %&gt;%\n  # See what categories exist\n  select(name, category) %&gt;%\n  unique()\n## # A tibble: 0 × 2\n## # ℹ 2 variables: name &lt;chr&gt;, category &lt;dbl&gt;\n\nIt looks like 2013 was just an incredibly quiet year for tropical activity.\n\n\n2013 may have been a quiet year for tropical activity in the Atlantic, but 2004 was not. So let’s just make sure our code works by checking out 2004.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2004) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 49 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 51 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 43 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 49 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 12 ncol 13\"\n\n\n\n\n\n\n\nIf we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code).\n\nfor (i in 1:5) {\n  # print(paste0(\"Category \", i, \" storms\"))\n  \n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  # print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n  #       # str(subdata) works too, but produces more clutter. I started\n  #       # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  \n  if (nrow(subdata) &gt; 0) print(plot) \n}\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you’ve found your problem, go back and delete or comment out your print statements, as they’re no longer necessary. If you think you may need them again, comment them out, otherwise, just delete them so that your code is neat, clean, and concise.\n\n15.5.2 After an error has occurred - traceback()\n\ntraceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This may help you identify which function is actually causing the problem, which is especially useful when you have nested functions or are using package functions that depend on other packages.\n\n\n\n\n\n\n15.5.2.1 Demo: Using traceback\n\n\n\n\n\nR\nPython\n\n\n\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n     stop('there was a problem')  # This generates an error\n    }\n    cc()\n  }\n  bb()\n}\n\naa()\n## Error in cc(): there was a problem\n\nFor more information, you could run traceback\n\ntraceback()\n\nWhich will provide the following output:\n4: stop(\"there was a problem\") at #4\n3: c() at #6\n2: b() at #8\n1: a()\nReading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code.\nIf you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error.\n\n\nBoth Show Traceback and Rerun with Debug are useful tools\n\nIf you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. \n\n\n\nimport sys,traceback\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      try: \n        return y + z + tuple()[0] # This generates an error\n      except IndexError:\n        exc_type, exc_value, exc_tb = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_tb, file = sys.stdout)\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\nPython’s traceback information is a bit more low-level and requires a bit more from the programmer than R’s version.\n\n\n\n\n\n\n15.5.3 Interactive Debugging\n\n\n\n\nIn R, the interactive debugging tool of choice is the browser() function. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on.\n\n\n\n\nIn python, the equivalent interactive debugger is ipdb. You can install it with pip install ipdb. If you want to run Python in the interactive ipython console, then you can invoke the ipdb debugging with %debug get_xkcd() (this may or may not work in your RStudio python via reticulate session). You can also insert ipdb.set_trace() in the middle of your function (before the error), and it will function more or less the same as browser() in R.\n\n\n\n\n\n\n15.5.3.1 Additional ipdb Options\n\n\n\n\n\nIf you’re working in Python in RStudio and you run into ipdb problems, you can get into debug mode in a more involved way. To run code using ipdb when your code hits an error, add from ipdb import launch_ipdb_on_exception to the top of your python code chunk. Then, at the bottom, put any lines that may trigger the error after these two lines:\nif __name__ == \"__main__\":\n  with launch_ipdb_on_exception():\n    &lt;your properly indented code goes here&gt;\nThis ensures that ipdb is launched when an error is reached.\n\n\n\n\n\n\n\n\n\nExample: Interactive Debugging with XKCD\n\n\n\n\n\nProblem\nR starter code\nR solution\nPython\nPython solution\n\n\n\nI wanted to write a function that will plot an xkcd comic, but it isn’t working. I’ve provided starter code for R and python in the next two tabs. Use the interactive browser command to figure out what is going on and fix the problem.\n\n\nI start with\n\nlibrary(png)\nlibrary(xml2)\nlibrary(dplyr)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  \n  readPNG(source = image)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n## Error in readPNG(source = image): unable to open //imgs.xkcd.com/comics/archaeology_research.png\n\n\n\nInserting `browser() at the top of the get_xkcd function (inside the function call) lets us explore what’s going wrong.\nThe first issue is that the URL for the image isn’t fully formed – it’s missing the https: component (or, it has 2 extra slashes at the front–take your pick).\nThen, instead of reading the PNG from the URL, we need to download it first and then read it in from the downloaded file.\nHere’s the final function, since this is somewhat hard to actually demonstrate because it’s interactive…\n\nlibrary(png)\nlibrary(xml2)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  \n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  # Fix image address so that we can access the image\n  # Remove the first two characters \n  # (start at character 3 and go to the end)\n  image &lt;- substr(image, 3, nchar(image))\n  \n  # Download the file to a temp file and read from there\n  file_location &lt;- tempfile(fileext = \".png\")\n  download.file(image, destfile = file_location, quiet = T)\n  \n  readPNG(source = file_location)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\nI start with\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\nfrom ipdb import launch_ipdb_on_exception\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## ValueError: unknown url type: '//imgs.xkcd.com/comics/archaeology_research.png'\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\nimport ipdb # load IPDB\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  ipdb.set_trace() # Set a breakpoint to enter debugging\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  \n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\nplt.show()\n\nThe first issue is that the URL for the image isn’t fully formed – it’s missing the https: component (or, it has 2 extra slashes at the front–take your pick).\nHere’s the final function:\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\nfrom ipdb import launch_ipdb_on_exception\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.5.3.2 Example: More Interactive Debugging with XKCD\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nEach xkcd has a corresponding ID number (ordered sequentially from 1 to 2722 at the time this was written). Modify the XKCD functions above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic.\nUse interactive debugging tools to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL.\nWhat things might you add to make this function “defensive programming” compatible?\n\n\n\n# get the most current xkcd or the specified number\nget_xkcd &lt;- function(id = NULL) {\n  if (is.null(id)) {\n    # Have to get the location of the image ourselves\n    url &lt;- \"http://xkcd.com\"\n  } else if (is.numeric(id)) {\n    url &lt;- paste0(\"http://xkcd.com/\", id, \"/\")\n  } else {\n    # only allow numeric or null input\n    stop(\"To get current xkcd, pass in NULL, otherwise, pass in a valid comic number\")\n  }\n\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  # Fix image address so that we can access the image\n  image &lt;- substr(image, 3, nchar(image)) # cut the first 2 characters off\n\n  # make temp file\n  location &lt;- tempfile(fileext = \"png\")\n  download.file(image, destfile = location, quiet = T)\n\n  # This checks to make sure we saved the file correctly\n  if (file.exists(location)) {\n    readPNG(source = location)\n  } else {\n    # Give a good informative error message\n    stop(paste(\"Something went wrong saving the image at \", image, \" to \", location))\n  }\n}\n\nget_xkcd(2259) %&gt;%\n  as.raster() %&gt;% \n  plot()\n\n\n\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd(id=''):\n  image = 0 # Defining a placeholder\n  \n  if id == '':\n    # Have to get the location of the image ourselves\n    url = \"http://xkcd.com\"\n  elif id.isnumeric():\n    url = \"http://xkcd.com/\" + id + \"/\"\n  else:\n    # only allow numeric or null input\n    raise TypeError(\"To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\")\n  \n  # Print debugging left in for your amusement\n  # print(type(id))\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imnode = soup.select('#comic &gt; img')\n  \n  try:\n    imlink = imnode[0].get('src')\n  except:\n    raise Exception(\"No comic could be found with number \" + id + \" (url = \"+ url+ \" )\")\n  \n  try: \n    # Format as a numpy array\n    image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n    return image\n  except: \n    raise Exception(\"Reading the image failed. Check to make sure an image exists at \" + url)\n    return(None)\n\n\nres = get_xkcd('')\nplt.imshow(res)\nplt.show()\n\n\n\n\n\n\nres = get_xkcd('3000')\n\nres = get_xkcd('abcd')\n\nTypeError: To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\n\n\n\n\n\n\n\n\n15.5.4 R debug()\n\nIn the traceback() Rstudio output, the other option is “rerun with debug”. In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow.\ndebug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code.\n\n\n\n\n\n\n15.5.4.1 Demo: debug() in R\n\n\n\n\ndata(iris)\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n## \n## Call:\n## lm(formula = Species ~ ., data = iris)\n## \n## Residuals:\n## Error in quantile.default(resid): (unordered) factors are not allowed\n\nWe get this weird warning, and then an error about factors when we use summary() to look at the coefficients.\n\ndebug(lm) # turn debugging on\n\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n\nundebug(lm) # turn debugging off\n\n\n\nThe first thing I see when I run lm after turning on debug (screenshot)\n\n\n\nThe variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function\n\nAfter pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function.\n\n\nStepping through the function. The arrow on the left side in the editor window shows which line of code we’re currently at.\n\nWe can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong.\nWe probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened.\n\n\nI can hit “Stop” or type “Q” to exit the debug environment.\n\nBut, until I run undebug(lm), every call to lm will take me into the debug window.\n\n\nundebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run.\n\n\n\n\n\n\n15.5.4.2 Example: debug in R\n\n\n\n\n\nProblem\nSolution\n\n\n\nlarger(x, y) is supposed to return the elementwise maximum of two vectors.\n\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  x[y.is.bigger] &lt;- y[y.is.bigger] \n  x\n} \n\nlarger(c(1, 5, 10), c(2, 4, 11))\n## [1]  2  5 11\n\n\nlarger(c(1, 5, 10), 6)\n## [1]  6 NA 10\n\nWhy is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it.\n\n\nI’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined\n\nx &lt;- c(1, 5, 10)\ny &lt;- 6\n\n# Inside of larger() with x = c(1, 5, 10), y = 6\n(y.is.bigger &lt;- y &gt; x ) # putting something in () prints it out\n## [1]  TRUE  TRUE FALSE\ny[y.is.bigger] # This isn't quite what we were going for, but it's what's causing the issue\n## [1]  6 NA\nx[y.is.bigger] # What gets replaced\n## [1] 1 5\n\n\n# Better option\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  ifelse(y.is.bigger, y, x)\n}",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#sec-debugging-refs",
    "href": "part-gen-prog/06-debugging.html#sec-debugging-refs",
    "title": "15  Debugging",
    "section": "\n15.6 References",
    "text": "15.6 References\n\n\n\n\n[1] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[2] \nH. Wickham et al., “Welcome to the tidyverse,” Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686. \n\n\n[3] \nK. Ushey, Renv: Project environments. 2022 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[4] \nH. Wickham and J. Bryan, R Packages: Organize, Test, Document, and Share Your Code, 1st ed. Sebastopol, CA: O’Reilly, 2015 [Online]. Available: https://r-pkgs.org/. [Accessed: Sep. 23, 2022]\n\n\n[5] \nT. Beuzen and T. Timbers, Python Packages, 1st edition. Boca Raton: Chapman; Hall/CRC, 2022 [Online]. Available: https://py-pkgs.org/\n\n\n\n[6] \nJ. Evans, “A debugging manifesto https://t.co/3eSOFQj1e1,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570060516839641092. [Accessed: Sep. 21, 2022]\n\n\n[7] \nNasser_Junior, “User.fist_name https://t.co/lxrf3IFO4x,” Twitter. Aug. 2020 [Online]. Available: https://twitter.com/Nasser_Junior/status/1295805928315531264. [Accessed: Sep. 21, 2022]\n\n\n[8] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nJ. Evans, “Debugging strategy: Reread the error message https://t.co/2BZHhPg04h,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570463473011920897. [Accessed: Sep. 21, 2022]\n\n\n[10] \nJ. Evans, “Debugging strategy: Shorten your feedback loop https://t.co/1cByDlafsK,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1549164800978059264. [Accessed: Sep. 21, 2022]\n\n\n[11] \nJ. Evans, “Debugging strategy: Write a tiny program https://t.co/Kajr5ZyeIp,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1547247776001654786. [Accessed: Sep. 21, 2022]\n\n\n[12] \nJ. Evans, “Debugging strategy: Change working code into broken code https://t.co/1T5uNDDFs0,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1545099244238946304. [Accessed: Sep. 21, 2022]\n\n\n[13] \nJ. Evans, “Debugging strategy: Come up with one question https://t.co/2Lytzl4laQ,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1554120424602193921. [Accessed: Sep. 21, 2022]\n\n\n[14] \nJ. Evans, “Debugging strategy: Write a unit test https://t.co/mC01DBNyM3,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1561718747504803842. [Accessed: Sep. 21, 2022]\n\n\n[15] \nT. Monteiro, “Improve how you code: Understanding rubber duck debugging. Duckly blog,” Oct. 31, 2019. [Online]. Available: https://duckly.com/blog/improve-how-to-code-with-rubber-duck-debugging/. [Accessed: Jan. 11, 2023]\n\n\n[16] \nS. Grimes, “This 500-Year-Old Piece of Advice Can Help You Solve Your Modern Problems,” Forge. Dec. 2019 [Online]. Available: https://forge.medium.com/the-500-year-old-piece-of-advice-that-will-change-your-life-1e580f115731. [Accessed: Sep. 21, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#footnotes",
    "href": "part-gen-prog/06-debugging.html#footnotes",
    "title": "15  Debugging",
    "section": "",
    "text": "Some people use cats, but I find that they don’t meet the nonjudgmental criteria. Of course, they’re equally judgmental whether your code works or not, so maybe that works if you’re a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you’re upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html",
    "href": "part-gen-prog/07-prog-data.html",
    "title": "16  Programming With Data",
    "section": "",
    "text": "Objectives\nAt this point, you’ve learned how to write functions. You know the basics of how to create new variables, how data frames and lists work, and how to use markdown.\nAnd yet… these are skills that take some practice when applied to new data. We’re going to take a break from the fire-hose of syntax you’ve learned and focus on applying what you’ve learned to problems related to data. The goal is to reinforce the skills you’ve already learned and help you find your feet a bit as you work through data analysis.\nI’ll provide sample code for tasks like basic plots and tables that we haven’t covered yet - you should feel free to modify and tinker with these chunks as you go along. This chapter will also provide a preview of some of the packages we’re going to work with in the next few sections (because I’m going to show you some code for e.g. summarizing a dataset and plot a few things, even without having covered that material).\nAs you’ve probably guessed by now, this section will primarily be focused on examples.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#objectives",
    "href": "part-gen-prog/07-prog-data.html#objectives",
    "title": "16  Programming With Data",
    "section": "",
    "text": "Write functions to create simple plots and data summaries\nApply syntax knowledge to reference variables and observations in common data structures\nCreate new variables and columns or reformat existing columns in provided data structures",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#artwork-dimensions",
    "href": "part-gen-prog/07-prog-data.html#artwork-dimensions",
    "title": "16  Programming With Data",
    "section": "\n16.1 Artwork Dimensions",
    "text": "16.1 Artwork Dimensions\nThe Tate Art Museum assembled a collection of 70,000 artworks (last updated in 2014). They cataloged information including accession number, artwork dimensions, units, title, date, medium, inscription, and even URLs for images of the art.\n\n16.1.1 Reading in the Data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\nartwork &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\nimport pandas as pd\nartwork = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\n\n16.1.2 Basic Summaries\nWhen you first access a new dataset, it’s fun to explore it a bit. I’ve shown a summary of the variables (character variables summarized with completion rates and # unique values, numeric variables summarized with quantiles and mean/sd) generated using the R skimr and Python skimpy packages (which we’ll talk about in the next chapter).\n\n\nR\nPython (pandas)\nPython (skimpy)\n\n\n\nYou may need to run install.packages(\"skimr\") in the R terminal if you have not used the package before.\n\nlibrary(skimr)\nskim(artwork)\n\n\nData summary\n\n\nName\nartwork\n\n\nNumber of rows\n69201\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nlogical\n1\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\naccession_number\n0\n1.00\n6\n7\n0\n69201\n0\n\n\nartist\n0\n1.00\n4\n120\n0\n3336\n0\n\n\nartistRole\n0\n1.00\n5\n24\n0\n19\n0\n\n\ntitle\n0\n1.00\n1\n320\n0\n43529\n0\n\n\ndateText\n0\n1.00\n4\n75\n0\n2736\n0\n\n\nmedium\n6384\n0.91\n3\n120\n0\n3401\n0\n\n\ncreditLine\n3\n1.00\n14\n820\n0\n3209\n0\n\n\ndimensions\n2433\n0.96\n4\n248\n0\n25575\n0\n\n\nunits\n3341\n0.95\n2\n2\n0\n1\n0\n\n\ninscription\n62895\n0.09\n14\n14\n0\n1\n0\n\n\nthumbnailUrl\n10786\n0.84\n55\n57\n0\n58415\n0\n\n\nurl\n0\n1.00\n48\n134\n0\n69201\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nthumbnailCopyright\n69201\n0\nNaN\n:\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1.00\n39148.03\n25980.47\n3\n19096.00\n37339\n54712\n129068\n▇▇▅▁▁\n\n\nartistId\n0\n1.00\n1201.06\n2019.42\n0\n558.00\n558\n1137\n19232\n▇▁▁▁▁\n\n\nyear\n5397\n0.92\n1867.23\n72.01\n1545\n1817.00\n1831\n1953\n2012\n▁▁▇▆▆\n\n\nacquisitionYear\n45\n1.00\n1910.65\n64.20\n1823\n1856.00\n1856\n1982\n2013\n▇▁▁▁▅\n\n\nwidth\n3367\n0.95\n323.47\n408.81\n3\n118.00\n175\n345\n11960\n▇▁▁▁▁\n\n\nheight\n3342\n0.95\n346.44\n538.04\n6\n117.00\n190\n359\n37500\n▇▁▁▁▁\n\n\ndepth\n66687\n0.04\n479.20\n1051.14\n1\n48.25\n190\n450\n18290\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n# Base pandas\nartwork.describe()\n##                   id      artistId  ...         depth  thumbnailCopyright\n## count   69201.000000  69201.000000  ...   2514.000000                 0.0\n## mean    39148.026213   1201.063251  ...    479.197772                 NaN\n## std     25980.468687   2019.422535  ...   1051.141734                 NaN\n## min         3.000000      0.000000  ...      1.000000                 NaN\n## 25%     19096.000000    558.000000  ...     48.250000                 NaN\n## 50%     37339.000000    558.000000  ...    190.000000                 NaN\n## 75%     54712.000000   1137.000000  ...    450.000000                 NaN\n## max    129068.000000  19232.000000  ...  18290.000000                 NaN\n## \n## [8 rows x 8 columns]\n\n\n\nYou may need to run pip install skimpy in the terminal if you have not used the package before.\n\n# Skimpy package - like skimr\nfrom skimpy import skim\nskim(artwork)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 69201  │ │ string      │ 12    │                       │\n## │ │ Number of columns │ 20     │ │ float64     │ 6     │                       │\n## │ └───────────────────┴────────┘ │ int64       │ 2     │                       │\n## │                                └─────────────┴───────┘                       │\n## │                                  All null                                    │\n## │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n## │ ┃ column                                     ┃ NA            ┃ NA %       ┃  │\n## │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n## │ │ thumbnailCopyright                         │         69201 │        100 │  │\n## │ └────────────────────────────────────────────┴───────────────┴────────────┘  │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━┳━━━━━━┳━━━━━┳━━━━━━┳━━━━━┓  │\n## │ ┃ colu ┃      ┃      ┃      ┃      ┃      ┃     ┃      ┃     ┃      ┃ his ┃  │\n## │ ┃ mn   ┃ NA   ┃ NA % ┃ mean ┃ sd   ┃ p0   ┃ p25 ┃ p50  ┃ p75 ┃ p100 ┃ t   ┃  │\n## │ ┡━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━╇━━━━━━╇━━━━━╇━━━━━━╇━━━━━┩  │\n## │ │ id   │    0 │    0 │ 3915 │ 2598 │    3 │ 191 │ 3734 │ 547 │ 1291 │ ▇██ │  │\n## │ │      │      │      │    0 │    0 │      │  00 │    0 │  10 │   00 │ ▁▁▁ │  │\n## │ │ arti │    0 │    0 │ 1201 │ 2019 │    0 │ 558 │  558 │ 113 │ 1923 │  █  │  │\n## │ │ stId │      │      │      │      │      │     │      │   7 │    0 │     │  │\n## │ │ year │ 5397 │ 7.79 │ 1867 │ 72.0 │ 1545 │ 181 │ 1831 │ 195 │ 2012 │     │  │\n## │ │      │      │ 9020 │      │    1 │      │   7 │      │   3 │      │ █▁▃ │  │\n## │ │      │      │ 2453 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │ 7217 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │    6 │      │      │      │     │      │     │      │     │  │\n## │ │ acqu │   45 │ 0.06 │ 1911 │ 64.2 │ 1823 │ 185 │ 1856 │ 198 │ 2013 │  █  │  │\n## │ │ isit │      │ 5027 │      │      │      │   6 │      │   2 │      │ ▁▂▄ │  │\n## │ │ ionY │      │ 9620 │      │      │      │     │      │     │      │     │  │\n## │ │ ear  │      │ 2367 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │  017 │      │      │      │     │      │     │      │     │  │\n## │ │ widt │ 3367 │ 4.86 │ 323. │ 408. │    3 │ 118 │  175 │ 345 │ 1196 │  █  │  │\n## │ │ h    │      │ 5536 │    5 │    8 │      │     │      │     │    0 │     │  │\n## │ │      │      │ 6251 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │ 9327 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │    7 │      │      │      │     │      │     │      │     │  │\n## │ │ heig │ 3342 │ 4.82 │ 346. │  538 │    6 │ 117 │  190 │ 359 │ 3750 │  █  │  │\n## │ │ ht   │      │ 9409 │    4 │      │      │     │      │     │    0 │     │  │\n## │ │      │      │ 9796 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │ 2457 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │    2 │      │      │      │     │      │     │      │     │  │\n## │ │ dept │ 6668 │ 96.3 │ 479. │ 1051 │    1 │ 48. │  190 │ 450 │ 1829 │  █  │  │\n## │ │ h    │    7 │ 6710 │    2 │      │      │  25 │      │     │    0 │     │  │\n## │ │      │      │ 4521 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │ 6109 │      │      │      │     │      │     │      │     │  │\n## │ │      │      │    5 │      │      │      │     │      │     │      │     │  │\n## │ └──────┴──────┴──────┴──────┴──────┴──────┴─────┴──────┴─────┴──────┴─────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┓  │\n## │ ┃      ┃      ┃       ┃      ┃       ┃      ┃       ┃ char ┃       ┃ tota ┃  │\n## │ ┃      ┃      ┃       ┃      ┃       ┃      ┃       ┃ s    ┃ words ┃ l    ┃  │\n## │ ┃ colu ┃      ┃       ┃ shor ┃ longe ┃      ┃       ┃ per  ┃ per   ┃ word ┃  │\n## │ ┃ mn   ┃ NA   ┃ NA %  ┃ test ┃ st    ┃ min  ┃ max   ┃ row  ┃ row   ┃ s    ┃  │\n## │ ┡━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━┩  │\n## │ │ acce │    0 │     0 │ A000 │ AR000 │ A000 │ T1386 │ 6.02 │     1 │ 6920 │  │\n## │ │ ssio │      │       │ 01   │ 01    │ 01   │ 9     │      │       │    1 │  │\n## │ │ n_nu │      │       │      │       │      │       │      │       │      │  │\n## │ │ mber │      │       │      │       │      │       │      │       │      │  │\n## │ │ arti │    0 │     0 │ Erté │ Art & │ ?Bri │ Štyrs │ 23.9 │   3.3 │ 2265 │  │\n## │ │ st   │      │       │      │ Langu │ tish │ ký,   │      │       │   31 │  │\n## │ │      │      │       │      │ age   │      │ Jindr │      │       │      │  │\n## │ │      │      │       │      │ (Terr │ Scho │ ich   │      │       │      │  │\n## │ │      │      │       │      │ y     │ ol   │       │      │       │      │  │\n## │ │      │      │       │      │ Atkin │      │       │      │       │      │  │\n## │ │      │      │       │      │ son,  │      │       │      │       │      │  │\n## │ │      │      │       │      │ born  │      │       │      │       │      │  │\n## │ │      │      │       │      │ 1939; │      │       │      │       │      │  │\n## │ │      │      │       │      │ David │      │       │      │       │      │  │\n## │ │      │      │       │      │ Bainb │      │       │      │       │      │  │\n## │ │      │      │       │      │ ridge │      │       │      │       │      │  │\n## │ │      │      │       │      │ ,     │      │       │      │       │      │  │\n## │ │      │      │       │      │ born  │      │       │      │       │      │  │\n## │ │      │      │       │      │ 1941; │      │       │      │       │      │  │\n## │ │      │      │       │      │ Micha │      │       │      │       │      │  │\n## │ │      │      │       │      │ el    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Baldw │      │       │      │       │      │  │\n## │ │      │      │       │      │ in,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ born  │      │       │      │       │      │  │\n## │ │      │      │       │      │ 1945; │      │       │      │       │      │  │\n## │ │      │      │       │      │ Harol │      │       │      │       │      │  │\n## │ │      │      │       │      │ d     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Hurre │      │       │      │       │      │  │\n## │ │      │      │       │      │ ll,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ born  │      │       │      │       │      │  │\n## │ │ arti │    0 │     0 │ afte │ doubt │ afte │ style │    6 │     1 │ 6950 │  │\n## │ │ stRo │      │       │ r    │ fully │ r    │ of    │      │       │    4 │  │\n## │ │ le   │      │       │      │       │      │       │      │       │      │  │\n## │ │      │      │       │      │ attri │      │       │      │       │      │  │\n## │ │      │      │       │      │ buted │      │       │      │       │      │  │\n## │ │      │      │       │      │  to   │      │       │      │       │      │  │\n## │ │ titl │    0 │     0 │ I    │ Dans  │ !    │ “What │ 29.9 │     5 │ 3460 │  │\n## │ │ e    │      │       │      │ plusi │ 1971 │ is to │      │       │   84 │  │\n## │ │      │      │       │      │ eurs  │      │ be    │      │       │      │  │\n## │ │      │      │       │      │ de    │      │ done? │      │       │      │  │\n## │ │      │      │       │      │ ces   │      │ ”     │      │       │      │  │\n## │ │      │      │       │      │ forêt │      │ 1984. │      │       │      │  │\n## │ │      │      │       │      │ s et  │      │ Alter │      │       │      │  │\n## │ │      │      │       │      │ de    │      │ nativ │      │       │      │  │\n## │ │      │      │       │      │ ces   │      │ e     │      │       │      │  │\n## │ │      │      │       │      │ bois, │      │ Techn │      │       │      │  │\n## │ │      │      │       │      │ il    │      │ ology │      │       │      │  │\n## │ │      │      │       │      │ n’y   │      │       │      │       │      │  │\n## │ │      │      │       │      │ avait │      │ Versu │      │       │      │  │\n## │ │      │      │       │      │ pas   │      │ s     │      │       │      │  │\n## │ │      │      │       │      │ seule │      │ Nucle │      │       │      │  │\n## │ │      │      │       │      │ ment  │      │ ar    │      │       │      │  │\n## │ │      │      │       │      │ des   │      │ Power │      │       │      │  │\n## │ │      │      │       │      │ villa │      │       │      │       │      │  │\n## │ │      │      │       │      │ ges   │      │       │      │       │      │  │\n## │ │      │      │       │      │ soute │      │       │      │       │      │  │\n## │ │      │      │       │      │ rrain │      │       │      │       │      │  │\n## │ │      │      │       │      │ s     │      │       │      │       │      │  │\n## │ │      │      │       │      │ group │      │       │      │       │      │  │\n## │ │      │      │       │      │ és    │      │       │      │       │      │  │\n## │ │      │      │       │      │ autou │      │       │      │       │      │  │\n## │ │      │      │       │      │ rs du │      │       │      │       │      │  │\n## │ │      │      │       │      │ terri │      │       │      │       │      │  │\n## │ │      │      │       │      │ er du │      │       │      │       │      │  │\n## │ │      │      │       │      │ chef  │      │       │      │       │      │  │\n## │ │      │      │       │      │ mais  │      │       │      │       │      │  │\n## │ │      │      │       │      │ il y  │      │       │      │       │      │  │\n## │ │      │      │       │      │ avait │      │       │      │       │      │  │\n## │ │      │      │       │      │ encor │      │       │      │       │      │  │\n## │ │      │      │       │      │ e de  │      │       │      │       │      │  │\n## │ │      │      │       │      │ vérit │      │       │      │       │      │  │\n## │ │      │      │       │      │ ables │      │       │      │       │      │  │\n## │ │      │      │       │      │       │      │       │      │       │      │  │\n## │ │      │      │       │      │ hamea │      │       │      │       │      │  │\n## │ │      │      │       │      │ ux de │      │       │      │       │      │  │\n## │ │      │      │       │      │ hutte │      │       │      │       │      │  │\n## │ │      │      │       │      │ s     │      │       │      │       │      │  │\n## │ │      │      │       │      │ basse │      │       │      │       │      │  │\n## │ │      │      │       │      │ s     │      │       │      │       │      │  │\n## │ │      │      │       │      │ caché │      │       │      │       │      │  │\n## │ │      │      │       │      │ s     │      │       │      │       │      │  │\n## │ │      │      │       │      │ sous  │      │       │      │       │      │  │\n## │ │      │      │       │      │ les   │      │       │      │       │      │  │\n## │ │      │      │       │      │ arbre │      │       │      │       │      │  │\n## │ │      │      │       │      │ s, et │      │       │      │       │      │  │\n## │ │      │      │       │      │ si    │      │       │      │       │      │  │\n## │ │      │      │       │      │ nombr │      │       │      │       │      │  │\n## │ │      │      │       │      │ eaux  │      │       │      │       │      │  │\n## │ │      │      │       │      │ que   │      │       │      │       │      │  │\n## │ │      │      │       │      │ parfo │      │       │      │       │      │  │\n## │ │      │      │       │      │ is la │      │       │      │       │      │  │\n## │ │      │      │       │      │ forêt │      │       │      │       │      │  │\n## │ │      │      │       │      │ en    │      │       │      │       │      │  │\n## │ │      │      │       │      │ était │      │       │      │       │      │  │\n## │ │      │      │       │      │ rempl │      │       │      │       │      │  │\n## │ │      │      │       │      │ ie.   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Souve │      │       │      │       │      │  │\n## │ │      │      │       │      │ nt    │      │       │      │       │      │  │\n## │ │      │      │       │      │ les   │      │       │      │       │      │  │\n## │ │      │      │       │      │ fumée │      │       │      │       │      │  │\n## │ │      │      │       │      │ s les │      │       │      │       │      │  │\n## │ │      │      │       │      │ trahi │      │       │      │       │      │  │\n## │ │      │      │       │      │ ssaie │      │       │      │       │      │  │\n## │ │      │      │       │      │ nt.   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Deux  │      │       │      │       │      │  │\n## │ │      │      │       │      │ de... │      │       │      │       │      │  │\n## │ │ date │    0 │     0 │ 1870 │ 1915– │ 1545 │ publi │ 6.27 │   1.2 │ 8528 │  │\n## │ │ Text │      │       │      │ 23,   │      │ shed  │      │       │    8 │  │\n## │ │      │      │       │      │ recon │      │ c.186 │      │       │      │  │\n## │ │      │      │       │      │ struc │      │ 0     │      │       │      │  │\n## │ │      │      │       │      │ tion  │      │       │      │       │      │  │\n## │ │      │      │       │      │ by    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Richa │      │       │      │       │      │  │\n## │ │      │      │       │      │ rd    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Hamil │      │       │      │       │      │  │\n## │ │      │      │       │      │ ton   │      │       │      │       │      │  │\n## │ │      │      │       │      │ 1965– │      │       │      │       │      │  │\n## │ │      │      │       │      │ 6,    │      │       │      │       │      │  │\n## │ │      │      │       │      │ lower │      │       │      │       │      │  │\n## │ │      │      │       │      │ panel │      │       │      │       │      │  │\n## │ │      │      │       │      │ remad │      │       │      │       │      │  │\n## │ │      │      │       │      │ e     │      │       │      │       │      │  │\n## │ │      │      │       │      │ 1985  │      │       │      │       │      │  │\n## │ │ medi │ 6384 │ 9.225 │ Wax  │ Acryl │ 10   │ ink   │ 21.7 │   3.4 │ 2351 │  │\n## │ │ um   │      │ 30021 │      │ ic    │ phot │ and   │      │       │   15 │  │\n## │ │      │      │ 24246 │      │ paint │ o-et │ gouac │      │       │      │  │\n## │ │      │      │    76 │      │ , oil │ chin │ he on │      │       │      │  │\n## │ │      │      │       │      │ paint │ gs   │ paper │      │       │      │  │\n## │ │      │      │       │      │ ,     │ on   │       │      │       │      │  │\n## │ │      │      │       │      │ shell │ pape │       │      │       │      │  │\n## │ │      │      │       │      │ ac,   │ r    │       │      │       │      │  │\n## │ │      │      │       │      │ earth │      │       │      │       │      │  │\n## │ │      │      │       │      │ ,     │      │       │      │       │      │  │\n## │ │      │      │       │      │ sand, │      │       │      │       │      │  │\n## │ │      │      │       │      │ wood, │      │       │      │       │      │  │\n## │ │      │      │       │      │ paper │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ glass │      │       │      │       │      │  │\n## │ │      │      │       │      │ on 2  │      │       │      │       │      │  │\n## │ │      │      │       │      │ canva │      │       │      │       │      │  │\n## │ │      │      │       │      │ ses,  │      │       │      │       │      │  │\n## │ │      │      │       │      │ lead, │      │       │      │       │      │  │\n## │ │      │      │       │      │ iron, │      │       │      │       │      │  │\n## │ │      │      │       │      │ books │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ other │      │       │      │       │      │  │\n## │ │      │      │       │      │ mater │      │       │      │       │      │  │\n## │ │      │      │       │      │ ia    │      │       │      │       │      │  │\n## │ │ cred │    3 │ 0.004 │ Purc │ Purch │ ARTI │ [unco │ 57.1 │    10 │ 6984 │  │\n## │ │ itLi │      │ 33519 │ hase │ ased  │ ST   │ vered │      │       │   69 │  │\n## │ │ ne   │      │ 74682 │ d    │ joint │ ROOM │       │      │       │      │  │\n## │ │      │      │ 44679 │ 1912 │ ly by │ S    │ durin │      │       │      │  │\n## │ │      │      │       │      │ Tate, │ Acqu │ g     │      │       │      │  │\n## │ │      │      │       │      │ with  │ ired │ remou │      │       │      │  │\n## │ │      │      │       │      │ assis │      │ nting │      │       │      │  │\n## │ │      │      │       │      │ tance │ join │  of   │      │       │      │  │\n## │ │      │      │       │      │  from │ tly  │ T0304 │      │       │      │  │\n## │ │      │      │       │      │ the   │ with │ 8]    │      │       │      │  │\n## │ │      │      │       │      │ Ameri │ the  │ 1980  │      │       │      │  │\n## │ │      │      │       │      │ can   │ Nati │       │      │       │      │  │\n## │ │      │      │       │      │ Patro │ onal │       │      │       │      │  │\n## │ │      │      │       │      │ ns    │      │       │      │       │      │  │\n## │ │      │      │       │      │ for   │ Gall │       │      │       │      │  │\n## │ │      │      │       │      │ Tate  │ erie │       │      │       │      │  │\n## │ │      │      │       │      │ and   │ s of │       │      │       │      │  │\n## │ │      │      │       │      │ the   │ Scot │       │      │       │      │  │\n## │ │      │      │       │      │ Latin │ land │       │      │       │      │  │\n## │ │      │      │       │      │ Ameri │      │       │      │       │      │  │\n## │ │      │      │       │      │ can   │ thro │       │      │       │      │  │\n## │ │      │      │       │      │ Acqui │ ugh  │       │      │       │      │  │\n## │ │      │      │       │      │ sitio │ The  │       │      │       │      │  │\n## │ │      │      │       │      │ ns    │ d'Of │       │      │       │      │  │\n## │ │      │      │       │      │ Commi │ fay  │       │      │       │      │  │\n## │ │      │      │       │      │ ttee; │ Dona │       │      │       │      │  │\n## │ │      │      │       │      │  and  │ tion │       │      │       │      │  │\n## │ │      │      │       │      │ Albri │      │       │      │       │      │  │\n## │ │      │      │       │      │ ght-K │ with │       │      │       │      │  │\n## │ │      │      │       │      │ nox   │ assi │       │      │       │      │  │\n## │ │      │      │       │      │ Art   │ stan │       │      │       │      │  │\n## │ │      │      │       │      │ Galle │ ce   │       │      │       │      │  │\n## │ │      │      │       │      │ ry,   │ from │       │      │       │      │  │\n## │ │      │      │       │      │ Buffa │ the  │       │      │       │      │  │\n## │ │      │      │       │      │ lo,   │ Nati │       │      │       │      │  │\n## │ │      │      │       │      │ with  │ onal │       │      │       │      │  │\n## │ │      │      │       │      │ funds │      │       │      │       │      │  │\n## │ │      │      │       │      │ from  │ Heri │       │      │       │      │  │\n## │ │      │      │       │      │ Charl │ tage │       │      │       │      │  │\n## │ │      │      │       │      │ es    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Clift │ Memo │       │      │       │      │  │\n## │ │      │      │       │      │ on,   │ rial │       │      │       │      │  │\n## │ │      │      │       │      │ James │      │       │      │       │      │  │\n## │ │      │      │       │      │ S.    │ Fund │       │      │       │      │  │\n## │ │      │      │       │      │ Ely,  │ and  │       │      │       │      │  │\n## │ │      │      │       │      │ Charl │ the  │       │      │       │      │  │\n## │ │      │      │       │      │ es W. │ Art  │       │      │       │      │  │\n## │ │      │      │       │      │ Goody │ Fund │       │      │       │      │  │\n## │ │      │      │       │      │ ear,  │ 2008 │       │      │       │      │  │\n## │ │      │      │       │      │ Sarah │      │       │      │       │      │  │\n## │ │      │      │       │      │ Norto │      │       │      │       │      │  │\n## │ │      │      │       │      │ n     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Goody │      │       │      │       │      │  │\n## │ │      │      │       │      │ ear,  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Dr.   │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mrs.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Clayt │      │       │      │       │      │  │\n## │ │      │      │       │      │ on    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Pieme │      │       │      │       │      │  │\n## │ │      │      │       │      │ r,    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Georg │      │       │      │       │      │  │\n## │ │      │      │       │      │ e     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Bello │      │       │      │       │      │  │\n## │ │      │      │       │      │ ws    │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Irene │      │       │      │       │      │  │\n## │ │      │      │       │      │ Pirso │      │       │      │       │      │  │\n## │ │      │      │       │      │ n     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Macdo │      │       │      │       │      │  │\n## │ │      │      │       │      │ nald  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Funds │      │       │      │       │      │  │\n## │ │      │      │       │      │ ; by  │      │       │      │       │      │  │\n## │ │      │      │       │      │ excha │      │       │      │       │      │  │\n## │ │      │      │       │      │ nge:  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Seymo │      │       │      │       │      │  │\n## │ │      │      │       │      │ ur H. │      │       │      │       │      │  │\n## │ │      │      │       │      │ Knox, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Jr.   │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ the   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Steve │      │       │      │       │      │  │\n## │ │      │      │       │      │ nson  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Famil │      │       │      │       │      │  │\n## │ │      │      │       │      │ y,    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fello │      │       │      │       │      │  │\n## │ │      │      │       │      │ ws    │      │       │      │       │      │  │\n## │ │      │      │       │      │ for   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Life  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mrs.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Georg │      │       │      │       │      │  │\n## │ │      │      │       │      │ e A.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Forma │      │       │      │       │      │  │\n## │ │      │      │       │      │ n,    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mrs.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Georg │      │       │      │       │      │  │\n## │ │      │      │       │      │ ia    │      │       │      │       │      │  │\n## │ │      │      │       │      │ M.G.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Forma │      │       │      │       │      │  │\n## │ │      │      │       │      │ n,    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Elisa │      │       │      │       │      │  │\n## │ │      │      │       │      │ beth  │      │       │      │       │      │  │\n## │ │      │      │       │      │ H.    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gates │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Charl │      │       │      │       │      │  │\n## │ │      │      │       │      │ es W. │      │       │      │       │      │  │\n## │ │      │      │       │      │ Goody │      │       │      │       │      │  │\n## │ │      │      │       │      │ ear   │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mrs.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Georg │      │       │      │       │      │  │\n## │ │      │      │       │      │ ia    │      │       │      │       │      │  │\n## │ │      │      │       │      │ M.G.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Forma │      │       │      │       │      │  │\n## │ │      │      │       │      │ n     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Edmun │      │       │      │       │      │  │\n## │ │      │      │       │      │ d     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Hayes │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Sherm │      │       │      │       │      │  │\n## │ │      │      │       │      │ an S. │      │       │      │       │      │  │\n## │ │      │      │       │      │ Jewet │      │       │      │       │      │  │\n## │ │      │      │       │      │ t     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Georg │      │       │      │       │      │  │\n## │ │      │      │       │      │ e B.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Jenny │      │       │      │       │      │  │\n## │ │      │      │       │      │ R.    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mathe │      │       │      │       │      │  │\n## │ │      │      │       │      │ ws    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Beque │      │       │      │       │      │  │\n## │ │      │      │       │      │ st of │      │       │      │       │      │  │\n## │ │      │      │       │      │ Arthu │      │       │      │       │      │  │\n## │ │      │      │       │      │ r B.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Micha │      │       │      │       │      │  │\n## │ │      │      │       │      │ el,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Mrs.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Seymo │      │       │      │       │      │  │\n## │ │      │      │       │      │ ur H. │      │       │      │       │      │  │\n## │ │      │      │       │      │ Knox, │      │       │      │       │      │  │\n## │ │      │      │       │      │ Sr.,  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Baron │      │       │      │       │      │  │\n## │ │      │      │       │      │ ess   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Alpho │      │       │      │       │      │  │\n## │ │      │      │       │      │ nse   │      │       │      │       │      │  │\n## │ │      │      │       │      │ de    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Roths │      │       │      │       │      │  │\n## │ │      │      │       │      │ child │      │       │      │       │      │  │\n## │ │      │      │       │      │ ,     │      │       │      │       │      │  │\n## │ │      │      │       │      │ Phili │      │       │      │       │      │  │\n## │ │      │      │       │      │ p J.  │      │       │      │       │      │  │\n## │ │      │      │       │      │ Wicks │      │       │      │       │      │  │\n## │ │      │      │       │      │ er    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Fund  │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Gift  │      │       │      │       │      │  │\n## │ │      │      │       │      │ of    │      │       │      │       │      │  │\n## │ │      │      │       │      │ the   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Winfi │      │       │      │       │      │  │\n## │ │      │      │       │      │ eld   │      │       │      │       │      │  │\n## │ │      │      │       │      │ Found │      │       │      │       │      │  │\n## │ │      │      │       │      │ ation │      │       │      │       │      │  │\n## │ │      │      │       │      │ ,     │      │       │      │       │      │  │\n## │ │      │      │       │      │ 2010  │      │       │      │       │      │  │\n## │ │ dime │ 2433 │ 3.515 │ 7min │ Six   │ 10.5 │ varia │ 22.3 │   5.3 │ 3639 │  │\n## │ │ nsio │      │ 84514 │      │ eleme │ ft x │ ble   │      │       │   37 │  │\n## │ │ ns   │      │ 67464 │      │ nts:  │ 20 x │       │      │       │      │  │\n## │ │      │      │   342 │      │ squar │ 10   │       │      │       │      │  │\n## │ │      │      │       │      │ e     │ -ove │       │      │       │      │  │\n## │ │      │      │       │      │ tube: │ rall │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │ disp │       │      │       │      │  │\n## │ │      │      │       │      │ 920   │ lay  │       │      │       │      │  │\n## │ │      │      │       │      │ mm;   │ dime │       │      │       │      │  │\n## │ │      │      │       │      │ recta │ nsio │       │      │       │      │  │\n## │ │      │      │       │      │ ngula │ ns   │       │      │       │      │  │\n## │ │      │      │       │      │ r     │ -var │       │      │       │      │  │\n## │ │      │      │       │      │ tube: │ iabl │       │      │       │      │  │\n## │ │      │      │       │      │ 230 x │ e    │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 920   │      │       │      │       │      │  │\n## │ │      │      │       │      │ mm;   │      │       │      │       │      │  │\n## │ │      │      │       │      │ cubic │      │       │      │       │      │  │\n## │ │      │      │       │      │ tube, │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460   │      │       │      │       │      │  │\n## │ │      │      │       │      │ mm;   │      │       │      │       │      │  │\n## │ │      │      │       │      │ angul │      │       │      │       │      │  │\n## │ │      │      │       │      │ ar    │      │       │      │       │      │  │\n## │ │      │      │       │      │ eleme │      │       │      │       │      │  │\n## │ │      │      │       │      │ nt,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ openi │      │       │      │       │      │  │\n## │ │      │      │       │      │ ng:   │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460   │      │       │      │       │      │  │\n## │ │      │      │       │      │ mm;   │      │       │      │       │      │  │\n## │ │      │      │       │      │ trans │      │       │      │       │      │  │\n## │ │      │      │       │      │ ition │      │       │      │       │      │  │\n## │ │      │      │       │      │       │      │       │      │       │      │  │\n## │ │      │      │       │      │ eleme │      │       │      │       │      │  │\n## │ │      │      │       │      │ nt,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ openi │      │       │      │       │      │  │\n## │ │      │      │       │      │ ngs:  │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460   │      │       │      │       │      │  │\n## │ │      │      │       │      │ and   │      │       │      │       │      │  │\n## │ │      │      │       │      │ T-pie │      │       │      │       │      │  │\n## │ │      │      │       │      │ ce,   │      │       │      │       │      │  │\n## │ │      │      │       │      │ openi │      │       │      │       │      │  │\n## │ │      │      │       │      │ ngs:  │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460 x │      │       │      │       │      │  │\n## │ │      │      │       │      │ 460   │      │       │      │       │      │  │\n## │ │      │      │       │      │ mm    │      │       │      │       │      │  │\n## │ │      │      │       │      │ Overa │      │       │      │       │      │  │\n## │ │      │      │       │      │ ll    │      │       │      │       │      │  │\n## │ │      │      │       │      │ displ │      │       │      │       │      │  │\n## │ │      │      │       │      │ ay    │      │       │      │       │      │  │\n## │ │ unit │ 3341 │ 4.827 │ mm   │ mm    │ mm   │ mm    │    2 │  0.95 │ 6586 │  │\n## │ │ s    │      │ 96491 │      │       │      │       │      │       │    0 │  │\n## │ │      │      │ 38018 │      │       │      │       │      │       │      │  │\n## │ │      │      │    23 │      │       │      │       │      │       │      │  │\n## │ │ insc │ 6289 │ 90.88 │ date │ date  │ date │ date  │   14 │  0.18 │ 1261 │  │\n## │ │ ript │    5 │ 74149 │ insc │ inscr │ insc │ inscr │      │       │    2 │  │\n## │ │ ion  │      │ 21749 │ ribe │ ibed  │ ribe │ ibed  │      │       │      │  │\n## │ │      │      │    68 │ d    │       │ d    │       │      │       │      │  │\n## │ │ thum │ 1078 │ 15.58 │ http │ http: │ http │ http: │   57 │  0.84 │ 5841 │  │\n## │ │ bnai │    6 │ 64799 │ ://w │ //www │ ://w │ //www │      │       │    5 │  │\n## │ │ lUrl │      │ 64162 │ ww.t │ .tate │ ww.t │ .tate │      │       │      │  │\n## │ │      │      │   368 │ ate. │ .org. │ ate. │ .org. │      │       │      │  │\n## │ │      │      │       │ org. │ uk/ar │ org. │ uk/ar │      │       │      │  │\n## │ │      │      │       │ uk/a │ t/ima │ uk/a │ t/ima │      │       │      │  │\n## │ │      │      │       │ rt/i │ ges/w │ rt/i │ ges/w │      │       │      │  │\n## │ │      │      │       │ mage │ ork/A │ mage │ ork/T │      │       │      │  │\n## │ │      │      │       │ s/wo │ /A00/ │ s/wo │ /T13/ │      │       │      │  │\n## │ │      │      │       │ rk/A │ A0000 │ rk/A │ T1386 │      │       │      │  │\n## │ │      │      │       │ R/AR │ 1_8.j │ /A00 │ 9_8.j │      │       │      │  │\n## │ │      │      │       │ 0000 │ pg    │ /A00 │ pg    │      │       │      │  │\n## │ │      │      │       │ 1_8. │       │ 001_ │       │      │       │      │  │\n## │ │      │      │       │ jpg  │       │ 8.jp │       │      │       │      │  │\n## │ │      │      │       │      │       │ g    │       │      │       │      │  │\n## │ │ url  │    0 │     0 │ http │ http: │ http │ http: │ 77.9 │     1 │ 6920 │  │\n## │ │      │      │       │ ://w │ //www │ ://w │ //www │      │       │    1 │  │\n## │ │      │      │       │ ww.t │ .tate │ ww.t │ .tate │      │       │      │  │\n## │ │      │      │       │ ate. │ .org. │ ate. │ .org. │      │       │      │  │\n## │ │      │      │       │ org. │ uk/ar │ org. │ uk/ar │      │       │      │  │\n## │ │      │      │       │ uk/a │ t/art │ uk/a │ t/art │      │       │      │  │\n## │ │      │      │       │ rt/a │ works │ rt/a │ works │      │       │      │  │\n## │ │      │      │       │ rtwo │ /naum │ rtwo │ /zyw- │      │       │      │  │\n## │ │      │      │       │ rks/ │ an-ra │ rks/ │ light │      │       │      │  │\n## │ │      │      │       │ lim- │ w-mat │ abak │ -t005 │      │       │      │  │\n## │ │      │      │       │ a-p1 │ erial │ anow │ 34    │      │       │      │  │\n## │ │      │      │       │ 1588 │ -wash │ icz- │       │      │       │      │  │\n## │ │      │      │       │      │ ing-h │ abak │       │      │       │      │  │\n## │ │      │      │       │      │ ands- │ an-o │       │      │       │      │  │\n## │ │      │      │       │      │ norma │ rang │       │      │       │      │  │\n## │ │      │      │       │      │ l-a-o │ e-t1 │       │      │       │      │  │\n## │ │      │      │       │      │ f-ab- │ 2980 │       │      │       │      │  │\n## │ │      │      │       │      │ raw-m │      │       │      │       │      │  │\n## │ │      │      │       │      │ ateri │      │       │      │       │      │  │\n## │ │      │      │       │      │ al-wa │      │       │      │       │      │  │\n## │ │      │      │       │      │ shing │      │       │      │       │      │  │\n## │ │      │      │       │      │ -hand │      │       │      │       │      │  │\n## │ │      │      │       │      │ s-nor │      │       │      │       │      │  │\n## │ │      │      │       │      │ mal-b │      │       │      │       │      │  │\n## │ │      │      │       │      │ -of-a │      │       │      │       │      │  │\n## │ │      │      │       │      │ b-ar0 │      │       │      │       │      │  │\n## │ │      │      │       │      │ 0579  │      │       │      │       │      │  │\n## │ └──────┴──────┴───────┴──────┴───────┴──────┴───────┴──────┴───────┴──────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\n\n16.1.3 Accessing one column\nFirst, let’s pull out the year for each piece of artwork in the dataset and see what we can do with it…\n\n\nR\nPython\n\n\n\n\nhead(artwork$year)\n## [1]   NA   NA 1785   NA 1826 1826\n\nWe reference a column of the dataset by name using dataset_name$column_name, and since our data is stored in artwork, and we want the column named year, we use artwork$year to get access to the data we want.\n\n\n\nartwork.year.head()\n## 0       NaN\n## 1       NaN\n## 2    1785.0\n## 3       NaN\n## 4    1826.0\n## Name: year, dtype: float64\n\nWe reference a column of the dataset by name using dataset_name.column_name or dataset_name['column_name'], and since our data is stored in artwork and we want the column year, we use artwork.year or artwork['year'] to access the data we want.\n\n\n\nI’ve used the head command to show only the first few values (so that the output isn’t overwhelming).\n\n16.1.4 Variable Summary\nWhen we have output like this, it is useful to summarize the output in some way:\n\n\nR\nPython\n\n\n\n\nsummary(artwork$year)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    1545    1817    1831    1867    1953    2012    5397\n\nThat’s much less output, but we might want to instead make a chart:\n\nhist(artwork$year, breaks = 30)\n\n\n\n\n\n\n\n\n\n\nartwork.year.describe()\n## count    63804.000000\n## mean      1867.227823\n## std         72.012718\n## min       1545.000000\n## 25%       1817.000000\n## 50%       1831.000000\n## 75%       1953.000000\n## max       2012.000000\n## Name: year, dtype: float64\n\nThe df.describe() command provides us with a 5-number summary and then some additional statistics.\nWe can also create a chart:\n\nartwork.year.hist(bins = 30)\n\n\n\n\n\n\n\n\n\n\nPersonally, I much prefer the graphical version. It’s informative (though it does leave out NA values) and shows that there are pieces going back to the 1500s, but that most pieces were made in the early 1800s or late 1900s.\n\n16.1.5 Create a Histogram (base graphics/matplotlib)\nWe might be interested in the aspect ratio of the artwork - let’s take a look at the input variables and define new variables related to aspect ratio(s).\n\n\nR\nPython\n\n\n\n\npar(mfrow=c(1, 3)) # 3 plots on one row\nhist(artwork$width, main = \"width\", breaks = 30)\nhist(artwork$depth, main = \"depth\", breaks = 30)\nhist(artwork$height, main = \"height\", breaks = 30)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=3) # 3 subplots\n\nartwork.width.hist(bins = 30, ax = axes[0])\nartwork.depth.hist(bins = 30, ax = axes[1])\nartwork.height.hist(bins = 30, ax= axes[2])\n\n# Set subplot titles\naxes[0].title.set_text(\"width\")\naxes[1].title.set_text(\"depth\")\naxes[2].title.set_text(\"height\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSo all of our variables are skewed quite a bit, and we know from the existence of the units column that they may not be in the same unit, either.\n\n16.1.6 Summary Tables\nLet’s make a table of the units column so that we can see what the frequency of various units are in the dataset.\n\n\nR\nPython\n\n\n\n\ntable(artwork$units, useNA = 'ifany')\n## \n##    mm  &lt;NA&gt; \n## 65860  3341\n\n\n\n\nartwork.units.value_counts(dropna=False)\n## units\n## mm     65860\n## NaN     3341\n## Name: count, dtype: int64\n\n\n\n\nEverything that has specified units is in mm. That makes things easier.\n\n16.1.7 Defining a new variable\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\naspect_hw &lt;- artwork$height/artwork$width\npar(mfrow = c(1, 2))\nhist(aspect_hw, breaks = 30)\nhist(log(aspect_hw), breaks = 30)\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nfig, axes = plt.subplots(nrows=1, ncols=2) # 2 subplots\n\naspect_hw = artwork.height/artwork.width\naspect_hw.hist(bins = 30, ax = axes[0])\nnp.log(aspect_hw).hist(bins = 30, ax = axes[1])\n\n\n\n\n\n\n\n\n\n\nMost things are pretty square-ish, but there are obviously quite a few exceptions in both directions.\nThe one problem with how we’ve done this is that we now have a data frame with all of our data in it, and a separate variable aspect_hw, that is not attached to our data frame. That’s not ideal - it’s easy to lose track of the variable, it’s easy to accidentally “sort” the variable so that the row order isn’t the same as in the original data frame… there are all sorts of potential issues.\n\n16.1.8 Adding a new column\nThe better way to define a new variable is to add a new column to the data frame:\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\nartwork$aspect_hw &lt;- artwork$height/artwork$width\n\n\n\n\nartwork['aspect_hw'] = artwork.height/artwork.width\n\nNote that when you create a new column in a pandas dataframe, you have to use df['colname'] on the left hand side, even if you use df.colname syntax on the right hand side.\n\n\n\n(We’ll learn a shorter way to do this later, but this is functional, if not pretty, for now).\nThe downside to this is that we have to write out artwork$aspect_hw or artwork.aspect_hw each time we want to reference the variable. That is a pain, but one that’s relatively temporary (we’ll get to a better way to do this in a couple of weeks). A little bit of extra typing is definitely worth it if you don’t lose data you want to keep.\n\n\n\n\n\n\nAssign your calculations to a variable or column!\n\n\n\nOne mistake I see people make frequently is to calculate height/width, but then not assign that value to a variable.\nIf you’re not using &lt;- in R1 or = in Python, then you’re not saving that information to be referenced later - you’re just calculating values temporarily and possibly printing them as output.\n\n\n\n16.1.9 Conclusions\nIt’s important to keep track of where you’re putting the pieces you create during an analysis - just as important as keeping track of the different sub-components when you’re putting a lego set together or making a complex recipe in the kitchen. Forgetting to assign your calculation to a variable is like dumping your glaze down the sink or throwing that small lego component into the trash.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#dogs-of-nyc",
    "href": "part-gen-prog/07-prog-data.html#dogs-of-nyc",
    "title": "16  Programming With Data",
    "section": "\n16.2 Dogs of NYC",
    "text": "16.2 Dogs of NYC\nNew York City provides a whole host of open-data resources, including a dataset of dogs licensed in the city on an annual basis (link is to the NYC Open Data Page).\nCSV link (this data is ~23 MB)\n\n16.2.1 Read in data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\n\nif (!file.exists(\"../data/NYC_dogs.csv\")) {\n  # if the file doesn't exist, download it!\n  download.file(\n    \"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\", # url for download\n    destfile = \"../data/NYC_dogs.csv\", # location to store the file\n    mode = \"wb\" # need this to get downloads to work on windows\n  )\n}\n\ndogs &lt;- read_csv(\"../data/NYC_dogs.csv\")\nhead(dogs)\n## # A tibble: 6 × 8\n##   AnimalName AnimalGender AnimalBirthYear BreedName    ZipCode LicenseIssuedDate\n##   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n## 1 PAIGE      F                       2014 American Pi…   10035 09/12/2014       \n## 2 YOGI       M                       2010 Boxer          10465 09/12/2014       \n## 3 ALI        M                       2014 Basenji        10013 09/12/2014       \n## 4 QUEEN      F                       2013 Akita Cross…   10013 09/12/2014       \n## 5 LOLA       F                       2009 Maltese        10028 09/12/2014       \n## 6 IAN        M                       2006 Unknown        10013 09/12/2014       \n## # ℹ 2 more variables: LicenseExpiredDate &lt;chr&gt;, `Extract Year` &lt;dbl&gt;\n\n\n\n\nfrom os.path import exists # to test whether files exist\nimport pandas as pd\nimport requests # to download a file\n\nif ~exists(\"../data/NYC_dogs.csv\"):\n  response = requests.get(\"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\")\n  open(\"../data/NYC_dogs.csv\", \"wb\").write(response.content)\n## 45416500\n\ndogs = pd.read_csv(\"../data/NYC_dogs.csv\")\ndogs.head()\n##   AnimalName AnimalGender  ... LicenseExpiredDate Extract Year\n## 0      PAIGE            F  ...         09/12/2017         2016\n## 1       YOGI            M  ...         10/02/2017         2016\n## 2        ALI            M  ...         09/12/2019         2016\n## 3      QUEEN            F  ...         09/12/2017         2016\n## 4       LOLA            F  ...         10/09/2017         2016\n## \n## [5 rows x 8 columns]\n\n\n\n\n\n16.2.2 Work with Dates\nOne thing we might want to do first is to transform the license dates (LicenseIssuedDate, LicenseExpiredDate) into actual dates instead of characters.\n\n\nR\nPython\n\n\n\nWe will use the lubridate package to do this, because it is designed to make working with dates and times very easy.\nYou may need to run install.packages(\"lubridate\") in the R console if you have not used the package before.\n\nlibrary(lubridate)\nhead(dogs$LicenseExpiredDate) # Dates are in month-day-year format\n## [1] \"09/12/2017\" \"10/02/2017\" \"09/12/2019\" \"09/12/2017\" \"10/09/2017\"\n## [6] \"10/30/2019\"\n\ndogs$LicenseExpiredDate &lt;- mdy(dogs$LicenseExpiredDate)\ndogs$LicenseIssuedDate &lt;- mdy(dogs$LicenseIssuedDate)\n\nhead(dogs$LicenseExpiredDate)\n## [1] \"2017-09-12\" \"2017-10-02\" \"2019-09-12\" \"2017-09-12\" \"2017-10-09\"\n## [6] \"2019-10-30\"\n\n\n\nYou may need to run pip install datetime in the terminal if you have not used the package before.\n\nfrom datetime import date\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # Before\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         09/12/2017        09/12/2014\n## 1         10/02/2017        09/12/2014\n## 2         09/12/2019        09/12/2014\n## 3         09/12/2017        09/12/2014\n## 4         10/09/2017        09/12/2014\n\nformat_str = \"%m/%d/%Y\" # date format in the dataset\n\ndogs['LicenseExpiredDate'] = pd.to_datetime(dogs.LicenseExpiredDate, format = format_str)\ndogs['LicenseIssuedDate'] = pd.to_datetime(dogs.LicenseIssuedDate, format = format_str)\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # After\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         2017-09-12        2014-09-12\n## 1         2017-10-02        2014-09-12\n## 2         2019-09-12        2014-09-12\n## 3         2017-09-12        2014-09-12\n## 4         2017-10-09        2014-09-12\n\n\n\n\nIt might be interesting to see when licenses have been issued over time, so let’s make a histogram. This time, I’m going to use ggplot graphics with the ggplot2 package in R and the plotnine package in python (which is the python version of the R package).\n\n16.2.3 Create a Histogram (ggplot2/plotnine)\n\n\nR\nPython\n\n\n\nYou may need to run install.packages(\"ggplot2\") in the R console if you have not used ggplot2 before.\n\nlibrary(ggplot2)\n\nggplot(\n  data = dogs, \n  aes(x = LicenseIssuedDate) # Specify we want LicenseIssueDate on the x-axis\n) + \n  geom_histogram() # Create a histogram\n\n\n\n\n\n\n\n\n\nYou may need to run pip install plotnine in the terminal if you have not used the package before.\n\nfrom plotnine import *\n\n(\n  ggplot(mapping = aes(x = 'LicenseIssuedDate'), data = dogs) + \n  geom_histogram() # Create a histogram\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f90937a1cd0&gt;\n\n\n\n\nThere is an interesting periodicity to the license issue dates.\n\n16.2.4 Compute License Length\nI’m also curious about how long a license tends to be held for - we can get this information by subtracting the issue date from the expiration date.\n\n\nR\nPython\n\n\n\n\ndogs$LicenseLength &lt;- dogs$LicenseExpiredDate - dogs$LicenseIssuedDate\nsummary(dogs$LicenseLength)\n## Time differences in days\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##     1.0   365.0   366.0   541.7   415.0  7913.0      82\nhead(dogs$LicenseLength)\n## Time differences in days\n## [1] 1096 1116 1826 1096 1123 1874\n\nWe can see that directly subtracting date-times gives us a license length in days. That’s useful enough, I guess, but it might be more useful in years… unfortunately, that’s not an option for difftime()\n\nlibrary(ggplot2)\ndogs$LicenseLength &lt;- difftime(dogs$LicenseExpiredDate, dogs$LicenseIssuedDate, units = \"weeks\")\n\n# 52 weeks in a year so we'll just convert as we plot\nggplot(data = dogs, aes(x = LicenseLength / 52 )) + geom_histogram() + \n  scale_x_continuous(limits = c(0,10))\n\n\n\n\n\n\n\n\n\n\ndogs[\"License_length\"] = dogs.LicenseExpiredDate - dogs.LicenseIssuedDate\n\ndogs.License_length.describe()\n## count                         722782\n## mean     541 days 15:49:38.083018136\n## std      423 days 20:41:36.018659752\n## min                  1 days 00:00:00\n## 25%                365 days 00:00:00\n## 50%                366 days 00:00:00\n## 75%                415 days 00:00:00\n## max               7913 days 00:00:00\n## Name: License_length, dtype: object\ndogs.License_length.head()\n## 0   1096 days\n## 1   1116 days\n## 2   1826 days\n## 3   1096 days\n## 4   1123 days\n## Name: License_length, dtype: timedelta64[ns]\n\ndogs[\"License_length_yr\"] = dogs.License_length.dt.days/365.25\n\n\n(\n  ggplot(mapping = aes(x = \"License_length_yr\"), data = dogs) + \n  geom_histogram(bins = 30)+\n  scale_x_continuous(limits = (0,10))\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f9069204f10&gt;\n\nIn python, we have to first access the “days” attribute of the timedelta64 data type (this gives us a number) using dogs.Licence_length.dt.days and then divide by 365.25 (number of days in a year, on average).\n\n\n\n\n16.2.5 Explore Boroughs\nAnother question that I have when looking at this dataset is a bit more superficial - are the characteristics of different areas different? The dogs data frame has a Borough column, but it’s not actually filled in, so we’ll need to get rid of it and then add Borough back in by zip code.\nTo look at this, we’ll need a bit more data. I found a list of NYC zip codes by borough, which we can merge in with the data we already have to get puppy registrations by borough. Then, we can see if e.g. the top 10 breeds are different for different boroughs. To simplify this, I’m going to link to a file to merge in, and not show you the specifics of how I read the table from this site.\n\n\nR\nPython\n\n\n\n\nborough_zip &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/nyc_zip_borough.csv\")\n\n# Remove the Borough column from dogs\ndogs &lt;- dogs[, which(names(dogs) != \"Borough\")]\ndogs &lt;- merge(dogs, borough_zip, by = \"ZipCode\")\nhead(dogs)\n##   ZipCode AnimalName AnimalGender AnimalBirthYear                BreedName\n## 1   10001      BARAK            M            2013                  Unknown\n## 2   10001       ESME            F            2020 Border Collie Crossbreed\n## 3   10001      ROCKY            M            2014               Rottweiler\n## 4   10001    UNKNOWN            M            2022                   Poodle\n## 5   10001      LOUIE            M            2020        Yorkshire Terrier\n## 6   10001       ABBY            F            2008       Labrador Retriever\n##   LicenseIssuedDate LicenseExpiredDate Extract Year   LicenseLength   Borough\n## 1        2016-12-28         2018-01-02         2016  52.85714 weeks Manhattan\n## 2        2021-07-31         2026-07-31         2024 260.85714 weeks Manhattan\n## 3        2018-10-16         2019-10-16         2018  52.14286 weeks Manhattan\n## 4        2022-08-21         2023-08-21         2023  52.14286 weeks Manhattan\n## 5        2020-07-08         2021-07-08         2022  52.14286 weeks Manhattan\n## 6        2016-08-01         2021-09-24         2016 268.57143 weeks Manhattan\n\n\n\n\nborough_zip = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/nyc_zip_borough.csv\")\ndogs = dogs.drop('Borough', axis = 1) # drop borough column\n## KeyError: \"['Borough'] not found in axis\"\ndogs = pd.merge(dogs, borough_zip, on = 'ZipCode')\ndogs.head()\n##   AnimalName AnimalGender  ... License_length_yr    Borough\n## 0      PAIGE            F  ...          3.000684  Manhattan\n## 1       YOGI            M  ...          3.055441      Bronx\n## 2        ALI            M  ...          4.999316  Manhattan\n## 3      QUEEN            F  ...          3.000684  Manhattan\n## 4       LOLA            F  ...          3.074606  Manhattan\n## \n## [5 rows x 11 columns]\n\n\n\n\nNow that we have borough, let’s write a function that will take a dataset and spit out a list of the top 5 dog breeds registered in that area.\n\n\nR\nPython\n\n\n\n\ntop_5_breeds &lt;- function(data) {\n  # Inside the function, our dataset is called data, not dogs\n  tmp &lt;- table(data$BreedName) \n  return(sort(tmp, decreasing = T)[1:5]) # top 5 breeds with counts\n}\n\n\n\n\ndef top_5_breeds(data):\n  tmp = pd.value_counts(data.BreedName)\n  return tmp.iloc[0:5]\n\n\n\n\nNow, using that function, lets write a for loop that loops through the 5 boroughs and spits out the top 5 breeds in each borough:\n\n\nR\nPython\n\n\n\n\nboroughs &lt;- unique(borough_zip$Borough) # get a list of the 5 boroughs\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- dogs[dogs$Borough == i,]\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  print(result)\n}\n##                Breed  Freq   Borough rank\n## 1            Unknown 18784 Manhattan    1\n## 2  Yorkshire Terrier  9196 Manhattan    2\n## 3          Chihuahua  9112 Manhattan    3\n## 4           Shih Tzu  8038 Manhattan    4\n## 5 Labrador Retriever  7873 Manhattan    5\n##                Breed Freq Borough rank\n## 1            Unknown 6722  Staten    1\n## 2           Shih Tzu 4051  Staten    2\n## 3  Yorkshire Terrier 3893  Staten    3\n## 4 Labrador Retriever 2713  Staten    4\n## 5            Maltese 2067  Staten    5\n##                                  Breed Freq Borough rank\n## 1                    Yorkshire Terrier 6814   Bronx    1\n## 2                              Unknown 6284   Bronx    2\n## 3                             Shih Tzu 5756   Bronx    3\n## 4                            Chihuahua 3675   Bronx    4\n## 5 American Pit Bull Mix / Pit Bull Mix 2845   Bronx    5\n##               Breed  Freq Borough rank\n## 1           Unknown 13622  Queens    1\n## 2 Yorkshire Terrier  9109  Queens    2\n## 3          Shih Tzu  8343  Queens    3\n## 4         Chihuahua  5638  Queens    4\n## 5           Maltese  5613  Queens    5\n##                           Breed  Freq  Borough rank\n## 1                       Unknown 17149 Brooklyn    1\n## 2             Yorkshire Terrier 10628 Brooklyn    2\n## 3                      Shih Tzu 10316 Brooklyn    3\n## 4                     Chihuahua  7480 Brooklyn    4\n## 5 Labrador Retriever Crossbreed  6311 Brooklyn    5\n\n\n\n\nboroughs = borough_zip.Borough.unique()\nfor i in boroughs:\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough == @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n\n  print(result)\n##                 count  count\n## 0             Unknown  18784\n## 1   Yorkshire Terrier   9196\n## 2           Chihuahua   9112\n## 3            Shih Tzu   8038\n## 4  Labrador Retriever   7873\n##             BreedName  count    Borough  rank\n## 0             Unknown  18784  Manhattan     1\n## 1   Yorkshire Terrier   9196  Manhattan     2\n## 2           Chihuahua   9112  Manhattan     3\n## 3            Shih Tzu   8038  Manhattan     4\n## 4  Labrador Retriever   7873  Manhattan     5\n##                 count  count\n## 0             Unknown   6722\n## 1            Shih Tzu   4051\n## 2   Yorkshire Terrier   3893\n## 3  Labrador Retriever   2713\n## 4             Maltese   2067\n##             BreedName  count Borough  rank\n## 0             Unknown   6722  Staten     1\n## 1            Shih Tzu   4051  Staten     2\n## 2   Yorkshire Terrier   3893  Staten     3\n## 3  Labrador Retriever   2713  Staten     4\n## 4             Maltese   2067  Staten     5\n##                                   count  count\n## 0                     Yorkshire Terrier   6814\n## 1                               Unknown   6284\n## 2                              Shih Tzu   5756\n## 3                             Chihuahua   3675\n## 4  American Pit Bull Mix / Pit Bull Mix   2845\n##                               BreedName  count Borough  rank\n## 0                     Yorkshire Terrier   6814   Bronx     1\n## 1                               Unknown   6284   Bronx     2\n## 2                              Shih Tzu   5756   Bronx     3\n## 3                             Chihuahua   3675   Bronx     4\n## 4  American Pit Bull Mix / Pit Bull Mix   2845   Bronx     5\n##                count  count\n## 0            Unknown  13622\n## 1  Yorkshire Terrier   9109\n## 2           Shih Tzu   8343\n## 3          Chihuahua   5638\n## 4            Maltese   5613\n##            BreedName  count Borough  rank\n## 0            Unknown  13622  Queens     1\n## 1  Yorkshire Terrier   9109  Queens     2\n## 2           Shih Tzu   8343  Queens     3\n## 3          Chihuahua   5638  Queens     4\n## 4            Maltese   5613  Queens     5\n##                            count  count\n## 0                        Unknown  17149\n## 1              Yorkshire Terrier  10628\n## 2                       Shih Tzu  10316\n## 3                      Chihuahua   7480\n## 4  Labrador Retriever Crossbreed   6311\n##                        BreedName  count   Borough  rank\n## 0                        Unknown  17149  Brooklyn     1\n## 1              Yorkshire Terrier  10628  Brooklyn     2\n## 2                       Shih Tzu  10316  Brooklyn     3\n## 3                      Chihuahua   7480  Brooklyn     4\n## 4  Labrador Retriever Crossbreed   6311  Brooklyn     5\n\nMore information on pandas query function (use \\@varname to use a variable in a query).\n\n\n\nIf we wanted to save these results as a summary data frame, we could totally do that!\n\n\nR\nPython\n\n\n\n\nbreeds_by_borough &lt;- data.frame() # create a blank data frame\n\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- subset(dogs, Borough == i)\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  breeds_by_borough &lt;- rbind(breeds_by_borough, result)\n}\n\nbreeds_by_borough\n##                                   Breed  Freq   Borough rank\n## 1                               Unknown 18784 Manhattan    1\n## 2                     Yorkshire Terrier  9196 Manhattan    2\n## 3                             Chihuahua  9112 Manhattan    3\n## 4                              Shih Tzu  8038 Manhattan    4\n## 5                    Labrador Retriever  7873 Manhattan    5\n## 6                               Unknown  6722    Staten    1\n## 7                              Shih Tzu  4051    Staten    2\n## 8                     Yorkshire Terrier  3893    Staten    3\n## 9                    Labrador Retriever  2713    Staten    4\n## 10                              Maltese  2067    Staten    5\n## 11                    Yorkshire Terrier  6814     Bronx    1\n## 12                              Unknown  6284     Bronx    2\n## 13                             Shih Tzu  5756     Bronx    3\n## 14                            Chihuahua  3675     Bronx    4\n## 15 American Pit Bull Mix / Pit Bull Mix  2845     Bronx    5\n## 16                              Unknown 13622    Queens    1\n## 17                    Yorkshire Terrier  9109    Queens    2\n## 18                             Shih Tzu  8343    Queens    3\n## 19                            Chihuahua  5638    Queens    4\n## 20                              Maltese  5613    Queens    5\n## 21                              Unknown 17149  Brooklyn    1\n## 22                    Yorkshire Terrier 10628  Brooklyn    2\n## 23                             Shih Tzu 10316  Brooklyn    3\n## 24                            Chihuahua  7480  Brooklyn    4\n## 25        Labrador Retriever Crossbreed  6311  Brooklyn    5\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\nbreeds_by_borough[order(breeds_by_borough$rank, \n                        breeds_by_borough$Borough),]\n##                                   Breed  Freq   Borough rank\n## 11                    Yorkshire Terrier  6814     Bronx    1\n## 21                              Unknown 17149  Brooklyn    1\n## 1                               Unknown 18784 Manhattan    1\n## 16                              Unknown 13622    Queens    1\n## 6                               Unknown  6722    Staten    1\n## 12                              Unknown  6284     Bronx    2\n## 22                    Yorkshire Terrier 10628  Brooklyn    2\n## 2                     Yorkshire Terrier  9196 Manhattan    2\n## 17                    Yorkshire Terrier  9109    Queens    2\n## 7                              Shih Tzu  4051    Staten    2\n## 13                             Shih Tzu  5756     Bronx    3\n## 23                             Shih Tzu 10316  Brooklyn    3\n## 3                             Chihuahua  9112 Manhattan    3\n## 18                             Shih Tzu  8343    Queens    3\n## 8                     Yorkshire Terrier  3893    Staten    3\n## 14                            Chihuahua  3675     Bronx    4\n## 24                            Chihuahua  7480  Brooklyn    4\n## 4                              Shih Tzu  8038 Manhattan    4\n## 19                            Chihuahua  5638    Queens    4\n## 9                    Labrador Retriever  2713    Staten    4\n## 15 American Pit Bull Mix / Pit Bull Mix  2845     Bronx    5\n## 25        Labrador Retriever Crossbreed  6311  Brooklyn    5\n## 5                    Labrador Retriever  7873 Manhattan    5\n## 20                              Maltese  5613    Queens    5\n## 10                              Maltese  2067    Staten    5\n\n\n\n\nbreeds_by_borough = pd.DataFrame() # Create a blank dataframe\n\nfor i in boroughs:\n  print(i)\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough== @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n  # Append to blank dataframe\n  breeds_by_borough = breeds_by_borough.append(result)\n## AttributeError: 'DataFrame' object has no attribute 'append'\n\nbreeds_by_borough.head()\n## Empty DataFrame\n## Columns: []\n## Index: []\nbreeds_by_borough.tail()\n## Empty DataFrame\n## Columns: []\n## Index: []\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\nbreeds_by_borough.sort_values(['rank', 'Borough'])\n## KeyError: 'rank'\n\n\n\n\nSoon we’ll learn a much shorter set of commands to get these types of summaries, but it’s important to know how a for loop connects to the concept of summarizing data by a factor (in this case, by borough).\n\n\n\n\n\n\nTry it out: NYC dogs\n\n\n\nLook at the name, age, or gender of dogs registered in NYC and see if you can come up with a similar function and way of summarizing the data in a for-loop. You may want to calculate the mean or quantiles (for numeric variables), or list the most common dog names/genders in each borough.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#swearing-in-tarantino-films",
    "href": "part-gen-prog/07-prog-data.html#swearing-in-tarantino-films",
    "title": "16  Programming With Data",
    "section": "\n16.3 Swearing in Tarantino Films",
    "text": "16.3 Swearing in Tarantino Films\nContent warning: This section contains analysis of swear words and deaths. I will not censor the words used in these movies, as they are legitimate data and could lead to an interesting analysis. Feel free to skip this example if it makes you uncomfortable.\n\nQuentin Jerome Tarantino (/ˌtærənˈtiːnoʊ/; born March 27, 1963) is an American film director, screenwriter, producer, actor, and author. His films are characterized by stylized violence, extended dialogue including a pervasive use of profanity, and references to popular culture. [1]\n\n\n16.3.1 Read in data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\n\ntarantino &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/tarantino/tarantino.csv\")\nhead(tarantino)\n## # A tibble: 6 × 4\n##   movie          type  word     minutes_in\n##   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 Reservoir Dogs word  dick           0.4 \n## 2 Reservoir Dogs word  dicks          0.43\n## 3 Reservoir Dogs word  fucked         0.55\n## 4 Reservoir Dogs word  fucking        0.61\n## 5 Reservoir Dogs word  bullshit       0.61\n## 6 Reservoir Dogs word  fuck           0.66\n\n\n\n\nimport pandas as pd\n\ntarantino = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/tarantino/tarantino.csv\")\ntarantino.head()\n##             movie  type      word  minutes_in\n## 0  Reservoir Dogs  word      dick        0.40\n## 1  Reservoir Dogs  word     dicks        0.43\n## 2  Reservoir Dogs  word    fucked        0.55\n## 3  Reservoir Dogs  word   fucking        0.61\n## 4  Reservoir Dogs  word  bullshit        0.61\n\n\n\n\n\n16.3.2 Create a Density Plot (ggplot2/plotnine)\n\n\nR\nPython\n\n\n\nYou may need to run install.packages(\"ggplot2\") in the R console if you have not used ggplot2 before.\n\nlibrary(ggplot2)\n\nggplot(\n  data = tarantino, \n  aes(x = minutes_in, color = type)\n) + \n  geom_density() + \n  scale_color_manual(values = c(\"black\", \"grey\")) +\n  facet_wrap(~movie)\n\n\n\n\n\n\n\n\n\nYou may need to run pip install plotnine in the terminal if you have not used the package before.\n\nfrom plotnine import *\n\nplot = ggplot(data = tarantino, mapping = aes(x = 'minutes_in', color = \"type\")) \nplot = plot + geom_density()\nplot = plot + scale_color_manual(values = [\"black\", \"grey\"]) \nplot = plot + facet_wrap(\"movie\")\nplot.show()\n\n\n\n\n\n\n\n\n\n\nSo, from these plots, we can see that in at least two movies, there are high spikes in deaths about 1/3 and 2/3 of the way in; in another movie, most of the deaths occur in the first 25 minutes. Swearing, on the other hand, seems to be fairly evenly distributed throughout the movies.\n\n16.3.3 Compare Swear Words Used by Movie\nAs there are a very large number of swear words and variants in Tarantino movies, let’s work with only the 6 most common swear words in the data set. To do this, we have to:\n\nSelect only rows that have words (as opposed to deaths)\nAssemble a list of the 6 most common words\nSelect only rows with those words\n\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n# Step 1\ntarantino_words &lt;- tarantino[tarantino$type == \"word\",]\n\n# Step 2\nword_freq &lt;- sort(table(tarantino_words$word), decreasing = T)\n# word_freq has the counts of how many times the words appear\n# we need the names that are above those counts\nswear6 &lt;- names(word_freq)[1:6]\n\n# Step 3\nword_6 &lt;- tarantino_words[tarantino_words$word %in% swear6,]\n\n\n\nggplot(\n  data = word_6, \n  aes(x = movie, fill = word)\n) + \n  geom_bar() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\n# Step 1 - remove deaths\ntarantino_words = tarantino.query(\"type == 'word'\")\n\n# Step 2 - 6 most common words\n\nplot = ggplot(tarantino, aes(x = 'minutes_in', color = 'movie'))\nplot = plot + geom_density() \nplot = plot + facet_wrap(\"type\")\n\nplot.show()\n\n\n\n\n\n\n\n\n\n\nXXX Under construction - I will add more as I get time.\n\n\n\n\n[1] \nWikipedia contributors, “Quentin Tarantino,” Wikipedia. Aug. 2023 [Online]. Available: https://en.wikipedia.org/wiki/Quentin_Tarantino. [Accessed: Aug. 29, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#footnotes",
    "href": "part-gen-prog/07-prog-data.html#footnotes",
    "title": "16  Programming With Data",
    "section": "",
    "text": "(or =, or -&gt; if you’re a total heathen)↩︎",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/00-wrangling.html",
    "href": "part-wrangling/00-wrangling.html",
    "title": "Part III: Data Wrangling",
    "section": "",
    "text": "References\nThis part of the textbook covers topics related to working with data. Every data set is messy in its own way [1], and this section is focused on providing you with some of the tools to deal with the most common types of messy data.\n17  Data Input covers how to read in data from many common formats, such as spreadsheets, web tables, and databases.\nThen, we start to talk about graphics.\n18  Data Visualization Basics provides a brief primer on how to create different charts and graphics in R and python for use in other sections.\n19  Exploratory Data Analysis talks about how to do exploratory data analysis using graphs, tables, and summary statistics.\n20  A Grammar of Graphics covers the in-depth about how to create different types of charts using R and python.\n21  Creating Good Charts discusses how to create good graphics - graphics that are easy to read, account for human perceptual quirks, and look nice.\nIn the next set of sections, we talk about how to rearrange, summarize, clean, and manipulate data.\n22  Data Cleaning covers verbs for transforming data - creating new variables, modifying existing variables, selecting specific rows and/or columns, and more.\n23  Working with Strings discusses string manipulations. Text data is some of the messiest data out there, and this chapter will give you some tools to help make text and character data more tidy.\n24  Reshaping Data discusses how to transform data from wide human-friendly formats to long comptuer-friendly formats for analysis and processing.\n26  Joining Data discusses how to join data sets together using common variables.\nFinally, we talk about a few specific types of data: dates and times and lists.\n27  Dates and Times discusses how to work with dates and times using R and Python.\n28  Functional Programming discusses how to work with data stored in nested lists efficiently, though additional information is available in 32  Record-based Data and List Processing Strategies, where these strategies are applied to record-based data.",
    "crumbs": [
      "Part III: Data Wrangling"
    ]
  },
  {
    "objectID": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "href": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "title": "Part III: Data Wrangling",
    "section": "",
    "text": "[1] H. Wickham, “Tidy data,” The Journal of Statistical Software, vol. 59, 2014 [Online]. Available: http://www.jstatsoft.org/v59/i10/",
    "crumbs": [
      "Part III: Data Wrangling"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html",
    "href": "part-wrangling/01-data-input.html",
    "title": "17  Data Input",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#objectives",
    "href": "part-wrangling/01-data-input.html#objectives",
    "title": "17  Data Input",
    "section": "",
    "text": "Read in data from common formats into R or Python\nIdentify delimiters, headers, and other essential components of files\n\n\n\n\n\n\n\nCheatsheets!\n\n\n\nThese may be worth printing off as you work through this module.\n\nR - tidyverse\nPython",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#overview-data-formats",
    "href": "part-wrangling/01-data-input.html#overview-data-formats",
    "title": "17  Data Input",
    "section": "\n17.1 Overview: Data Formats",
    "text": "17.1 Overview: Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs.\n\n[1] describes why binary file formats exist, and why they’re not necessarily optimal.\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there’s this amazing comic:\n\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which you may come across:\n\nWeb data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. See Chapter 28 for some tools to work with these files.\nSpatial files: Shapefiles are the most common version of spatial files, though there are a seemingly infinite number of different formats, and new formats pop up at the most inconvenient times. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. \n\nTo be minimally functional in R and Python, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly discuss binary files and databases, but it is less critical to remember how to read these in without consulting an online reference.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#text-files",
    "href": "part-wrangling/01-data-input.html#text-files",
    "title": "17  Data Input",
    "section": "\n17.2 Text Files",
    "text": "17.2 Text Files\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say?\n\n17.2.1 Fixed-width files\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory).\n\n17.2.1.1 Fixed Width File IO\n\n\nBase R\nreadr\nPython\n\n\n\nIn base R (no extra packages), you can read fixed-width files in using read.fwf, but you must specify the column breaks yourself, which can be painful.\n\nurl &lt;- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\n\n\ndata &lt;- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you.\n\n\n\n\n\n\nCode for counting field width\n\n\n\n\n\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp &lt;- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars &lt;- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars &lt;- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space &lt;- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space &lt;- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks &lt;- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths &lt;- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata &lt;- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names &lt;- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) &lt;- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n\n\n\n\nYou can also write fixed-width files if you really want to:\n\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\nlibrary(gdata)\nwrite.fwf(mtcars, file = tempfile())\n\n\n\nThe readr package creates data-frame like objects called tibbles (a souped-up data frame), but it is much friendlier to use.\n\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 × 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # ℹ 110 more rows\n## # ℹ 12 more variables: PRES &lt;dbl&gt;, SRAD &lt;dbl&gt;, TA9M &lt;dbl&gt;, WS2M &lt;dbl&gt;,\n## #   TS10 &lt;dbl&gt;, TB10 &lt;dbl&gt;, TS05 &lt;dbl&gt;, TS25 &lt;dbl&gt;, TS60 &lt;dbl&gt;, TR05 &lt;dbl&gt;,\n## #   TR25 &lt;dbl&gt;, TR60 &lt;dbl&gt;\n\n\n\nBy default, pandas’ read_fwf will guess at the format of your fixed-width file.\n\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\ndata.head()\n##    STID   STNM  TIME  RELH  TAIR  WSPD  ...  TS05  TS25   TS60  TR05  TR25    TR60\n## 0  ACME  110.0   0.0  53.0  31.8   5.2  ...  31.6  25.2   21.7  3.09  2.22    1.48\n## 1  ADAX    1.0   0.0  55.0  32.4   1.0  ...  29.6  26.8 -998.0  2.61  1.88 -998.00\n## 2  ALTU    2.0   0.0  31.0  35.6   8.9  ...  30.7  26.1 -998.0  3.39  2.47 -998.00\n## 3  ALV2  116.0   0.0  27.0  35.8   6.7  ...  25.6  22.6 -998.0  2.70  1.60 -998.00\n## 4  ANT2  135.0   0.0  73.0  27.8   0.0  ...  30.2  26.8   23.8  1.96  1.73    1.33\n## \n## [5 rows x 24 columns]\n\n\n\n\n\n17.2.2 Delimited Text Files\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file.\n\n17.2.2.1 Comma Delimited Files\n\n\nBase R\nreadr\nPython\n\n\n\n\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\npokemon_info &lt;- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n  gen pokedex_no\n1   1          1\n2   1          2\n3   1          3\n4   1          3\n5   1          4\n6   1          5\n                                                               img_link\n1     https://img.pokemondb.net/sprites/sword-shield/icon/bulbasaur.png\n2       https://img.pokemondb.net/sprites/sword-shield/icon/ivysaur.png\n3      https://img.pokemondb.net/sprites/sword-shield/icon/venusaur.png\n4 https://img.pokemondb.net/sprites/sword-shield/icon/venusaur-mega.png\n5    https://img.pokemondb.net/sprites/sword-shield/icon/charmander.png\n6    https://img.pokemondb.net/sprites/sword-shield/icon/charmeleon.png\n        name variant         type\n1  Bulbasaur    &lt;NA&gt; Grass,Poison\n2    Ivysaur    &lt;NA&gt; Grass,Poison\n3   Venusaur    &lt;NA&gt; Grass,Poison\n4   Venusaur    Mega Grass,Poison\n5 Charmander    &lt;NA&gt;         Fire\n6 Charmeleon    &lt;NA&gt;         Fire\n\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\nThe most common delimited text format is CSV: comma-separated value.\n\nlibrary(readr)\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\npokemon_info &lt;- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n# A tibble: 6 × 6\n    gen pokedex_no img_link                                  name  variant type \n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1     1          1 https://img.pokemondb.net/sprites/sword-… Bulb… &lt;NA&gt;    Gras…\n2     1          2 https://img.pokemondb.net/sprites/sword-… Ivys… &lt;NA&gt;    Gras…\n3     1          3 https://img.pokemondb.net/sprites/sword-… Venu… &lt;NA&gt;    Gras…\n4     1          3 https://img.pokemondb.net/sprites/sword-… Venu… Mega    Gras…\n5     1          4 https://img.pokemondb.net/sprites/sword-… Char… &lt;NA&gt;    Fire \n6     1          5 https://img.pokemondb.net/sprites/sword-… Char… &lt;NA&gt;    Fire \n\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\n\nimport pandas as pd\n\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\nTypeError: bad operand type for unary -: 'str'\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n\n[123 rows x 0 columns]\n\n\n\n\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can’t let it keep us from accessing the data.\n\n17.2.2.2 Other Character Delimited Files\n\n\nBase R\nreadr\nPython\n\n\n\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n## Error in download.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", : cannot open URL 'https://geonames.usgs.gov/docs/stategaz/NE_Features.zip'\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations &lt;- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n## Error in download.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", : cannot open URL 'https://geonames.usgs.gov/docs/stategaz/NE_Features.zip'\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations &lt;- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre… Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n\nWe can actually read in the file without unzipping it, so long as we download it first - readr does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\nnebraska_locations &lt;- read_delim(\"../data/NE_Features.zip\", delim = \"|\")\n## Error: '../data/NE_Features.zip' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-wrangling').\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre… Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\nPandas can access zipped data files and unzip them while reading the data in, so we don’t have to download the file and unzip it first.\n\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\n## urllib.error.HTTPError: HTTP Error 503: Service Unavailable\nnebraska_locations\n## NameError: name 'nebraska_locations' is not defined\n\n\n\n\n\n\n\n\n\n\nTry it out: Reading CSV files\n\n\n\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveniently available at their download page. Because these data sets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV.\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe readr package and pandas can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n\n\nlibrary(readr)\nlegosets &lt;- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"../data/lego_sets.csv\")\n\n\n\n\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"../data/lego_sets_py.csv\")",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#spreadsheets",
    "href": "part-wrangling/01-data-input.html#spreadsheets",
    "title": "17  Data Input",
    "section": "\n17.3 Spreadsheets",
    "text": "17.3 Spreadsheets\n\n17.3.1 Spreadsheet IO\nThis example uses from NYC SLice. The author maintains a google sheet of the slices he has photographed, which we can download as an excel sheet and import.\n\n\nR readxl\nPython\n\n\n\nIn R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/1EY3oi9ttxybG0A0Obtwey6BFu7QLqdHe02JApijgztg/export?format=xlsx\"\n# Only download the data if it doesn't exist in the data folder\nif (!file.exists(\"../data/nyc_slice.xlsx\")) {\n  download.file(url, destfile = \"../data/nyc_slice.xlsx\", mode = \"wb\")\n}\n\n# Read in the downloaded data\npizza_data &lt;- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1)\npizza_data[1:10, 1:6]\n## # A tibble: 10 × 6\n##    `Link to IG Post`                       Name  location_lat location_lng Date \n##    &lt;chr&gt;                                   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;\n##  1 https://www.instagram.com/p/CjszJ-fOP5… Ange… 40.6232544   -73.9379223… 2022…\n##  2 https://www.instagram.com/p/CjdcPNAufP… Ozon… 40.6808917   -73.8426307  2022…\n##  3 https://www.instagram.com/p/CjQdNsaOZl… Pino… 40.6000148   -73.9994551  2022…\n##  4 https://www.instagram.com/p/Ci5XblnOnM… La R… 40.7133354   -73.8294102  2022…\n##  5 https://www.instagram.com/p/CiiLAtkON_… Rony… 40.7482509   -73.9923498  2022…\n##  6 https://www.instagram.com/p/CiS-44nucN… John… 40.8545616   -73.8658818… 2022…\n##  7 https://www.instagram.com/p/CiSmQnjutQ… Preg… 40.8631291   -73.8585108  2022…\n##  8 https://www.instagram.com/p/CiIO6oFuxp… N & … 40.6004632   -73.9430723… 2022…\n##  9 https://www.instagram.com/p/ChaZUsxuFs… Pepp… 40.9036613   -73.8504667… 2022…\n## 10 https://www.instagram.com/p/ChNd9wqOqG… Rocc… 40.8676344   -73.8836046  2022…\n## # ℹ 1 more variable: `Date Expanded (times in EST)` &lt;chr&gt;\n\n\n\n\nimport pandas as pd\n\npizza_data = pd.read_excel(\"../data/nyc_slice.xlsx\")\npizza_data\n##                               Link to IG Post  ... Notes\n## 0    https://www.instagram.com/p/CjszJ-fOP5o/  ...   NaN\n## 1    https://www.instagram.com/p/CjdcPNAufPj/  ...   NaN\n## 2    https://www.instagram.com/p/CjQdNsaOZlY/  ...   NaN\n## 3    https://www.instagram.com/p/Ci5XblnOnMA/  ...   NaN\n## 4    https://www.instagram.com/p/CiiLAtkON_1/  ...   NaN\n## ..                                        ...  ...   ...\n## 459   https://www.instagram.com/p/rqCdE_hp3N/  ...   NaN\n## 460   https://www.instagram.com/p/rnT-kRhp4h/  ...   NaN\n## 461   https://www.instagram.com/p/rh17_NBp7a/  ...   NaN\n## 462   https://www.instagram.com/p/rfnZKmBp3B/  ...   NaN\n## 463   https://www.instagram.com/p/rfGr-RBp4U/  ...   NaN\n## \n## [464 rows x 11 columns]\n\n\n\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n\n\nurl &lt;- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates &lt;- read_xls(path = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 × 6\n##    Age   \\nOperator's \\nLicense …¹ Operator's\\nLicense …² Motor-\\ncycle\\nLicen…³\n##    &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n##  1 &lt;NA&gt;                         NA                     NA                     NA\n##  2 14                            0                      0                      0\n##  3 15                            0                      0                      0\n##  4 16                            0                      0                      0\n##  5 17                          961                     33                      0\n##  6 18                        18903                    174                      0\n##  7 19                        22159                    251                      0\n##  8 20                        22844                    326                      1\n##  9 21                        21589                    428                      0\n## 10 22                        22478                    588                      0\n## # ℹ abbreviated names: ¹​`\\nOperator's \\nLicense -\\nClass O`,\n## #   ²​`Operator's\\nLicense - \\nClass O/\\nMotorcycle\\nClass M`,\n## #   ³​`Motor-\\ncycle\\nLicense /\\nClass M`\n## # ℹ 2 more variables: `Commercial Driver's License` &lt;chr&gt;, ...6 &lt;chr&gt;\n\n\n\nYou may need to install xlrd via pip for this code to work.\n\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\nne_plates\n##     Unnamed: 0  ... Total\\nLicensed\\n Drivers\n## 0          NaN  ...                       NaN\n## 1          NaN  ...                    3279.0\n## 2          NaN  ...                   14902.0\n## 3          NaN  ...                   22339.0\n## 4          NaN  ...                   24341.0\n## 5          NaN  ...                   21447.0\n## 6          NaN  ...                   23761.0\n## 7          NaN  ...                   24269.0\n## 8          NaN  ...                   23039.0\n## 9          NaN  ...                   23990.0\n## 10         NaN  ...                   24717.0\n## 11         NaN  ...                   25283.0\n## 12         NaN  ...                  125153.0\n## 13         NaN  ...                  128563.0\n## 14         NaN  ...                  126432.0\n## 15         NaN  ...                  117878.0\n## 16         NaN  ...                  102261.0\n## 17         NaN  ...                  104622.0\n## 18         NaN  ...                  111702.0\n## 19         NaN  ...                  118860.0\n## 20         NaN  ...                  106939.0\n## 21         NaN  ...                   86649.0\n## 22         NaN  ...                   56675.0\n## 23         NaN  ...                   35481.0\n## 24         NaN  ...                   20288.0\n## 25         NaN  ...                    8555.0\n## 26         NaN  ...                    1864.0\n## 27         NaN  ...                      82.0\n## 28         NaN  ...                 1483162.0\n## \n## [29 rows x 16 columns]\n\n\n\n\n\n\n\n17.3.2 Google Sheets\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We’re not going to get into the power of these packages just now, but it’s worth a look if you’re working with collaborators who use Google sheets.\n\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via GSpread or googlesheets4 in python and R respectively.\n\n\n17.3.2.1 Reading Google Sheets\nLet’s demonstrate reading in data from google sheets in R and python using the Data Is Plural archive.\n\n\nPython\nR\nR: googlesheets4\nPython: GSpread\n\n\n\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in [2].\n\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n\nThis method would likely work just as well in R and would not require the googlesheets4 package.\n\n\nThis method is described in [2] for Python, but I have adapted the code to use in R.\n\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n\n\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\nCopy this code and run it on your computer to read in a sheet from google drive directly. You will see a prompt in the R console that you’ll have to interact with, and there may also be a browser pop-up where you will need to sign in to google.\n\n\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n\n\n\nThese instructions are derived from [3]. We will have to install the GSpread package: type pip install gspread into the terminal.\nThen, you will need to obtain a client token JSON file following these instructions.\n\nimport gspread as gs\nimport pandas as pd\n\nI’ve stopped here because I can’t get the authentication working, but the method seems solid if you’re willing to fiddle around with it. \n\n\n\n\n\n\n\n\n\nTry It Out!\n\n\n\n\n\nProblem\nSolution\n\n\n\nUsing a method of your choice, read in this spreadsheet of dog sizes and make a useful plot of dog height and weight ranges by breed.\n\n\nComing soon!",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#binary-files",
    "href": "part-wrangling/01-data-input.html#binary-files",
    "title": "17  Data Input",
    "section": "\n17.4 Binary Files",
    "text": "17.4 Binary Files\nR has binary file formats which store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see [4].\n\n17.4.1 Binary File IO\n\n\nR formats in R\nR formats in Python\nSAS format in R\nSAS format in Python\n\n\n\n.Rdata is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\nlegos &lt;- read_csv(\"../data/lego_sets.csv\")\nmy_var &lt;- \"This variable contains a string\"\nsave(legos, my_var, file = \"../data/R_binary.Rdata\")\n\nIf we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious.\nWe can load the R binary file back in using the load() function.\n\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legosets\"           \"mesodata\"          \n##  [7] \"mesodata_names\"     \"ne_plates\"          \"nebraska_locations\"\n## [10] \"pizza_data\"         \"pokemon_info\"       \"sheet_id\"          \n## [13] \"sheet_name\"         \"tmdb_key\"           \"tmp\"               \n## [16] \"tmp_chars\"          \"tmp_chars_space\"    \"tmp_space\"         \n## [19] \"url\"                \"widths\"\n\nload(\"../data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"legosets\"          \n##  [7] \"mesodata\"           \"mesodata_names\"     \"my_var\"            \n## [10] \"ne_plates\"          \"nebraska_locations\" \"pizza_data\"        \n## [13] \"pokemon_info\"       \"sheet_id\"           \"sheet_name\"        \n## [16] \"tmdb_key\"           \"tmp\"                \"tmp_chars\"         \n## [19] \"tmp_chars_space\"    \"tmp_space\"          \"url\"               \n## [22] \"widths\"\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\nsaveRDS(legos, \"../data/RDSlego.rds\")\n\nother_lego &lt;- readRDS(\"../data/RDSlego.rds\")\n\nBecause RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately.\n\n\nWe first need to install the pyreadr package by running pip install pyreadr in the terminal.\n\nimport pyreadr\n\nrdata_result = pyreadr.read_r('../data/R_binary.Rdata')\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n##             set_num  ...                                            img_url\n## 0      0003977811-1  ...  https://cdn.rebrickable.com/media/sets/0003977...\n## 1             001-1  ...   https://cdn.rebrickable.com/media/sets/001-1.jpg\n## 2            0011-2  ...  https://cdn.rebrickable.com/media/sets/0011-2.jpg\n## 3            0011-3  ...  https://cdn.rebrickable.com/media/sets/0011-3.jpg\n## 4            0012-1  ...  https://cdn.rebrickable.com/media/sets/0012-1.jpg\n## ...             ...  ...                                                ...\n## 25508   YODACHRON-1  ...  https://cdn.rebrickable.com/media/sets/yodachr...\n## 25509        YOTO-1  ...  https://cdn.rebrickable.com/media/sets/yoto-1.jpg\n## 25510        YOTO-2  ...  https://cdn.rebrickable.com/media/sets/yoto-2.jpg\n## 25511    YTERRIER-1  ...  https://cdn.rebrickable.com/media/sets/yterrie...\n## 25512      ZX8000-1  ...  https://cdn.rebrickable.com/media/sets/zx8000-...\n## \n## [25513 rows x 6 columns]\nrdata_result[\"my_var\"]\n##                             my_var\n## 0  This variable contains a string\n\nrds_result = pyreadr.read_r('../data/RDSlego.rds')\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n##             set_num  ...                                            img_url\n## 0      0003977811-1  ...  https://cdn.rebrickable.com/media/sets/0003977...\n## 1             001-1  ...   https://cdn.rebrickable.com/media/sets/001-1.jpg\n## 2            0011-2  ...  https://cdn.rebrickable.com/media/sets/0011-2.jpg\n## 3            0011-3  ...  https://cdn.rebrickable.com/media/sets/0011-3.jpg\n## 4            0012-1  ...  https://cdn.rebrickable.com/media/sets/0012-1.jpg\n## ...             ...  ...                                                ...\n## 25508   YODACHRON-1  ...  https://cdn.rebrickable.com/media/sets/yodachr...\n## 25509        YOTO-1  ...  https://cdn.rebrickable.com/media/sets/yoto-1.jpg\n## 25510        YOTO-2  ...  https://cdn.rebrickable.com/media/sets/yoto-2.jpg\n## 25511    YTERRIER-1  ...  https://cdn.rebrickable.com/media/sets/yterrie...\n## 25512      ZX8000-1  ...  https://cdn.rebrickable.com/media/sets/zx8000-...\n## \n## [25513 rows x 6 columns]\n\n\n\nFirst, let’s download the NHTS data.\n\nlibrary(httr)\n# Download the file and write to disk\nres &lt;- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"../data/cen10pub.sas7bdat\", overwrite = T))\n\nYou can see more information about this data here [5].\n\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata &lt;- read.sas7bdat(\"../data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured.\n\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"../data/cen10pub.sas7bdat\")\n## '../data/cen10pub.sas7bdat'\n\nYou can see more information about this data here [5].\nTo read SAS files, we use the read_sas function in Pandas.\n\nimport pandas as pd\n\ndata = pd.read_sas(\"../data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\nIn RStudio, go to Session -&gt; Clear Workspace. (This will clear your environment)\nNow, using your RDS files, load the objects back into R with different names.\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious)\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n\n\nlibrary(readxl)\nlibrary(readr)\npizza &lt;- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1, guess_max = 7000)\nlegos &lt;- read_csv(\"../data/lego_sets.csv\")\n\nsave(pizza, legos, file = \"../data/04_Try_Binary.Rdata\")\nsaveRDS(pizza, \"../data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"../data/04_Try_Binary2.rds\")\n\nrm(pizza, legos) # Limited clearing of workspace... \n\n\nload(\"../data/04_Try_Binary.Rdata\")\n\npizza_compare &lt;- readRDS(\"../data/04_Try_Binary1.rds\")\nlego_compare &lt;- readRDS(\"../data/04_Try_Binary2.rds\")\n\nall.equal(pizza, pizza_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n\n\n\n\nimport pyreadr\n\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary.Rdata' does not exist!\npizza = robjs[\"pizza\"]\n## NameError: name 'robjs' is not defined\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n## NameError: name 'robjs' is not defined\n\npizza_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary1.rds' does not exist!\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary2.rds' does not exist!\n\npizza.equals(pizza_compare)\n## NameError: name 'pizza' is not defined\nlegos.equals(lego_compare)\n## NameError: name 'legos' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#learn-more",
    "href": "part-wrangling/01-data-input.html#learn-more",
    "title": "17  Data Input",
    "section": "\n17.5 Learn more",
    "text": "17.5 Learn more\n\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#references",
    "href": "part-wrangling/01-data-input.html#references",
    "title": "17  Data Input",
    "section": "\n17.6 References",
    "text": "17.6 References\n\n\n\n\n[1] \nBetterExplained, “A little diddy about binary file formats – BetterExplained. Better explained,” 2017. [Online]. Available: https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/. [Accessed: Jan. 13, 2023]\n\n\n[2] \nM. Schäfer, “Read Data from Google Sheets into Pandas without the Google Sheets API,” Towards Data Science. Dec. 2020 [Online]. Available: https://towardsdatascience.com/read-data-from-google-sheets-into-pandas-without-the-google-sheets-api-5c468536550. [Accessed: Jun. 07, 2022]\n\n\n[3] \nM. Clarke, “How to read Google Sheets data in Pandas with GSpread,” Practical Data Science. Jun. 2021 [Online]. Available: https://web.archive.org/web/20211025204025/https://practicaldatascience.co.uk/data-science/how-to-read-google-sheets-data-in-pandas-with-gspread. [Accessed: Jun. 07, 2022]\n\n\n[4] \nC. Maierle, “Loading binary data to NumPy/Pandas,” Towards Data Science. Jul. 2020 [Online]. Available: https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672. [Accessed: Jun. 07, 2022]\n\n\n[5] \nUS Department of Transportation, “National Household Travel Survey (NHTS) 2009,” data.world. Mar. 2018 [Online]. Available: https://data.world/dot/national-household-travel-survey-nhts-2009. [Accessed: Jun. 13, 2022]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html",
    "href": "part-wrangling/02-basic-data-vis.html",
    "title": "18  Data Visualization Basics",
    "section": "",
    "text": "Objectives\nThis section is intended as a very light overview of how you might create charts in R and python. Chapter 20 will be much more in depth.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#objectives",
    "href": "part-wrangling/02-basic-data-vis.html#objectives",
    "title": "18  Data Visualization Basics",
    "section": "",
    "text": "Use ggplot2/seaborn to create a chart\nBegin to identify issues with data formatting that need to be resolved before creating a chart.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#package-installation",
    "href": "part-wrangling/02-basic-data-vis.html#package-installation",
    "title": "18  Data Visualization Basics",
    "section": "\n18.1 Package Installation",
    "text": "18.1 Package Installation\nYou will need the seaborn (python) and ggplot2 (R) packages for this section.\n\ninstall.packages(\"ggplot2\")\n\nTo install seaborn, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 10.3.2.1)\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install seaborn matplotlib\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don’t try to install packages this way).\n\nreticulate::py_install(c(\"seaborn\", \"matplotlib\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called “IPython magic” commands, so if it doesn’t work for you, try the System Terminal method instead.\n\n%pip install seaborn matplotlib\n\nOnce you have run this command, please comment it out so that you don’t reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#first-steps",
    "href": "part-wrangling/02-basic-data-vis.html#first-steps",
    "title": "18  Data Visualization Basics",
    "section": "\n18.2 First Steps",
    "text": "18.2 First Steps\nNow that you can read data in to R and python and define new variables, you can create plots! Data visualization is a skill that takes a lifetime to learn, but for now, let’s start out easy: let’s talk about how to make (basic) plots in R (with ggplot2) and in python (with seaborn, which has a similar approach to charts). You can read more about this approach, called the grammar of graphics in Chapter 20.\n\n18.2.1 Graphing HBCU Enrollment\nLet’s work with Historically Black College and University enrollment.\n\n18.2.1.1 Loading Libraries\n\n\nR\nPython\n\n\n\n\nhbcu_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nlibrary(ggplot2)\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nhbcu_all = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\n\n\n\n\n18.2.2 Making a Line Chart\nggplot2 and seaborn work with data frames.\nIf you pass a data frame in as the data argument, you can refer to columns in that data with “bare” column names (you don’t have to reference the full data object using df$name or df.name; you can instead use name or \"name\").\n\n\n\nR\nPython\n\n\n\n\nggplot(hbcu_all, aes(x = Year, y = `4-year`)) + geom_line() +\n  ggtitle(\"4-year HBCU College Enrollment\")\n\n\n\n\n\n\n\n\n\n\nplot = sns.lineplot(hbcu_all, x = \"Year\", y = \"4-year\")\nplot.set_title(\"4-year HBCU College Enrollment\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n18.2.3 Data Formatting\nIf your data is in the right format, ggplot2 is very easy to use; if your data aren’t formatted neatly, it can be a real pain. If you want to plot multiple lines, you need to either list each variable you want to plot, one by one, or (more likely) you want to get your data into “long form”. We’ll learn more about how to do this type of data transition when we talk about reshaping data.\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s helpful to start thinking about what format your data is in, and what format you would want it to be in in order to plot it. Sketching a data frame for the “as is” condition and the “to plot” condition is a useful skill to cultivate.\n\n\nYou don’t need to know exactly how this works, but it is helpful to see the difference in the two datasets:\n\n\nR\nPython\nOriginal Data\nLong Data\n\n\n\n\nlibrary(tidyr)\nhbcu_long &lt;- pivot_longer(hbcu_all, -Year, names_to = \"type\", values_to = \"value\")\n\n\n\n\nhbcu_long = pd.melt(hbcu_all, id_vars = ['Year'], value_vars = hbcu_all.columns[1:11])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTotal enrollment\nMales\nFemales\n4-year\n2-year\nTotal - Public\n4-year - Public\n2-year - Public\nTotal - Private\n4-year - Private\n2-year - Private\n\n\n\n1976\n222613\n104669\n117944\n206676\n15937\n156836\n143528\n13308\n65777\n63148\n2629\n\n\n1980\n233557\n106387\n127170\n218009\n15548\n168217\n155085\n13132\n65340\n62924\n2416\n\n\n1982\n228371\n104897\n123474\n212017\n16354\n165871\n151472\n14399\n62500\n60545\n1955\n\n\n1984\n227519\n102823\n124696\n212844\n14675\n164116\n151289\n12827\n63403\n61555\n1848\n\n\n1986\n223275\n97523\n125752\n207231\n16044\n162048\n147631\n14417\n61227\n59600\n1627\n\n\n1988\n239755\n100561\n139194\n223250\n16505\n173672\n158606\n15066\n66083\n64644\n1439\n\n\n\n\n\n\n\n\n\n\n\nYear\ntype\nvalue\n\n\n\n1976\nTotal enrollment\n222613\n\n\n1976\nMales\n104669\n\n\n1976\nFemales\n117944\n\n\n1976\n4-year\n206676\n\n\n1976\n2-year\n15937\n\n\n1976\nTotal - Public\n156836\n\n\n1976\n4-year - Public\n143528\n\n\n1976\n2-year - Public\n13308\n\n\n1976\nTotal - Private\n65777\n\n\n1976\n4-year - Private\n63148\n\n\n1976\n2-year - Private\n2629\n\n\n1980\nTotal enrollment\n233557\n\n\n1980\nMales\n106387\n\n\n1980\nFemales\n127170\n\n\n1980\n4-year\n218009\n\n\n1980\n2-year\n15548\n\n\n1980\nTotal - Public\n168217\n\n\n1980\n4-year - Public\n155085\n\n\n1980\n2-year - Public\n13132\n\n\n1980\nTotal - Private\n65340\n\n\n1980\n4-year - Private\n62924\n\n\n1980\n2-year - Private\n2416\n\n\n1982\nTotal enrollment\n228371\n\n\n1982\nMales\n104897\n\n\n1982\nFemales\n123474\n\n\n1982\n4-year\n212017\n\n\n1982\n2-year\n16354\n\n\n1982\nTotal - Public\n165871\n\n\n1982\n4-year - Public\n151472\n\n\n1982\n2-year - Public\n14399\n\n\n1982\nTotal - Private\n62500\n\n\n1982\n4-year - Private\n60545\n\n\n1982\n2-year - Private\n1955\n\n\n1984\nTotal enrollment\n227519\n\n\n1984\nMales\n102823\n\n\n1984\nFemales\n124696\n\n\n1984\n4-year\n212844\n\n\n1984\n2-year\n14675\n\n\n1984\nTotal - Public\n164116\n\n\n1984\n4-year - Public\n151289\n\n\n1984\n2-year - Public\n12827\n\n\n1984\nTotal - Private\n63403\n\n\n1984\n4-year - Private\n61555\n\n\n1984\n2-year - Private\n1848\n\n\n1986\nTotal enrollment\n223275\n\n\n1986\nMales\n97523\n\n\n1986\nFemales\n125752\n\n\n1986\n4-year\n207231\n\n\n1986\n2-year\n16044\n\n\n1986\nTotal - Public\n162048\n\n\n1986\n4-year - Public\n147631\n\n\n1986\n2-year - Public\n14417\n\n\n1986\nTotal - Private\n61227\n\n\n1986\n4-year - Private\n59600\n\n\n\n\n\nIn the long form of the data, we have a row for each data point (year x measurement type), not for each year. I’ve shown the same amount of data (6 years, 9 measurements) in this table as in the original data, but this takes up much more vertical space!\n\n\n\n\n\n18.2.4 Making a (Better) Line Chart\nIf we had wanted to show all of the available data before, we would have needed to add a separate line for each column, coloring each one manually, and then we would have wanted to create a legend manually (which is a pain). Converting the data to long form means we can use ggplot2/seaborn to do all of this for us with only a single plot statement (geom_line or sns.lineplot). Having the data in the right form to plot is very important if you want to get the plot you’re imagining with relatively little effort.\n\n\n\nR\nPython\n\n\n\n\nggplot(hbcu_long, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment\")\n\n\n\n\n\n\n\n\n\n\n\nplot = sns.lineplot(hbcu_long, x = \"Year\", y = \"value\", hue = \"variable\")\nplot.set_title(\"4-year HBCU College Enrollment\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n18.2.5 Highlighting Key Insights\nExamining the charts in the previous section, it seems that there are at least three different contrasts which are easily made: 2 year vs. 4 year (vs. Total), Public vs. Private, and Male vs. Female enrollment. The 2 year vs. 4 year vs. Total and Public vs. Private variables seem to be “crossed” - we have values for every combination of these variables. The Male and Female numbers are not broken out further.\nWhen creating charts, it’s useful to think about key comparisons and insights that the user may wish to explore, and then to explicitly highlight those comparisons with separate charts. It’s never enough to just make one chart – any data that is complex enough to plot deserves to be explored from multiple angles.\n\n\n\n\n\n\n18.2.5.1 Sketching Possible Comparisons\n\n\n\nIt is helpful to sketch out the data structure and then use that sketch to identify key comparisons. You do not have to know what the data looks like at this stage – it is enough to think through what might be interesting and then to test whether or not the comparison is interesting by generating the chart.\n\n\nData Structure\nChart 1\nChart 2\nChart 4\n\n\n\n\n\nSketching the data structure and identifying interesting comparisons can help to guide your data analysis and exploration of the data visually.\n\n\n\n\n\nThis sketch shows what we might see by breaking the data out by gender.\n\n\n\n{fig-alt=” There are three subplots shown - Total, Public, and Private. In the Total pane, lines are broken out by public and private, while in the public and private panes they are broken out by 2y vs. 4y.”} ##### Chart 3\n{fig-alt=” There are four subplots shown, arranged in a 2x2 table, with columns Public and Private, and rows 2yr and 4yr. In each cell, there is a subplot with a single line drawn.”}\n\n\n{fig-alt=” There are two subplots shown, arranged in a 1x2 table, with columns Public and Private. In each cell there is a chart with two lines: 4yr and 2yr.”}\n\n\n\nWhen considering which version of a chart to generate, it is helpful to think about which comparisons are most natural for each chart. When lines are close together and share the same scale, comparisons are easier to make. So, if the goal is to highlight the difference between 2yr enrollment and 4yr enrollment, then Chart 4 is particularly effective, as those comparisons are easiest to make in that chart. Chapter 20 discusses this in more detail.\n\n\n\n18.2.5.2 HBCU Enrollment by Gender\nFirst, let’s peel off the gender-specific enrollment data. This requires us to find only types “Females” and “Males”, which we can obtain by filtering or subsetting the data.\n\n\n\nR\nPython\n\n\n\n\nhbcu_gender &lt;- hbcu_long |&gt;\n  dplyr::filter(type %in% c(\"Females\", \"Males\", \"Total enrollment\"))\n\nggplot(hbcu_gender, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment by Gender\")\n\n\n\n\n\n\n\n\n\n\nplt.clf() # Clear previous plot\nrel_types=[\"Total enrollment\", \"Females\", \"Males\"]\nhbcu_gender = hbcu_long.query('variable.isin(@rel_types)')\nplot = sns.lineplot(hbcu_gender, x = \"Year\", y = \"value\", hue = \"variable\")\nplot.set_title(\"HBCU College Enrollment\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSeeing only the gender related data (with total enrollment as a comparison) helps to highlight that the same temporal trends seem to apply across males and females, though in some cases growth is more shallow in males.\n\n18.2.5.3 Institution Types\nIt may also be useful to consider the type of institution when we consider enrollment numbers – public or private? 2 year or 4 year? In this case, it may be most useful to aim for creating some “small multiples” – multiple charts that are similarly constructed and placed together systematically.\n\n\n\n\n\n\nDemo\n\n\n\n\n\nR\nPython\n\n\n\n\nhbcu_inst &lt;- hbcu_all |&gt;\n  dplyr::select(1:2, 5:12) |&gt;\n  dplyr::rename(\n    \"Total-Total\" = \"Total enrollment\",\n    \"Total-4year\" = \"4-year\", \n    \"Total-2year\"=\"2-year\", \n    \"Public-Total\" = \"Total - Public\", \n    \"Public-4year\" = \"4-year - Public\", \n    \"Public-2year\" = \"2-year - Public\", \n    \"Private-Total\" = \"Total - Private\", \n    \"Private-4year\" = \"4-year - Private\", \n    \"Private-2year\" = \"2-year - Private\") |&gt;\n  tidyr::pivot_longer(2:10, names_to=\"variable\", values_to=\"enrollment\") |&gt;\n  tidyr::separate(\"variable\", into = c(\"Type\", \"Degree\")) \n\nggplot(hbcu_inst, aes(x = Year, y = enrollment, color = Degree)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment by Institution Type and Degree Length\") + \n  facet_wrap(~Type)\n\n\n\n\n\n\n\n\nggplot(hbcu_inst, aes(x = Year, y = enrollment, color = Degree)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment by Institution Type and Degree Length\") + \n  facet_wrap(~Degree)\n\n\n\n\n\n\n\n\n\n\nhbcu_inst = hbcu_all.iloc[:,[0,1,4,5,6,7,8,9,10,11]]\nhbcu_inst_names=[\"Year\", \"Total-Total\", \"Total-4year\", \"Total-2year\", \"Public-Total\", \"Public-4year\", \"Public-2year\",\"Private-Total\", \"Private-4year\", \"Private-2year\"]\nhbcu_inst.columns = hbcu_inst_names\nhbcu_inst_long = pd.melt(hbcu_inst, id_vars = ['Year'], value_vars = hbcu_inst.columns[1:10])\nhbcu_inst_long[['Type','Degree']]=list(hbcu_inst_long[\"variable\"].str.split(\"-\"))\n\nplt.clf() # Clear previous plot\nplot = sns.FacetGrid(hbcu_inst_long, col=\"Type\")\nplot.map_dataframe(sns.lineplot,x = \"Year\", y = \"value\", hue = \"Degree\")\n\n\n\n\n\n\nplot.add_legend()\n\n\n\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.clf() # Clear previous plot\nplot = sns.FacetGrid(hbcu_inst_long, col=\"Degree\")\nplot.map_dataframe(sns.lineplot,x = \"Year\", y = \"value\", hue = \"Type\")\n\n\n\n\n\n\nplot.add_legend()\n\n\n\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny of these plots could be customized by e.g. removing Totals (if desired), changing axis parameters so that facets don’t have the same axis values, etc., but this is enough to get the basic idea of how to work with data.\n\n\n\n18.2.6 Key Takeaways\nFrom this example of HBCU enrollment, a few things are clear:\n\nThe form of the data is important to be able to plot the data easily\nIt can be helpful to break measurements down into disjoint combinations (where the data allows) to create plots that thoughtfully compare variables\nPlotting subsets of variables and rows can help us understand effects in the data better",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#sec-graphics-intro-refs",
    "href": "part-wrangling/02-basic-data-vis.html#sec-graphics-intro-refs",
    "title": "18  Data Visualization Basics",
    "section": "\n18.3 References",
    "text": "18.3 References",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html",
    "href": "part-wrangling/02a-eda.html",
    "title": "19  Exploratory Data Analysis",
    "section": "",
    "text": "Objectives\nMajor components of Exploratory Data Analysis (EDA):\nEDA is an iterative process. It is like brainstorming - you start with an idea or question you might have about the data, investigate, and then generate new ideas. EDA is useful even when you are relatively familiar with the type of data you’re working with: in any dataset, it is good to make sure that you know the quality of the data as well as the relationships between the variables in the dataset.\nEDA is important because it helps us to know what challenges a particular data set might bring, what we might do with it. Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence.\nWhile in many classes you’ll be given mostly clean data, you do need to know how to clean your own data up so that you can use more interesting data sets for projects (and for fun!). EDA is an important component to learning how to work with messy data.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#objectives",
    "href": "part-wrangling/02a-eda.html#objectives",
    "title": "19  Exploratory Data Analysis",
    "section": "",
    "text": "Understand the main goals of exploratory data analysis\nGenerate and answer questions about a new dataset using charts, tables, and numerical summaries",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#package-installation",
    "href": "part-wrangling/02a-eda.html#package-installation",
    "title": "19  Exploratory Data Analysis",
    "section": "\n19.1 Package Installation",
    "text": "19.1 Package Installation\nYou will need the seaborn and matplotlib (python) and ggplot2 (R) packages for this section.\n\ninstall.packages(\"ggplot2\")\n\nTo install python graphing libraries, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 10.3.2.1)\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install matplotlib seaborn\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don’t try to install packages this way).\n\nreticulate::py_install(c(\"matplotlib\", \"seaborn\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called “IPython magic” commands, so if it doesn’t work for you, try the System Terminal method instead.\n\n%pip install matplotlib seaborn\n\nOnce you have run this command, please comment it out so that you don’t reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#extra-reading",
    "href": "part-wrangling/02a-eda.html#extra-reading",
    "title": "19  Exploratory Data Analysis",
    "section": "Extra Reading",
    "text": "Extra Reading\nThe EDA chapter in R for Data Science [1] is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. Much of the material in this chapter is based at least in part on the R4DS chapter.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "href": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "title": "19  Exploratory Data Analysis",
    "section": "\n19.2 A Note on Language Philosophies",
    "text": "19.2 A Note on Language Philosophies\nIt is usually relatively easy to get summary statistics from a dataset, but the “flow” of EDA is somewhat different depending on the language patterns.\n\nYou must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of “give me every possible statistic that could be calculated from this model, whether or not it makes sense”. The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. – Douglas Bates\n\nI provide this as a historical artifact, but it does explain the difference between the approach to EDA and model output in R and Python, and the approach in SAS, which you may see in your other statistics classes. This is not (at least, in my opinion) a criticism – the SAS philosophy dates back to the mainframe and punch card days, and the syntax and output still bear evidence of that – but it is worth noting.\nIn R and in Python, you will have to specify each piece of output you want, but in SAS you will get more than you ever wanted with a single command. Neither approach is wrong, but sometimes one is preferable over the other for a given problem.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#generating-eda-questions",
    "href": "part-wrangling/02a-eda.html#generating-eda-questions",
    "title": "19  Exploratory Data Analysis",
    "section": "\n19.3 Generating EDA Questions",
    "text": "19.3 Generating EDA Questions\nI very much like the two quotes in the [1] section on EDA Questions:\n\nThere are no routine statistical questions, only questionable statistical routines. — Sir David Cox\n\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. — John Tukey\n\nAs statisticians, we are concerned with variability by default. This is also true during EDA: we are interested in variability (or sometimes, lack thereof) in the variables in our dataset, including the co-variability between multiple variables.\nWe may assess variability using pictures or numerical summaries:\n\nhistograms or density plots (continuous variables)\ncolumn plots (categorical variables)\nboxplots\n5 number summaries (min, 25%, mean, 75%, max)\ntabular data summaries (for categorical variables)\n\nIn many cases, this gives us a picture of both variability and the “typical” value of our variable.\nSometimes we may also be interested in identifying unusual values: outliers, data entry errors, and other points which don’t conform to our expectations. These unusual values may show up when we generate pictures and the axis limits are much larger than expected.\nWe also are usually concerned with missing values - in many cases, not all observations are complete, and this missingness can interfere with statistical analyses. It can be helpful to keep track of how much missingness there is in any particular variable and any patterns of missingness that would impact the eventual data analysis1.\nIf you are having trouble getting started on EDA, [3] provides a nice checklist to get you thinking:\n\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat’s missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "href": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "title": "19  Exploratory Data Analysis",
    "section": "\n19.4 Useful EDA Techniques",
    "text": "19.4 Useful EDA Techniques\n\n\n\n\nNintendo, Creatures, Game Freak, The Pokémon Company, Public domain, via Wikimedia Commons\n\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %&gt;%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\n\n\nThis data has several categorical and continuous variables that should allow for a reasonable demonstration of a number of techniques for exploring data.\n\n19.4.1 Numerical Summary Statistics\n\n\nR: summary\nPython: describe\nR: skimr\npython: skimpy\n\n\n\nThe first, and most basic EDA command in R is summary().\nFor numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all “other” category.\n\n# Make types into factors to demonstrate the difference\npoke &lt;- tidyr::separate(poke, type, into = c(\"type_1\", \"type_2\"), sep = \",\")\npoke$type_1 &lt;- factor(poke$type_1)\npoke$type_2 &lt;- factor(poke$type_2)\n\nsummary(poke)\n##       gen          pokedex_no       img_link             name          \n##  Min.   :1.000   Min.   :   1.0   Length:1526        Length:1526       \n##  1st Qu.:2.000   1st Qu.: 226.2   Class :character   Class :character  \n##  Median :4.000   Median : 484.0   Mode  :character   Mode  :character  \n##  Mean   :4.478   Mean   : 487.9                                        \n##  3rd Qu.:7.000   3rd Qu.: 726.8                                        \n##  Max.   :9.000   Max.   :1008.0                                        \n##                                                                        \n##    variant               type_1        type_2        total       \n##  Length:1526        Water   :179   Flying :157   Min.   : 175.0  \n##  Class :character   Normal  :156   Psychic: 61   1st Qu.: 345.8  \n##  Mode  :character   Psychic :123   Ghost  : 57   Median : 475.0  \n##                     Electric:119   Ground : 57   Mean   : 450.3  \n##                     Grass   :113   Steel  : 55   3rd Qu.: 525.0  \n##                     Bug     :107   (Other):466   Max.   :1125.0  \n##                     (Other) :729   NA's   :673                   \n##        hp             attack          defense         sp_attack     \n##  Min.   :  1.00   Min.   :  5.00   Min.   :  5.00   Min.   : 10.00  \n##  1st Qu.: 50.25   1st Qu.: 60.00   1st Qu.: 55.00   1st Qu.: 50.00  \n##  Median : 70.00   Median : 80.00   Median : 70.00   Median : 70.00  \n##  Mean   : 71.18   Mean   : 82.05   Mean   : 75.66   Mean   : 75.05  \n##  3rd Qu.: 85.00   3rd Qu.:100.00   3rd Qu.: 95.00   3rd Qu.: 98.00  \n##  Max.   :255.00   Max.   :190.00   Max.   :250.00   Max.   :194.00  \n##                                                                     \n##    sp_defense         speed         species             height_m     \n##  Min.   : 20.00   Min.   :  5.0   Length:1526        Min.   : 0.100  \n##  1st Qu.: 55.00   1st Qu.: 50.0   Class :character   1st Qu.: 0.500  \n##  Median : 70.00   Median : 70.0   Mode  :character   Median : 1.000  \n##  Mean   : 73.84   Mean   : 72.5                      Mean   : 1.233  \n##  3rd Qu.: 90.00   3rd Qu.: 95.0                      3rd Qu.: 1.500  \n##  Max.   :250.00   Max.   :200.0                      Max.   :20.000  \n##                                                                      \n##    weight_kg        generation \n##  Min.   :  0.10   1      :285  \n##  1st Qu.:  8.00   5      :237  \n##  Median : 29.25   3      :193  \n##  Mean   : 68.25   4      :178  \n##  3rd Qu.: 78.50   8      :134  \n##  Max.   :999.90   7      :133  \n##                   (Other):366\n\nOne common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis. We can see in this summary that 673 pokemon don’t have a second type.\nWe also look for extreme values. There is at least one pokemon who appears to have a weight of 999.90 kg. Let’s investigate further:\n\npoke[poke$weight_kg &gt; 999,] \n## # A tibble: 2 × 18\n##     gen pokedex_no img_link       name  variant type_1 type_2 total    hp attack\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1     7        790 https://img.p… Cosm… NA      Psych… &lt;NA&gt;     400    43     29\n## 2     7        797 https://img.p… Cele… NA      Steel  Flying   570    97    101\n## # ℹ 8 more variables: defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;,\n## #   generation &lt;fct&gt;\n# Show any rows where weight_kg is extreme\n\nThis is the last row of our data frame, and this pokemon appears to have many missing values.\n\n\nThe most basic EDA command in pandas is df.describe() (which operates on a DataFrame named df). Like summary() in R, describe() provides a 5-number summary for numeric variables. For categorical variables, describe() provides the number of unique values, the most common value, and the frequency of that common value.\n\n# Split types into two columns\npoke[['type_1', 'type_2']] = poke.type.str.split(\",\", expand = True)\n# Make each one categorical\npoke['type_1'] = pd.Categorical(poke.type_1)\npoke['type_2'] = pd.Categorical(poke.type_2)\n\npoke.iloc[:,:].describe() # describe only shows numeric variables by default\n##                gen   pokedex_no  ...     height_m    weight_kg\n## count  1526.000000  1526.000000  ...  1526.000000  1526.000000\n## mean      4.477720   487.863041  ...     1.232962    68.249607\n## std       2.565182   290.328644  ...     1.289446   121.828015\n## min       1.000000     1.000000  ...     0.100000     0.100000\n## 25%       2.000000   226.250000  ...     0.500000     8.000000\n## 50%       4.000000   484.000000  ...     1.000000    29.250000\n## 75%       7.000000   726.750000  ...     1.500000    78.500000\n## max       9.000000  1008.000000  ...    20.000000   999.900000\n## \n## [8 rows x 11 columns]\n\n# You can get categorical variables too if that's all you give it to show\npoke['type_1'].describe()\n## count      1526\n## unique       18\n## top       Water\n## freq        179\n## Name: type_1, dtype: object\npoke['type_2'].describe()\n## count        853\n## unique        18\n## top       Flying\n## freq         157\n## Name: type_2, dtype: object\n\n\n\nAn R package that is incredibly useful for this type of dataset exploration is skimr.\n\nlibrary(skimr)\nskim(poke)\n\n\nData summary\n\n\nName\npoke\n\n\nNumber of rows\n1526\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nfactor\n3\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nimg_link\n0\n1\n59\n84\n0\n1192\n0\n\n\nname\n0\n1\n3\n12\n0\n1008\n0\n\n\nvariant\n0\n1\n2\n22\n0\n105\n0\n\n\nspecies\n0\n1\n11\n21\n0\n708\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ntype_1\n0\n1.00\nFALSE\n18\nWat: 179, Nor: 156, Psy: 123, Ele: 119\n\n\ntype_2\n673\n0.56\nFALSE\n18\nFly: 157, Psy: 61, Gho: 57, Gro: 57\n\n\ngeneration\n0\n1.00\nFALSE\n9\n1: 285, 5: 237, 3: 193, 4: 178\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ngen\n0\n1\n4.48\n2.57\n1.0\n2.00\n4.00\n7.00\n9.0\n▇▇▅▅▅\n\n\npokedex_no\n0\n1\n487.86\n290.33\n1.0\n226.25\n484.00\n726.75\n1008.0\n▇▆▇▇▆\n\n\ntotal\n0\n1\n450.29\n120.59\n175.0\n345.75\n475.00\n525.00\n1125.0\n▅▇▂▁▁\n\n\nhp\n0\n1\n71.18\n26.53\n1.0\n50.25\n70.00\n85.00\n255.0\n▃▇▁▁▁\n\n\nattack\n0\n1\n82.05\n32.41\n5.0\n60.00\n80.00\n100.00\n190.0\n▂▇▇▂▁\n\n\ndefense\n0\n1\n75.66\n30.21\n5.0\n55.00\n70.00\n95.00\n250.0\n▃▇▂▁▁\n\n\nsp_attack\n0\n1\n75.05\n33.88\n10.0\n50.00\n70.00\n98.00\n194.0\n▅▇▅▂▁\n\n\nsp_defense\n0\n1\n73.84\n27.72\n20.0\n55.00\n70.00\n90.00\n250.0\n▇▇▁▁▁\n\n\nspeed\n0\n1\n72.50\n30.74\n5.0\n50.00\n70.00\n95.00\n200.0\n▃▇▆▁▁\n\n\nheight_m\n0\n1\n1.23\n1.29\n0.1\n0.50\n1.00\n1.50\n20.0\n▇▁▁▁▁\n\n\nweight_kg\n0\n1\n68.25\n121.83\n0.1\n8.00\n29.25\n78.50\n999.9\n▇▁▁▁▁\n\n\n\n\n\nskim provides a beautiful table of summary statistics along with a sparklines-style histogram of values, giving you a sneak peek at the distribution.\n\n\nThere is a similar package to skimr in R called skimpy in Python.\n\nfrom skimpy import skim\nskim(poke)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 1526   │ │ int64       │ 9     │                       │\n## │ │ Number of columns │ 19     │ │ string      │ 5     │                       │\n## │ └───────────────────┴────────┘ │ category    │ 3     │                       │\n## │                                │ float64     │ 2     │                       │\n## │                                └─────────────┴───────┘                       │\n## │        Categories                                                            │\n## │ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                                    │\n## │ ┃ Categorical Variables ┃                                                    │\n## │ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                                    │\n## │ │ generation            │                                                    │\n## │ │ type_1                │                                                    │\n## │ │ type_2                │                                                    │\n## │ └───────────────────────┘                                                    │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┓  │\n## │ ┃ colu ┃    ┃      ┃      ┃      ┃     ┃      ┃      ┃      ┃      ┃      ┃  │\n## │ ┃ mn   ┃ NA ┃ NA % ┃ mean ┃ sd   ┃ p0  ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ hist ┃  │\n## │ ┡━━━━━━╇━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━┩  │\n## │ │ gen  │  0 │    0 │ 4.47 │ 2.56 │   1 │    2 │    4 │    7 │    9 │ █▄▃▇ │  │\n## │ │      │    │      │    8 │    5 │     │      │      │      │      │  ▃▅  │  │\n## │ │ poke │  0 │    0 │ 487. │ 290. │   1 │ 226. │  484 │ 726. │ 1008 │ █▅▇▇ │  │\n## │ │ dex_ │    │      │    9 │    3 │     │    2 │      │    8 │      │  ▆▆  │  │\n## │ │ no   │    │      │      │      │     │      │      │      │      │      │  │\n## │ │ tota │  0 │    0 │ 450. │ 120. │ 175 │ 345. │  475 │  525 │ 1125 │ ▅█▇▁ │  │\n## │ │ l    │    │      │    3 │    6 │     │    8 │      │      │      │      │  │\n## │ │ hp   │  0 │    0 │ 71.1 │ 26.5 │   1 │ 50.2 │   70 │   85 │  255 │ ▁█▃  │  │\n## │ │      │    │      │    8 │    3 │     │    5 │      │      │      │      │  │\n## │ │ atta │  0 │    0 │ 82.0 │ 32.4 │   5 │   60 │   80 │  100 │  190 │ ▂▇█▆ │  │\n## │ │ ck   │    │      │    5 │    1 │     │      │      │      │      │  ▂▁  │  │\n## │ │ defe │  0 │    0 │ 75.6 │ 30.2 │   5 │   55 │   70 │   95 │  250 │ ▂█▄▁ │  │\n## │ │ nse  │    │      │    6 │    1 │     │      │      │      │      │      │  │\n## │ │ sp_a │  0 │    0 │ 75.0 │ 33.8 │  10 │   50 │   70 │   98 │  194 │ ▄█▆▃ │  │\n## │ │ ttac │    │      │    5 │    8 │     │      │      │      │      │  ▁   │  │\n## │ │ k    │    │      │      │      │     │      │      │      │      │      │  │\n## │ │ sp_d │  0 │    0 │ 73.8 │ 27.7 │  20 │   55 │   70 │   90 │  250 │ ▅█▃  │  │\n## │ │ efen │    │      │    4 │    2 │     │      │      │      │      │      │  │\n## │ │ se   │    │      │      │      │     │      │      │      │      │      │  │\n## │ │ spee │  0 │    0 │ 72.5 │ 30.7 │   5 │   50 │   70 │   95 │  200 │ ▃▆█▂ │  │\n## │ │ d    │    │      │      │    4 │     │      │      │      │      │  ▁   │  │\n## │ │ heig │  0 │    0 │ 1.23 │ 1.28 │ 0.1 │  0.5 │    1 │  1.5 │   20 │  █   │  │\n## │ │ ht_m │    │      │    3 │    9 │     │      │      │      │      │      │  │\n## │ │ weig │  0 │    0 │ 68.2 │ 121. │ 0.1 │    8 │ 29.2 │ 78.5 │ 999. │  █▁  │  │\n## │ │ ht_k │    │      │    5 │    8 │     │      │    5 │      │    9 │      │  │\n## │ │ g    │    │      │      │      │     │      │      │      │      │      │  │\n## │ └──────┴────┴──────┴──────┴──────┴─────┴──────┴──────┴──────┴──────┴──────┘  │\n## │                                  category                                    │\n## │ ┏━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━┓  │\n## │ ┃ column         ┃ NA    ┃ NA %                    ┃ ordered    ┃ unique  ┃  │\n## │ ┡━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━┩  │\n## │ │ generation     │     0 │                       0 │ False      │       9 │  │\n## │ │ type_1         │     0 │                       0 │ False      │      18 │  │\n## │ │ type_2         │   673 │       44.10222804718217 │ False      │      19 │  │\n## │ └────────────────┴───────┴─────────────────────────┴────────────┴─────────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┓  │\n## │ ┃      ┃      ┃      ┃       ┃      ┃       ┃      ┃       ┃ word ┃       ┃  │\n## │ ┃      ┃      ┃      ┃       ┃      ┃       ┃      ┃ chars ┃ s    ┃       ┃  │\n## │ ┃ colu ┃      ┃      ┃ short ┃ long ┃       ┃      ┃ per   ┃ per  ┃ total ┃  │\n## │ ┃ mn   ┃ NA   ┃ NA % ┃ est   ┃ est  ┃ min   ┃ max  ┃ row   ┃ row  ┃ words ┃  │\n## │ ┡━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━┩  │\n## │ │ img_ │    0 │    0 │ https │ http │ https │ http │  65.9 │    1 │  1526 │  │\n## │ │ link │      │      │ ://im │ s:// │ ://im │ s:// │       │      │       │  │\n## │ │      │      │      │ g.pok │ img. │ g.pok │ img. │       │      │       │  │\n## │ │      │      │      │ emond │ poke │ emond │ poke │       │      │       │  │\n## │ │      │      │      │ b.net │ mond │ b.net │ mond │       │      │       │  │\n## │ │      │      │      │ /spri │ b.ne │ /spri │ b.ne │       │      │       │  │\n## │ │      │      │      │ tes/s │ t/sp │ tes/s │ t/sp │       │      │       │  │\n## │ │      │      │      │ word- │ rite │ carle │ rite │       │      │       │  │\n## │ │      │      │      │ shiel │ s/sw │ t-vio │ s/sw │       │      │       │  │\n## │ │      │      │      │ d/ico │ ord- │ let/i │ ord- │       │      │       │  │\n## │ │      │      │      │ n/muk │ shie │ con/a │ shie │       │      │       │  │\n## │ │      │      │      │ .png  │ ld/i │ nnihi │ ld/i │       │      │       │  │\n## │ │      │      │      │       │ con/ │ lape. │ con/ │       │      │       │  │\n## │ │      │      │      │       │ darm │ png   │ zyga │       │      │       │  │\n## │ │      │      │      │       │ anit │       │ rde- │       │      │       │  │\n## │ │      │      │      │       │ an-g │       │ comp │       │      │       │  │\n## │ │      │      │      │       │ alar │       │ lete │       │      │       │  │\n## │ │      │      │      │       │ ian- │       │ .png │       │      │       │  │\n## │ │      │      │      │       │ stan │       │      │       │      │       │  │\n## │ │      │      │      │       │ dard │       │      │       │      │       │  │\n## │ │      │      │      │       │ .png │       │      │       │      │       │  │\n## │ │ name │    0 │    0 │ Muk   │ Crab │ Aboma │ Zyga │  7.55 │    1 │  1551 │  │\n## │ │      │      │      │       │ omin │ snow  │ rde  │       │      │       │  │\n## │ │      │      │      │       │ able │       │      │       │      │       │  │\n## │ │ vari │ 1071 │ 70.1 │ Fan   │ Gala │ 10%   │ Zero │  7.51 │ 0.39 │   598 │  │\n## │ │ ant  │      │ 8348 │       │ rian │       │      │       │      │       │  │\n## │ │      │      │ 6238 │       │      │       │      │       │      │       │  │\n## │ │      │      │ 5321 │       │ Stan │       │      │       │      │       │  │\n## │ │      │      │    1 │       │ dard │       │      │       │      │       │  │\n## │ │      │      │      │       │      │       │      │       │      │       │  │\n## │ │      │      │      │       │ Mode │       │      │       │      │       │  │\n## │ │ type │    0 │    0 │ Bug   │ Elec │ Bug   │ Wate │  9.03 │    1 │  1526 │  │\n## │ │      │      │      │       │ tric │       │ r,St │       │      │       │  │\n## │ │      │      │      │       │ ,Fig │       │ eel  │       │      │       │  │\n## │ │      │      │      │       │ htin │       │      │       │      │       │  │\n## │ │      │      │      │       │ g    │       │      │       │      │       │  │\n## │ │ spec │    0 │    0 │ Fox   │ Mega │ Abund │ Zen  │  15.4 │  2.3 │  3469 │  │\n## │ │ ies  │      │      │ Pokém │ Fire │ ance  │ Char │       │      │       │  │\n## │ │      │      │      │ on    │ Pig  │ Pokém │ m    │       │      │       │  │\n## │ │      │      │      │       │ Poké │ on    │ Poké │       │      │       │  │\n## │ │      │      │      │       │ mon  │       │ mon  │       │      │       │  │\n## │ └──────┴──────┴──────┴───────┴──────┴───────┴──────┴───────┴──────┴───────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\n\n19.4.2 Assessing Distributions\nWe are often also interested in the distribution of values.\n\n19.4.2.1 Categorical Variables\nOne useful way to assess the distribution of values is to generate a cross-tabular view of the data. This is mostly important for variables with a relatively low number of categories - otherwise, it is usually easier to use a graphical summary method.\nTabular Summaries\n\n\nR\nPython\n\n\n\nWe can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). We can even generate cross-tabular views for a combination of two variables (or theoretically more, but this gets hard to read and track).\n\ntable(poke$generation)\n## \n##   1   2   3   4   5   6   7   8   9 \n## 285 124 193 178 237 119 133 134 123\n\ntable(poke$type_1, poke$type_2)\n##           \n##            Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass\n##   Bug        0    1      0        4     2        5    2     14     1     8\n##   Dark       0    0      4        0     3        2    4      8     2     2\n##   Dragon     0    1      0        1     1        2    1      6     3     0\n##   Electric   0    4      3        0     2        2    6     19     6    10\n##   Fairy      0    0      0        0     0        1    0      6     0     0\n##   Fighting   0    3      1        1     0        0    4      3     2     0\n##   Fire       2    1      2        0     0        7    0     11     7     0\n##   Flying     0    1      2        0     0        1    0      0     0     0\n##   Ghost      0    1      4        0     3        0    3      6     0    11\n##   Grass      0    5      6        0     5        7    1      8     4     0\n##   Ground     0    3      2        2     0        1    1      6     5     2\n##   Ice        2    0      0        0     2        0    4      3     1     0\n##   Normal     0    0      1        0     5        5    0     33     4     8\n##   Poison     1    7      4        0     2        3    2      3     0     0\n##   Psychic    0    2      3        0    11        3    1     14     9     4\n##   Rock       2    2      2        7     3        1    2      8     0     2\n##   Steel      0    0      9        0     4        1    0      2     7     0\n##   Water      2    9      6        2     4        6    0      7     6     3\n##           \n##            Ground Ice Normal Poison Psychic Rock Steel Water\n##   Bug           4   0      0     12       3    4    13     3\n##   Dark          1   4      9      3       2    0     3     0\n##   Dragon       13  12      1      0       4    0     0     9\n##   Electric      1   7      2      5       2    0     4     6\n##   Fairy         0   0      0      0       1    0     5     0\n##   Fighting      0   1      0      2       3    0     4     6\n##   Fire          3   0      2      1       6    5     1     1\n##   Flying        0   0      0      0       0    0     1     3\n##   Ghost         2   0      0      4       0    0     0     0\n##   Grass         1   3      3     15       3    0     3     0\n##   Ground        0   0      1      0       2    3     8     0\n##   Ice           3   0      0      0       5    2     4     4\n##   Normal        1   0      0      0       6    0     0     1\n##   Poison        5   0      2      0       4    0     0     3\n##   Psychic       0   3      4      0       0    0     4     0\n##   Rock          9   2      0      3       2    0     4     6\n##   Steel         2   0      0      2       7    3     0     0\n##   Water        12   4      0      4      11    6     1     0\n\n\n\n\nimport numpy as np\n# For only one factor, use .groupby('colname')['colname'].count()\npoke.groupby(['generation'])['generation'].count()\n## generation\n## 1    285\n## 2    124\n## 3    193\n## 4    178\n## 5    237\n## 6    119\n## 7    133\n## 8    134\n## 9    123\n## Name: generation, dtype: int64\n\n# for two or more factors, use pd.crosstab\npd.crosstab(index = poke['type_1'], columns = poke['type_2'])\n## type_2    Bug  Dark  Dragon  Electric  ...  Psychic  Rock  Steel  Water\n## type_1                                 ...                             \n## Bug         0     1       0         4  ...        3     4     13      3\n## Dark        0     0       4         0  ...        2     0      3      0\n## Dragon      0     1       0         1  ...        4     0      0      9\n## Electric    0     4       3         0  ...        2     0      4      6\n## Fairy       0     0       0         0  ...        1     0      5      0\n## Fighting    0     3       1         1  ...        3     0      4      6\n## Fire        2     1       2         0  ...        6     5      1      1\n## Flying      0     1       2         0  ...        0     0      1      3\n## Ghost       0     1       4         0  ...        0     0      0      0\n## Grass       0     5       6         0  ...        3     0      3      0\n## Ground      0     3       2         2  ...        2     3      8      0\n## Ice         2     0       0         0  ...        5     2      4      4\n## Normal      0     0       1         0  ...        6     0      0      1\n## Poison      1     7       4         0  ...        4     0      0      3\n## Psychic     0     2       3         0  ...        0     0      4      0\n## Rock        2     2       2         7  ...        2     0      4      6\n## Steel       0     0       9         0  ...        7     3      0      0\n## Water       2     9       6         2  ...       11     6      1      0\n## \n## [18 rows x 18 columns]\n\n\n\n\nFrequency Plots\n\n\nBase R\nR: ggplot2\nPython: matplotlib\n\n\n\n\nplot(table(poke$generation)) # bar plot\n\n\n\n\n\n\n\n\n\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(generation))) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\")\n\n\n\n\n\n\n\n\n\nWe generate a bar chart using the contingency table we generated earlier combined with matplotlib’s plt.bar().\n\nimport matplotlib.pyplot as plt\n\ntab = poke.groupby(['generation'])['generation'].count()\n\nplt.bar(tab.keys(), tab.values, color = 'grey')\nplt.xlabel(\"Generation\")\nplt.ylabel(\"# Pokemon\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n19.4.2.2 Quantitative Variables\nWe covered some numerical summary statistics in the numerical summary statistic section above. In this section, we will primarily focus on visualization methods for assessing the distribution of quantitative variables.\n\n\n\n\n\n\nNote: R pipe\n\n\n\nThe code in this section uses the R pipe, %&gt;%. The left side of the pipe is passed as an argument to the right side. This makes code easier to read because it becomes a step-wise “recipe” instead of a nested mess of functions and parentheses.\n\n\nIn each step, the left hand side of the pipe is put into the first argument of the function. Source: Arthur Welle (Github)\n\n\n\nWe can generate histograms2 or kernel density plots (a continuous version of the histogram) to show us the distribution of a continuous variable.\n\n\nBase R\nPython: matplotlib\nR: ggplot2\nPython: seaborn\n\n\n\nBy default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desirable result. If we do not specify breaks, R will pick them for us.\n\nhist(poke$hp)\n\n\n\n\n\n\n\nFor continuous variables, we can use histograms, or we can examine kernel density plots.\nlibrary(magrittr) # This provides the pipe command, %&gt;%\n\nhist(poke$weight_kg)\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Take the log - will transformation be useful w/ modeling?\n  hist(main = \"Histogram of Log10 Weight (Kg)\") # create a histogram\n\npoke$weight_kg %&gt;%\n  density(na.rm = T) %&gt;% # First, we compute the kernel density\n  # (na.rm = T says to ignore NA values)\n  plot(main = \"Density of Weight (Kg)\") # Then, we plot the result\n\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Transform the variable\n  density(na.rm = T) %&gt;% # Compute the density ignoring missing values\n  plot(main = \"Density of Log10 pokemon weight in Kg\") # Plot the result,\n    # changing the title of the plot to a meaningful value\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots with separate axes\n# This uses python multi-assignment to assign figures, axes\n# variables all in one go\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\npoke.weight_kg.plot.hist(ax = ax1) # first plot\nax1.set_title(\"Histogram of Weight (kg)\")\n\n\nnp.log10(poke.weight_kg).plot.hist(ax = ax2)\nax2.set_title(\"Histogram of Log10 Weight (kg)\")\n\npoke.weight_kg.plot.density(ax = ax3)\nax3.set_title(\"Density of Weight (kg)\")\n\nnp.log10(poke.weight_kg).plot.density(ax = ax4)\nax4.set_title(\"Density of Log10 Weight (kg)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\nlibrary(ggplot2)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\nggplot(poke, aes(x = height_m)) +\n  geom_density()\nggplot(poke, aes(x = height_m)) +\n  geom_density() +\n  scale_x_log10()\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\nNotice that in ggplot2, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots with separate axes\n# This uses python multi-assignment to assign figures, axes\n# variables all in one go\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\nsns.histplot(poke, x = \"height_m\", bins = 30, ax = ax1)\nsns.histplot(poke, x = \"height_m\", bins = 30, log_scale = True, ax = ax2)\nsns.kdeplot(poke, x = \"height_m\", bw_adjust = 1, ax = ax3)\nsns.kdeplot(poke, x = \"height_m\", bw_adjust = 1, log_scale = True, ax = ax4)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n19.4.3 Relationships Between Variables\n\n19.4.3.1 Categorical - Categorical Relationships\n\n\nR: ggplot2\nBase R\nPython: matplotlib\nPython: seaborn\n\n\n\nWe can generate a (simple) mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I’ve flipped the axes using coord_flip so that you can read the labels more easily.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip()\n\n\n\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\nggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +\n  # Shade tiles according to the number of things in the bin\n  geom_tile(aes(fill = after_stat(count)), stat = \"bin2d\") +\n  # Add the number of things in the bin to the top of the tile as text\n  geom_text(aes(label = after_stat(count)), stat = 'bin2d') +\n  # Scale the tile fill\n  scale_fill_gradient2(limits = c(0, 100), low = \"white\", high = \"blue\", na.value = \"white\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\n\n\n\n\n\nBase R mosaic plots aren’t nearly as pretty as the ggplot version, but I will at least show you how to create them.\n\nplot(table(poke$type_1, poke$type_2)) \n\n\n\n\n\n\n# mosaic plot - hard to read b/c too many categories\n\n\n\nTo get a mosaicplot, we need an additional library, called statsmodels, which we install with pip install statsmodels in the terminal.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(poke, ['type_1', 'type_2'], title = \"Pokemon Types\")\nplt.show()\n\n\n\n\n\n\n\nThis obviously needs a bit of cleaning up to remove extra labels, but it’s easy to get to and relatively functional. Notice that it does not, by default, show NA values.\n\n\nSeaborn doesn’t appear to have built-in mosaic plots; the example I found used the statsmodels function shown in the maptlotlib example and used seaborn to create facets. As the point here isn’t to display facets, borrowing the code from that example doesn’t add any value here.\n\n\n\n\n19.4.3.2 Categorical - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: seaborn\n\n\n\nIn R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms.\nTo examine the relationship between a categorical variable and a continuous variable, we might look at box plots:\n\npar(mfrow = c(1, 2)) # put figures in same row\nboxplot(log10(height_m) ~ type_1, data = poke)\nboxplot(total ~ generation, data = poke)\n\n\n\n\n\n\n\nIn the second box plot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn’t usually about creating pretty plots (or we’d be using ggplot right now) but rather about identifying things which may come up in the analysis later.\n\n\n\nggplot(data = poke, aes(x = type_1, y = height_m)) + \n  geom_boxplot() + \n  scale_y_log10()\n\nggplot(data = poke, aes(x = factor(generation), y = total)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure()\n\n# Create a list of vectors of height_m by type_1\npoke['height_m_log'] = np.log(poke.height_m)\nheight_by_type = poke.groupby('type_1', group_keys = True).height_m_log.apply(list)\n\n# Plot each object in the list\nplt.boxplot(height_by_type, labels = height_by_type.index)\n## {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x7f94f260b890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23c4050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23c7610&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23c7e10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d3350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d3c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23df010&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23df890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23eed50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ef550&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23feb50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ff450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240e9d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240f2d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f241a990&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f241b1d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2426690&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2426f90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2435950&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2391690&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2445c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f24464d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23b3bd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3bcefd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f39a2310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f39a09d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f288c850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f288eb90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2813790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a50d90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d2490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d1c10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2788e10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2788c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27f8f50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27fa690&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x7f94f23c49d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23c5350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d0710&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d0fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23dc450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23dcd50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ec150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ec950&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23efe50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23fc750&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ffcd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240c650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240fbd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f24184d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f241b9d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2424310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2427850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2434090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f24371d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2437a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2446d90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2447590&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27df5d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27dc0d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f39a18d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f39a2e10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f288fc10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2813d50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a52cd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a51150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d0750&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d1e90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f278a490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f278b210&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27fb6d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28a8c10&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x7f94f3eecfd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23c6dd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d2a50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23de750&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ee4d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23fe2d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240e150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f241a090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2425dd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2435ad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f24453d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2458f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27dfe90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f288c950&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2812050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a53bd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27884d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27f8e50&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x7f94f23c5d50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d18d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23dd590&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23ed250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23fcfd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240cf90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2418d50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2424b50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f24347d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2444310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2447d90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27dc910&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f39a3f50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2812a10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a51f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d3190&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27f9bd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28a8290&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x7f94f23c6650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23d2150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23dde50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23edb50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f23fd950&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f240d850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2419710&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2425450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2435250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2444ad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2458690&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27dce50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f288e690&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2810150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a52f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28d0290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27f9510&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28a9410&gt;], 'means': []}\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.figure()\n\n# Create a list of vectors of total by generation\ntotal_by_gen = poke.groupby('generation', group_keys = True).total.apply(list)\n\n# Plot each object in the list\nplt.boxplot(total_by_gen, labels = total_by_gen.index)\n## {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x7f94f27336d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2968890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f296b450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28ba050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28bba10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28bb550&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27afe50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f282a7d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2682490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26812d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26dc510&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26ddf90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27e8d50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27eb350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a47c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a6610&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2970510&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2973110&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x7f94f296b0d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f296a790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28b9210&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28b8090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27ac150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27ac0d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2829310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2829110&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2680c10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2682750&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26df290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26dc310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a44e90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a45650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a5d50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a4050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f29704d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2971b10&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x7f94f2732e10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f296aa10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28ba4d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27af050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2683110&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26de3d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27e8f90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a46c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a7c10&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x7f94f2968c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28b8fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27ac910&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f282a4d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2683310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27e89d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a44b10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a54d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2973450&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x7f94f2969a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f28b9fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27ac250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f282b790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26dda50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f27eae90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f3a45e50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f26a6f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f94f2973810&gt;], 'means': []}\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nplt.figure()\nsns.boxplot(poke, x = \"generation\", y = \"height_m\", log_scale = True)\nplt.show()\n\n\n\n\n\n\n\nAs a higher-level graphics library, seaborn allows you to transform the scale shown on the axes instead of having to manually transform the data. This is a more natural presentation, as the values on the scale are (at least in theory) a bit easier to read. In practice, I’d still probably customize the labels on the y-axis if I were hoping to use this for publication.\n\nimport seaborn as sns\nplt.figure()\nsns.boxplot(poke, x = \"generation\", y = \"total\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nYou can find more on boxplots and ways to customize boxplots in the Graphics chapter.\n\n19.4.3.3 Continuous - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: pandas\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nplot(defense ~ attack, data = poke, type = \"p\")\n\n\n\n\n\n\n\ncor(poke$defense, poke$attack)\n## [1] 0.4259168\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. In other cases, like in the plot below, we may discover an interesting correlation that sticks out - the identity line \\(y=x\\) seems to stand out from the cloud here.\n\nplot(x = poke$sp_attack, y = poke$attack, type = \"p\")\n\n\n\n\n\n\n\nA scatterplot matrix can also be a useful way to visualize relationships between several variables.\n\npairs(poke[,c(\"hp\", \"attack\", \"defense\", \"sp_attack\", \"sp_defense\")]) # hp - sp_defense columns\n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s more information on how to customize base R scatterplot matrices here.\n\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nlibrary(ggplot2)\nggplot(poke, aes(x = attack, y = defense)) + geom_point()\n\n\n\n\n\n\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. When this happens, it can be a good idea to use geom_jitter to provide some “wiggle” in the data so that you can still see the point density. Changing the point transparency (alpha = .5) can also help with overplotting.\nIn other cases, we might find that there is a prominent feature of a scatterplot (in this case, the line \\(y=x\\) seems to stand out a bit from the overall point cloud). We can highlight this feature by adding a line at \\(y=x\\) in red behind the points.\n\nggplot(poke, aes(x = attack, y = sp_attack)) + geom_point()\n\n\n\n\n\n\n\nggplot(poke, aes(x = attack, y = sp_attack)) + \n  geom_abline(slope = 1, color = \"red\") + \n  geom_jitter(alpha = 0.5)\n\n\n\n\n\n\n\n\nlibrary(GGally) # an extension to ggplot2\nggpairs(poke[,c(\"hp\", \"attack\", \"defense\", \"sp_attack\", \"sp_defense\")], \n        # hp - sp_defense columns\n        lower = list(continuous = wrap(\"points\", alpha = .15)),\n        progress = F) \n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\nggpairs can also handle continuous variables, if you want to explore the options available.\n\n\nBelieve it or not, you don’t have to go to matplotlib to get plots in python - you can get some plots from pandas directly, even if you are still using matplotlib under the hood (this is why you have to run plt.show() to get the plot to appear if you’re working in markdown).\n\nimport matplotlib.pyplot as plt\n\npoke.plot.scatter(x = 'attack', y = 'defense')\nplt.show()\n\n\n\n\n\n\n\nPandas also includes a nice scatterplot matrix method.\n\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\n\nvars = [6, 7,14,15]\nscatter_matrix(poke.iloc[:,vars], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')\n## array([[&lt;Axes: xlabel='total', ylabel='total'&gt;,\n##         &lt;Axes: xlabel='hp', ylabel='total'&gt;,\n##         &lt;Axes: xlabel='height_m', ylabel='total'&gt;,\n##         &lt;Axes: xlabel='weight_kg', ylabel='total'&gt;],\n##        [&lt;Axes: xlabel='total', ylabel='hp'&gt;,\n##         &lt;Axes: xlabel='hp', ylabel='hp'&gt;,\n##         &lt;Axes: xlabel='height_m', ylabel='hp'&gt;,\n##         &lt;Axes: xlabel='weight_kg', ylabel='hp'&gt;],\n##        [&lt;Axes: xlabel='total', ylabel='height_m'&gt;,\n##         &lt;Axes: xlabel='hp', ylabel='height_m'&gt;,\n##         &lt;Axes: xlabel='height_m', ylabel='height_m'&gt;,\n##         &lt;Axes: xlabel='weight_kg', ylabel='height_m'&gt;],\n##        [&lt;Axes: xlabel='total', ylabel='weight_kg'&gt;,\n##         &lt;Axes: xlabel='hp', ylabel='weight_kg'&gt;,\n##         &lt;Axes: xlabel='height_m', ylabel='weight_kg'&gt;,\n##         &lt;Axes: xlabel='weight_kg', ylabel='weight_kg'&gt;]], dtype=object)\nplt.show()\n\n\n\n\n\n\n\n\nvars = [6, 7,14,15]\n\nplt.figure()\nsns.set_theme(style=\"ticks\")\nsns.pairplot(data = poke.iloc[:,vars])\n\n\n\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I’m cheating a bit by mentioning it now, but it’s just so useful!)\n\n\n\n\n\n\nTry it out: EDA\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nExplore the variables present in the Lancaster County Assessor Housing Sales Data Documentation.\nNote that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\ndownload.file(\"https://github.com/srvanderplas/datasets/blob/main/raw/Lancaster%20County,%20NE%20-%20Assessor.xlsx?raw=true\", destfile = \"../data/lancaster-housing.xlsx\")\nhousing_lincoln &lt;- read_xlsx(\"../data/lancaster-housing.xlsx\", sheet = 1, guess_max = 7000)\n\n\nimport pandas as pd\nhousing_lincoln = pd.read_excel(\"../data/lancaster-housing.xlsx\")\n\n\n\n\nhousing_lincoln$TLA &lt;- readr::parse_number(housing_lincoln$`TLA (Sqft)`)\nhousing_lincoln$Assd_Value &lt;- readr::parse_number(housing_lincoln$Assd_Value)\n\nskim(housing_lincoln)\n\n\nData summary\n\n\nName\nhousing_lincoln\n\n\nNumber of rows\n6918\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nParcel_ID\n0\n1\n17\n17\n0\n6740\n0\n\n\nAddress\n0\n1\n29\n50\n0\n6740\n0\n\n\nOwner\n0\n1\n6\n67\n0\n6435\n0\n\n\nOwner Address\n0\n1\n25\n93\n0\n6184\n0\n\n\nImp_Type\n0\n1\n2\n3\n0\n39\n0\n\n\nTLA (Sqft)\n0\n1\n3\n5\n0\n1767\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nYr_Blt\n0\n1\n1950.85\n22.55\n1900\n1933\n1954.0\n1963\n2023\n▂▃▇▂▁\n\n\nAssd_Value\n0\n1\n229956.23\n96272.53\n36500\n174925\n214500.0\n259175\n1404800\n▇▁▁▁▁\n\n\nTLA\n0\n1\n1379.08\n599.09\n400\n966\n1235.5\n1612\n6819\n▇▂▁▁▁\n\n\n\n\n\nLet’s examine the numeric variables first:\n\nhist(housing_lincoln$Assd_Value)\n\n\n\n\n\n\n\nhist(housing_lincoln$Yr_Blt)\n\n\n\n\n\n\n\nLet’s look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nhousing_lincoln$decade &lt;- 10*floor(housing_lincoln$Yr_Blt/10)\n\ntable(housing_lincoln$decade, useNA = 'ifany')\n## \n## 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 \n##  245  295  885  414  647 1949 1429  282  550  116   72   24   10\ntable(housing_lincoln$Imp_Type)\n## \n##   BL   BN   C1   C2   CA   CB  CXF  CXU  CYF  CYU   D1   D2   D3   D4   D5   D6 \n##  163  764   11   39   48   17    4    3    7   25    2   40    9  232   45   10 \n##   DA   HC   M1   R1   R2   RA   RB   RR   RS  RXF  RXU  RYF  RYU   T1   T2   T3 \n##    2  132    1 3165  492  628   23   15  218  257   79   31   73  160    9   14 \n##   T4   T5   T6   T7   TA   TS  TYF \n##   17   37    5   21  110    9    1\ntable(housing_lincoln$Imp_Type, housing_lincoln$decade)\n##      \n##       1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\n##   BL     0    0    0    0    0    0  119   42    0    1    1    0    0\n##   BN    77   84  413  176   14    0    0    0    0    0    0    0    0\n##   C1     1    0    8    0    1    0    0    0    1    0    0    0    0\n##   C2    14    9   12    1    1    2    0    0    0    0    0    0    0\n##   CA     7   14   17    5    4    1    0    0    0    0    0    0    0\n##   CB     5   11    0    0    0    1    0    0    0    0    0    0    0\n##   CXF    0    1    2    0    1    0    0    0    0    0    0    0    0\n##   CXU    0    0    1    2    0    0    0    0    0    0    0    0    0\n##   CYF    0    5    2    0    0    0    0    0    0    0    0    0    0\n##   CYU    7    9    7    2    0    0    0    0    0    0    0    0    0\n##   D1     0    0    0    0    0    0    1    1    0    0    0    0    0\n##   D2     0    0    0    0    0    5   26    8    1    0    0    0    0\n##   D3     0    0    0    0    0    0    1    1    2    3    2    0    0\n##   D4     0    0    1    3   29   91   87    8    5    5    1    2    0\n##   D5     0    2    1   10    9   12    0    5    1    2    3    0    0\n##   D6     0    0    0    0    0    2    0    2    0    6    0    0    0\n##   DA     0    0    1    0    0    1    0    0    0    0    0    0    0\n##   HC     0    0    0    0    0    0   83    8   41    0    0    0    0\n##   M1     0    0    0    1    0    0    0    0    0    0    0    0    0\n##   R1     3    1   10    8  382 1641  902  133   52   10    3   10   10\n##   R2    33   46   76   42   19   11   48   25  163   27    0    2    0\n##   RA    47   51  165   87  115  104   13    7   26    6    4    3    0\n##   RB     4    5    8    5    1    0    0    0    0    0    0    0    0\n##   RR     0    0    0    0    1    9    3    0    0    1    1    0    0\n##   RS     0    0    2    0    4   33  145   26    8    0    0    0    0\n##   RXF   16   13  101   53   45   28    1    0    0    0    0    0    0\n##   RXU    3    8   31   12   18    7    0    0    0    0    0    0    0\n##   RYF   12   11    6    0    1    1    0    0    0    0    0    0    0\n##   RYU   16   25   21    7    2    0    0    0    2    0    0    0    0\n##   T1     0    0    0    0    0    0    0    6  124    8   22    0    0\n##   T2     0    0    0    0    0    0    0    0    3    0    0    6    0\n##   T3     0    0    0    0    0    0    0    4   10    0    0    0    0\n##   T4     0    0    0    0    0    0    0    0   16    0    0    1    0\n##   T5     0    0    0    0    0    0    0    0    8   29    0    0    0\n##   T6     0    0    0    0    0    0    0    0    4    1    0    0    0\n##   T7     0    0    0    0    0    0    0    0    5   13    3    0    0\n##   TA     0    0    0    0    0    0    0    0   74    4   32    0    0\n##   TS     0    0    0    0    0    0    0    6    3    0    0    0    0\n##   TYF    0    0    0    0    0    0    0    0    1    0    0    0    0\n\nplot(table(housing_lincoln$decade, housing_lincoln$Imp_Type),\n     main = \"Year Built and Improvement Type\")\n\n\n\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_lincoln %&gt;%\n  subset(Imp_Type %in% c(\"BN\", \"R1\", \"R2\", \"RA\")) %&gt;%\n  boxplot(TLA ~ Imp_Type, data = .)\n\n\n\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house.\n\n\n\nhousing_lincoln[\"TLA\"] = housing_lincoln[\"TLA (Sqft)\"].str.replace(\"[,\\$]\", \"\", regex = True)\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA\"] = housing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA (Sqft)\"]\nhousing_lincoln[\"TLA\"] = pd.to_numeric(housing_lincoln[\"TLA\"], errors = 'coerce')\n\nhousing_lincoln[\"Assessed\"] = housing_lincoln[\"Assd_Value\"].str.replace(\"[,\\$]\", \"\", regex = True)\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assessed\"] = housing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assd_Value\"]\nhousing_lincoln[\"Assessed\"] = pd.to_numeric(housing_lincoln[\"Assessed\"], errors = 'coerce')\n\nhousing_lincoln = housing_lincoln.drop([\"TLA (Sqft)\", \"Assd_Value\"], axis = 1)\n\n# housing_lincoln.describe()\nskim(housing_lincoln)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 6918   │ │ string      │ 5     │                       │\n## │ │ Number of columns │ 8      │ │ int64       │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━┓  │\n## │ ┃ colu ┃    ┃      ┃      ┃      ┃      ┃      ┃      ┃      ┃      ┃ his ┃  │\n## │ ┃ mn   ┃ NA ┃ NA % ┃ mean ┃ sd   ┃ p0   ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ t   ┃  │\n## │ ┡━━━━━━╇━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━┩  │\n## │ │ Yr_B │  0 │    0 │ 1951 │ 22.5 │ 1900 │ 1933 │ 1954 │ 1963 │ 2023 │ ▃▃█ │  │\n## │ │ lt   │    │      │      │    5 │      │      │      │      │      │ ▄▂  │  │\n## │ │ TLA  │  0 │    0 │ 1379 │ 599. │  400 │  966 │ 1236 │ 1612 │ 6819 │ █▃▁ │  │\n## │ │      │    │      │      │    1 │      │      │      │      │      │     │  │\n## │ │ Asse │  0 │    0 │ 2300 │ 9627 │ 3650 │ 1749 │ 2145 │ 2592 │ 1405 │ █▂  │  │\n## │ │ ssed │    │      │   00 │    0 │    0 │   00 │   00 │   00 │  000 │     │  │\n## │ └──────┴────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴─────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┓  │\n## │ ┃       ┃    ┃      ┃       ┃       ┃       ┃       ┃ char ┃       ┃ tota ┃  │\n## │ ┃       ┃    ┃      ┃       ┃       ┃       ┃       ┃ s    ┃ words ┃ l    ┃  │\n## │ ┃ colum ┃    ┃      ┃ short ┃ longe ┃       ┃       ┃ per  ┃ per   ┃ word ┃  │\n## │ ┃ n     ┃ NA ┃ NA % ┃ est   ┃ st    ┃ min   ┃ max   ┃ row  ┃ row   ┃ s    ┃  │\n## │ ┡━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━┩  │\n## │ │ Parce │  0 │    0 │ 10-25 │ 10-25 │ 10-25 │ 17-34 │   17 │     1 │ 6918 │  │\n## │ │ l_ID  │    │      │ -125- │ -125- │ -125- │ -241- │      │       │      │  │\n## │ │       │    │      │ 007-0 │ 007-0 │ 007-0 │ 001-0 │      │       │      │  │\n## │ │       │    │      │ 00    │ 00    │ 00    │ 00    │      │       │      │  │\n## │ │ Addre │  0 │    0 │ 2011  │ 1100  │ 100   │ 964 S │ 33.8 │   7.4 │ 5141 │  │\n## │ │ ss    │    │      │ L ST, │ SILVE │ SYCAM │ 49TH  │      │       │    0 │  │\n## │ │       │    │      │ LINCO │ R     │ ORE   │ ST,   │      │       │      │  │\n## │ │       │    │      │ LN,   │ RIDGE │ DR,   │ LINCO │      │       │      │  │\n## │ │       │    │      │ NE    │ RD,   │ LINCO │ LN,   │      │       │      │  │\n## │ │       │    │      │ 68510 │ UNIT  │ LN,   │ NE    │      │       │      │  │\n## │ │       │    │      │       │ #13,  │ NE    │ 68510 │      │       │      │  │\n## │ │       │    │      │       │ LINCO │ 68510 │       │      │       │      │  │\n## │ │       │    │      │       │ LN,   │       │       │      │       │      │  │\n## │ │       │    │      │       │ NE    │       │       │      │       │      │  │\n## │ │       │    │      │       │ 68510 │       │       │      │       │      │  │\n## │ │ Owner │  0 │    0 │ NBK   │ GOVAE │ 1     │ ZWIEB │   23 │   4.2 │ 2929 │  │\n## │ │       │    │      │ PC    │ RTS,  │ CHRON │ EL,   │      │       │    4 │  │\n## │ │       │    │      │       │ KENNE │ 29:11 │ THOMA │      │       │      │  │\n## │ │       │    │      │       │ TH    │ LLC   │ S E   │      │       │      │  │\n## │ │       │    │      │       │ CHARL │       │       │      │       │      │  │\n## │ │       │    │      │       │ ES &  │       │       │      │       │      │  │\n## │ │       │    │      │       │ KATHR │       │       │      │       │      │  │\n## │ │       │    │      │       │ YN    │       │       │      │       │      │  │\n## │ │       │    │      │       │ ANN   │       │       │      │       │      │  │\n## │ │       │    │      │       │ REVOC │       │       │      │       │      │  │\n## │ │       │    │      │       │ ABLE  │       │       │      │       │      │  │\n## │ │       │    │      │       │ LIVIN │       │       │      │       │      │  │\n## │ │       │    │      │       │ G     │       │       │      │       │      │  │\n## │ │       │    │      │       │ TRUST │       │       │      │       │      │  │\n## │ │       │    │      │       │ , THE │       │       │      │       │      │  │\n## │ │ Owner │  0 │    0 │ 3419  │ Attn: │ 100 N │ UNION │ 33.8 │   7.6 │ 5229 │  │\n## │ │ Addre │    │      │ J     │ US    │ 12 ST │ BANK- │      │       │    2 │  │\n## │ │ ss    │    │      │ LINCO │ BANK  │ #UNIT │ ANDRE │      │       │      │  │\n## │ │       │    │      │ LN,   │ NATIO │ 1005  │ W     │      │       │      │  │\n## │ │       │    │      │ NE    │ NAL   │ LINCO │ KAFKA │      │       │      │  │\n## │ │       │    │      │ 68510 │ ASSOC │ LN,   │ PO    │      │       │      │  │\n## │ │       │    │      │       │ IATIO │ NE    │ BOX   │      │       │      │  │\n## │ │       │    │      │       │ N C/O │ 68508 │ 82535 │      │       │      │  │\n## │ │       │    │      │       │ YVONN │       │ LINCO │      │       │      │  │\n## │ │       │    │      │       │ E     │       │ LN,   │      │       │      │  │\n## │ │       │    │      │       │ LUNNE │       │ NE    │      │       │      │  │\n## │ │       │    │      │       │ Y 233 │       │ 68501 │      │       │      │  │\n## │ │       │    │      │       │ S 13  │       │       │      │       │      │  │\n## │ │       │    │      │       │ ST    │       │       │      │       │      │  │\n## │ │       │    │      │       │ #STE  │       │       │      │       │      │  │\n## │ │       │    │      │       │ 1011  │       │       │      │       │      │  │\n## │ │       │    │      │       │ LINCO │       │       │      │       │      │  │\n## │ │       │    │      │       │ LN,   │       │       │      │       │      │  │\n## │ │       │    │      │       │ NE    │       │       │      │       │      │  │\n## │ │       │    │      │       │ 68508 │       │       │      │       │      │  │\n## │ │ Imp_T │  0 │    0 │ BN    │ RXU   │ BL    │ TYF   │ 2.07 │     1 │ 6918 │  │\n## │ │ ype   │    │      │       │       │       │       │      │       │      │  │\n## │ └───────┴────┴──────┴───────┴───────┴───────┴───────┴──────┴───────┴──────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\nLet’s examine the numeric and date variables first:\n\nhousing_lincoln[\"TLA\"].plot.hist()\nplt.show()\n\n\n\n\n\n\n\nhousing_lincoln[\"Yr_Blt\"].plot.hist()\nplt.show()\n\n\n\n\n\n\n\nLet’s look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nimport numpy as np\nhousing_lincoln['decade'] = 10*np.floor(housing_lincoln.Yr_Blt/10)\n\nhousing_lincoln[\"decade\"].groupby(housing_lincoln[\"decade\"]).count()\n## decade\n## 1900.0     245\n## 1910.0     295\n## 1920.0     885\n## 1930.0     414\n## 1940.0     647\n## 1950.0    1949\n## 1960.0    1429\n## 1970.0     282\n## 1980.0     550\n## 1990.0     116\n## 2000.0      72\n## 2010.0      24\n## 2020.0      10\n## Name: decade, dtype: int64\nhousing_lincoln[\"Imp_Type\"].groupby(housing_lincoln[\"Imp_Type\"]).count()\n## Imp_Type\n## BL      163\n## BN      764\n## C1       11\n## C2       39\n## CA       48\n## CB       17\n## CXF       4\n## CXU       3\n## CYF       7\n## CYU      25\n## D1        2\n## D2       40\n## D3        9\n## D4      232\n## D5       45\n## D6       10\n## DA        2\n## HC      132\n## M1        1\n## R1     3165\n## R2      492\n## RA      628\n## RB       23\n## RR       15\n## RS      218\n## RXF     257\n## RXU      79\n## RYF      31\n## RYU      73\n## T1      160\n## T2        9\n## T3       14\n## T4       17\n## T5       37\n## T6        5\n## T7       21\n## TA      110\n## TS        9\n## TYF       1\n## Name: Imp_Type, dtype: int64\n\npd.crosstab(index = housing_lincoln[\"decade\"], columns = housing_lincoln[\"Imp_Type\"])\n## Imp_Type   BL   BN  C1  C2  CA  CB  CXF  CXU  ...  T3  T4  T5  T6  T7  TA  TS  TYF\n## decade                                        ...                                 \n## 1900.0      0   77   1  14   7   5    0    0  ...   0   0   0   0   0   0   0    0\n## 1910.0      0   84   0   9  14  11    1    0  ...   0   0   0   0   0   0   0    0\n## 1920.0      0  413   8  12  17   0    2    1  ...   0   0   0   0   0   0   0    0\n## 1930.0      0  176   0   1   5   0    0    2  ...   0   0   0   0   0   0   0    0\n## 1940.0      0   14   1   1   4   0    1    0  ...   0   0   0   0   0   0   0    0\n## 1950.0      0    0   0   2   1   1    0    0  ...   0   0   0   0   0   0   0    0\n## 1960.0    119    0   0   0   0   0    0    0  ...   0   0   0   0   0   0   0    0\n## 1970.0     42    0   0   0   0   0    0    0  ...   4   0   0   0   0   0   6    0\n## 1980.0      0    0   1   0   0   0    0    0  ...  10  16   8   4   5  74   3    1\n## 1990.0      1    0   0   0   0   0    0    0  ...   0   0  29   1  13   4   0    0\n## 2000.0      1    0   0   0   0   0    0    0  ...   0   0   0   0   3  32   0    0\n## 2010.0      0    0   0   0   0   0    0    0  ...   0   1   0   0   0   0   0    0\n## 2020.0      0    0   0   0   0   0    0    0  ...   0   0   0   0   0   0   0    0\n## \n## [13 rows x 39 columns]\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(housing_lincoln, [\"decade\", \"Imp_Type\"], title = \"Housing Built by Type, Decade\")\nplt.show()\n\n\n\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_subcat = [\"BN\", \"R1\", \"RA\", \"R2\"]\n\nhousing_sub = housing_lincoln.loc[housing_lincoln[\"Imp_Type\"].isin(housing_subcat)]\nhousing_sub = housing_sub.assign(Imp_cat = pd.Categorical(housing_sub[\"Imp_Type\"], categories = housing_subcat))\n\nhousing_sub.boxplot(\"TLA\", by = \"Imp_cat\")\nplt.show()\n\n\n\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house; we would expect an increase in square footage with each additional floor of the house (broadly speaking).\n\n\n\n\n\n\n\n\n\n\n\nLearn More: Janitor R package\n\n\n\nThe janitor package [4] has some very convenient functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing.\n\n\njanitor and clean_names() by Allison Horst",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#sec-eda-refs",
    "href": "part-wrangling/02a-eda.html#sec-eda-refs",
    "title": "19  Exploratory Data Analysis",
    "section": "\n19.5 References",
    "text": "19.5 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nN. Tierney, D. Cook, M. McBain, and C. Fay, Naniar: Data structures, summaries, and visualisations for missing data. 2021 [Online]. Available: https://CRAN.R-project.org/package=naniar\n\n\n\n[3] \nDaniel Bourke, “A Gentle Introduction to Exploratory Data Analysis,” Daniel Bourke. Jan. 2019 [Online]. Available: https://www.mrdbourke.com/a-gentle-introduction-to-exploratory-data-analysis/. [Accessed: Jun. 13, 2022]\n\n\n[4] \nS. Firke, Janitor: Simple tools for examining and cleaning dirty data. 2021 [Online]. Available: https://CRAN.R-project.org/package=janitor",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#footnotes",
    "href": "part-wrangling/02a-eda.html#footnotes",
    "title": "19  Exploratory Data Analysis",
    "section": "",
    "text": "One package for this process in R is naniar [2].↩︎\nA histogram is a chart which breaks up a continuous variable into ranges, where the height of the bar is proportional to the number of items in the range. A bar chart is similar, but shows the number of occurrences of a discrete variable.↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html",
    "href": "part-wrangling/02b-graphics.html",
    "title": "20  A Grammar of Graphics",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#objectives",
    "href": "part-wrangling/02b-graphics.html#objectives",
    "title": "20  A Grammar of Graphics",
    "section": "",
    "text": "Describe charts using the grammar of graphics\nCreate charts designed to communicate specific aspects of the data\nCreate layered graphics that highlight multiple aspects of the data",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#introduction",
    "href": "part-wrangling/02b-graphics.html#introduction",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. I’m going to be opinionated - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics, and more specifically the implementation in ggplot2, which is one of the more complete and extensible implementations available currently.\n\n\nVisualization and statistical graphics are also my research area, so I’m more passionate about this material, which means there’s going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested.\nThis is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That’s why I’m showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.\n\n\n\n\n\n\nGuides and Resources\n\n\n\n\n\nGraph galleries contain sample code to create many different types of charts. Similar galaries are available in R and Python.\nCheat Sheets:\n\nPython\nGgplot2\nBase R\n\nYoutube Playlist: a bunch of different “how to plot” tutorials on YouTube I found helpful\n\n\n\n\n\n\n\n\n20.1.1 Package Installation\nThis chapter will cover the following graphics libraries1:\n\n\nggplot2 in R [2]\n\n\nseaborn in python [3], with some information about its next-generation interface that is much closer to a grammar-of-graphics implementation [4]\n\n\nTo a lesser degree, we will also cover some details of more basic plotting libraries:\n\n\nmatplotlib in python [5]\n\nBase R plotting libraries [6]\n\n\n\ninstall.packages(\"ggplot2\")\n\nTo install python graphics packages, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 10.3.2.1).\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install matplotlib seaborn\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don’t try to install packages this way).\n\nreticulate::py_install(c(\"matplotlib\", \"seaborn\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called “IPython magic” commands, so if it doesn’t work for you, try the System Terminal method instead.\n\n%pip install matplotlib seaborn\n\nOnce you have run this command, please comment it out so that you don’t reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "href": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.2 Why do we create graphics?",
    "text": "20.2 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John W. Tukey [7]\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\n\n\n\n\n\n\nThought Experiment\n\n\n\nYou have a simple data set - 2 variables, about 150 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\n\n\nDataset\nSummary statistics\nPlot\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur brains are very good at processing large amounts of visual information quickly. Evolution is good at optimizing for survival, and it’s important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIn addition, it’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "href": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.3 Thinking Critically About Graphics",
    "text": "20.3 Thinking Critically About Graphics\nWhen we create graphics, we want to enable people to think about relationships between the variables in our dataset.\nFor this example, let’s consider the relationship between different physical measurements of penguins. We’ll use the palmerpenguins package in R, which is also available in python.\n\n20.3.1 Load (and examine) the data\n\n\nR\nPython\n\n\n\n\n## install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n## 1 Adelie  Torgersen           39.1          18.7               181        3750\n## 2 Adelie  Torgersen           39.5          17.4               186        3800\n## 3 Adelie  Torgersen           40.3          18                 195        3250\n## 4 Adelie  Torgersen           NA            NA                  NA          NA\n## 5 Adelie  Torgersen           36.7          19.3               193        3450\n## 6 Adelie  Torgersen           39.3          20.6               190        3650\n## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nIn your system console:\n\npip3 install palmerpenguins\n\nIn Python:\n\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\npenguins.head()\n##   species     island  bill_length_mm  ...  body_mass_g     sex  year\n## 0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n## 1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n## 2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n## 3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n## 4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n## \n## [5 rows x 8 columns]\n\n\n\n\nHow do physical measurements (bill width, bill depth, flipper length, and body mass) relate to each other?\n\n20.3.2 Initial plots\n\n\nR - ggplot2\nPy - Seaborn\nPy - Seaborn Objects\nR base\nPy - Matplotlib\n\n\n\n\nlibrary(ggplot2)\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm)) + geom_point()\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm, color = species)) + geom_point()\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in ggplot2 is mapped to a plot feature using the aes() (aesthetic) function. This function automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in red, Chinstrap in green, and Gentoo in blue.\nThat is, ggplot2 allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\n\nimport seaborn as sns \nimport matplotlib.pyplot\n\nsns.set_theme() # default theme\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\"\n)\n\n\n\n\n\n\n\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). In seaborn, this is called a relational plot - that is, it shows the relationship between the variables on the x and y axis. We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn, you begin to construct a plot by identifying the type of plot you want - whether it’s a relational plot, like a scatterplot or a lineplot, a distributional plot, like a histogram, density, CDF, or rug plot, or a categorical plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\", hue = \"species\"\n)\n\n\n\n\n\n\n\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn relational plot is mapped to a plot feature. This function automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2, seaborn allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\n\nimport seaborn.objects as so\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\")\n  .add(so.Dot())\n  .show()\n)\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn objects interface, you begin to construct a plot by identifying the primary relationship of interest: you define what variables go on each axis (body mass on x, bill length on y) and then create a layer that shows the actual data (so.Dot()).\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\", color = \"species\")\n  .add(so.Dot())\n  .show()\n)\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn objects plot declaration is mapped to a plot feature. Seaborn then automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2 and plotnine, seaborn’s objects interface allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\nYou may notice that this is much more similar to ggplot2 in syntax than the default Seaborn interface.\n\n\n\nplot(penguins$body_mass_g, penguins$bill_length_mm)\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nNotice that in base R, you have to reference each column of the data frame separately, as there is not an included data argument in the plot function.\nWhen we add color to the plot, it gets a little more complicated - to get a legend, we have to know a bit about how base R plots are constructed. When we pass in a factor as a color, R will automatically assign colors to each level of the factor.\n\nplot(penguins$body_mass_g, penguins$bill_length_mm, col = penguins$species)\nlegend(5500, 40,unique(penguins$species),col=1:3,pch=1)\n\n\n\n\n\n\n\nTo create a legend, we need to then identify where the legend should go (x = 5500, y = 40), and then the factor labels (unique(penguins$species)), and then the color levels assigned to those labels (1:3), and finally the point shape (pch = 1).\nThis manual legend creation process is a bit odd if you start from a grammar-of-graphics approach, but was for a long time the only way to make graphics in essentially any plotting system.\nPersonally, I still think the base plotting system creates charts that look a bit … ancient …, but there are some people who very much prefer it to ggplot2 [8].\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots() # Create a new plot\n\nax.scatter(penguins.body_mass_g, penguins.bill_length_mm)\nplt.show()\n\n\n\n\n\n\n\nplt.close() # close the figure\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWhen we add color to the plot, it gets a little more complicated. Matplotlib does not automatically assign factors to colors - we have to instead create a dictionary of colors for each factor label, and then use the .map function to apply our dictionary to our factor variable.\nTo create a legend, we define handles and use the dictionary to create the legend. Then we have to manually position the legend in the lower right of the plot. Note that here, the legend position is defined using bbox_to_anchor in plot coordinates (x = 1, y = 0), and then the orientation of that box is defined using the loc parameter.\n\nfrom matplotlib.lines import Line2D  # for legend handle\nfig, ax = plt.subplots() # Create a new plot\n\n# Define a color mapping\ncolors = {'Adelie':'tab:blue', 'Gentoo':'tab:orange', 'Chinstrap':'tab:green'}\n\nax.scatter(x = penguins.body_mass_g, y = penguins.bill_length_mm, c = penguins.species.map(colors))\n# add a legend\nhandles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in colors.items()]\nax.legend(title='Species', handles=handles, bbox_to_anchor=(1, 0), loc='lower right')\n\nplt.show()\n\n\n\n\n\n\n\nplt.close() # close the figure\n\nMatplotlib is prettier than the Base R graphics, but we again have to manually create our legend, which is not ideal. Matplotlib is great for lower-level control over your plot, but that means you have to do a lot of the work manually. Personally, I much prefer to focus my attention on creating the right plot, and let sensible defaults take over whenever possible - this is the idea with both ggplot2 and seaborn objects.\n\n\n\n\n20.3.3 A graphing template\nIt would be convenient if we could create a template for each of these libraries that would help us create many different types of plots. Unfortunately, that’s going to be a bit difficult, because there are a number of libraries covered here that are not built on the idea of a grammar of graphics - that is, a set of vocabulary that can define a bunch of different types of charts. In this section, I’ll show you basic templates for charts where such templates exist, and otherwise, I will try to give you a set of functions that may get you started.\n\n\nggplot2\nSeaborn\nSeaborn Objects\nR base\nMatplotlib\n\n\n\nThe gg in ggplot2 stands for the grammar of graphics. In ggplot2, we can describe any plot using a template that looks like this:\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM&gt;(mapping = aes(&lt;MAPPINGS&gt;), \n         position = &lt;POSITION&gt;, \n         stat = &lt;STAT&gt;) + \n  &lt;FACET&gt; + \n  &lt;COORD&gt; + \n  &lt;THEME&gt;\nGeoms are geometric objects, like points (geom_point), lines (geom_line), and rectangles (geom_rect, geom_bar, geom_column). Mappings are relationships between plot characteristics and variables in the dataset - setting x, y, color, fill, size, shape, linetype, and more. Critically, when a visual characteristic, such as color, fill, shape, or size, is a constant, it is set outside of an aes() statement; when it is mapped to a variable, it must be provided inside the aes() statement. Read more about aesthetics\nIn ggplot2, statistics can be used within geoms to summarize data. Additional modifications of plots in ggplot2 include changing the coordinate system, adjusting positions, and adding subplots using facets. \n\n\nIn seaborn, we can describe plots using three different basic templates:\n\n\nSeaborn API module organization. Source\n\nsns.relplot(data = &lt;DATA&gt;, x = &lt;X var&gt;, y = &lt;Y var&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\nsns.displot(data = &lt;DATA&gt;, x = &lt;X var&gt;, y = &lt;Y var, optional&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\nsns.catplot(data = &lt;DATA&gt;, x = &lt;Categorical X var&gt;, y = &lt;Y var&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\n# General version\nsns.&lt;PLOTTYPE&gt;(data = &lt;DATA&gt;, &lt;MAPPINGS&gt;, \n               kind = &lt;PLOT SUBTYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\nHere, the kind argument is somewhat similar to the geom argument in ggplot2, but there are different functions used for different purposes. This seems to be a way to handle statistical transformations without the full grammar-of-graphics implementation of statistics that is found in ggplot2. Read more about statistical relationships, distributions, and categorical data in the seaborn tutorial.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\nWe can come up with a generic plot template that looks something like this:\n(\n  so.Plot(&lt;DATA&gt;, &lt;MAPPINGS&gt;, &lt;GROUPS&gt;)\n  .add(so.&lt;GEOM&gt;(&lt;ARGS&gt;), so.&lt;TRANSFORMATION&gt;, so.&lt;POSITION&gt;)\n  .show()\n)\n\n\nR’s base graphics library is decidedly non-grammar-of-graphics like. Each plot is defined using plot-specific arguments that are not particularly consistent across different functions.\nThe following is as close as I can get, but it’s still not accurate for many plots.\n&lt;PLOT NAME&gt;(&lt;MAPPINGS&gt;, &lt;ARGS&gt;)\n&lt;LEGEND&gt;(&lt;MAPPINGS&gt;, &lt;ARGS&gt;)\nAny statistics are either computed as part of the plot function (e.g. hist()) or there are plot methods to accompany the statistic calculation (plot(density(...))). Facets/subplots are sometimes created using par(mfrow=(...)) to define sub-plots, and are sometimes created by the plot command itself, in the case of e.g. passing a numeric matrix to plot(), which creates a scatterplot matrix.\nUseful commands: plot() for scatterplots, lines (type = ‘l’), scatterplot matrices, etc. hist() for histograms. plot(density(...)) for density plots. boxplot() for boxplots, barplot() for bar plots, …\n\n\nI’m not even going to try to create a grammar summary for matplotlib… there just isn’t one. It wasn’t designed with the grammar of graphics in mind, and was built by computer scientists [9], not statisticians. As a result, even retrofitting some sort of “grammar” interpretation doesn’t work so well.\nFor a primer on matplotlib, I recommend [10], Ch 9.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "href": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.4 Advanced: Charts, Graphics, and the Grammar of Graphics",
    "text": "20.4 Advanced: Charts, Graphics, and the Grammar of Graphics\nThere are two general approaches to generating statistical graphics computationally:\n\nManually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.\nBase R, matplotlib, SAS graphics\nDescribe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.\nggplot2, seaborn (sort of), seaborn objects\n\n\n\nThere is a difference between low-level plotting libraries (base R, matplotlib) and high-level plotting libraries (ggplot2, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it’s probably a low level library.\nIn the introduction to the Grammar of Graphics [11], Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects”. His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics”.\n\n\nThere are other graphics systems (namely, lattice in R, seaborn in Python, and some web-based rendering engines like Observable or d3) that you could explore, but it’s far more important that you know how to functionally create plots in R and/or Python. I don’t recommend you try to become proficient in all of them. Pick one (two at most) and get familiar with those libraries, then google for the rest.\nBefore we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years [12], so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible in Chapter 21.\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %&gt;%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\nOnce the data is read in, we can start plotting:\n\n\nggplot2\nBase R\nMatplotlib\nSeaborn\n\n\n\nIn ggplot2, we start by specifying which variables we want to be mapped to which features of the data.\nIn a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack”. So it’s easiest to specify our x variable as a constant, ““. We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable.\nThen, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”).\n\nlibrary(ggplot2)\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") \n\n\n\n\n\n\n\nIf we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\).\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") + \n  coord_polar(\"y\", start = 0)\n\n\n\n\n\n\n\nNotice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\npie(tmp$n, labels = tmp$generation)\n\n\n\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\n# Matrix is necessary for a stacked bar chart\nmatrix(tmp$n, nrow = 9, ncol = 1, dimnames = list(tmp$generation)) %&gt;%\nbarplot(beside = F, legend.text = T, main = \"Generations of Pokemon\")\n\n\n\n\n\n\n\nThere’s not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)\n## ([&lt;matplotlib.patches.Wedge object at 0x7f11c1f3e050&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bc097b10&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa15a10&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa15290&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa2ccd0&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa2e850&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa2ffd0&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa398d0&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bfa3b290&gt;], [Text(-0.6090073012006626, 0.9160295339585321, '1'), Text(-1.0954901637626666, -0.09950528176557247, '2'), Text(-0.6165298897093509, -0.9109834768506923, '3'), Text(0.18481487009430594, -1.0843631604734758, '4'), Text(0.7975739189399522, -0.757545935126555, '5'), Text(1.0758366616678645, -0.2292934308072192, '6'), Text(1.044470095507754, 0.34508291697796867, '7'), Text(0.7443080329240818, 0.8099416967440831, '8'), Text(0.26679773552389424, 1.0671546131275085, '9')], [Text(-0.3321858006549068, 0.4996524730682902, '18.7%'), Text(-0.5975400893250908, -0.05427560823576679, '15.5%'), Text(-0.33628903075055494, -0.49690007828219573, '12.6%'), Text(0.10080811096053051, -0.591470814803714, '11.7%'), Text(0.43504031942179205, -0.4132068737053936, '8.8%'), Text(0.5868199972733805, -0.125069144076665, '8.7%'), Text(0.5697109611860476, 0.18822704562434653, '8.1%'), Text(0.4059861997767718, 0.4417863800422271, '8.1%'), Text(0.14552603755848775, 0.5820843344331864, '7.8%')])\nax1.axis('equal')\n## (np.float64(-1.0999996109024037), np.float64(1.0999999852979658), np.float64(-1.0999997040730163), np.float64(1.099999985908239))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts()\nsizes = sizes.sort_index()\n\n# Find location of bottom of the bar for each bar\ncumulative_sizes = sizes.cumsum() - sizes\nwidth = 1\n\nfig, ax = plt.subplots()\n\nfor i in sizes.index:\n  ax.bar(\"Generation\", sizes[i-1], width, label=i, bottom = cumulative_sizes[i-1])\n## KeyError: 0\n\nax.set_ylabel('# Pokemon')\nax.set_title('Pokemon Distribution by Generation')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs with Plotnine, seaborn does not support pie charts due to the same underlying issue. The best option is to create a pie chart in matplotlib and use seaborn colors [13].\n\n# Load the seaborn package\n# the alias \"sns\" stands for Samuel Norman Seaborn\n# from \"The West Wing\" television show\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Initialize seaborn styling; context\nsns.set_style('white')\nsns.set_context('notebook')\n\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n#define Seaborn color palette to use\ncolors = sns.color_palette()[0:9]\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, colors = colors, autopct='%1.1f%%', startangle = 90)\n## ([&lt;matplotlib.patches.Wedge object at 0x7f11bf80ec50&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf846a10&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf854850&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf847e90&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf860450&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf8623d0&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf86c090&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf86e050&gt;, &lt;matplotlib.patches.Wedge object at 0x7f11bf860250&gt;], [Text(-0.6090073012006626, 0.9160295339585321, '1'), Text(-1.0954901637626666, -0.09950528176557247, '2'), Text(-0.6165298897093509, -0.9109834768506923, '3'), Text(0.18481487009430594, -1.0843631604734758, '4'), Text(0.7975739189399522, -0.757545935126555, '5'), Text(1.0758366616678645, -0.2292934308072192, '6'), Text(1.044470095507754, 0.34508291697796867, '7'), Text(0.7443080329240818, 0.8099416967440831, '8'), Text(0.26679773552389424, 1.0671546131275085, '9')], [Text(-0.3321858006549068, 0.4996524730682902, '18.7%'), Text(-0.5975400893250908, -0.05427560823576679, '15.5%'), Text(-0.33628903075055494, -0.49690007828219573, '12.6%'), Text(0.10080811096053051, -0.591470814803714, '11.7%'), Text(0.43504031942179205, -0.4132068737053936, '8.8%'), Text(0.5868199972733805, -0.125069144076665, '8.7%'), Text(0.5697109611860476, 0.18822704562434653, '8.1%'), Text(0.4059861997767718, 0.4417863800422271, '8.1%'), Text(0.14552603755848775, 0.5820843344331864, '7.8%')])\nax1.axis('equal')\n## (np.float64(-1.0999996109024037), np.float64(1.0999999852979658), np.float64(-1.0999997040730163), np.float64(1.099999985908239))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeaborn doesn’t have the autopct option that matplotlib uses, so we have to aggregate the data ourselves before creating a barchart.\n\npoke['generation'] = pd.Categorical(poke['generation'])\n\nplt.cla() # clear out matplotlib buffer\nsns.catplot(data = poke, x = 'generation', kind = \"count\")\n\n\n\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNintendo, Creatures, Game Freak, The Pokémon Company, Public domain, via Wikimedia Commons\n\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book [11]. Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the dataset itself relates to the components of the chart.\n\n\nBuilding a masterpiece, by Allison Horst\n\nThis has a few advantages:\n\nIt’s relatively easy to represent the same dataset with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: [14]\n\n\n\n\n\n\n\nNote\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\n\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from [14]).\n\n\nSketch\nggplot2\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing)\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point(alpha = .2)\n\n\n\n\n\n\n\n\n\n\nfrom plotnine.data import txhousing\nimport seaborn.objects as so\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .show()\n)",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "href": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.5 Exploratory Data Analysis and Plot Customization",
    "text": "20.5 Exploratory Data Analysis and Plot Customization\nWhen you first get a new dataset, it is critical that you, as the analyst, get a feel for the data. John Tukey, in his book [15], likens exploratory data analysis to detective work, and says that in both cases the analyst needs two things: tools, and understanding. Tools, like fingerprint powder or types of charts, are essential to collect evidence; understanding helps the analyst to know where to apply the tools and what to look out for.\nTukey claims that while tools are constantly evolving, and there may not be one set of “best” tools, understanding is a bit different. There are situational elements – you might need different base knowledge to solve a crime in a small village in the UK than you need to solve a crime in New York City or Buenos Aires. However, the broad strokes of what questions are asked during an investigation will be similar across different locations and types of crimes.\nThe same thing is true in exploratory data analysis - while it is helpful to have a basic understanding of the dataset, and being intimately familiar with the type of data and data collection processes can give the analyst an advantage, the same basic detective skills will be useful across a wide variety of data sets.\nThere is one other aspect of the analogy between EDA and detective work that is useful: the detective gathers evidence, but does not try the case or make decisions about what should happen to the accused. Similarly, the individual conducting an exploratory data analysis should not move too quickly to hypothesis testing and other confirmatory data analysis techniques. In exploratory data analysis, the goal is to lay the foundation, assess the evidence (data) for interesting clues, and to try to understand the whole story. Only once this process is complete should we move to any sort of confirmatory analysis.\nIn this section, we’ll primarily use charts as a tool for exploratory data analysis. Pay close attention not only to the tools, but to the process of inquiry.\n\n20.5.1 Texas Housing Data\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of the features of each plotting library.\n\n20.5.2 Starting Chart\nBefore we start exploring, let’s start with a basic scatterplot, and add a title and label our axes, so that we’re creating good, informative charts.\n\n\nSketch\nggplot2\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\n\n\n\n\n20.5.3 Exploring Trends\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\n\n\nSketch\nggplot2\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .add(so.Line(color = \"black\"), so.PolyFit(order = 2))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\nI haven’t yet figured out a way to do locally weighted regression in Seaborn using the objects interface, but I’m sure that will come as the interface develops.\n\n\n\n\n20.5.4 Adding Complexity with Moderating Variables\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n\n\nSketch\nggplot2\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\nThis is one of the first places we see differences in Python and R’s graphs - python doesn’t allocate sufficient space for the legend by default. In Python, you have to manually adjust the theme to show the legend (or plot the legend separately).\n\n\n\n\n20.5.5 Data Reduction Strategies\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities).\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\n\n\nggplot2\nSeaborn\n\n\n\n\ncitylist &lt;- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub &lt;- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\")\n  .facet(col = \"city\")\n  .show()\n)\n\n\n\n\n\n\n\n\n\n\n\n20.5.6 Adding Additional Complexity\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\n\nggplot2\nSeaborn\n\n\n\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  # Remove extra information from the legend -\n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',\n                             override.aes = list(linetype = NA,\n                                                 fill = 'transparent'))) +\n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\nComing soon!\n\n\n\n\n20.5.7 Exploring Other Variables and Relationships\nUp to this point, we’ve used the same position information - date for the y axis, median sale price for the y axis. Let’s switch that up a bit so that we can play with some transformations on the x and y axis and add variable mappings to a continuous variable.\n\n\nggplot2\nplotnine\n\n\n\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation2. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"listings\", y = \"sales\", color = \"city\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ scale_x_log10()\n+ scale_y_log10()\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## NameError: name 'ggplot' is not defined\n\nNotice that the gridlines included in python by default are different than those in ggplot2 by default (personally, I vastly prefer the python version - it makes it obvious that we’re using a log scale).\n\n\n\n\n20.5.8 Adding Additional Moderating Variables\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in x and y, and mapping the size or color of the point to number of sales.\n\nggplot2\n\n\n\nhouston &lt;- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) +\n  geom_point(shape = 1) +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_legend(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) +\n  geom_point() +\n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Date\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\nIs that easier or harder to read?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "href": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.6 What type of chart to use?",
    "text": "20.6 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type.\nConsider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through catalogs like the R Graph Gallery or the Python Graph Gallery to see what visualizations match your data and use-case.\nChapter 21 talks in more depth about considerations for creating good charts; these considerations may also inform your decisions.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#additional-reading",
    "href": "part-wrangling/02b-graphics.html#additional-reading",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.7 Additional Reading",
    "text": "20.7 Additional Reading\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nR graph cookbook\n\nData Visualization in R (@ramnathv)\nPython graphics\n\n\nMatplotlib documentation - Matplotlib is the base that plotnine uses to replicate ggplot2 functionality\n\n\nVisualization with Matplotlib chapter of Python Data Science\n\n\nScientific Visualization with Python",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#sec-graphics-b-refs",
    "href": "part-wrangling/02b-graphics.html#sec-graphics-b-refs",
    "title": "20  A Grammar of Graphics",
    "section": "\n20.8 References",
    "text": "20.8 References\n\n\n\n\n[1] \nH. Kibirige, “A grammar of graphics for python. Plotnine 0.10.1 documentation,” 2022. [Online]. Available: https://plotnine.readthedocs.io/en/stable/. [Accessed: Feb. 06, 2023]\n\n\n[2] \nH. Wickham, ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016 [Online]. Available: https://ggplot2.tidyverse.org\n\n\n\n[3] \nM. Waskom, “An introduction to seaborn. Seaborn 0.12.2 documentation,” 2022. [Online]. Available: https://seaborn.pydata.org/tutorial/introduction.html. [Accessed: Feb. 06, 2023]\n\n\n[4] \nM. Waskom, “Next-generation seaborn interface. Seaborn nextgen documentation,” 2022. [Online]. Available: https://seaborn.pydata.org/nextgen/. [Accessed: Aug. 29, 2022]\n\n\n[5] \nThe matplotlib development team, “Matplotlib — visualization with python. Matplotlib,” 2023. [Online]. Available: https://matplotlib.org/. [Accessed: Feb. 06, 2023]\n\n\n[6] \nR. D. Peng, “The base plotting system,” in Exploratory data analysis with r, 1st ed., leanpub, 2020 [Online]. Available: https://bookdown.org/rdpeng/exdata/. [Accessed: Feb. 06, 2023]\n\n\n[7] \nJ. W. Tukey, “Data-Based Graphics: Visual Display in the Decades to Come,” Statistical Science, vol. 5, no. 3, pp. 327–339, Aug. 1990, doi: 10.1214/ss/1177012101. [Online]. Available: https://projecteuclid.org/journals/statistical-science/volume-5/issue-3/Data-Based-Graphics--Visual-Display-in-the-Decades-to/10.1214/ss/1177012101.full. [Accessed: Aug. 22, 2022]\n\n\n[8] \nN. Yau, “Comparing ggplot2 and r base graphics. FlowingData,” Mar. 22, 2016. [Online]. Available: https://flowingdata.com/2016/03/22/comparing-ggplot2-and-r-base-graphics/. [Accessed: Feb. 06, 2023]\n\n\n[9] \nW. Koehrsen, “The next level of data visualization in python. Medium,” Jan. 24, 2019. [Online]. Available: https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e. [Accessed: Feb. 06, 2023]\n\n\n[10] \nW. McKinney, Python for data analysis, 3rd ed. O’Reilly, 2022 [Online]. Available: https://wesmckinney.com/book/. [Accessed: Feb. 06, 2023]\n\n\n[11] \nL. Wilkinson, The grammar of graphics. New York: Springer, 1999. \n\n\n[12] \nF. E. Croxton and R. E. Stryker, “Bar Charts Versus Circle Diagrams,” Journal of the American Statistical Association, vol. 22, no. 160, pp. 473–482, 1927, doi: 10.2307/2276829. \n\n\n[13] \nZach, “How to Create a Pie Chart in Seaborn,” Statology. Jul. 2021 [Online]. Available: https://www.statology.org/seaborn-pie-chart/. [Accessed: Sep. 19, 2022]\n\n\n[14] \nD. (DJ). Sarkar, “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-dimensional…,” Medium. Sep. 2018 [Online]. Available: https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149. [Accessed: Apr. 11, 2022]\n\n\n[15] \nJ. Tukey, Exploratory data analysis. Addison-Wesley Publishing Company, 1977.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#footnotes",
    "href": "part-wrangling/02b-graphics.html#footnotes",
    "title": "20  A Grammar of Graphics",
    "section": "",
    "text": "This chapter used to contain examples of plotnine [1], which is a python ggplot2 clone, but it has not been updated in a while, so it has been removed to ensure compatibility with up-to-date python packages.↩︎\nThis isn’t necessarily a good thing, but you should know how to do it. The jury is still very much out on whether log transformations make data easier to read and understand↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html",
    "href": "part-wrangling/02c-good-graphics.html",
    "title": "21  Creating Good Charts",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#objectives",
    "href": "part-wrangling/02c-good-graphics.html#objectives",
    "title": "21  Creating Good Charts",
    "section": "",
    "text": "Understand what features make graphics effective\nEvaluate existing charts for accessibility and readability\nMake improvements to charts to increase comprehension and accessibility",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#introduction",
    "href": "part-wrangling/02c-good-graphics.html#introduction",
    "title": "21  Creating Good Charts",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart. A simple and disposable chart created during an exploratory data analysis process may be useful even if it is not nicely formatted and publication ready, because its purpose is to guide an interactive process. This is very different than a chart created for communicating with the public – for instance, a forecast map showing possible paths and intensities of a hurricane that would inform resident decisions about storm preparation and/or evacuation.\nComprehensive advice on creating good charts is difficult, too, because what works for one dataset may not work for another, even if the variable types are similar. We have some established conventions that should usually be followed (for instance, time usually is placed on the x-axis, with a dependent variable on the y-axis), but there are usually situations where it is reasonable to break those conventions.\nFinally, what makes a chart “good” requires some additional knowledge beyond statistics and programming. To make good charts, we have to understand how those charts will be interpreted, which means we need at least some basic information about human perception and cognition. The human visual system is incredibly powerful - it has a bandwidth that would make even modern computers jealous, and many computations are performed instantaneously and without requiring any process management (e.g. the calculations happen so fast and so automatically that you aren’t really aware that they’re happening). This comes with some tradeoffs, though - evolutionary optimizations that ensure that you can spot predators quickly weren’t as concerned with your ability to accurately determine the height of a two-dimensional drawing of a three-dimensional object. So, while the visual system has some amazing strengths and is a very useful medium to communicate about data, it is important to understand the limitations of the visual system’s sensors, software, short-term and long term memory, and attention.\nWe’ll start with a short exploration of some foundational concepts related to perception in Section 21.2. Section 21.3 will discuss the design process and how to leverage the grammar of graphics to provide both the full data and visual summaries that highlight key features. Where available, this section incorporates conclusions from empirical studies of charts and graphs to guide design decisions.?sec-annotation expands on this discussion, demonstrating effective use of annotations to provide contextual information that can assist viewers with the interpretation of the data. Finally, Section 21.4 provides a guide to evaluating graphics for clarity, effectiveness, accessibility, and common design pitfalls.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#sec-cognition-perception",
    "href": "part-wrangling/02c-good-graphics.html#sec-cognition-perception",
    "title": "21  Creating Good Charts",
    "section": "\n21.2 Cognitive and Perceptual Foundations",
    "text": "21.2 Cognitive and Perceptual Foundations\nBefore we discuss how to create good charts, there is a certain amount of background information that must be considered. Charts make use of the visual system within the human brain, which means we need to understand some basic attributes of human perception and cognition in order to make the best use of this “wetware” [1] processing power.\n\nFirst, let’s set the stage. Information in the form of light bounces off objects in the world and lands on our retina (there are lenses and focusing mechanisms that we’ll skip). There are four types of light detectors in the retina: three types of cones that respond to red, green, and blue light wavelengths, and rods, which respond to light intensity across wavelengths. Cones are concentrated in one area, while rods are spread across the surface of the retina. The rods and cones turn light into neural impulses, which are transmitted along the optic nerve to the visual cortex located in the back of the brain (roughly where your head would hit the floor when you are lying down). The visual cortex contains special neurons called feature detectors which organize the information from the retina and reconstruct this information into a mental representation of the world. Some feature detectors respond to specific angles, signals from specific parts of the retina corresponding to specific parts of the outside world, and many other low-level features. Signals from these feature detectors are then aggregated into higher-level concepts that form our visual experience of the world.\nThe initial light signals and lower-level information are sometimes called “sensation”, and the ability to detect higher-level concepts is called “perception”. We can also think of “top-down” perception, where our experience shapes what we perceive and how we experience the world, compared to “bottom-up” perception, where we construct higher-level concepts solely from lower-level signals.\nPerception tends to require a few other mental resources beyond the visual detection and processing equipment (eyes, visual cortex, etc.): attention and memory (short and long-term) are vital for processing the visual input and making sense out of it. The next few subsections provide specific examples of why it’s important to understand the basics of the visual system when thinking about how to construct charts and graphs.\n\n21.2.1 Color\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum, as shown in Figure 21.1. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\n\n\n\nFigure 21.1: Sensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient [2]. We have 3 cones used for color detection, as well as cells called rods, which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, 0.2% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\n\n\n\n\nDemo: Types of Colorblindness\n\n\n\n\n\nSimulations of the same rainbow color scheme map under different types of colorblindness, generated using CoBliS. While colorblindness simulations can be useful, as colorblindness is a result of one of many different mutations, simulators do not cover all of the different color vision mutations which exist.\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nFigure 21.2: Original (Normal Color Vision)\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nFigure 21.3: Protanomaly (Weak red vision)\n\n\n\n\n\n\n\n\n\nFigure 21.4: Deuteranomaly (Weak green vision)\n\n\n\n\n\n\n\n\n\nFigure 21.5: Tritanomaly (Weak blue vision)\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.6: Protanopia (No red cone)\n\n\n\n\n\n\n\n\n\nFigure 21.7: Deuteranopia (No green cone)\n\n\n\n\n\n\n\n\n\nFigure 21.8: Tritanopia (No blue cone)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColorblindness Testing\n\n\n\n\n\nYou can take a test designed to screen for colorblindness here. Your monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n\n\n\n\n\n\n\nMy results on one monitor\n\n\n\n\n\nMy results on a monitor that has a different tech and supposedly higher color fidelity\n\n\n\n\n\nThe Munsell colorblindness test\n\n\n\n\nIn reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\n\n \n\n\n\n\n\n\n\n(a) The original illusion\n\n\n\n\n \n\n\n\n\n\n\n\n(b) The illusion with the checkerboard and shadow removed\n\n\n\n\n \n\n\n\n\nFigure 21.9: The color constancy illusion. The squares marked A and B are actually the same color.\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown in Figure 21.9 - because the brain automatically corrects for the shadow, B looks lighter than A, even though when the context is removed they are clearly the same shade.\nTakeaways\n\nDo not use rainbow color gradient schemes\n\nbecause of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\n\n\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for\n\nblue for cold, orange/red for hot is a natural scale\nred = Republican and blue = Democrat in the US (since ~1980)\nwhite -&gt; blue gradients for showing rainfall totals\n\n\nSome colors can can provoke emotional responses that may not be desirable.1\n\nConsider the social baggage that certain color schemes may have\n\npink/blue color scheme often used for gender may be polarizing\nConsider using a cooler color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women2.\n\n\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n21.2.2 Preattentive Features\nYou’ve almost certainly noticed that some graphical tasks are easier than others. Part of the reason for this is that certain tasks require active engagement and attention to search through the visual stimulus; others, however, just “pop” out of the background. We call these features that just “pop” without active work preattentive features; technically, they are detected within the first 250ms of viewing a stimulus [3].\nTake a look at Figure 21.10; can you spot the point that is different?\n\n\n\n\n\n\n\n\n\n(a) Shape\n\n\n\n\n\n\n\n\n\n(b) Color\n\n\n\n\n\n\nFigure 21.10: Two scatterplots with one point that is different. Can you easily spot the different point?\n\n\nColor and shape are commonly used graphical features that are processed pre-attentively. Some people suggest utilizing this to pack more dimensions into multivariate visualizations [4], but in general, knowing which features are processed more quickly (color/shape) and which are processed more slowly (combinations of preattentively processed features) allows you to design a chart that requires less cognitive effort to read.\nAs awesome as it is to be able to use preattentive features to process information, we cannot use combinations of preattentive features to show different variables - these combinations are no longer processed preattentively. Take a look at Figure 21.11 - part (a) shows the same grouping in color and shape, part (b) shows color and shape used to encode different variables.\n\n\n\n\n\n\n\n\n\n(a) Shape and Color (dual encoded)\n\n\n\n\n\n\n\n\n\n(b) Shape and Color (different variables)\n\n\n\n\n\n\nFigure 21.11: Two scatterplots. Can you easily spot the different point(s)?\n\n\nHere, it is easy to differentiate the points in Figure 21.11(a), because they are dual-encoded. However, it is very difficult to pick out the different groups of points in Figure 21.11(b) because the combination of preattentive features requires active attention to sort out.\nTakeaways\nCareful use of preattentive features can reduce the cognitive effort required for viewers to perceive a chart.\nEncode only one variable using preattentive features, as combinations of preattentive features are not processed preattentively.\n\n21.2.3 Short Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\n\n\n\n\n\n\nTry it out!\n\n\n\n\n\n\n\n\n\nClick here, read the information, and then click to hide it.\n\n\n\n\n\n1 4 2 2 3 9 8 0 7 8\n\n\n\n\n\n\n\n\n\nWait a few seconds, then expand this section\n\n\n\n\n\nWhat was the third number?\n\n\n\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. If you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nTakeaways\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n21.2.4 Grouping and Sense-making\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\nOur brains have to be very good at imposing order on visual chaos – there is a huge amount of information being processed by the visual system all the time, and some basic heuristics (guesses/shortcuts) are important in this process.\nWhen we create charts, it becomes important to understand these heuristics so that we can make it easier for people to understand the data. Working with the natural sense making algorithms in the brain requires less cognitive effort, which leaves more space for thinking about the data.\nLet’s start with a few examples that show how the brain constructs meaning from ambiguous or conflicting stimuli.\n\n\n\n\n\n\n21.2.4.1 Demo: Constructing Visual Meaning\n\n\n\n\n\nAmbiguous Image\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does Figure 21.12 look like to you?\n\n\n\n\n\nFigure 21.12: Is it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options. Sometimes, the brain will even flip between the possible options, as in the Necker cube illusion.\n\n\n\n\nConsider this image - what do you see?\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\nThis illusory contour image is closely related to the Gestalt concepts of closure and “good figure”.\n\n\nConsider the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\nOne of the first tasks we have when confronted with a visual scene is to separate the important part of the image (the figure) from the background. In most cases this is straightforward, but occasionally, artificial images (as opposed to real world scenes) can be hard to interpret. The zoo logo shown above leverages this ambiguity to capture your visual attention.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter.\n\n\nThe Gestalt heuristics attempt to explain how our brains group and order visual stimuli to make sense of the world. You can read about the gestalt rules here, but they are also demonstrated in Figure 21.13.\n\n\n\n\n\nFigure 21.13: The Gestalt Heuristics help us to impose order on ambiguous visual stimuli.\n\n\nIn graphics, we make use of the Gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\n\n\n\n\n\nDemo: Grouping and Geometries\n\n\n\nSuppose I want to emphasize the change in life expectancy between 1982 and 2007. For this, we’ll use the Gapminder [5] data which is found in the gapminder packages in R and python.\n\n\nR\nPython\n\n\n\nShow the codelibrary(gapminder)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ngapminder %&gt;%\n  filter(year %in% c(1982, 2007)) %&gt;%\n  filter(country %in% c(\"Korea, Rep.\", \"China\", \"Afghanistan\", \"India\")) %&gt;%\n  ggplot(aes(x = country, y = lifeExp, fill = factor(year))) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Life Expectancy\")\n\ngapminder %&gt;%\n  filter(year %in% c(1982, 2007)) %&gt;%\n  filter(country %in% c(\"Korea, Rep.\", \"China\", \"Afghanistan\", \"India\")) %&gt;%\n  ggplot(aes(x = year, y = lifeExp, color = country)) +\n  geom_line() +\n  ylab(\"Life Expectancy\")\n\ngapminder |&gt;\n  filter(year %in% c(1982, 2007)) |&gt;\n  ggplot(aes(x = factor(year), y = lifeExp)) + \n  geom_boxplot() + \n  ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\n\n(a) A bar chart allows comparisons of change over time for each country as well as cross-country comparisons.\n\n\n\n\n\n\n\n\n\n(b) A line chart centers changes over time for each country.\n\n\n\n\n\n\n\n\n\n(c) A boxplot shows overall change in aggregate over time, but does not show individual country data.\n\n\n\n\n\n\nFigure 21.14: Three different charts created from the same data. Which one best demonstrates that in every country, the life expectancy increased between 1982 and 2007?\n\n\n\n\nShow the code# %pip install gapminder\nfrom gapminder import gapminder\nimport pandas as pd\nimport seaborn as sns\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt # to clear plots\n\nmy_gap = gapminder.query('year.isin([1982,2007])')\nmy_gap = my_gap.query('country.isin([\"Korea, Rep.\", \"China\", \"Afghanistan\", \"India\"])')\nmy_gap = my_gap.assign(yearFactor=pd.Categorical(my_gap.year))\n\nplot = so.Plot(my_gap, x = \"country\", y = \"lifeExp\", color = \"yearFactor\").\\\n  add(so.Bar(), so.Dodge()).\\\n  label(y = \"Life Expectancy\")\nplot.show()\nplt.clf() # Clear plot workspace\n\nplot = so.Plot(my_gap, x = \"year\", y = \"lifeExp\", color = \"country\").\\\n  add(so.Lines()).\\\n  label(y = \"Life Expectancy\")\nplot.show()\nplt.clf()  # Clear plot workspace\n\nsns.boxplot(data = my_gap, x = \"year\", y = \"lifeExp\")\nplt.show()\n\n\n\n\n\n\n\n\n\n(a) A bar chart allows comparisons of change over time for each country as well as cross-country comparisons.\n\n\n\n\n\n\n\n\n\n(b) A line chart centers changes over time for each country.\n\n\n\n\n\n\n\n\n\n(c) A boxplot shows overall change in aggregate over time, but does not show individual country data.\n\n\n\n\n\n\nFigure 21.15: Three different charts created from the same data. Which one best demonstrates that in every country, the life expectancy increased between 1982 and 2007?\n\n\n\n\n\nIf the goal is to emphasize that every single country had an increase in life expectancy over the period, the best chart is the line chart - we can see upward slopes for each country leading to the conclusion that life expectancy increased. This leverages the Gestalt principles of “similarity” and “common fate”. Similarity, in that all lines point in the same direction, and common fate (often used for motion, e.g. a flock of birds are a group because they move together) because the lines are all “moving together”.\nWe can derive the same information from the bar chart, but we have to work a bit more for it, because we naturally group bars together by country (proximity) and by year (similarity). We have to then notice that the 2007 bar is bigger for each country to come to the same conclusion – this takes a bit more cognitive effort.\nWe cannot get the specifics from the box plot, because we cannot see individual country data. This is a case where a summary statistic actually destroys the conclusion we might want to draw from the data and leaves us with weaker information - we can see that there is an increase in the minimum, median, and maximum life expectancy, but it is possible to have this and still have a single-country decrease in life expectancy, so we cannot draw the same conclusion from the box plot that we can from the bar or line charts.\n\n\nThe geometric mappings and aesthetic choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining plots. Choosing the wrong geometry or statistic can obscure the point you want to make using the data, leading your reader to draw a conclusion that is unsupported, less important, or misleading.\nOn the other hand, using aesthetic mappings to highlight information can ensure that viewers see the important information you’re trying to communicate 3, and can even tilt the balance when two equally valid conclusions are present in a chart. This power should be used responsibly.\n\n\n\n\n\n\nDemo: Facets and Grouping\n\n\n\nWhen creating a visualization that involves many different (usually categorical) variables, it is important to decide which variable is the primary comparison of interest. This variable is the one which should be shown in the most easily comparable way – usually, directly on the same plot.\nLet’s look at the palmerpenguins data. We have several categorical variables: species, island, year (technically numerical, but there are only 3 years), and sex. The most interesting part of this data set is how the morphology (measurements) of penguins changes based on their sex and species, so let’s explore what different charts examining bill_len and body_mass might look like across those variables. For this example, I’ll drop penguins with an unidentified sex for visual simplicity.\n\n\n\n\n\n \n\n\n\n\n\n\n\n(a) This plot uses shape, color, and linetype to distinguish the different groups. It’s cluttered, but it is possible to make comparisons across species and sex\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n(b) This plot facets by species, allowing the user to make easy comparisons across sex for each species separately. Cross species comparisons are somewhat more difficult in this plot because the viewer must shift back and forth between panels.\n\n\n\n\n\n\n\n\n\n(c) This plot facets by sex, allowing the user to make easy judgments about the effect of species on measurements. Comparisons between sexes must be made across panels, and require more cognitive effort.\n\n\n\n\n\n\nFigure 21.16: Three views of the palmerpenguins data comparing bill_len and body_mass across sex and species. Click on figures to enlarge.\n\n\n\nWhich plot makes it easier to answer the following questions:\n\nWhich species of penguin has the most overlap between the size of males and females?\nWhich species of penguin is the most physically different across both males and females?\nWhat is the overall relationship across species for the size of males vs. females?\n\nDifferent aesthetic mappings and facets can lead to different overall conclusions from the same data. It is important when you are exploring data to generate many different plots, so that you get a more comprehensive picture of your data. When you want to explain the data to others, it is equally important to carefully choose the most important findings from the data and present charts that back up those findings.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#sec-design-process",
    "href": "part-wrangling/02c-good-graphics.html#sec-design-process",
    "title": "21  Creating Good Charts",
    "section": "\n21.3 Designing Good Graphics Using the Grammar",
    "text": "21.3 Designing Good Graphics Using the Grammar\n\n21.3.1 Representing Data Accurately\nIn order to read data off of a chart correctly, several things must happen in sequence:\n\nThe data must be accurately written to the chart, that is, the transformation from data -&gt; geometry must be accurate\nThe geometric objects that make up the chart must be perceived accurately - the mapping from geometric object size or location to mental model of geometric object size or location must be correct.\nThe mental model of geometric object size or location must be accurately converted to a numerical value. We know that this isn’t lossless, but we hope that this is at least reasonably accurate.\n\nIf step 1 is not done correctly, the chart is misleading or inaccurate. However, steps 2 and 3 depend on our brains accurately perceiving and estimating information mentally. These steps can involve a lot of effort, and as mental effort increases, we tend to take shortcuts. Sometimes, these shortcuts work well, but not always.\nWhen you design a chart, it’s good to consider what mental tasks viewers of your chart need to perform. Then, ask yourself whether there is an equivalent way to represent the data that requires fewer mental operations, or a different representation that requires easier mental calculations.\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. The three lines have the same length in both panels, but the operation is much more difficult when the lines do not start or end at the same place.\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level. See this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgments of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts tend not to be a good general option – people must compare values using area or angle instead of position or length, which is a more difficult judgment under most circumstances. When there are a limited number of categories (2-4) and you have data that is easily compared to quarters of a circle, it may be justifiable to use a pie chart over a stacked bar chart - some studies have shown that pie charts are preferable under these conditions. As a general rule, though, we have an easier time comparing position than angle or area.\n\n\n\n\n\n\n\n\nStacked bar chart\n\n\n\n\n\nPie chart\n\n\n\n\n\nStacked bar and pie charts showing the relative proportion of people in North America living in the US, Canada, and Mexico in 2007. Which chart is easier to read relative information (e.g. there are about 3x as many people living in Mexico as Canada) from? Which chart is easier to estimate raw proportions (e.g. the US makes up about 70% of the population of North America) from?\n\n\n\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nConsider the hierarchy of graphical tasks again. You may notice a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice can be misleading. Just how misleading depends a lot on the type of chart you’re using. Most of the time, the addition of an extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual numerical value being represented.\n\n\n\n\n\nFigure 21.17: Here, the area and height both encode the same variable, leading to a far disproportionate number of pixels allocated to “Stocks” than “Cash Investments” (h/t Junk Charts). In the first chart, stocks make up 60% of the portfolio, but have 67.5% of the pixels; Cash makes up 5% of the portfolio but those investments represent 2.3% of the pixels.\n\n\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration).\n\n\n\nBusiness Insider: The Worst Graphs Ever\n\n21.3.2 Show the Data\nTODO: Example showing the importance of raw data and how to add summary statistics to show model results on top of the raw data.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#sec-checklist",
    "href": "part-wrangling/02c-good-graphics.html#sec-checklist",
    "title": "21  Creating Good Charts",
    "section": "\n21.4 Evaluating Graphics: Effectiveness, Clarity, and Comprehension",
    "text": "21.4 Evaluating Graphics: Effectiveness, Clarity, and Comprehension\nWhen evaluating a chart that you hope to use for presentation, it is important to first think through what the goal of the chart is. Then, it can be helpful to work through the grammar of graphics to ensure that each element of the chart contributes to the overall goal.\n\nMessage\n\n\nWhat is the message you hope to convey?\nWhat elements of the data are necessary for that message?\nWhat level of audience are you targeting? What education, expectations, culture, and perceptual abilities or challenges do they have?\n\n\nMappings\n\n\nAre mappings appropriate for the type of variable?\nDo the \\(x\\) and \\(y\\) mappings show variables that are most important for accurate quantitative comparisons?\nAre aesthetic mappings carefully chosen to minimize clutter?\nAre aesthetic mappings chosen for accessibility? If color is used, is another aesthetic also used to dual-encode the information?\n\n\nGeometries\n\n\nAre appropriate geoms used for the data type and comparison goals?\n\nIf there are multiple layers,\n\nDo the layers work together to show the same aspects of the data at different levels?\nDo layers enhance clarity, or increase clutter?\n\n\nAre key patterns visible?\n\n\nScales and Transformations\n\n\nLocation scales\n\n\nIs the data well distributed across the full range of the scale?\n\nIf most of the data is compressed into a small part of the scale consider a scale transformation (log, sqrt) or a plot-within-a-plot (ggpp) to show both the compressed area and the total range.\n\n\nIf the \\(x\\) and \\(y\\) axis scales show the same units, does the aspect ratio of the plot reflect this?\n\n\nColor scales\n\nIs the color scale used the most accessible scale for the data?\nIf color is mapped to a continuous value, is the scale transformed to maximize the contrast across the range of the variable? Log transforms can often be useful to increase contrast between different orders of magnitude of a continuous value.\n\n\nTransformations\n\nAre any transformations used clearly identified? e.g. a log scale should have ticks that are not uniformly spaced to provide an additional cue that the transformation is present.\nAre transformed elements labeled clearly?\nDoes the transformation improve comprehension?\n\n\nCoordinate Systems\n\nIs the coordinate system appropriate for the message? If polar coordinates are used, is there a clear justification for deviating from better-understood cartesian coordinates?\n\n\n\n\nFacets and Layout\n\n\nDoes the layout support the comparisons of interest? Are facets that viewers may want to compare ordered so that they are adjacent?\nHow much effort is required to make secondary comparisons? Consider visual distance traveled between elements in the secondary comparison, alignment of axes, etc.\nAre there too many facets shown? How does the number of facets required for the comparison compare to the overall increase in visual clutter?\nAre facet labels clear and easy to understand?\n\n\nAnnotations\n\n\nAre key points annotated clearly?\nDo annotations reduce clutter?\nWould it be more effective to replace legends with on-plot labels, or would this increase visual clutter?\nDoes the design support the overall message of the plot?\nAre artifacts in the data annotated and explained?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#sec-good-graphics-refs",
    "href": "part-wrangling/02c-good-graphics.html#sec-good-graphics-refs",
    "title": "21  Creating Good Charts",
    "section": "\n21.5 References",
    "text": "21.5 References\n\n\n\n\n[1] \nWikipedia contributors, “Wetware (brain),” Wikipedia, Aug. 2025 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Wetware_(brain)&oldid=1306946759. [Accessed: Sep. 03, 2025]\n\n\n[2] \nWikipedia contributors, “Color blindness,” Wikipedia, May 2023. \n\n\n[3] \nA. Treisman, “Preattentive processing in vision,” Computer Vision, Graphics, and Image Processing, vol. 31, no. 2, pp. 156–177, Aug. 1985, doi: 10.1016/S0734-189X(85)80004-9. \n\n\n[4] \nC. G. Healey, K. S. Booth, and J. T. Enns, “High-speed visual estimation using preattentive processing,” ACM Transactions on Computer-Human Interaction (TOCHI), vol. 3, no. 2, pp. 107–135, 1996, doi: 10.1145/230562.230563. \n\n\n[5] \nJ. Bryan, Gapminder: Data from gapminder. 2023 [Online]. Available: https://CRAN.R-project.org/package=gapminder",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#footnotes",
    "href": "part-wrangling/02c-good-graphics.html#footnotes",
    "title": "21  Creating Good Charts",
    "section": "",
    "text": "When the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.↩︎\nLisa Charlotte Rost. What to consider when choosing colors for data visualization.↩︎\nSee this paper for more details. This is the last chapter of my dissertation, for what it’s worth. It was a lot of fun. (no sarcasm, seriously, it was fun!)↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html",
    "href": "part-wrangling/03-data-cleaning.html",
    "title": "22  Data Cleaning",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#objectives",
    "href": "part-wrangling/03-data-cleaning.html#objectives",
    "title": "22  Data Cleaning",
    "section": "",
    "text": "Identify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately\nApply data manipulation verbs to prepare data for analysis\nUnderstand the consequences of data cleaning steps for statistical analysis\nCreate summaries of data appropriate for analysis or display using data manipulation techniques",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#introduction",
    "href": "part-wrangling/03-data-cleaning.html#introduction",
    "title": "22  Data Cleaning",
    "section": "\n22.1 Introduction",
    "text": "22.1 Introduction\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\n\n\n\n\nData wrangling (by Allison Horst)\n\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\nIn R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework).\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nIn Python, most data manipulation tasks are handled using pandas[1]. In the interests of using a single consistent “language” for describing data manipulation tasks, I’ll use the tidyverse “verbs” to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\nThere is also the datar python package[2], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, datar may be more useful if you prefer the dplyr way of handling things using a data-first API.\n\n\nI haven’t had the chance to add the datar package to this book, but it looks promising and may be worth your time to figure out. It’s a bit too new for me to teach right now - I want packages that will be maintained long-term if I’m going to teach them to others.\n\n\n\n\n\n\nNote\n\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.\nHere is a data wrangling with pandas cheatsheet that is formatted similarly to the dplyr cheat sheet.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#tidy-data",
    "href": "part-wrangling/03-data-cleaning.html#tidy-data",
    "title": "22  Data Cleaning",
    "section": "\n22.2 Tidy Data",
    "text": "22.2 Tidy Data\nThere are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\nThese attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your clothes are folded, but they aren’t clean just because they’re folded; you could have folded a pile of dirty clothes).\nWe’ll get more into how to work with different “messy” data configurations in Chapter 24 and Chapter 26, but it’s worth keeping rules 1 and 3 in mind while working through this module.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "href": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "title": "22  Data Cleaning",
    "section": "\n22.3 Filter: Subset rows",
    "text": "22.3 Filter: Subset rows\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\ndplyr filter() by Allison Horst\n\n\n\n\n\n\n\nExample: starwars\n\n\n\nLet’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies.\nIn the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by CSV files. You can access the csv data here.\n\n\nR\nPython\n\n\n\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n##  3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n##  4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n##  9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n## 10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n## # ℹ 77 more rows\n## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\nWe have to use the exported CSV data in python.\n\nimport pandas as pd\nstarwars = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padmé Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\n\nfrom skimpy import skim\nskim(starwars)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 87     │ │ string      │ 8     │                       │\n## │ │ Number of columns │ 11     │ │ float64     │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━┳━━━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┓  │\n## │ ┃ colu ┃    ┃       ┃      ┃       ┃    ┃      ┃     ┃      ┃      ┃      ┃  │\n## │ ┃ mn   ┃ NA ┃ NA %  ┃ mean ┃ sd    ┃ p0 ┃ p25  ┃ p50 ┃ p75  ┃ p100 ┃ hist ┃  │\n## │ ┡━━━━━━╇━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━╇━━━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━━┩  │\n## │ │ heig │  6 │ 6.896 │ 174. │ 34.77 │ 66 │  167 │ 180 │  191 │  264 │  ▁   │  │\n## │ │ ht   │    │ 55172 │    4 │       │    │      │     │      │      │ ▁█▂  │  │\n## │ │      │    │ 41379 │      │       │    │      │     │      │      │      │  │\n## │ │      │    │    31 │      │       │    │      │     │      │      │      │  │\n## │ │ mass │ 28 │ 32.18 │ 97.3 │ 169.5 │ 15 │ 55.6 │  79 │ 84.5 │ 1358 │  █   │  │\n## │ │      │    │ 39080 │    1 │       │    │      │     │      │      │      │  │\n## │ │      │    │ 45977 │      │       │    │      │     │      │      │      │  │\n## │ │      │    │    01 │      │       │    │      │     │      │      │      │  │\n## │ │ birt │ 44 │ 50.57 │ 87.5 │ 154.7 │  8 │   35 │  52 │   72 │  896 │  █   │  │\n## │ │ h_ye │    │ 47126 │    7 │       │    │      │     │      │      │      │  │\n## │ │ ar   │    │ 43678 │      │       │    │      │     │      │      │      │  │\n## │ │      │    │    16 │      │       │    │      │     │      │      │      │  │\n## │ └──────┴────┴───────┴──────┴───────┴────┴──────┴─────┴──────┴──────┴──────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━━┳━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┓  │\n## │ ┃       ┃    ┃       ┃       ┃      ┃       ┃      ┃       ┃ word ┃       ┃  │\n## │ ┃       ┃    ┃       ┃       ┃      ┃       ┃      ┃ chars ┃ s    ┃       ┃  │\n## │ ┃ colum ┃    ┃       ┃ short ┃ long ┃       ┃      ┃ per   ┃ per  ┃ total ┃  │\n## │ ┃ n     ┃ NA ┃ NA %  ┃ est   ┃ est  ┃ min   ┃ max  ┃ row   ┃ row  ┃ words ┃  │\n## │ ┡━━━━━━━╇━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━┩  │\n## │ │ name  │  0 │     0 │ Rey   │ Jabb │ Ackba │ Zam  │  10.3 │  1.8 │   157 │  │\n## │ │       │    │       │       │ a    │ r     │ Wese │       │      │       │  │\n## │ │       │    │       │       │ Desi │       │ ll   │       │      │       │  │\n## │ │       │    │       │       │ liji │       │      │       │      │       │  │\n## │ │       │    │       │       │ c    │       │      │       │      │       │  │\n## │ │       │    │       │       │ Tiur │       │      │       │      │       │  │\n## │ │       │    │       │       │ e    │       │      │       │      │       │  │\n## │ │ hair_ │  5 │ 5.747 │ none  │ aubu │ aubur │ whit │  4.84 │ 0.98 │    85 │  │\n## │ │ color │    │ 12643 │       │ rn,  │ n     │ e    │       │      │       │  │\n## │ │       │    │ 67816 │       │ whit │       │      │       │      │       │  │\n## │ │       │    │    09 │       │ e    │       │      │       │      │       │  │\n## │ │ skin_ │  0 │     0 │ red   │ grey │ blue  │ yell │  5.97 │  1.2 │   106 │  │\n## │ │ color │    │       │       │ ,    │       │ ow   │       │      │       │  │\n## │ │       │    │       │       │ gree │       │      │       │      │       │  │\n## │ │       │    │       │       │ n,   │       │      │       │      │       │  │\n## │ │       │    │       │       │ yell │       │      │       │      │       │  │\n## │ │       │    │       │       │ ow   │       │      │       │      │       │  │\n## │ │ eye_c │  0 │     0 │ red   │ gree │ black │ yell │   5.1 │    1 │    89 │  │\n## │ │ olor  │    │       │       │ n,   │       │ ow   │       │      │       │  │\n## │ │       │    │       │       │ yell │       │      │       │      │       │  │\n## │ │       │    │       │       │ ow   │       │      │       │      │       │  │\n## │ │ sex   │  4 │ 4.597 │ male  │ herm │ femal │ none │  4.51 │ 0.95 │    83 │  │\n## │ │       │    │ 70114 │       │ aphr │ e     │      │       │      │       │  │\n## │ │       │    │ 94252 │       │ odit │       │      │       │      │       │  │\n## │ │       │    │    87 │       │ ic   │       │      │       │      │       │  │\n## │ │ gende │  4 │ 4.597 │ femin │ masc │ femin │ masc │   8.8 │ 0.95 │    83 │  │\n## │ │ r     │    │ 70114 │ ine   │ ulin │ ine   │ ulin │       │      │       │  │\n## │ │       │    │ 94252 │       │ e    │       │ e    │       │      │       │  │\n## │ │       │    │    87 │       │      │       │      │       │      │       │  │\n## │ │ homew │ 10 │ 11.49 │ Tund  │ Cato │ Alder │ Zola │  7.14 │ 0.98 │    85 │  │\n## │ │ orld  │    │ 42528 │       │ Neim │ aan   │ n    │       │      │       │  │\n## │ │       │    │ 73563 │       │ oidi │       │      │       │      │       │  │\n## │ │       │    │   218 │       │ a    │       │      │       │      │       │  │\n## │ │ speci │  4 │ 4.597 │ Dug   │ Yoda │ Aleen │ Zabr │  6.17 │ 0.99 │    86 │  │\n## │ │ es    │    │ 70114 │       │ 's   │ a     │ ak   │       │      │       │  │\n## │ │       │    │ 94252 │       │ spec │       │      │       │      │       │  │\n## │ │       │    │    87 │       │ ies  │       │      │       │      │       │  │\n## │ └───────┴────┴───────┴───────┴──────┴───────┴──────┴───────┴──────┴───────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Section 11.2.1, but here we’ll focus on using specific functions to perform the same operation.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nThe dplyr verb for selecting rows is filter. filter takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  3 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  4 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  5 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  6 Biggs D…    183    84 black      light      brown           24   male  mascu…\n##  7 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n##  8 Anakin …    188    84 blond      fair       blue            41.9 male  mascu…\n##  9 Wilhuff…    180    NA auburn, g… fair       blue            64   male  mascu…\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu…\n## # ℹ 25 more rows\n## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n## 1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n## 2 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n## 3 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n## 4 Beru Whi…    165    75 brown      light      blue            47   fema… femin…\n## 5 Biggs Da…    183    84 black      light      brown           24   male  mascu…\n## 6 Anakin S…    188    84 blond      fair       blue            41.9 male  mascu…\n## 7 Shmi Sky…    163    NA black      fair       brown           72   fema… femin…\n## 8 Cliegg L…    183    NA brown      fair       blue            82   male  mascu…\n## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cordé   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dormé   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padmé Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\n\n# Get only the people who come from Tattoine\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n\n\n\nIn base R, you would perform a filtering operation using subset\n\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n##  2 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n##  3 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n##  4 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n##  5 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n##  6 Biggs D…    183    84 black      light      brown           24   male  mascu…\n##  7 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n##  8 Anakin …    188    84 blond      fair       blue            41.9 male  mascu…\n##  9 Wilhuff…    180    NA auburn, g… fair       blue            64   male  mascu…\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu…\n## # ℹ 25 more rows\n## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n## 1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n## 2 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n## 3 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n## 4 Beru Whi…    165    75 brown      light      blue            47   fema… femin…\n## 5 Biggs Da…    183    84 black      light      brown           24   male  mascu…\n## 6 Anakin S…    188    84 blond      fair       blue            41.9 male  mascu…\n## 7 Shmi Sky…    163    NA black      fair       brown           72   fema… femin…\n## 8 Cliegg L…    183    NA brown      fair       blue            82   male  mascu…\n## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nNotice that with subset, you have to use & to join two logical statements; it does not by default take multiple successive arguments.\n\n\n\n\n\n\n22.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I’ll do my best to show you easy ways to handle each task in each language.\n\n22.3.1.1 Filtering by row number\n\n\nR: dplyr\nPython\nBase R\n\n\n\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nlibrary(readr)\nlibrary(dplyr)\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\nfilter(poke, (row_number() %% 2 == 0)) \n## # A tibble: 763 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          2 https://img.… Ivys… &lt;NA&gt;    Gras…   405    60     62      63\n##  2     1          3 https://img.… Venu… Mega    Gras…   625    80    100     123\n##  3     1          5 https://img.… Char… &lt;NA&gt;    Fire    405    58     64      58\n##  4     1          6 https://img.… Char… Mega  X Fire…   634    78    130     111\n##  5     1          7 https://img.… Squi… &lt;NA&gt;    Water   314    44     48      65\n##  6     1          9 https://img.… Blas… &lt;NA&gt;    Water   530    79     83     100\n##  7     1         10 https://img.… Cate… &lt;NA&gt;    Bug     195    45     30      35\n##  8     1         12 https://img.… Butt… &lt;NA&gt;    Bug,…   395    60     45      50\n##  9     1         14 https://img.… Kaku… &lt;NA&gt;    Bug,…   205    45     25      50\n## 10     1         15 https://img.… Beed… Mega    Bug,…   495    65    150      40\n## # ℹ 753 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n\n\n\nIn python, the easiest way to accomplish filtering by row number is by using .iloc. But, up until now, we’ve only talked about how Python creates slices using start:(end+1) notation. There is an additional option with slicing - start:(end+1):by. So if we want to get only even rows, we can use the index [::2], which will give us row 0, 2, 4, 6, … through the end of the dataset, because we didn’t specify the start and end portions of the slice.\nBecause Python is 0-indexed, using ::2 will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke.iloc[0::2]\n##       gen  pokedex_no  ... height_m weight_kg\n## 0       1           1  ...      0.7       6.9\n## 2       1           3  ...      2.0     100.0\n## 4       1           4  ...      0.6       8.5\n## 6       1           6  ...      1.7      90.5\n## 8       1           6  ...      1.7      90.5\n## ...   ...         ...  ...      ...       ...\n## 1516    9         999  ...      0.3       5.0\n## 1518    9        1001  ...      1.5      74.2\n## 1520    9        1003  ...      2.7     699.7\n## 1522    9        1005  ...      2.0     380.0\n## 1524    9        1007  ...      2.5     303.0\n## \n## [763 rows x 16 columns]\n\nIf we want to get only odd rows, we can use the index [1::2], which will start at row 1 and give us 1, 3, 5, …\n\n\nIn base R, we’d use seq() to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use subset, which requires a logical condition, and use 1:nrow(poke) to create an index which we then use for deciding whether each row is even or odd.\n\npoke[seq(1, nrow(poke), 2),]\n## # A tibble: 763 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          1 https://img.… Bulb… &lt;NA&gt;    Gras…   318    45     49      49\n##  2     1          3 https://img.… Venu… &lt;NA&gt;    Gras…   525    80     82      83\n##  3     1          4 https://img.… Char… &lt;NA&gt;    Fire    309    39     52      43\n##  4     1          6 https://img.… Char… &lt;NA&gt;    Fire…   534    78     84      78\n##  5     1          6 https://img.… Char… Mega  Y Fire…   634    78    104      78\n##  6     1          8 https://img.… Wart… &lt;NA&gt;    Water   405    59     63      80\n##  7     1          9 https://img.… Blas… Mega    Water   630    79    103     120\n##  8     1         11 https://img.… Meta… &lt;NA&gt;    Bug     205    50     20      55\n##  9     1         13 https://img.… Weed… &lt;NA&gt;    Bug,…   195    40     35      30\n## 10     1         15 https://img.… Beed… &lt;NA&gt;    Bug,…   395    65     90      40\n## # ℹ 753 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## # A tibble: 763 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          2 https://img.… Ivys… &lt;NA&gt;    Gras…   405    60     62      63\n##  2     1          3 https://img.… Venu… Mega    Gras…   625    80    100     123\n##  3     1          5 https://img.… Char… &lt;NA&gt;    Fire    405    58     64      58\n##  4     1          6 https://img.… Char… Mega  X Fire…   634    78    130     111\n##  5     1          7 https://img.… Squi… &lt;NA&gt;    Water   314    44     48      65\n##  6     1          9 https://img.… Blas… &lt;NA&gt;    Water   530    79     83     100\n##  7     1         10 https://img.… Cate… &lt;NA&gt;    Bug     195    45     30      35\n##  8     1         12 https://img.… Butt… &lt;NA&gt;    Bug,…   395    60     45      50\n##  9     1         14 https://img.… Kaku… &lt;NA&gt;    Bug,…   205    45     25      50\n## 10     1         15 https://img.… Beed… Mega    Bug,…   495    65    150      40\n## # ℹ 753 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nThis is less fun than using dplyr because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you’re going. The real power of dplyr is in the collection of the full set of verbs with a consistent user interface; nothing done in dplyr is so special that it can’t be done in base R as well.\n\n\n\n\n22.3.1.2 Sorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n\nR: dplyr\nPython\nBase R\n\n\n\narrange() is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\narrange(poke, desc(total))\n## # A tibble: 1,526 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.… Eter… Eterna… Pois…  1125   255    115     250\n##  2     1        150 https://img.… Mewt… Mega  X Psyc…   780   106    190     100\n##  3     1        150 https://img.… Mewt… Mega  Y Psyc…   780   106    150      70\n##  4     3        384 https://img.… Rayq… Mega    Drag…   780   105    180     100\n##  5     3        382 https://img.… Kyog… Primal  Water   770   100    150      90\n##  6     3        383 https://img.… Grou… Primal  Grou…   770   100    180     160\n##  7     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  8     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  9     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n## 10     4        493 https://img.… Arce… &lt;NA&gt;    Norm…   720   120    120     120\n## # ℹ 1,516 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\nIn pandas, we use the sort_values function, which has an argument ascending. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\npoke.sort_values(['total'], ascending = False)\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 584     3         384  ...      7.0     206.5\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 580     3         382  ...      4.5     352.0\n## ...   ...         ...  ...      ...       ...\n## 1336    8         872  ...      0.3       3.8\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## \n## [1526 rows x 16 columns]\n\n\n\nThe sort() function in R can be used to sort a vector, but when sorting a data frame we usually want to use the order() function instead. This is because sort() orders the values of the argument directly, where order() returns a sorted index.\n\nx &lt;- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n\nWhen working with a data frame, we want to sort the entire data frame’s rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\npoke[order(poke$total, decreasing = T),]\n## # A tibble: 1,526 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.… Eter… Eterna… Pois…  1125   255    115     250\n##  2     1        150 https://img.… Mewt… Mega  X Psyc…   780   106    190     100\n##  3     1        150 https://img.… Mewt… Mega  Y Psyc…   780   106    150      70\n##  4     3        384 https://img.… Rayq… Mega    Drag…   780   105    180     100\n##  5     3        382 https://img.… Kyog… Primal  Water   770   100    150      90\n##  6     3        383 https://img.… Grou… Primal  Grou…   770   100    180     160\n##  7     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  8     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  9     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n## 10     4        493 https://img.… Arce… &lt;NA&gt;    Norm…   720   120    120     120\n## # ℹ 1,516 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\n\n\n22.3.1.3 Keep the top \\(n\\) values of a variable\n\n\nR: dplyr\nPython\nBase R\n\n\n\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nslice_max(poke, order_by = total, n = 5)\n## # A tibble: 6 × 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p… Eter… Eterna… Pois…  1125   255    115     250\n## 2     1        150 https://img.p… Mewt… Mega  X Psyc…   780   106    190     100\n## 3     1        150 https://img.p… Mewt… Mega  Y Psyc…   780   106    150      70\n## 4     3        384 https://img.p… Rayq… Mega    Drag…   780   105    180     100\n## 5     3        382 https://img.p… Kyog… Primal  Water   770   100    150      90\n## 6     3        383 https://img.p… Grou… Primal  Grou…   770   100    180     160\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nBy default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows.\n\nslice_max(poke, order_by = total, n = 5, with_ties = F) \n## # A tibble: 5 × 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p… Eter… Eterna… Pois…  1125   255    115     250\n## 2     1        150 https://img.p… Mewt… Mega  X Psyc…   780   106    190     100\n## 3     1        150 https://img.p… Mewt… Mega  Y Psyc…   780   106    150      70\n## 4     3        384 https://img.p… Rayq… Mega    Drag…   780   105    180     100\n## 5     3        382 https://img.p… Kyog… Primal  Water   770   100    150      90\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nOf course, there is a similar slice_min() function as well:\n\nslice_min(poke, order_by = total, n = 5)\n## # A tibble: 5 × 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     7        746 https://img.p… Wish… Solo    Water   175    45     20      20\n## 2     7        746 https://img.p… Wish… Solo    Water   175    45     20      20\n## 3     2        191 https://img.p… Sunk… &lt;NA&gt;    Grass   180    30     30      30\n## 4     8        824 https://img.p… Blip… &lt;NA&gt;    Bug     180    25     20      20\n## 5     8        872 https://img.p… Snom  &lt;NA&gt;    Ice,…   185    30     25      35\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nslice_max(poke, order_by = total, prop = .01)\n## # A tibble: 30 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.… Eter… Eterna… Pois…  1125   255    115     250\n##  2     1        150 https://img.… Mewt… Mega  X Psyc…   780   106    190     100\n##  3     1        150 https://img.… Mewt… Mega  Y Psyc…   780   106    150      70\n##  4     3        384 https://img.… Rayq… Mega    Drag…   780   105    180     100\n##  5     3        382 https://img.… Kyog… Primal  Water   770   100    150      90\n##  6     3        383 https://img.… Grou… Primal  Grou…   770   100    180     160\n##  7     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  8     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  9     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n## 10     4        493 https://img.… Arce… &lt;NA&gt;    Norm…   720   120    120     120\n## # ℹ 20 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\nIn Python, nlargest and nsmallest work roughly the same as dplyr’s slice_max and slice_min for integer counts.\n\npoke.nlargest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## \n## [5 rows x 16 columns]\npoke.nsmallest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## \n## [5 rows x 16 columns]\n\nTo get proportions, though, we have to do some math:\n\npoke.nlargest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## 582     3         383  ...      3.5     950.0\n## 1257    7         800  ...      2.4     230.0\n## 1258    7         800  ...      2.4     230.0\n## 1259    7         800  ...      2.4     230.0\n## 779     4         493  ...      3.2     320.0\n## 1126    6         718  ...      5.0     305.0\n## 1127    6         718  ...      5.0     305.0\n## 1128    6         718  ...      5.0     305.0\n## 405     2         248  ...      2.0     202.0\n## 567     3         373  ...      1.5     102.6\n## \n## [15 rows x 16 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## 465     3         298  ...      0.2       2.0\n## 616     4         401  ...      0.3       2.2\n## 13      1          10  ...      0.3       2.9\n## 16      1          13  ...      0.3       3.2\n## 431     3         265  ...      0.3       3.6\n## 446     3         280  ...      0.4       6.6\n## 248     1         129  ...      0.9      10.0\n## 524     3         349  ...      0.6       7.4\n## 1032    6         664  ...      0.3       2.5\n## 1237    7         789  ...      0.2       0.1\n## \n## [15 rows x 16 columns]\n\n\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing. In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\npoke[order(poke$total, decreasing = T)[1:5],]\n## # A tibble: 5 × 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p… Eter… Eterna… Pois…  1125   255    115     250\n## 2     1        150 https://img.p… Mewt… Mega  X Psyc…   780   106    190     100\n## 3     1        150 https://img.p… Mewt… Mega  Y Psyc…   780   106    150      70\n## 4     3        384 https://img.p… Rayq… Mega    Drag…   780   105    180     100\n## 5     3        382 https://img.p… Kyog… Primal  Water   770   100    150      90\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\npoke[order(poke$total, decreasing = T)[1:round(nrow(poke)*.01)],]\n## # A tibble: 15 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.… Eter… Eterna… Pois…  1125   255    115     250\n##  2     1        150 https://img.… Mewt… Mega  X Psyc…   780   106    190     100\n##  3     1        150 https://img.… Mewt… Mega  Y Psyc…   780   106    150      70\n##  4     3        384 https://img.… Rayq… Mega    Drag…   780   105    180     100\n##  5     3        382 https://img.… Kyog… Primal  Water   770   100    150      90\n##  6     3        383 https://img.… Grou… Primal  Grou…   770   100    180     160\n##  7     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  8     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n##  9     7        800 https://img.… Necr… Ultra   Psyc…   754    97    167      97\n## 10     4        493 https://img.… Arce… &lt;NA&gt;    Norm…   720   120    120     120\n## 11     6        718 https://img.… Zyga… Comple… Drag…   708   216    100     121\n## 12     6        718 https://img.… Zyga… Comple… Drag…   708   216    100     121\n## 13     6        718 https://img.… Zyga… Comple… Drag…   708   216    100     121\n## 14     2        248 https://img.… Tyra… Mega    Rock…   700   100    164     150\n## 15     3        373 https://img.… Sala… Mega    Drag…   700    95    145     130\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\nTry it out: Filtering\n\n\n\n\n\nProblem\nR: dplyr\nPython\nBase R\n\n\n\nUse the Pokemon data to accomplish the following:\n\ncreate a new data frame that has only water type Pokemon\nwrite a filter statement that looks for any Pokemon which has water type for either type1 or type2\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nfilter(poke, str_detect(type, \"Water\"))[,c('gen', 'type', 'weight_kg')] \n## Error in `filter()`:\n## ℹ In argument: `str_detect(type, \"Water\")`.\n## Caused by error in `str_detect()`:\n## ! could not find function \"str_detect\"\n\nstr_detect looks for Water in the type entry – Pokemon can have one or two types, and in type, these are separated by a comma. Instead of splitting the types apart (which we could do), it’s easier to just check to see if water exists in the entire string.\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke.query(\"type.str.contains('Water')\")[['gen', 'type', 'weight_kg']]\n##       gen          type  weight_kg\n## 9       1         Water        9.0\n## 10      1         Water       22.5\n## 11      1         Water       85.5\n## 12      1         Water       85.5\n## 98      1         Water       19.6\n## ...   ...           ...        ...\n## 1488    9  Dragon,Water        8.0\n## 1489    9  Dragon,Water        8.0\n## 1490    9  Dragon,Water        8.0\n## 1491    9  Dragon,Water        8.0\n## 1505    9     Ice,Water       11.0\n## \n## [221 rows x 3 columns]\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nsubset(poke, grepl(\"Water\", type))\n## # A tibble: 221 × 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          7 https://img.… Squi… &lt;NA&gt;    Water   314    44     48      65\n##  2     1          8 https://img.… Wart… &lt;NA&gt;    Water   405    59     63      80\n##  3     1          9 https://img.… Blas… &lt;NA&gt;    Water   530    79     83     100\n##  4     1          9 https://img.… Blas… Mega    Water   630    79    103     120\n##  5     1         54 https://img.… Psyd… &lt;NA&gt;    Water   320    50     52      48\n##  6     1         55 https://img.… Gold… &lt;NA&gt;    Water   500    80     82      78\n##  7     1         60 https://img.… Poli… &lt;NA&gt;    Water   300    40     50      40\n##  8     1         61 https://img.… Poli… &lt;NA&gt;    Water   385    65     65      65\n##  9     1         62 https://img.… Poli… &lt;NA&gt;    Wate…   510    90     95      95\n## 10     1         72 https://img.… Tent… &lt;NA&gt;    Wate…   335    40     40      35\n## # ℹ 211 more rows\n## # ℹ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\ngrepl is a function that searches a string (grep) and returns a logical value (grep+l). It’s very useful for subsetting.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "href": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "title": "22  Data Cleaning",
    "section": "\n22.4 Select: Pick columns",
    "text": "22.4 Select: Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\n\n\n\n\nWays to select variables in dplyr\n\n\n\n\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(&lt;set of vars 1&gt;) & (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\n\n\nLet’s try these selector functions out and see what we can accomplish!\n\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\nWe’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n\n\n\n\n\nTip\n\n\n\nYou might want to try out your EDA (Exploratory Data Analysis) skills to see what you can find out about the dataset, before seeing how select() works.\n\n\nWe could get a data frame of departure information for each flight:\n\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # ℹ 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nflights %&gt;%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ℹ 336,766 more rows\n## # ℹ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\nNote that everything() won’t duplicate columns you’ve already added.\nExploring the difference between bare name selection and all_of()/any_of()\n\nflights %&gt;%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 × 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ℹ 336,766 more rows\n## # ℹ 2 more variables: air_time &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nvarlist &lt;- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %&gt;%\n  select(all_of(varlist))\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ℹ 336,766 more rows\n## # ℹ 1 more variable: air_time &lt;dbl&gt;\n\nvarlist &lt;- c(varlist, \"whoops\")\n\nflights %&gt;%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()` at magrittr/R/pipe.R:136:3:\n## ℹ In argument: `all_of(varlist)`.\n## Caused by error in `all_of()` at rlang/R/eval-tidy.R:121:3:\n## ! Can't subset elements that don't exist.\n## ✖ Element `whoops` doesn't exist.\n\nflights %&gt;%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ℹ 336,766 more rows\n## # ℹ 1 more variable: air_time &lt;dbl&gt;\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\nFirst, let’s install the nycflights13 package[3] in python by typing the following into your system terminal.\n\npip install nycflights13\n## Requirement already satisfied: nycflights13 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (0.0.3)\n## Requirement already satisfied: pandas&gt;=0.24.0 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from nycflights13) (2.3.1)\n## Requirement already satisfied: numpy&gt;=1.23.2 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2.3.2)\n## Requirement already satisfied: python-dateutil&gt;=2.8.2 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2.9.0.post0)\n## Requirement already satisfied: pytz&gt;=2020.1 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2025.2)\n## Requirement already satisfied: tzdata&gt;=2022.7 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2025.2)\n## Requirement already satisfied: six&gt;=1.5 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.24.0-&gt;nycflights13) (1.17.0)\n\nThen, we can load the flights data from the nycflights13 package.\n\nfrom nycflights13 import flights\n\nSelect operations are not as easy in python as they are when using dplyr::select() with helpers, but of course you can accomplish the same tasks.\n\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\nList Comprehensions\nIn Python, there are certain shorthands called “list comprehensions” [4] that can perform similar functions to e.g. the matches() function in dplyr.\nSuppose we want to get all columns containing the word ‘time’. We could iterate through the list of columns (flights.columns) and add the column name any time we detect the word ‘time’ within. That is essentially what the following code does:\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n\nExplaining the code:\n\n\nfor col in flights.columns iterates through the list of columns, storing each column name in the variable col\n\n\nif 'time' in col detects the presence of the word ‘time’ in the column name stored in col\n\nthe col out front adds the column name in the variable col to the array of columns to keep\nSelecting columns in Python\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\nselcols.extend([\"whoops\"])\nselcols\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\n\n# Subset the data frame\nflights.loc[:,selcols]\n## KeyError: \"['whoops'] not in index\"\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\n\n\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. matches in dplyr.\ngrepl is a shorthand function for grep, which searches for a pattern in a vector of strings. grepl returns a logical vector indicating whether the pattern (\"dep\", in this case) was found in the vector (names(flights), in this case).\n\ndepcols &lt;- names(flights)[grepl(\"dep\", names(flights))]\ncollist &lt;- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # ℹ 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nnew_order &lt;- names(flights)\nnew_order &lt;- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ℹ 336,766 more rows\n## # ℹ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\nThis is less convenient than dplyr::everything in part because it depends on us to get the column indexes right.\n\n\n\n\n\n22.4.1 Rearranging Columns\n\n\ndplyr::relocate\nPython\n\n\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ℹ 336,766 more rows\n## # ℹ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\n# move numeric variables to the front\nflights %&gt;% relocate(where(is.numeric))\n## # A tibble: 336,776 × 19\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # ℹ 336,766 more rows\n## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, flight &lt;int&gt;, air_time &lt;dbl&gt;,\n## #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n## #   origin &lt;chr&gt;, dest &lt;chr&gt;, time_hour &lt;dttm&gt;\n\n\n\nThere are similar ways to rearrange columns in pandas, but they are a bit harder to work with - you have to specify the column names (in some way) and then perform the operation yourself.\n\nimport numpy as np\ncols = list(flights.columns.values) # get column names\n\n# Move flight specific info to the front\nflightcols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']\nflights[flightcols + list(flights.drop(flightcols, axis = 1))]\n##        carrier  flight tailnum  ... hour minute             time_hour\n## 0           UA    1545  N14228  ...    5     15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5     29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5     40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5     45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6      0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...    ...                   ...\n## 336771      9E    3393     NaN  ...   14     55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22      0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12     10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11     59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8     40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\n# move numeric variables to the front\nnumcols = list(flights.select_dtypes(include = np.number).columns.values)\nflights[numcols + list(flights.drop(numcols, axis = 1))]\n##         year  month  day  dep_time  ...  tailnum  origin  dest             time_hour\n## 0       2013      1    1     517.0  ...   N14228     EWR   IAH  2013-01-01T10:00:00Z\n## 1       2013      1    1     533.0  ...   N24211     LGA   IAH  2013-01-01T10:00:00Z\n## 2       2013      1    1     542.0  ...   N619AA     JFK   MIA  2013-01-01T10:00:00Z\n## 3       2013      1    1     544.0  ...   N804JB     JFK   BQN  2013-01-01T10:00:00Z\n## 4       2013      1    1     554.0  ...   N668DN     LGA   ATL  2013-01-01T11:00:00Z\n## ...      ...    ...  ...       ...  ...      ...     ...   ...                   ...\n## 336771  2013      9   30       NaN  ...      NaN     JFK   DCA  2013-09-30T18:00:00Z\n## 336772  2013      9   30       NaN  ...      NaN     LGA   SYR  2013-10-01T02:00:00Z\n## 336773  2013      9   30       NaN  ...   N535MQ     LGA   BNA  2013-09-30T16:00:00Z\n## 336774  2013      9   30       NaN  ...   N511MQ     LGA   CLE  2013-09-30T15:00:00Z\n## 336775  2013      9   30       NaN  ...   N839MQ     LGA   RDU  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "href": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "title": "22  Data Cleaning",
    "section": "\n22.5 Mutate: Add and transform variables",
    "text": "22.5 Mutate: Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate, but in base R and python, we’ll simply use assignment to add columns to our data frames.\n\n\n\n\nMutate (by Allison Horst)\n\nWe’ll use the Pokemon data to demonstrate. Some Pokemon have a single “type”, which is usually elemental, such as Water, Ice, Fire, etc., but others have two. Let’s add a column that indicates how many types a pokemon has.\n\n\nBase R\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\n# This splits type_1,type_2 into two separate variables. \n# Don't worry about the string processing (gsub) just now\n# Focus on how variables are defined.\npoke$type_1 &lt;- gsub(\",.*$\", \"\", poke$type) # Replace anything after comma with ''\npoke$type_2 &lt;- gsub(\"^.*,\", \"\", poke$type) # Use the 2nd type\npoke$type_2[poke$type_1 == poke$type_2] &lt;- NA # Type 2 only exists if not same as Type 1\n\npoke$no_types &lt;- 1 # set a default value\npoke$no_types[grepl(\",\", poke$type)] &lt;- 2 # set the value if there's not a comma in type\n\n# This is a bit faster\npoke$no_types &lt;- ifelse(grepl(\",\", poke$type), 2, 1)\n\n# Sanity check\n# This checks number of types vs. value of type_2\n# If type 2 is NA, then number of types should be 1\nt(table(poke$type_2, poke$no_types, useNA = 'ifany'))\n##    \n##     Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass Ground Ice\n##   1   0    0      0        0     0        0    0      0     0     0      0   0\n##   2   9   40     49       17    47       47   31    157    57    50     57  36\n##    \n##     Normal Poison Psychic Rock Steel Water &lt;NA&gt;\n##   1      0      0       0    0     0     0  673\n##   2     24     51      61   23    55    42    0\n\nNotice that we had to type the name of the dataset at least 3 times to perform the operation we were looking for. I could reduce that to 2x with the ifelse function, but it’s still a lot of typing.\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke &lt;- poke %&gt;%\n  # This splits type into type_1,type_2 : two separate variables. \n  # Don't worry about the string processing (str_extract) just now\n  # Focus on how variables are defined: \n  #   we use a function on the type column\n  #   within the mutate statement.\n  mutate(type_1 = str_extract(type, \"^(.*),\", group = 1),\n         type_2 = str_extract(type, \"(.*),(.*)\", group = 2)) %&gt;%\n  mutate(no_types = if_else(is.na(type_2), 1, 2))\n## Error in `mutate()` at dplyr/R/mutate.R:146:3:\n## ℹ In argument: `type_1 = str_extract(type, \"^(.*),\", group = 1)`.\n## Caused by error in `str_extract()`:\n## ! could not find function \"str_extract\"\n\nselect(poke, type_2, no_types) %&gt;% table(useNA = 'ifany') %&gt;% t()\n## Error in `select()`:\n## ! Can't select columns that don't exist.\n## ✖ Column `type_2` doesn't exist.\n\nThe last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab.\n\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with).\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\n# This splits type into two columns, type_1 and type_2, based on \",\"\npoke[[\"type_1\", \"type_2\"]] = poke[\"type\"].apply(lambda x: pd.Series(str(x).split(\",\")))\n\n# This defines number of types\npoke[\"no_types\"] = 1 # default value\npoke.loc[~poke.type_2.isna(), \"no_types\"] = 2 # change those with a defined type 2\n\n\npoke.groupby([\"no_types\", \"type_2\"], dropna=False).size()\n## no_types  type_2  \n## 1         NaN         673\n## 2         Bug           9\n##           Dark         40\n##           Dragon       49\n##           Electric     17\n##           Fairy        47\n##           Fighting     47\n##           Fire         31\n##           Flying      157\n##           Ghost        57\n##           Grass        50\n##           Ground       57\n##           Ice          36\n##           Normal       24\n##           Poison       51\n##           Psychic      61\n##           Rock         23\n##           Steel        55\n##           Water        42\n## dtype: int64\n# When type_2 is NaN, no_types is 1\n# When type_2 is defined, no_types is 2\n\nAnother function that may be useful is the assign function, which can be used to create new variables if you don’t want to use the [\"new_col\"] notation. In some circumstances, .assign(var = ...) is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I’d like to not get into right now for simplicity’s sake).\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables.\n\n\n\n\n\n\nMutate and new challenges\n\n\n\n\n\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation, google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\nSource\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a quick table of places to look in R and python to solve some of the more common problems.\n\n\nProblem\nR\nPython\n\n\n\nDates and Times\n\nlubridate package (esp. ymd_hms() and variants, decimal_date(), and other convenience functions)\n\npandas has some date time support by default; see the datetime module for more functionality.\n\n\nString manipulation\n\nstringr package\nQuick Tips [5], Whirlwind Tour of Python chapter [6]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarize",
    "href": "part-wrangling/03-data-cleaning.html#summarize",
    "title": "22  Data Cleaning",
    "section": "\n22.6 Summarize",
    "text": "22.6 Summarize\nThe next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we’ve used e.g. group_by + count in Python to perform certain tasks as well.\nHere (in a trivial example), I compute the overall average HP of a Pokemon in each generation, as well as the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes.\n\n\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %&gt;%\n  mutate(name_chr = nchar(name)) %&gt;%\n  summarize(n = max(pokedex_no), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 1 × 3\n##       n    hp name_chr\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1008  71.2     7.55\n\n\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as mean. You can also build custom summary functions [7], or use the agg() function to define multiple summary variables. agg() will even let you use different summary functions for each variable, just like summarize.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke[['hp', 'name_length']].mean()\n## hp             71.178244\n## name_length     7.545216\n## dtype: float64\npoke[['hp', 'name_length']].agg(['mean', 'min'])\n##              hp  name_length\n## mean  71.178244     7.545216\n## min    1.000000     3.000000\npoke[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n## pokedex_no     1008.000000\n## hp               71.178244\n## name_length       7.545216\n## dtype: float64\n\n\n\n\nThe real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#group-by-power",
    "href": "part-wrangling/03-data-cleaning.html#group-by-power",
    "title": "22  Data Cleaning",
    "section": "\n22.7 Group By + (?) = Power!",
    "text": "22.7 Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\nsplit my dataset into smaller datasets - one for each day\ncompute summary values for each smaller dataset\nput my summarized data back together into a single dataset\n\nThis is known as the split-apply-combine [9] or sometimes, map-reduce [10] strategy (though map-reduce is usually on specifically large datasets and performed in parallel).\nIn tidy parlance, group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well.\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\n\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %&gt;%\n  mutate(name_chr = nchar(name)) %&gt;%\n  group_by(gen) %&gt;%\n  summarize(n = length(unique(pokedex_no)), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 9 × 4\n##     gen     n    hp name_chr\n##   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1   151  65.3     7.23\n## 2     2   100  71.0     7.36\n## 3     3   135  65.8     7.16\n## 4     4   107  69.4     6.85\n## 5     5   156  75.8     7.77\n## 6     6    72  72.9     7.47\n## 7     7    88  73.2     8.04\n## 8     8    96  77.9     8.01\n## 9     9   103  75.8     8.66\n\n\n\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke.groupby('gen')[['hp', 'name_length']].mean()\n##             hp  name_length\n## gen                        \n## 1    65.333333     7.231579\n## 2    71.024194     7.362903\n## 3    65.777202     7.160622\n## 4    69.382022     6.853933\n## 5    75.827004     7.767932\n## 6    72.882353     7.470588\n## 7    73.233083     8.037594\n## 8    77.940299     8.014925\n## 9    75.756098     8.658537\npoke.groupby('gen')[['hp', 'name_length']].agg(['mean', 'min'])\n##             hp     name_length    \n##           mean min        mean min\n## gen                               \n## 1    65.333333  10    7.231579   3\n## 2    71.024194  20    7.362903   4\n## 3    65.777202   1    7.160622   4\n## 4    69.382022  20    6.853933   4\n## 5    75.827004  30    7.767932   4\n## 6    72.882353  38    7.470588   5\n## 7    73.233083  25    8.037594   6\n## 8    77.940299  25    8.014925   4\n## 9    75.756098  10    8.658537   5\npoke.groupby('gen')[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n##      pokedex_no         hp  name_length\n## gen                                    \n## 1           151  65.333333     7.231579\n## 2           100  71.024194     7.362903\n## 3           135  65.777202     7.160622\n## 4           107  69.382022     6.853933\n## 5           156  75.827004     7.767932\n## 6            72  72.882353     7.470588\n## 7            88  73.233083     8.037594\n## 8            96  77.940299     8.014925\n## 9           103  75.756098     8.658537\n\n\n\n\nWhen you group_by a variable, your result carries this grouping with it. In R, summarize will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command. In Python, you should consider using reset_index or grouped_thing.obj() to access the original information[11].\n\n\n\n\n\n\nStorms Example\n\n\n\nLet’s try a non-trivial example, using the storms dataset that is part of the dplyr package.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 19,537 × 13\n##    name   year month   day  hour   lat  long status      category  wind pressure\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n##  1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n##  8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n## # ℹ 19,527 more rows\n## # ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n\n\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\", on_bad_lines='skip')\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n\n\n\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes).\nOne thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way:\n\nThe points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause).\nIf there’s a tie, we might want to know when the maximum wind speed occurred.\nIf that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred.\n\nLet’s start by translating these criteria into basic operations. I’ll use dplyr function names here, but I’ll also specify what I mean when there’s a conflict (e.g. filter in dplyr means something different than filter in python).\nInitial attempt:\n\n\nFor each storm (group_by),\nwe need the point where the storm has lowest atmospheric pressure. (filter - pick the row with the lowest pressure).\n\nThen we read the next part: “If there is a tie, pick the maximum wind speed.”\n\ngroup_by\n\narrange by ascending pressure and descending wind speed\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\ngroup_by\n\narrange by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nsummarize - compute the average time and category (if there are multiple rows)\n\nLet’s write the code, now that we have the order of operations straight!\n\n\nR\nPython\n\n\n\n\nmax_power_storm &lt;- storms %&gt;%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %&gt;%\n  filter(pressure == min(pressure, na.rm = T)) %&gt;%\n  filter(wind == max(wind, na.rm = T)) %&gt;%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %&gt;%\n  arrange(time) %&gt;%\n  ungroup()\nmax_power_storm\n## # A tibble: 665 × 7\n##    name      year pressure  wind category status         time               \n##    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;          &lt;dttm&gt;             \n##  1 Amy       1975      981    60       NA tropical storm 1975-07-02 12:00:00\n##  2 Blanche   1975      980    75        1 hurricane      1975-07-28 00:00:00\n##  3 Caroline  1975      963   100        3 hurricane      1975-08-31 06:00:00\n##  4 Doris     1975      965    95        2 hurricane      1975-09-02 21:00:00\n##  5 Eloise    1975      955   110        3 hurricane      1975-09-23 12:00:00\n##  6 Faye      1975      977    75        1 hurricane      1975-09-28 18:00:00\n##  7 Gladys    1975      939   120        4 hurricane      1975-10-02 15:00:00\n##  8 Hallie    1975     1002    45       NA tropical storm 1975-10-27 03:00:00\n##  9 Belle     1976      957   105        3 hurricane      1976-08-09 00:00:00\n## 10 Dottie    1976      996    45       NA tropical storm 1976-08-20 06:00:00\n## # ℹ 655 more rows\n\n\n\n\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n\n\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## &lt;plotnine.ggplot.ggplot object at 0x7fb3eae75b10&gt;\n\n\n\n\nWe could also look to see whether there has been any change over time in pressure.\n\n\nR\nPython\n\n\n\n\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## &lt;plotnine.ggplot.ggplot object at 0x7fb3ea855810&gt;\n\n\n\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic.\n\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n\nR\nPython\n\n\n\n\nstorm_strength_duration &lt;- storms %&gt;%\n  group_by(name, year) %&gt;%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %&gt;%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n## Error in `geom_boxplot()`:\n## ! Problem while computing stat.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `if (...) NULL`:\n## ! missing value where TRUE/FALSE needed\n\n\n\n\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## TypeError: ggplot.__init__() got multiple values for argument 'data'\n\n\n\n\nYou don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with.\nWe could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate)\nDiameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables).\n\n\nR\nPython\n\n\n\nNote the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted.\n\nstorm_evolution &lt;- storms %&gt;%\n  filter(!is.na(hurricane_force_diameter)) %&gt;%\n  group_by(name, year) %&gt;%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %&gt;%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## &lt;plotnine.ggplot.ggplot object at 0x7fb3eb316350&gt;\n\n\n\n\nFor this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms.\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n\nR\nPython\n\n\n\n\nstorm_evolution %&gt;%\n  filter(year == 2008) %&gt;%\n  arrange(desc(time_since_start))\n## # A tibble: 548 × 15\n##    name    year month   day  hour   lat  long status     category  wind pressure\n##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n##  1 Bertha  2008     7    21     6  58.5 -27   extratrop…       NA    45      990\n##  2 Bertha  2008     7    21     0  55.1 -31   extratrop…       NA    50      990\n##  3 Bertha  2008     7    20    18  53   -34   extratrop…       NA    55      990\n##  4 Bertha  2008     7    20    12  50   -37   extratrop…       NA    60      985\n##  5 Bertha  2008     7    20     6  47.6 -40   tropical …       NA    60      985\n##  6 Bertha  2008     7    20     0  45.3 -42.4 tropical …       NA    60      990\n##  7 Bertha  2008     7    19    18  43.7 -44.3 hurricane         1    65      989\n##  8 Bertha  2008     7    19    12  42.1 -46.3 hurricane         1    65      989\n##  9 Bertha  2008     7    19     6  40.4 -48   hurricane         1    65      989\n## 10 Bertha  2008     7    19     0  38.6 -49.7 hurricane         1    65      989\n## # ℹ 538 more rows\n## # ℹ 4 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;, time &lt;dttm&gt;, time_since_start &lt;drtn&gt;\n\nstorm_evolution %&gt;% filter(name == \"Ike\") %&gt;%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n##      name  year  ...                time              age\n## 8000  Ike  2008  ... 2008-09-14 06:00:00 13 days 00:00:00\n## 7999  Ike  2008  ... 2008-09-14 00:00:00 12 days 18:00:00\n## 7998  Ike  2008  ... 2008-09-13 18:00:00 12 days 12:00:00\n## 7997  Ike  2008  ... 2008-09-13 12:00:00 12 days 06:00:00\n## 7996  Ike  2008  ... 2008-09-13 07:00:00 12 days 01:00:00\n## \n## [5 rows x 12 columns]\n\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## &lt;plotnine.ggplot.ggplot object at 0x7fb3ea6dfb10&gt;\n\n\n\n\n\n\n\n\n\nRadar coverage map from 1995, from [12]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "href": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "title": "22  Data Cleaning",
    "section": "\n22.8 Summarizing Across Multiple Variables",
    "text": "22.8 Summarizing Across Multiple Variables\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead (in dplyr).\n\n\nR\nPython\n\n\n\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time.\n\n\ndplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)\n\n\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity &lt;- storms %&gt;%\n  filter(status == \"hurricane\") %&gt;%\n  group_by(name) %&gt;%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %&gt;%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %&gt;%\n  arrange(desc(wind)) %&gt;% \n  # get top 10\n  filter(row_number() &lt;= 10) %&gt;%\n  knitr::kable() # Make into a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nmonth\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\nAllen\n1980\n8.000000\n122.9688\n941.0312\nNaN\nNaN\n\n\nIrma\n2017\n8.942308\n118.8462\n941.6154\n249.4231\n75.96154\n\n\nAndrew\n1992\n8.000000\n118.2609\n946.6522\nNaN\nNaN\n\n\nMitch\n1998\n10.000000\n115.9091\n945.3182\nNaN\nNaN\n\n\nRita\n2005\n9.000000\n114.7368\n931.6316\n265.2941\n97.05882\n\n\nIsabel\n2003\n9.000000\n112.1875\n946.5417\nNaN\nNaN\n\n\nGilbert\n1988\n9.000000\n110.8929\n945.4286\nNaN\nNaN\n\n\nLuis\n1995\n8.928571\n110.5952\n948.6190\nNaN\nNaN\n\n\nWilma\n2005\n10.000000\n110.3030\n939.4242\n349.8333\n118.33333\n\n\nMatthew\n2016\n9.880952\n109.5238\n952.1190\n263.5714\n62.02381\n\n\n\n\n\n\n\nStackoverflow reference\nWe can use python’s list comprehensions in combination with .agg to accomplish the same task as dplyr’s across function.\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a lambda function - basically, a “dummy” function that has some arguments but not all of the arguments. Here, our lambda function is a function of x, and we calculate x.mean(skipna=True) for each x passed in (so, for each column).\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "href": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "title": "22  Data Cleaning",
    "section": "\n22.9 Try it out - Data Cleaning",
    "text": "22.9 Try it out - Data Cleaning\nYou can read about the gapminder project here.\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. The gapminder package in python (install with pip install gapminder) is a port of the R package, but doesn’t contain the unfiltered data, so we’ll instead use a CSV export.\n\n\n\n\n\n\nRead in the Data\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ℹ 3,303 more rows\n\n\n\n\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 1: How Bad is It?\n\n\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nUsing your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information.\n\n\n\ngapminder_unfiltered %&gt;% \n  group_by(country) %&gt;% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %&gt;%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 × 5\n##    country        n missinglifeExp missingpop missingGDP\n##    &lt;fct&gt;      &lt;int&gt;          &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # ℹ 73 more rows\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2: Exclude any data which isn’t at 5-year increments\n\n\n\n\n\nStart in 1952 (so 1952, 1957, 1962, …, 2007).\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ℹ 2,003 more rows\n\n\n\nReminder about python list comprehensions\nExplanation of the query @ statement\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3: Exclude any countries that don’t have a full set of observations\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5)) %&gt;%\n  group_by(country) %&gt;%\n  mutate(nobs = n()) %&gt;% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %&gt;%\n  select(-nobs)\n## # A tibble: 1,704 × 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ℹ 1,694 more rows\n\n\n\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##                       country continent  year  lifeExp       pop   gdpPercap\n## country                                                                     \n## Afghanistan 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n##             1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n##             2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n##             3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n##             4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...                       ...       ...   ...      ...       ...         ...\n## Zimbabwe    3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n##             3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n##             3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n##             3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n##             3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#additional-resources",
    "href": "part-wrangling/03-data-cleaning.html#additional-resources",
    "title": "22  Data Cleaning",
    "section": "\n22.10 Additional Resources",
    "text": "22.10 Additional Resources\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nBase R data manipulation\n\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.\n\n\nTidyTuesday Python github repo - replicating Tidy Tuesday analyses in Python with Pandas",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#references",
    "href": "part-wrangling/03-data-cleaning.html#references",
    "title": "22  Data Cleaning",
    "section": "\n22.11 References",
    "text": "22.11 References\n\n\n\n\n[1] \nPandas, “Indexing and selecting data,” Pandas 1.4.3 Documentation. 2022 [Online]. Available: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing. [Accessed: Jun. 30, 2022]\n\n\n[2] \npwwang, “Datar: A Grammar of Data Manipulation in python.” May 2022 [Online]. Available: https://pwwang.github.io/datar/. [Accessed: Jun. 30, 2022]\n\n\n[3] \nM. Chow, “nycflights13: A data package for nyc flights (the nycflights13 R package).” 2020 [Online]. Available: https://github.com/tidyverse/nycflights13. [Accessed: Jun. 30, 2022]\n\n\n[4] \nPython Foundation, “Data Structures,” Python 3.10.5 documentation. Jun. 2022 [Online]. Available: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions. [Accessed: Jun. 30, 2022]\n\n\n[5] \nC. Nguyen, “Tips for String Manipulation in Python,” Towards Data Science. Sep. 2021 [Online]. Available: https://towardsdatascience.com/tips-for-string-manipulation-in-python-92b1fc3f4d9f. [Accessed: Jul. 01, 2022]\n\n\n[6] \nJ. VanderPlas, “String Manipulation and Regular Expressions,” in A Whirlwind Tour of Python, O’Reilly Media, 2016 [Online]. Available: https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html. [Accessed: Jul. 01, 2022]\n\n\n[7] \nC. Whorton, “Applying Custom Functions to Groups of Data in Pandas,” Medium. Jul. 2021 [Online]. Available: https://towardsdatascience.com/applying-custom-functions-to-groups-of-data-in-pandas-928d7eece0aa. [Accessed: Jul. 01, 2022]\n\n\n[8] \nH. Wickham, “The split-apply-combine strategy for data analysis,” Journal of statistical software, vol. 40, pp. 1–29, 2011. \n\n\n[9] \n\n“Group by: Split-apply-combine,” in Pandas 1.4.3 documentation, Python, 2022 [Online]. Available: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html. [Accessed: Jul. 01, 2022]\n\n\n[10] \nJ. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008, doi: 10.1145/1327452.1327492. \n\n\n[11] \nM. Dancho, “Answer to \"Is there an \"ungroup by\" operation opposite to .groupby in pandas?\",” Stack Overflow. Mar. 2021 [Online]. Available: https://stackoverflow.com/a/66879388/2859168. [Accessed: Jul. 01, 2022]\n\n\n[12] \nC. Mass, “The pacific northwest has the worst coastal weather radar coverage in the continental u.s.: Documentation of the problem and a call for action. Department of atmospheric sciences,” Jan. 12, 2006. [Online]. Available: https://www.atmos.washington.edu/~cliff/coastalradarold.html. [Accessed: Jan. 14, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#footnotes",
    "href": "part-wrangling/03-data-cleaning.html#footnotes",
    "title": "22  Data Cleaning",
    "section": "",
    "text": "See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎\nThe philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎\nIt accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html",
    "href": "part-wrangling/04-strings.html",
    "title": "23  Working with Strings",
    "section": "",
    "text": "Objectives\nOne of the most common types of “messy” data involves strings. If the data is input by humans, well, … we suck at spelling [1], typing, and data input, so… it’s going to be messy. But, another type of messy data involves situations where multiple variables are stored in the same column, or where the same variable is stored across two different columns. Usually, when this type of messy data occurs, the data is stored in a string/character variable.\nThis chapter will teach you how to work with both messy spelling/data entry and multiple values in a single string, but we’ll focus on the second - you’ll learn the tools to handle the first case along the way.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#objectives",
    "href": "part-wrangling/04-strings.html#objectives",
    "title": "23  Working with Strings",
    "section": "",
    "text": "Use functions to perform find-and-replace operations\nUse functions to split string data into multiple columns/variables\nUse functions to join string data from multiple columns/variables into a single column/variable\n\n\n\nPerhaps one day you’ll be able to put this knowledge to use in a practical setting!",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#basic-operations",
    "href": "part-wrangling/04-strings.html#basic-operations",
    "title": "23  Working with Strings",
    "section": "\n23.1 Basic Operations",
    "text": "23.1 Basic Operations\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\nThe stringr cheatsheet by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n\nTable 23.1: Table of string functions in R and python. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\n\n\nTask\nR\nPython\n\n\n\nReplace pattern with replacement\n\n\nbase: gsub(pattern, replacement, x)\nstringr: str_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\npandas: x.str.replace(pattern, replacement) (not vectorized over pattern or replacement)\n\n\nConvert case\n\nbase: tolower(x), toupper(x)\nstringr: str_to_lower(x), str_to_upper(x) , str_to_title(x)\n\npandas: x.str.lower(), x.str.upper()\n\n\n\nStrip whitespace from start/end\n\nbase: trimws(x)\nstringr: str_trim(x) , str_squish(x)\n\npandas: x.str.strip()\n\n\n\nPad strings to a specific length\n\nbase: sprintf(format, x)\nstringr: str_pad(x, …)\n\npandas: x.str.pad()\n\n\n\nTest if the string contains a pattern\n\nbase: grep(pattern, x) or grepl(pattern, x)\nstringr: str_detect(x, pattern)\n\npandas: x.str.contains(pattern)\n\n\n\nCount how many times a pattern appears in the string\n\nbase: gregexpr(pattern, x) + sapply to count length of the returned list\nstringi: stri_count(x, pattern)\nstringr: str_count(x, pattern)\n\npandas: x.str.count(pattern)\n\n\n\nFind the first appearance of the pattern within the string\n\nbase: regexpr(pattern, x)\nstringr: str_locate(x, pattern)\n\npandas: x.str.find(pattern)\n\n\n\nFind all appearances of the pattern within the string\n\nbase: gregexpr\nstringr: str_locate_all(x, pattern)\n\npandas: x.str.findall(pattern)\n\n\n\nDetect a match at the start/end of the string\n\nbase: use regular expr.\nstringr: str_starts(x, pattern) ,str_ends(x, pattern)\n\npandas: x.str.startswith(pattern) , x.str.endswith(pattern)\n\n\n\nSubset a string from index a to b\n\nbase: substr(x, a, b)\nstringr: str_sub(x, a, b)\n\npandas: x.str.slice(a, b, step)\n\n\n\nConvert string encoding\n\nbase: iconv(x, encoding)\nstringr: str_conv(x, encoding)\n\npandas: x.str.encode(encoding)\n\n\n\n\n\n\n\nIn Table 23.1, multiple functions are provided for e.g. common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the stringr package, which is included in the tidyverse1.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "href": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "title": "23  Working with Strings",
    "section": "\n23.2 Converting strings to numbers",
    "text": "23.2 Converting strings to numbers\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\nTable 23.2: Different “messy” number formats\n\n\n\n\nint_col\nfloat_col\nmix_col\nmissing_col\nmoney_col\neu_numbers\nboolean_col\ncustom\n\n\n\n0\n1\n1.1\na\n1\n£1,000.00\n1.000.000,00\nTrue\nY\n\n\n1\n2\n1.2\n2\n2\n£2,400.00\n2.000.342,00\nFalse\nY\n\n\n2\n3\n1.3\n3\n3\n£2,400.00\n3.141,59\nTrue\nN\n\n\n3\n4\n4.7\n4\nnan\n£2,400.00\n34,25\nTrue\nN\n\n\n\n\n\n\nNumbers, currencies, dates, and times are written differently based on what country you’re in [2]. In computer terms, this is the locale, and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [3].\nLocales are something you may want to skip if you’re just starting out and you don’t work with code written by people in other countries. If you’re collaborating internationally, however, you may want to at least skim the section below to be aware of potential issues when locale-related problems crop up.\nIf you’ve never had to deal with the complexities of working on a laptop designed for one country using another country’s conventions, know that it isn’t necessarily the easiest thing to do.\nAdvanced: Locales\nFind your locale\n\n\n Type Get-WinSystemLocale into your CMD or powershell terminal.\n\n (10.4 and later) and  Type locale into your terminal\nGet set up to work with locales\nWhile this isn’t required, it may be useful and is definitely good practice if you’re planning to work with data generated internationally.\nThis article tells you how to set things up in linux . The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n\n Installing languages\n\n\n Change locales. Installing or creating new locales seems to be more complicated, and since I do not have a mac, I can’t test this out easily myself.\nWe’ll use Table 23.2 to explore different string operations focused specifically on converting strings to numbers.\n\n\nGet the data: Python\nR\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/number-formats.csv\")\n\n\n\n\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/number-formats.csv\", colClasses = \"character\")\n\nBy default, R tries to outsmart us and read the data in as numbers. I’ve disabled this behavior by setting colClasses='character' so that you can see how these functions work… but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don’t know how to disable it.\n\n\n\n\n\n\n\n\n\nConverting Columns Using Your Best Guess\n\n\n\nBoth R and Python have ways to “guess” what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you’re working with data that is already read in, how do you get R and Python to guess what type something is?\n\n\nR\nPython\n\n\n\nHere, R gets everything “right” except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn’t clearly numeric or doesn’t match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle mix_col, we would have to explicitly convert to numeric, causing the a to be converted to NA\n\nlibrary(dplyr)\nlibrary(readr)\ndf_guess &lt;- type_convert(df)\nstr(df_guess)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\nThe type_convert function has a locale argument; readr includes a locale() function that you can pass to type_convert that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\nlibrary(dplyr)\nlibrary(readr)\nfixed_df &lt;- type_convert(df) \nfixed_df2 &lt;- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\n\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\nAs in R, we can set the locale in Python to change how things are read in.\n\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Columns Directly\n\n\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in Section 8.4. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of ‘missing’ or NA variables that are not necessarily compatible.\n\n\nR\nPython\n\n\n\nHere, we use the across helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don’t work here, because they contain characters R doesn’t recognize as numeric characters.\n\nlibrary(dplyr)\n\ndf_numeric &lt;- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  NA NA NA NA\n##  $ custom     : num  NA NA NA NA\n\n\n\n\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Converting Y/N data\n\n\n\nThe next thing we might want to do is convert our custom column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\nWe could use factors/categorical variables, which have numeric values “under the hood”, but show up as labeled.\nWe could (in this particular case) test for equality with “Y”, but this approach would not generalize well if we had more than 2 categories.\nWe could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I’ve used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n\nR\nPython\n\n\n\n\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %&gt;%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %&gt;%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':    4 obs. of  11 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n\n\n\nWe’ve already done a brief demonstration of string methods in Python when we trimmed off the £ character. In this situation, it’s better to use the pandas replace method, which allows you to pass in a list of values and a list of replacements.\n\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers      float64\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#find-and-replace",
    "href": "part-wrangling/04-strings.html#find-and-replace",
    "title": "23  Working with Strings",
    "section": "\n23.3 Find and replace",
    "text": "23.3 Find and replace\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution2, and may introduce bugs if you use the same code to analyze new data with characters you haven’t anticipated, but in so many cases it’s also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\nI’ll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading [4] so that you know why find-and-replace isn’t (necessarily) the best option for locale-specific formatting.\n\n\n\n\n\n\nExample: find and replace\n\n\n\nLet’s start with the money column.\n\n\nR\nPython\n\n\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn’t work by default with type_convert, but as long as we mutate and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ‘,’ for decimal and ‘.’ for thousands separators.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n\n\n\nIn python, a similar approach doesn’t work out, because the pound sign is not handled correctly.\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## babel.numbers.NumberFormatError: '£1,000.00' is not a valid decimal number\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n# Remove £ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"£\")\n# Then parse the number\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n23.3.1 Example: Locale find-and-replace\nWe could also handle the locale issues using find-and-replace, if we wanted to…\n\n\nR\nPython\n\n\n\nNote that str_remove is shorthand for str_replace(x, pattern, \"\"). There is a little bit of additional complexity in switching “,” for “.” and vice versa - we have to change “,” to something else first, so that we can replace “.” with “,”. This is not elegant but it does work. It also doesn’t generalize - it will mess up numbers formatted using the US/UK convention, and it won’t handle numbers formatted using other conventions from other locales.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = str_remove(money_col, \"£\") %&gt;% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %&gt;%\n           parse_number())\n\n\n\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n## ValueError: Unable to parse string \"1.000.000.00\" at position 0\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"£\").\\\nstr.replace(',', '')\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...    eu_numbers  boolean_col custom\n## 0        1        1.1       a  ...  1.000.000.00         True      Y\n## 1        2        1.2       2  ...  2.000.342.00        False      Y\n## 2        3        1.3       3  ...      3.141.59         True      N\n## 3        4        4.7       4  ...         34.25         True      N\n## \n## [4 rows x 8 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "href": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "title": "23  Working with Strings",
    "section": "\n23.4 Separating multi-variable columns",
    "text": "23.4 Separating multi-variable columns\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\nIn pandas, we use x.str.split() to split columns in a DataFrame, in R we use the tidyr package’s separate_wider_xxx() series of functions.\n\n\n\n\n\n\nExample: Separating columns\n\n\n\nWe’ll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to separate the rate column into two new columns, cases and population.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate_wider_delim(table3, rate, delim = \"/\", names = c('cases', 'pop'), cols_remove = F)\n## # A tibble: 6 × 5\n##   country      year cases  pop        rate             \n##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            \n## 1 Afghanistan  1999 745    19987071   745/19987071     \n## 2 Afghanistan  2000 2666   20595360   2666/20595360    \n## 3 Brazil       1999 37737  172006362  37737/172006362  \n## 4 Brazil       2000 80488  174504898  80488/174504898  \n## 5 China        1999 212258 1272915272 212258/1272915272\n## 6 China        2000 213766 1280428583 213766/1280428583\n\n\n\n\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country    year               rate   cases         pop\n## 0  Afghanistan  1999.0       745/19987071     745    19987071\n## 1  Afghanistan  2000.0      2666/20595360    2666    20595360\n## 2       Brazil  1999.0    37737/172006362   37737   172006362\n## 3       Brazil  2000.0    80488/174504898   80488   174504898\n## 4        China  1999.0  212258/1272915272  212258  1272915272\n## 5        China  2000.0  213766/1280428583  213766  1280428583\n\nThis uses python’s multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g. cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#joining-columns",
    "href": "part-wrangling/04-strings.html#joining-columns",
    "title": "23  Working with Strings",
    "section": "\n23.5 Joining columns",
    "text": "23.5 Joining columns\nIt’s also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a ‘name’ column that has both pieces of information together.\n\n\n\n\n\n\nExample: Joining columns\n\n\n\nWe’ll use the table5 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table5.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to join the century and year columns into a new column, yyyy.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %&gt;%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 × 5\n##   country      yyyy century year  rate             \n##   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n\n\n\n\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#regular-expressions",
    "href": "part-wrangling/04-strings.html#regular-expressions",
    "title": "23  Working with Strings",
    "section": "\n23.6 Regular Expressions",
    "text": "23.6 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears &lt;- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n\n\n\n\n\nTo generate #1 albums, ‘jay –help’ recommends the -z flag. XKCD comics by Randall Munroe CC-A-NC 2.5.\n\nA regular expression is a sequence of characters that specify a match pattern to search for in a larger text [5]. Regular expressions may be used to specify find or find-and-replace operations on strings.\nRegular expressions can be extremely useful for cleaning and extracting data: they can replace misspellings, extract pieces of information from longer strings, and flexibly handle different ways people may input data. They may be incredibly powerful, but they can also be complicated to create and the expressions themselves may be cryptic and nearly impossible to decode.\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\nHere are some useful regular expressions3:\n\nValidate a phone number [6]: ^\\(*\\d{3}\\)*( |-)*\\d{3}( |-)*\\d{4}$\n\nCheck for first and last names [7]: ^[\\w'\\-,.][^0-9_!¡?÷?¿/\\\\+=@#$%ˆ&*(){}|~&lt;&gt;;:[\\]]{2,}$\n(This is a tricky proposition and this regular expression does make some assumptions about what characters are valid for names.)\nMatch a 5 or 9 digit zip code: (^\\d{5}$)|(^\\d{9}$)|(^\\d{5}-\\d{4}$)\n\n\nThese tasks are all well-suited for regular expressions. More complicated tasks, such as validating an email address, are less suited for regular expressions, though there are regular expressions that exist [8] for that task.\n\n\nI’ve assembled a YouTube playlist of different explanations of regular expressions, if you prefer that type of tutorial.\n\n\n\n\nThe following demonstrations are intended for advanced students: if you are just learning how to program, you may want to come back to these when you need them.\nThere is also an excellent site which helps you learn regular expressions via interactive tutorials, [9]. Another useful tool is [10]\n\n23.6.1 Regular Expression Basics\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\n\nR\nPython\n\n\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn &lt;- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n\n\n\nIn python, a regular expression is indicated by putting the character ‘r’ right before the quoted expression. This tells python that any backslashes in the string should be left alone – if R had that feature, we wouldn’t have to escape all the backslashes!\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\n\n\n\n\n\n23.6.2 Specifying repetition\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times – most useful when you’re looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\n\nR\nPython\n\n\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn &lt;- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone &lt;- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid &lt;- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance &lt;- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n\n\n\n\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## &lt;re.Match object; span=(0, 6), match='banana'&gt;\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## &lt;re.Match object; span=(0, 2), match='ba'&gt;\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## &lt;re.Match object; span=(1, 5), match='anan'&gt;\n\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## &lt;re.Match object; span=(7, 19), match='123-456-7890'&gt;\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## &lt;re.Match object; span=(27, 35), match='12345678'&gt;\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## &lt;re.Match object; span=(77, 91), match='$50,000,000.23'&gt;\n\n\n\n\n\n23.6.3 Matching Locations\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n\nR\nPython\n\n\n\n\naddress &lt;- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num &lt;- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet &lt;- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number\n\ncity &lt;- str_extract(address, \",.*,\") %&gt;% str_remove_all(\",\") %&gt;% str_trim()\n\nzip &lt;- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\nPython match objects contain 3 things: .span(), which has the start and end positions of the match, .string, which contains the original string passed into the function, and .group(), which contains the actual matching portion of the string.\n\nimport re\n\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n\n\n\n\n\n23.6.4 Capturing Information\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences.\nIn most languages, backreferences look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\n\n\nR\nPython\n\n\n\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants &lt;- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears &lt;- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n\n\n\n\nimport pandas as pd\nimport re\n\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n## ValueError: Cannot use a compiled regex as replacement pattern with regex=False\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n\n\n\n\n\n23.6.5 Putting it all Together\nWe can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n\nR\nPython\n\n\n\n\nstrings &lt;- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex &lt;- \"(walk|treat)\"\naddr_regex &lt;- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex &lt;- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 × 5\n##   text                                                   phone dog   addr  abab \n##   &lt;chr&gt;                                                  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\nimport pandas as pd\nimport re\n\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#sec-strings-refs",
    "href": "part-wrangling/04-strings.html#sec-strings-refs",
    "title": "23  Working with Strings",
    "section": "\n23.7 References",
    "text": "23.7 References\n\n\n\n\n[1] \nP. Norvig, “How to write a spelling corrector. Norvig.com,” Feb. 01, 2007. [Online]. Available: http://norvig.com/spell-correct.html. [Accessed: Mar. 08, 2023]\n\n\n[2] \nM. Ashour, “A Concise Guide to Number Localization,” Phrase. Feb. 2022 [Online]. Available: https://phrase.com/blog/posts/number-localization/. [Accessed: Jul. 25, 2022]\n\n\n[3] \nWikipedia Contributors, “Locale (computer software),” Wikipedia. Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Locale_(computer_software)&oldid=1082900932. [Accessed: Jul. 25, 2022]\n\n\n[4] \nA. Herrmann, “How to deal with international data formats in Python,” herrmann.tech. Feb. 2021 [Online]. Available: https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html. [Accessed: Jul. 25, 2022]\n\n\n[5] \n\n“Regular expression,” Wikipedia. Feb. 28, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Regular_expression&oldid=1142135804. [Accessed: Mar. 09, 2023]\n\n\n[6] \nJ. Atwood, “Regex use vs. Regex abuse. Coding horror,” Feb. 16, 2005. [Online]. Available: https://blog.codinghorror.com/regex-use-vs-regex-abuse/. [Accessed: Mar. 09, 2023]\n\n\n[7] \nCristianGuerrero, “Answer to \"regular expression for first and last name\". Stack overflow,” Aug. 24, 2017. [Online]. Available: https://stackoverflow.com/a/45871742. [Accessed: Mar. 09, 2023]\n\n\n[8] \nL. Ristic, “Validate email addresses with regular expressions in JavaScript. Stack abuse,” Oct. 14, 2021. [Online]. Available: https://stackabuse.com/validate-email-addresses-with-regular-expressions-in-javascript/. [Accessed: Mar. 09, 2023]\n\n\n[9] \nRegexOne, “RegexOne.” [Online]. Available: https://regexone.com/. [Accessed: Apr. 20, 2023]\n\n\n[10] \nLea Verou, “Reg explained. /a(b)/g,” 2017. [Online]. Available: https://projects.verou.me/regexplained/. [Accessed: Apr. 20, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#footnotes",
    "href": "part-wrangling/04-strings.html#footnotes",
    "title": "23  Working with Strings",
    "section": "",
    "text": "Many functions from stringr have somewhat faster functional equivalents in the stringi package, but the stringi package has a less “tidy” API, so it may be worth the slight slowdown to use stringr if your data isn’t huge because your code will be more readable.↩︎\nIt’s particularly hackish when you’re working with locale-specific settings [4], and in many cases you can handle locale issues much more elegantly.↩︎\nNote that these are written in generic regular expression text - to use them in R you will have to escape each and every \\ with another \\.↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html",
    "href": "part-wrangling/05-data-reshape.html",
    "title": "24  Reshaping Data",
    "section": "",
    "text": "Objectives\nBroadly, your objective while reading this chapter is to be able to identify datasets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should be master the following concepts:\nYou have learned some of the skills to tidy data in Chapter 23, and you’ll learn more in Chapter 26, but by the end of this chapter you will have many of the skills needed to wrangle the most common “messy” data sets into “tidy” form.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#objectives",
    "href": "part-wrangling/05-data-reshape.html#objectives",
    "title": "24  Reshaping Data",
    "section": "",
    "text": "Determine what data format is necessary to generate a desired plot or statistical model\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "href": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "title": "24  Reshaping Data",
    "section": "\n24.1 Tidy and Messy Data",
    "text": "24.1 Tidy and Messy Data\n\n24.1.1 Motivating Example\nConsider the spreadsheet screenshot in Figure 24.1.\n\n\n\n\n\nFigure 24.1: Spreadsheet intended for human consumption, from [1] (Chapter 3)\n\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. Figure 24.2 shows a restructured version of this data in a more compact rectangular format.\n\n\n\n\n\nFigure 24.2: Spreadsheet reorganized for data analysis\n\n\nIn Figure 24.2, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as “tidy” data: it has a single column for each variable and a single row for each observation.\n\n24.1.2 Defining Tidy data\nThe illustrations below are lifted from an excellent blog post [2] about tidy data; they’re reproduced here because\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\n\n\n\nFigure 24.3: Tidy data format, illustrated.\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the data set, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own data set.\n\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\n\nFigure 24.4: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\nExamples: Messy Data\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n\n\n\n\n\n\nTry it out: Classifying Messy Data\n\n\n\n\n\nProblem\nTable 1\n2\n3\n4\n5\n\n\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nEach variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first).\n\n\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach data set goes into its own table (or tibble, if you are using R)\nEach variable gets its own column",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#additional-reading",
    "href": "part-wrangling/05-data-reshape.html#additional-reading",
    "title": "24  Reshaping Data",
    "section": "\n24.2 Additional reading",
    "text": "24.2 Additional reading\n[3] - IBM SPSS ad that talks about the perils of spreadsheets\n[4] - assembled news stories involving spreadsheet mishaps",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#sec-pivot-operations",
    "href": "part-wrangling/05-data-reshape.html#sec-pivot-operations",
    "title": "24  Reshaping Data",
    "section": "\n24.3 Pivot operations",
    "text": "24.3 Pivot operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\n\nWide and Long format data. Source\n\nThe two operations we’ll learn here are wide -&gt; long and long -&gt; wide.\n\n\nPivoting from wide to long (and back) Source\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n24.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n##   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n##   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntba &lt;- table4a %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb &lt;- table4b %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %&gt;%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\n\nselect variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x &lt;- table4a %&gt;% mutate(id = \"cases\")\ntable4b.x &lt;- table4b %&gt;% mutate(id = \"population\")\n# Create one table\ntable4 &lt;- bind_rows(table4a.x, table4b.x)\n\ntable4_long &lt;- table4 %&gt;%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %&gt;%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy &lt;- table4_long %&gt;%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\nIn Pandas, pandas.melt(...) takes id_vars, value_vars, var_name, and value_name. Otherwise, it functions nearly exactly the same as pivot_longer; the biggest difference is that column selection works differently in python than it does in the tidyverse.\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n\nHere’s the melt/pivot_longer + pivot/pivot_wider version:\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year     cases    population\n## 0   Afghanistan  1999     745.0  1.998707e+07\n## 1   Afghanistan  2000    2666.0  2.059536e+07\n## 2        Brazil  1999   37737.0  1.720064e+08\n## 3        Brazil  2000   80488.0  1.745049e+08\n## 4         China  1999  212258.0  1.272915e+09\n## 5         China  2000  213766.0  1.280429e+09\n\n\n\n\n\n24.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\ntable2 %&gt;%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country    year     cases    population\n## 0     Afghanistan  1999.0     745.0  1.998707e+07\n## 1     Afghanistan  2000.0    2666.0  2.059536e+07\n## 2          Brazil  1999.0   37737.0  1.720064e+08\n## 3          Brazil  2000.0   80488.0  1.745049e+08\n## 4           China  1999.0  212258.0  1.272915e+09\n## 5           China  2000.0  213766.0  1.280429e+09\n\n\n\n\n\n\n\n\n\n\nTry it Out!\n\n\n\nIn the next section, we’ll be using the WHO surveillance of disease incidence data (link). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form (link). For demonstration purposes, I’ll continue using the messier 2020 data, but the link is no longer available on the WHO’s site.\nIt will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest.\n\n\nPreprocessing\nProblem\nR solution\nPython solution\n\n\n\nYou don’t have to understand what this code is doing just yet.\n\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\nlibrary(tibble)\nlibrary(dplyr)\n\ndownload.file(\"https://github.com/srvanderplas/datasets/raw/main/raw/2020_WHO_incidence_series.xls\", \"../data/2020_WHO_incidence_series.xls\")\nsheets &lt;- excel_sheets(\"../data/2020_WHO_incidence_series.xls\")\nsheets &lt;- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence &lt;- map_df(sheets, ~read_xls(path =\"../data/2020_WHO_incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 &lt;- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 &lt;- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"../data/2020_WHO_incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"../data/2020_who_disease_incidence.csv\")\n\n\n\nDownload the exported data here and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\nCan you make a line plot of cases of measles in Bangladesh over time?\n\nhead(disease_incidence)\n## # A tibble: 6 × 43\n##   WHO_REGION ISO_code Cname    Disease `2018` `2017` `2016` `2015` `2014` `2013`\n##   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 EMR        AFG      Afghani… CRS         NA     NA     NA      0      0      0\n## 2 EUR        ALB      Albania  CRS          0      0     NA     NA     NA      0\n## 3 AFR        DZA      Algeria  CRS         NA     NA      0      0     NA     NA\n## 4 EUR        AND      Andorra  CRS          0      0      0     NA     NA      0\n## 5 AFR        AGO      Angola   CRS         NA     NA     NA     NA     NA     NA\n## 6 AMR        ATG      Antigua… CRS          0      0      0      0      0      0\n## # ℹ 33 more variables: `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;,\n## #   `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;,\n## #   `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `1999` &lt;dbl&gt;,\n## #   `1998` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1994` &lt;dbl&gt;,\n## #   `1993` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1989` &lt;dbl&gt;,\n## #   `1988` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1984` &lt;dbl&gt;,\n## #   `1983` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1980` &lt;dbl&gt;\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(stringr)\nwho_disease &lt;- read_csv(\"../data/2020_who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long &lt;- who_disease %&gt;%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %&gt;%\n  rename(Country = Cname) %&gt;%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year),\n         cases = as.numeric(cases))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %&gt;%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"../data/2020_who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "href": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "title": "24  Reshaping Data",
    "section": "\n24.4 Example: Gas Prices Data",
    "text": "24.4 Example: Gas Prices Data\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table. \n\n\nGas prices at US EIA site\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\nBut looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n24.4.1 Setup: Gas Price Data Cleaning\nFor the next example, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl &lt;- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc &lt;- read_html(url)\ngas_prices_html &lt;- html_table(htmldoc, fill = T, trim = T)[[5]][,1:11]\n\n\n\n\nFirst 6 rows of gas prices data as read into R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-Month\nWeek 1\nWeek 1\nWeek 2\nWeek 2\nWeek 3\nWeek 3\nWeek 4\nWeek 4\nWeek 5\nWeek 5\n\n\n\nYear-Month\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\n\n\n1994-Nov\n\n\n\n\n\n\n11/28\n1.175\n\n\n\n\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.110\n01/30\n1.109\n\n\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n\n\n\n\n(‘Year-Month’, ‘Year-Month’)\n(‘Week 1’, ‘End Date’)\n(‘Week 1’, ‘Value’)\n(‘Week 2’, ‘End Date’)\n(‘Week 2’, ‘Value’)\n(‘Week 3’, ‘End Date’)\n(‘Week 3’, ‘Value’)\n(‘Week 4’, ‘End Date’)\n(‘Week 4’, ‘Value’)\n(‘Week 5’, ‘End Date’)\n(‘Week 5’, ‘Value’)\n(‘Unnamed: 11_level_0’, ‘Unnamed: 11_level_1’)\n(‘Unnamed: 12_level_0’, ‘Unnamed: 12_level_1’)\n\n\n\n0\n1994-Nov\nnan\nnan\nnan\nnan\nnan\nnan\n11/28\n1.175\nnan\nnan\nnan\nnan\n\n\n1\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\nnan\nnan\nnan\nnan\n\n\n2\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n3\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.11\n01/30\n1.109\nnan\nnan\n\n\n4\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Manual Formatting in Excel\n\n\n\n\n\nProblem\nSolution\nVideo\n\n\n\nAn excel spreadsheet of the data as downloaded in January 2023 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\nWhat steps are involved?\n\n\n\nCopy the year-month column, creating one vertical copy for every set of columns\nMove each block of two columns down to the corresponding vertical copy\nDelete empty rows\nFormat dates\nDelete empty columns\n\n\n\n\n\n\n\n\n\nFigure 24.5: Here is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting with Pivot Operations\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\nSteps to work through the gas prices data cleaning process\n\n\n\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names &lt;- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %&gt;%\n    # trim leading/trailing spaces\n    str_trim() %&gt;%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 11\n##   Year.Month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;      \n## 1 1994-Nov   \"\"          \"\"           \"\"          \"\"           \"\"         \n## 2 1994-Dec   \"12/05\"     \"1.143\"      \"12/12\"     \"1.118\"      \"12/19\"    \n## 3 1995-Jan   \"01/02\"     \"1.104\"      \"01/09\"     \"1.111\"      \"01/16\"    \n## 4 1995-Feb   \"02/06\"     \"1.103\"      \"02/13\"     \"1.099\"      \"02/20\"    \n## 5 1995-Mar   \"03/06\"     \"1.103\"      \"03/13\"     \"1.096\"      \"03/20\"    \n## 6 1995-Apr   \"04/03\"     \"1.116\"      \"04/10\"     \"1.134\"      \"04/17\"    \n## # ℹ 5 more variables: Week.3.Value &lt;chr&gt;, Week.4.Date &lt;chr&gt;,\n## #   Week.4.Value &lt;chr&gt;, Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;\n\n\n# gas_prices_raw &lt;- select(gas_prices_raw, -c(X, Date))\ngas_prices_long &lt;- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 × 3\n##   Year.Month variable     value\n##   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n\n\ngas_prices_sep &lt;- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %&gt;%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 × 4\n##   Year.Month week  variable value\n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n\n\ngas_prices_wide &lt;- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 × 4\n##   Year.Month week  Date    Value  \n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n\n\ngas_prices_date &lt;- gas_prices_wide %&gt;%\n  filter(nchar(Value) &gt; 0) %&gt;%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %&gt;%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %&gt;%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 × 2\n##   Date       Value\n##   &lt;chr&gt;      &lt;chr&gt;\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n\n\nlibrary(lubridate)\ngas_prices &lt;- gas_prices_date %&gt;%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %&gt;%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       Price.per.gallon\n##   &lt;date&gt;                &lt;dbl&gt;\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html.copy()\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n## MultiIndex([(         'Year-Month',          'Year-Month'),\n##             (             'Week 1',            'End Date'),\n##             (             'Week 1',               'Value'),\n##             (             'Week 2',            'End Date'),\n##             (             'Week 2',               'Value'),\n##             (             'Week 3',            'End Date'),\n##             (             'Week 3',               'Value'),\n##             (             'Week 4',            'End Date'),\n##             (             'Week 4',               'Value'),\n##             (             'Week 5',            'End Date'),\n##             (             'Week 5',               'Value'),\n##             ('Unnamed: 11_level_0', 'Unnamed: 11_level_1'),\n##             ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')],\n##            )\n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n## ['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date', 'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date', 'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\n\n# Set new column names\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\n# Drop extra columns on the end\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\ngas_prices_raw.head()\n##   Year-Month Week.1.Date  Week.1.Value  ... Week.4.Value  Week.5.Date Week.5.Value\n## 0   1994-Nov         NaN           NaN  ...        1.175          NaN          NaN\n## 1   1994-Dec       12/05         1.143  ...        1.088          NaN          NaN\n## 3   1995-Jan       01/02         1.104  ...        1.110        01/30        1.109\n## 4   1995-Feb       02/06         1.103  ...        1.101          NaN          NaN\n## 5   1995-Mar       03/06         1.103  ...        1.102          NaN          NaN\n## \n## [5 rows x 11 columns]\n\n\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\ngas_prices_long.head()\n##   Year-Month     variable  value\n## 0   1994-Nov  Week.1.Date    NaN\n## 1   1994-Dec  Week.1.Date  12/05\n## 2   1995-Jan  Week.1.Date  01/02\n## 3   1995-Feb  Week.1.Date  02/06\n## 4   1995-Mar  Week.1.Date  03/06\n\n\ngas_prices_sep = gas_prices_long\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\ngas_prices_sep.head()\n##   Year-Month variable  value week\n## 0   1994-Nov     Date    NaN    1\n## 1   1994-Dec     Date  12/05    1\n## 2   1995-Jan     Date  01/02    1\n## 3   1995-Feb     Date  02/06    1\n## 4   1995-Mar     Date  03/06    1\n\n\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\ngas_prices_wide.head()\n## variable          Date  Value\n## Year-Month week              \n## 1994-Dec   1     12/05  1.143\n##            2     12/12  1.118\n##            3     12/19  1.099\n##            4     12/26  1.088\n##            5       NaN    NaN\n\n\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n\ngas_prices_date.head()\n## variable Year-Month week       Date  Value  Year Month\n## 0          1994-Dec    1 1994-12-05  1.143  1994   Dec\n## 1          1994-Dec    2 1994-12-12  1.118  1994   Dec\n## 2          1994-Dec    3 1994-12-19  1.099  1994   Dec\n## 3          1994-Dec    4 1994-12-26  1.088  1994   Dec\n## 4          1994-Nov    4 1994-11-28  1.175  1994   Nov\n\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\ngas_prices['Price_per_gallon'] = gas_prices.Value\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\ngas_prices.head()\n## variable       Date Price_per_gallon\n## 0        1994-12-05            1.143\n## 1        1994-12-12            1.118\n## 2        1994-12-19            1.099\n## 3        1994-12-26            1.088\n## 4        1994-11-28            1.175\n\n\n\n\n\n\nWe’ll return to this example in Section 26.4 to demonstrate how you can use pivot operations and database merges together to complete this operation in a slightly different way.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#other-resources",
    "href": "part-wrangling/05-data-reshape.html#other-resources",
    "title": "24  Reshaping Data",
    "section": "\n24.5 Other resources",
    "text": "24.5 Other resources\n[5] - very nice task-oriented chapter that’s below the level addressed in this course but still useful",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#references",
    "href": "part-wrangling/05-data-reshape.html#references",
    "title": "24  Reshaping Data",
    "section": "\n24.6 References",
    "text": "24.6 References\n\n\n\n\n[1] \nQ. E. McCallum, Bad data handbook: Mapping the world of data problems, 1. ed. Beijing, Köln: O’Reilly, 2013. \n\n\n[2] \nJ. Lowndes and A. Horst, “Tidy data for efficiency, reproducibility, and collaboration,” Openscapes. Oct. 2020 [Online]. Available: https://www.openscapes.org/blog/2020/10/12/tidy-data//. [Accessed: Jul. 21, 2022]\n\n\n[3] \nInternational Business Machines, “The risks of using spreadsheets for statistical analysis,” The risks of using spreadsheets for statistical analysis. Nov. 2018 [Online]. Available: https://web.archive.org/web/20240415181938/https://www.statwks.com/wp-content/uploads/2018/11/ytw03240-usen-03\\_YTW03240USEN.pdf. [Accessed: Nov. 03, 2024]\n\n\n[4] \nP. O’Beirne, F. Hermans, T. Cheng, and M. P. Campbell, “Horror Stories,” European Spreadsheet Risks Interest Group Horror Stories. Oct. 2020 [Online]. Available: https://eusprig.org/research-info/horror-stories/. [Accessed: Nov. 03, 2024]\n\n\n[5] \nJ. Dougherty and I. Ilyankou, “Clean Up Messy Data,” in Hands-On Data Visualization, 1st ed., O’Reilly Media, 2021, p. 480 [Online]. Available: https://handsondataviz.org/. [Accessed: Nov. 03, 2024]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html",
    "href": "part-wrangling/05b-normal-forms.html",
    "title": "25  Normal Forms of Data",
    "section": "",
    "text": "Objectives\nThis chapter provides a bit of an ‘under-the-hood’ view of reshaping data and links it to some concepts in database design and data organization from the computer science view point.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#objectives",
    "href": "part-wrangling/05b-normal-forms.html#objectives",
    "title": "25  Normal Forms of Data",
    "section": "",
    "text": "Understand the origin of fsome of the parameter names in data manipulation functions, such as\n\nthe parameters key and value in pandas.melt\n\nwhy the long form of many data sets is called “tidy”\n\n\nDetermine the key variable(s) in a dataset",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#introduction",
    "href": "part-wrangling/05b-normal-forms.html#introduction",
    "title": "25  Normal Forms of Data",
    "section": "\n25.1 Introduction",
    "text": "25.1 Introduction\nNormal forms were developed in the 1970s by E.F. Codd as a theoretical framework to describe data structures in (relational) data bases. For the purpose of the considerations here, you can think of a data base as a bundle of data sets that are loosely connected - e.g. the data sets describe (different) aspects of the same objects, or they share some aspects (such as the same time or the same geography).\nWhat we mean by a data set, is a spread-sheet style (rectangular) representation, with rows representing observations and columns representing variables. We will assume that the first row in the spread sheet contains the names of the variables. Similarly, we will assume there are no row names - any existing row names can be made into a column in the data set.\nFor the purpose of assessing the normal form of a data set. we distinguish between two types of variables: keys and non-key variables.\n\n\n\n\n\n\nKey and Non-Key Variables\n\n\n\nThe key of a data frame is defined as the (set of) variable(s) that uniquely identifies each row.\nSimilarly, any variable that is not a key variable is a non-key variable.\n\n\nThere are various ways of recognizing key variables in a dataset: the easiest might be by their name; oftentimes a key variable is called an ‘identifier’, so watch out for variables with the letters ID in their name. Generally, the idea of a designed study and key variables are related: in a designed study, the combination of all design variables form a key. Any variables with values that are observed during the experiment, or collected after the study design is determined, are non-key variables.\nIn order to determine whether a set of variables forms a key, we will need to determine that there are no duplications in their combined values.\n\n\n\n\n\n\nDemo: is it a key?\n\n\n\nLet us assume, that we have the following dataset on measuring (repeatedly) different aspects and body parts of Wallabies. This data is part of the Australian data and story library OzDASL. See the help page for information about this data set and each of its variables.\n\nlibrary(knitr)\nwallaby &lt;- read.csv(\"../data/wallaby.csv\") \nhead(wallaby) |&gt; kable(caption=\"First few lines of the wallaby data.\")\n\n\nFirst few lines of the wallaby data.\n\nAnim\nSex\nLoca\nLeng\nHead\nEar\nArm\nLeg\nPes\nTail\nWeight\nAge\n\n\n\n45\n1\nG\nNA\n123\nNA\n59\n69\n59\n93\nNA\n14\n\n\n45\n1\nG\nNA\n178\n54\n90\n120\n92\n185\nNA\n28\n\n\n45\n1\nG\nNA\n250\n92\n130\n210\n142\n307\nNA\n49\n\n\n45\n1\nG\nNA\n324\n108\n174\n284\n205\n454\n290\n69\n\n\n45\n1\nG\nNA\n369\n129\n198\n340\n257\n568\n410\n83\n\n\n45\n1\nG\nNA\n408\n155\n237\n411\n308\n648\n570\n97\n\n\n\n\n\n\n\nStrategy\nIs Anim the key?\nIs Anim + Age the key?\n\n\n\nWhen determining whether this data set has a key, we might at first consider the variable Anim (animal number). However, the first couple of rows already show us that this variable is not uniquely describing a row/observation in the data. What about the combination of Anim and Age? In order for these two variables to be a key, their combination needs to be unique, i.e. for each animal, we can only have one set of measurements at any age. We can check whether that condition is fulfilled by tallying up the combination of Anim and Age.\n\n\n\nlibrary(dplyr)\nwallaby |&gt; \n  count(Anim, sort = TRUE) |&gt; \n  head() |&gt; \n  kable(caption=\"Anim by itself is not uniquely identifying rows in the data.\")\n\n\nAnim by itself is not uniquely identifying rows in the data.\n\nAnim\nn\n\n\n\n53\n50\n\n\n45\n49\n\n\n47\n49\n\n\n57\n49\n\n\n55\n47\n\n\n65\n42\n\n\n\n\n\nAnim by itself is not a key variable, because for some animal ids we have multiple sets of measurements.\n\n\n\nlibrary(dplyr)\nwallaby |&gt; \n  count(Anim, Age, sort = TRUE) |&gt; \n  head() |&gt; \n  kable(caption=\"All combinations of animal ID and an animal's age only refer to one set of measurements.\")\n\n\nAll combinations of animal ID and an animal’s age only refer to one set of measurements.\n\nAnim\nAge\nn\n\n\n\n44\n8\n1\n\n\n44\n22\n1\n\n\n44\n43\n1\n\n\n44\n63\n1\n\n\n44\n77\n1\n\n\n44\n91\n1\n\n\n\n\n\nThe combination of Anim and Age uniquely describes each observation, and is therefore a key for the data set.\n\n\n\n\n\n\n25.1.1 Benefits of Normal forms\nNormal forms are used to describe the state that a particular data set is in – lower normal forms can always be transformed into higher forms. This process is called normalization of data. Normalizing data has various benefits:\nNormalization\n\navoids data redundancies,\nreveals inconsistencies,\nsimplifies the data design,\nincreases lookup speeds (in data bases),\nand makes sets of data easier to maintain.\n\nMost often we are only interested in the first three normal forms. For memorization you can think of these forms as going along with\n\n\n\n\nImage of two keys as a memorization help for the three normal forms of data.\n\n\nThe key\nThe whole key\nand nothing but the key",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#first-normal-form",
    "href": "part-wrangling/05b-normal-forms.html#first-normal-form",
    "title": "25  Normal Forms of Data",
    "section": "\n25.2 First Normal Form",
    "text": "25.2 First Normal Form\nA table is in first normal form if\n\nevery entry in the table is a single value, and\nthe table has a key.\n\n\n\n\n\n\n\nDemo: Wallabies First Normal Form\n\n\n\nBelow is a snapshot of a reshaped version of the previous example, where all measurements for each animal are captured in the list variable measurements. While Anim now should be a key variable (presumably it uniquely identifies each animal), the data set is still not in first normal form, because the entries in the variable measurements are data sets by themselves, not just single values.\n\nlibrary(tidyr)\n\nwallaby2 &lt;- wallaby |&gt; \n  nest(.by = c(\"Anim\", \"Sex\", \"Loca\"), .key=\"measurements\") \nwallaby2 |&gt; head() \n## # A tibble: 6 × 4\n##    Anim   Sex Loca  measurements     \n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;list&gt;           \n## 1    45     1 G     &lt;tibble [49 × 9]&gt;\n## 2    47     1 G     &lt;tibble [49 × 9]&gt;\n## 3    53     2 G     &lt;tibble [50 × 9]&gt;\n## 4    57     2 G     &lt;tibble [49 × 9]&gt;\n## 5    65     2 G     &lt;tibble [42 × 9]&gt;\n## 6    79     2 K     &lt;tibble [39 × 9]&gt;\n\nIs Anim the key variable of wallaby2? For that we check whether the Anim variable is unique - and find out that it is not unique!\n\nwallaby2 |&gt; count(Anim, sort=TRUE) |&gt; head() |&gt;\n  knitr::kable(caption=\"We see in the frequency breakdown of `Anim`, that the animal ID for 125 is used twice, i.e. 125 seems to describe two different animals. This would indicate that animal numbers do not refer to an individual animal as the data description suggests.\")\n\n\nWe see in the frequency breakdown of Anim, that the animal ID for 125 is used twice, i.e. 125 seems to describe two different animals. This would indicate that animal numbers do not refer to an individual animal as the data description suggests.\n\nAnim\nn\n\n\n\n125\n2\n\n\n44\n1\n\n\n45\n1\n\n\n46\n1\n\n\n47\n1\n\n\n48\n1\n\n\n\n\n\nThis finding is a sign of an inconsistency in the data set - and just a first example of why we care about normal forms. Here, we identify the first entry in the results below as a completely different animal - it is male and lives in a different location. Most likely, this is a wrongly identified animal.\n\nwallaby |&gt; \n  filter(Anim == 125) |&gt; head() |&gt; \n  knitr::kable(caption=\"Based on the listing for the values of animal 125, the very first entry does not fit in well with the other values.\")\n\n\nBased on the listing for the values of animal 125, the very first entry does not fit in well with the other values.\n\nAnim\nSex\nLoca\nLeng\nHead\nEar\nArm\nLeg\nPes\nTail\nWeight\nAge\n\n\n\n125\n1\nHa\nNA\n566\n276\n354\n766\n642\n1310\n1590\n166\n\n\n125\n2\nH1\nNA\n112\nNA\n53\n62\n48\n93\nNA\n10\n\n\n125\n2\nH1\nNA\n166\n54\n80\n112\n79\n156\nNA\n26\n\n\n125\n2\nH1\nNA\n208\n74\n108\n157\n105\n222\nNA\n38\n\n\n125\n2\nH1\nNA\n255\n104\n134\n204\n147\n326\n160\n52\n\n\n125\n2\nH1\nNA\n301\n117\n160\n260\n187\n389\n240\n66\n\n\n\n\n\nSome detective work shows us that the additional entry for animal 125 is probably a missing entry for animal 126.\nWith a bit of detective work, we can identify animal 126 as the most likely candidate for the set of measurements wrongly attributed to animal 125 (see Figure 25.1). The orange point corresponds to the entry wrongly assigned to animal 125. Its timing (Age) and value (Weight) make it the best fit for animal 126.\n\n\n\n\n\n\n\nFigure 25.1: Scatterplots of Weight by Age, facetted by animal number. Only male specimen in location Ha are considered. The orange point shows the measurements of the entry wrongly assigned to animal 125 (which is female and lives in a different location).\n\n\n\n\nIn Figure 25.2 all measurements for wallaby 126 are shown by age (in days) when they were taken. The additional measurement wrongly assigned to animal 125 is shown in orange. All of the values are sensible within the grow curves of animal 126.\n\n\n\n\n\n\n\nFigure 25.2: Growth curves of animal 126 for all the measurements taken between days 50 and 200. In orange, the additional entry for animal 125 is shown. The values are consistent with animal 126’s growth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning the Wallaby data\n\n\n\nAs a direct result of the normalization step, we make a change (!!!) to the orignial data.\n\n# Cleaning Step 1\nwallaby_cleaner &lt;- wallaby |&gt; \n  mutate(Anim = ifelse(Anim==125 & Sex==1, 126, Anim))\n\nMaking any changes to a dataset should never be done light-heartedly, always be well argued and well-documented. For the present example, we keep the above investigation as argument, and the code to show the exact nature of the cleaning step.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#second-normal-form",
    "href": "part-wrangling/05b-normal-forms.html#second-normal-form",
    "title": "25  Normal Forms of Data",
    "section": "\n25.3 Second Normal Form",
    "text": "25.3 Second Normal Form\nA data set is in second normal form if\n\nit is in first normal form, and\nall non-key variables depend on all parts of the key (no split key).\n\nNote, that tables in 1st normal form with a single key variable are automatically in 2nd normal form.\nRegarding the example of the wallaby dataset, we see the dataset in its basic form is not in 2nd normal form, because the two non-key variables Sex (biological sex of the animal) and the animal’s location (Loca) only depend on the animal number Anim, and not on the Age variable.\n\nwallaby2 |&gt; group_by(Anim) \n## # A tibble: 78 × 4\n## # Groups:   Anim [77]\n##     Anim   Sex Loca  measurements     \n##    &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;list&gt;           \n##  1    45     1 G     &lt;tibble [49 × 9]&gt;\n##  2    47     1 G     &lt;tibble [49 × 9]&gt;\n##  3    53     2 G     &lt;tibble [50 × 9]&gt;\n##  4    57     2 G     &lt;tibble [49 × 9]&gt;\n##  5    65     2 G     &lt;tibble [42 × 9]&gt;\n##  6    79     2 K     &lt;tibble [39 × 9]&gt;\n##  7    81     2 K     &lt;tibble [36 × 9]&gt;\n##  8    87     1 G     &lt;tibble [34 × 9]&gt;\n##  9    92     2 G     &lt;tibble [28 × 9]&gt;\n## 10    93     2 W     &lt;tibble [28 × 9]&gt;\n## # ℹ 68 more rows\n\n\n25.3.1 Normalization: 1st NF to 2nd NF\nWe can bring any data set that is in 1st normal form into second normal form by splitting the data set into two parts: all non-key elements that only depend on a part of the key are moved into a second data set, together with a copy of the part of the key that those elements rely on. All duplicate rows in the second dataset then need to be removed.\nThis construction results in two tables that are in 2nd normal form.\n\n\n\n\n\n\nDemo: Getting the wallaby data into 2nd normal form\n\n\n\nIn the example of the wallaby data, we have identified the non-key variables Sex and Loca to only depend on the animal’s number - i.e. the values of these variables are animal-specific demographics, that do not change over the course of their lifetime.\nWe separate those variables into the data set wallaby_demographics and reduce the number of rows by finding a tally of the number of rows we summarize.\n\nwallaby_demographics &lt;- wallaby_cleaner |&gt; \n  select(Anim, Sex, Loca) |&gt;\n  count(Anim, Sex, Loca) \n# Don't need the total number\n\nOnce we have verified that Anim is a key for wallaby_demographics, we know that this table is in 2nd normal form.\n\nwallaby_demographics |&gt; \n  count(Anim, sort=TRUE) |&gt; \n  head()\n##   Anim n\n## 1   44 1\n## 2   45 1\n## 3   46 1\n## 4   47 1\n## 5   48 1\n## 6   53 1\n\nWith the key-splitting variables Sex and Loca being taken care of in the second dataset, we can safely remove those variables from the original data. To preserve the original, we actually create a separate copy called wallaby_measurements:\n\nwallaby_measurements &lt;- wallaby_cleaner |&gt; \n  select(-Sex, -Loca) \n\nwallaby_measurements |&gt;\n  head()\n##   Anim Leng Head Ear Arm Leg Pes Tail Weight Age\n## 1   45   NA  123  NA  59  69  59   93     NA  14\n## 2   45   NA  178  54  90 120  92  185     NA  28\n## 3   45   NA  250  92 130 210 142  307     NA  49\n## 4   45   NA  324 108 174 284 205  454    290  69\n## 5   45   NA  369 129 198 340 257  568    410  83\n## 6   45   NA  408 155 237 411 308  648    570  97\n\nThe wallaby_measurements dataset has the combination of Anim and Age as a key, and all of the variables are measurement that depend on both the animal and its age. This data set is therefore also in second normal form.\n\n\nIn the normalization process we spotted one inconsistency (animal 125), which we resolved earlier, and we reduced the overall size of the data has been reduced from 12 x 1463 = 17,556 to the sum of 4 x 77 = 308 and 10 x 1463 = 14,630 for the wallaby_demographics and the wallaby_measurements data, respectively.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#third-normal-form",
    "href": "part-wrangling/05b-normal-forms.html#third-normal-form",
    "title": "25  Normal Forms of Data",
    "section": "Third Normal Form",
    "text": "Third Normal Form\nA table is in third normal form if\n\nthe table is in 2nd normal form, and\nno non-key variable determines the values of another non-key variable.\n\nBeing able to determine whether any (combination of) non-key variables determine(s) the values of other non-key variables is a hard task. We would need to be the data experts in a lot of areas, and even then we might miss some dependencies and declare a table to be in third normal form when, in fact, it is not.\nRather than this form of the third normal form, we often employ a stricter normal form that only allows a single non-key variable to ensure that there are no dependencies left between non-key variables. This form is also called key-value pairs\n\n25.3.2 Key-Value Pairs (KVP)\nA table is in key-value representation if\n\nthe table is in 2nd form, and\nthere is only a single non-key column.\n\nNow we are finally at the point that we are connecting to the previous section on reshaping data. The way to bring data into key-value representation, is a transformation from a wide data form to a long form as described in Section 24.3 on pivot operations.\n\n\n\n\n\n\nDemo: Wallaby key-value-pairs\n\n\n\nIn the previous section, we separated the original wallaby data into two parts: the wallaby_demographics and wallaby_measurements data sets.\nThere are two different ways of bringing datasets into key-value pairs using these datasets.\n\n\nSplitting into parts\nPivoting (R)\nPivoting (Python)\n\n\n\nFor the wallaby_demographics, we will split the data into two parts: wallaby_sex and wallaby_location:\n\nwallaby_sex &lt;- wallaby_demographics |&gt; select(Anim, Sex)\nwallaby_Location &lt;- wallaby_demographics |&gt; select(Anim, Loca)\n\n\n\nAnother approach to bringing a dataset into key-value pairs is to summarize the values of a set of variables by introducing a new key for the variable names.\n\nwallaby_measurements_long &lt;- wallaby_measurements |&gt; \n  pivot_longer(cols = 'Leng':'Weight', names_to=\"Traits\", values_to=\"Measurements\", \n               values_drop_na = TRUE)\n\ndim(wallaby_measurements_long)\n## [1] 9905    4\nhead(wallaby_measurements_long)\n## # A tibble: 6 × 4\n##    Anim   Age Traits Measurements\n##   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;\n## 1    45    14 Head            123\n## 2    45    14 Arm              59\n## 3    45    14 Leg              69\n## 4    45    14 Pes              59\n## 5    45    14 Tail             93\n## 6    45    28 Head            178\n\nBy specifying values_drop_na as TRUE we exclude all measurements that could not be taken, such as Weight for about the first 50 days of age, and Leng for the animal until the joey is ready to leave the safety of the pouch. Note, that while this normalization step might save space, the structural nature of missing values might be further hidden.\n\n\nAnother approach to bringing a dataset into key-value pairs is to summarize the values of a set of variables by introducing a new key for the variable names.\n\nimport pandas as pd\n\nwallaby_measurements_long = pd.melt(r.wallaby_measurements, \n  id_vars=['Anim', 'Age'], \n  var_name = 'Traits',\n  value_name = 'Measurements')\n  \nwallaby_measurements_long = wallaby_measurements_long[\n    wallaby_measurements_long['Measurements'] &gt;= 0\n  ]\n\nwallaby_measurements_long  \n##         Anim  Age  Traits  Measurements\n## 24      45.0  377    Leng           844\n## 25      45.0  405    Leng           855\n## 26      45.0  432    Leng           873\n## 27      45.0  482    Leng           902\n## 28      45.0  517    Leng           905\n## ...      ...  ...     ...           ...\n## 11699  125.0  289  Weight         19840\n## 11700  125.0  297  Weight         19330\n## 11701  125.0  310  Weight         20910\n## 11702  125.0  332  Weight         23770\n## 11703  125.0  486  Weight         25000\n## \n## [9905 rows x 4 columns]\n\nMissing values in the R data set are interpreted as the value -2147483648 in python. We need to ensure that we remove all of these values (or convert them to NaN)",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#next-steps",
    "href": "part-wrangling/05b-normal-forms.html#next-steps",
    "title": "25  Normal Forms of Data",
    "section": "\n25.4 Next steps",
    "text": "25.4 Next steps\nUnits of measurements\nInvestigation of missing values: missing at random?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#other-resources",
    "href": "part-wrangling/05b-normal-forms.html#other-resources",
    "title": "25  Normal Forms of Data",
    "section": "\n25.5 Other resources",
    "text": "25.5 Other resources\nFrom Decomplexify’s Youtube channel: Learn Database Normalization - 1NF, 2NF, 3NF, 4NF, 5NF",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05b-normal-forms.html#references",
    "href": "part-wrangling/05b-normal-forms.html#references",
    "title": "25  Normal Forms of Data",
    "section": "\n25.6 References",
    "text": "25.6 References",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Normal Forms of Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html",
    "href": "part-wrangling/06-data-join.html",
    "title": "26  Joining Data",
    "section": "",
    "text": "Objectives\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#objectives",
    "href": "part-wrangling/06-data-join.html#objectives",
    "title": "26  Joining Data",
    "section": "",
    "text": "Identify columns (keys) which can be used to join separate but related tables\nSketch/plan out join operations based on matching keys and given objectives\nImplement planned join operations in R or python\nIdentify when join operations have not completed successfully by looking for duplicated rows, number of rows/columns in the finished object, and missing value counts.\n\n\n\n\n\n\n\nRelational Data Example: Primary School Records\n\n\n\n\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational:\n\neach class has both a teacher and a set of students\neach class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#vocabulary",
    "href": "part-wrangling/06-data-join.html#vocabulary",
    "title": "26  Joining Data",
    "section": "\n26.1 Vocabulary",
    "text": "26.1 Vocabulary\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nKeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#illustrating-joins",
    "href": "part-wrangling/06-data-join.html#illustrating-joins",
    "title": "26  Joining Data",
    "section": "\n26.2 Illustrating Joins",
    "text": "26.2 Illustrating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nThe next several sections will show animations demonstrating the different types of joins.\n\n26.2.1 Mutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\n26.2.1.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nJoins in base R are accomplished with the merge command.\nSpecify the keys to join by using by (or by.x and by.y if the column names are different in the two tables). Specify the rows to keep using all or all.x and all.y. By default, R will merge on any variables that have the same names in each table.\n\nmerge(x, y, by = \"v1\", all = F) # inner join\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nmerge(x, y, by = \"v1\", all = T) # full join\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\nmerge(x, y, by = \"v1\", all.x = T) # left join\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 &lt;NA&gt;\nmerge(x, y, by = \"v1\", all.y = T) # right join\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 &lt;NA&gt; y4\n\n\n\ndplyr contains functions that specifically implement mutating joins separately, primarily for code readability.\n\nlibrary(dplyr)\ninner_join(x, y)\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nleft_join(x, y)\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 &lt;NA&gt;\nright_join(x, y)\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 &lt;NA&gt; y4\nfull_join(x, y)\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\n\n\n\nMutating joins in pandas are accomplished with the merge command. The join type can be specified using the how parameter (left, right, outer, inner, cross). Specify the keys to join by using on (or left_on and right_on if the column names are different in the two tables).\n\nimport pandas as pd\npd.merge(x, y) # inner join (how = 'inner' is default)\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\npd.merge(x, y, how = 'left')\n##    v1  v2   v3\n## 0   1  x1   y1\n## 1   2  x2   y2\n## 2   3  x3  NaN\npd.merge(x, y, how = 'right')\n##     v1   v2  v3\n## 0  1.0   x1  y1\n## 1  2.0   x2  y2\n## 2  4.0  NaN  y4\npd.merge(x, y, how = 'outer') # full join\n##     v1   v2   v3\n## 0  1.0   x1   y1\n## 1  2.0   x2   y2\n## 2  3.0   x3  NaN\n## 3  4.0  NaN   y4\n\n\n\n\n\n26.2.2 Demo: Mutating Joins\n\n\nR\nPython\n\n\n\n\nlibrary(tibble)\nlibrary(dplyr)\nt1 &lt;- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 &lt;- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n## # A tibble: 2 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n## # A tibble: 4 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n\n\n\n\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n\nAn outer join keeps everything - all rows, all columns.\n\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  C  NaN  4.0\n## 3  D  3.0  5.0\n\n\n\n\nI’ve included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins here [1].\n\n26.2.3 Filtering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\n26.2.3.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nSemi and anti joins aren’t available by default in base R. You have to do multiple stages of operations to get either one to work.\n\n## Semi-join\n# First, do an inner join\ninnerxy = merge(x, y, all = F)\ninnerxy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nsemixy = innerxy[,names(innerxy)%in% names(x)]\nsemixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n## Anti-join\n# First, do an outer join\nouterxy = merge(x, y, all = T)\nouterxy\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\n# Then, drop any rows with NAs\nantixy = na.omit(outerxy)\nantixy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nantixy = antixy[,names(antixy) %in% names(x)]\nantixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n\n\n\nlibrary(dplyr)\nsemi_join(x, y)\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\nanti_join(x, y)\n##   v1 v2\n## 1  3 x3\n\n\n\nIn pandas, we have to be a bit tricky to get semi and anti joins.\n\nimport pandas as pd\n# First, we merge the two data frames (inner by default)\nsemixy = pd.merge(x, y) # Semi join\nsemixy\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\n\n# Then, we drop the extra columns\nsemixy = semixy[semixy.columns.intersection(x.columns)]\nsemixy\n##    v1  v2\n## 0   1  x1\n## 1   2  x2\n\n\n# This syntax keeps track of which rows are from which table\nouter = x.merge(y, how='outer', indicator=True)\nouter\n##     v1   v2   v3      _merge\n## 0  1.0   x1   y1        both\n## 1  2.0   x2   y2        both\n## 2  3.0   x3  NaN   left_only\n## 3  4.0  NaN   y4  right_only\n# Then we drop any rows that aren't 'left_only'\nantixy = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\nantixy\n##     v1  v2   v3\n## 2  3.0  x3  NaN\n# Then we drop any cols that aren't in x\nantixy = antixy[antixy.columns.intersection(x.columns)]\nantixy\n##     v1  v2\n## 2  3.0  x3\n\n\n\n\n\n26.2.4 Set Operations\nWhen talking about set operations, we start with two different data frames than those used above:\n\n\n\nUnion\nUnion All\nIntersection\nSet Difference\n\n\n\nAll unique rows from x and y\n\nOr, all unique rows from y and x.\n\n\n\nAll rows from x and y, keeping duplicate rows.\n\nThis is fundamentally the same as an rbind or bind_rows operation.\n\n\nCommon rows in x and y, keeping only unique rows.\n\n\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n\n\n\n\n\n26.2.4.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\n\nunionxy = unique(rbind(x, y))\nunionxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 5  2  b\n\nunionallxy = rbind(x, y)\nunionallxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  1  a\n## 5  2  b\n\nintersectxy = merge(x, y, all = F)\nintersectxy\n##   v1 v2\n## 1  1  a\n\nIt is possible to get set difference and intersection for data frames by applying the base methods setdiff and intersect, but dplyr does this by overriding those defaults, so it’s easier to just use that.\n\n\n\nlibrary(dplyr)\nunion(x, y)\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  2  b\nunionall(x, y)\n## Error in unionall(x, y): could not find function \"unionall\"\nsetdiff(x, y)\n##   v1 v2\n## 1  1  b\n## 2  2  a\nsetdiff(y, x)\n##   v1 v2\n## 1  2  b\nintersect(x, y)\n##   v1 v2\n## 1  1  a\n\n\n\n\nimport pandas as pd\n\n# Union\npd.concat([x, y]).drop_duplicates(keep = False)\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n## 1  2.0  b\n\n# Union all\npd.concat([x, y])\n##     v1 v2\n## 0  1.0  a\n## 1  1.0  b\n## 2  2.0  a\n## 0  1.0  a\n## 1  2.0  b\n\n# Intersection\nintersect = x.merge(y, how='inner')\nintersect\n##     v1 v2\n## 0  1.0  a\n\n# Set Difference\nsetdiffxy = x.merge(y, how='outer', indicator=True)\nsetdiffxy = setdiffxy[(setdiffxy._merge=='left_only')].drop('_merge', axis = 1)\nsetdiffxy\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n\nsetdiffyx = x.merge(y, how='outer', indicator=True)\nsetdiffyx = setdiffyx[(setdiffyx._merge=='right_only')].drop('_merge', axis = 1)\nsetdiffyx\n##     v1 v2\n## 3  2.0  b",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#example-nyc-flights",
    "href": "part-wrangling/06-data-join.html#example-nyc-flights",
    "title": "26  Joining Data",
    "section": "\n26.3 Example: NYC Flights",
    "text": "26.3 Example: NYC Flights\nWe’ll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you’ll recall, I had to use a small sample the last time we played with this data…). So before we can work with this data, we have to load the tables into Python.\n\n\n\n\n\n\nLoading Data\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"../data/\")\n## &lt;SQLiteConnection&gt;\n##   Path: /home/susan/Projects/Class/stat-computing-r-python/data/nycflights13.sqlite\n##   Extensions: TRUE\n\n\n\n\nimport sqlite3\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\ncur = con.cursor()\n\n\n\n\n\n\nI am not going to cover SQLITE commands here - I’m just going to use the bare minimum, but you can find a very nice introduction to python and SQLITE at datacarpentry [2], and an introduction to the dbplyr package for a nice R-SQLITE interface.\n\n\n\n\n\n\nTry it out: Understanding Relational Data\n\n\n\n\n\nProblem\nSolution\n\n\n\nSketch a diagram of which fields in each table match fields in other tables. Use the data documentation to help you with your sketch.\n\n\nhere (scroll down a bit).\n\n\n\n\n\n\n\n\n\n\n\nExample: Mutating Joins\n\n\n\nThese functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let’s determine whether there’s a relationship between the age of a plane and its delays.\n\n\nR\nPython\n\n\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age &lt;- planes %&gt;%\n  mutate(age = 2013 - year) %&gt;% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane &lt;- flights %&gt;%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres &lt;- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there’s much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren’t enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f615c297cd0&gt;\n\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f615c297710&gt;",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "href": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "title": "26  Joining Data",
    "section": "\n26.4 Example: Gas Prices Data",
    "text": "26.4 Example: Gas Prices Data\nLet’s return to the gas price data introduced in Section 24.4. I’ve repeated the setup chunks here for you to read in the data appropriately.\n\n26.4.1 Setup: Gas Price Data Cleaning\nFor the next example, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting using merge + pivot\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\nYou can start with the gas_prices_raw\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\n\nWe’ll use the same data cleaning function as before:\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n## Error in set_names(., fix_gas_names(names(.))): could not find function \"set_names\"\n\nhead(gas_prices_raw)\n## Error: object 'gas_prices_raw' not found\n\n\ngas_prices_dates &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\n## Error: object 'gas_prices_raw' not found\ngas_prices_values &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n## Error: object 'gas_prices_raw' not found\n\nhead(gas_prices_dates)\n## Error: object 'gas_prices_dates' not found\nhead(gas_prices_values)\n## Error: object 'gas_prices_values' not found\n\n\ngas_prices_dates_long &lt;- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\n## Error: object 'gas_prices_dates' not found\ngas_prices_values_long &lt;- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n## Error: object 'gas_prices_values' not found\n\nhead(gas_prices_dates_long)\n## Error: object 'gas_prices_dates_long' not found\nhead(gas_prices_values_long)\n## Error: object 'gas_prices_values_long' not found\n\n\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean &lt;- gas_prices_dates_long %&gt;%\n  filter(month_day != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %&gt;% \n           ymd())\n## Error: object 'gas_prices_dates_long' not found\n\ngas_prices_values_long_clean &lt;- gas_prices_values_long %&gt;%\n  filter(price_per_gallon != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n## Error: object 'gas_prices_values_long' not found\n\nhead(gas_prices_dates_long_clean)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices_values_long_clean)\n## Error: object 'gas_prices_values_long_clean' not found\n\n\ngas_prices &lt;- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %&gt;%\n  select(Date, price_per_gallon)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices)\n## Error: object 'gas_prices' not found\n\n\n\n\ngas_prices_raw = gas_prices_html.copy()\n## NameError: name 'gas_prices_html' is not defined\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n## NameError: name 'gas_prices_raw' is not defined\n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## NameError: name 'fix_gas_names' is not defined\ncolnames\n## NameError: name 'colnames' is not defined\n\n# Set new column names\ngas_prices_raw.columns = colnames\n## NameError: name 'colnames' is not defined\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_raw.head()\n## NameError: name 'gas_prices_raw' is not defined\n\n\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_dates.head()\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values.head()\n## NameError: name 'gas_prices_values' is not defined\n\n\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n## NameError: name 'gas_prices_values' is not defined\n\ngas_prices_dates_long.head()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_values_long.head()\n## NameError: name 'gas_prices_values_long' is not defined\n\n\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna().copy()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n## NameError: name 'gas_prices_dates_long_clean' is not defined\n\n\ngas_prices_values_long_clean = gas_prices_values_long.dropna().copy()\n## NameError: name 'gas_prices_values_long' is not defined\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## NameError: name 'gas_prices_values_long_clean' is not defined\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n## NameError: name 'gas_prices_values_long_clean' is not defined\n\ngas_prices_dates_long_clean.head()\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_values_long_clean.head()\n## NameError: name 'gas_prices_values_long_clean' is not defined\n\n\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices.head()\n## NameError: name 'gas_prices' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#sec-data-join-refs",
    "href": "part-wrangling/06-data-join.html#sec-data-join-refs",
    "title": "26  Joining Data",
    "section": "\n26.5 References",
    "text": "26.5 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nThe Carpentries, “Accessing SQLite Databases Using Python and Pandas,” Data Analysis and Visualization in Python for Ecologists. 2022 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql.html. [Accessed: Nov. 03, 2024]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html",
    "href": "part-wrangling/07-datetime.html",
    "title": "27  Dates and Times",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#objectives",
    "href": "part-wrangling/07-datetime.html#objectives",
    "title": "27  Dates and Times",
    "section": "",
    "text": "Understand the complexities of working with datetime data\nCreate datetime formatted data from character and numeric encodings\nFormat/print datetime data in the desired format",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "href": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "title": "27  Dates and Times",
    "section": "\n27.1 Why Dates and Times are hard",
    "text": "27.1 Why Dates and Times are hard\nI’m going to let Tom Scott deliver this portion of the material for me, as his times and timezones video is excellent and entertaining.\n\n\n\n\nThere is also an excellent StackOverflow question [1] and answers [3] demonstrating exactly how times and time zones can get very confusing even in relatively simple circumstances.\nLong story short, we will be using libraries in R and python which handle some of these complexities for us, because dates, times, and timezones are hard and we really don’t want to know exactly how hard they are. The libraries I’ve chosen for this are datetime in Python (used by Pandas), and lubridate in R.\n\n\n\n\n\n\nTry It Out - Getting Set up\n\n\n\n\n\nR\nPython\n\n\n\n\ninstall.packages(\"lubridate\")\n\n\nlibrary(lubridate)\n# get current date/time\ntoday()\n## [1] \"2025-08-16\"\nnow()\n## [1] \"2025-08-16 10:48:22 CDT\"\n\nLubridate cheat sheet Lubridate documentation\n\n\n\npip install datetime\n\nOr, if you prefer IPython magic…\n\n%pip install datetime\n\n\nimport datetime\n\ntoday = datetime.date.today()\ntoday\n## datetime.date(2025, 8, 16)\nprint(today)\n## 2025-08-16\n\nnow = datetime.datetime.now()\nnow\n## datetime.datetime(2025, 8, 16, 10, 48, 23, 162897)\nprint(now)\n## 2025-08-16 10:48:23.162897\n\npandas datetime documentation",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#getting-started",
    "href": "part-wrangling/07-datetime.html#getting-started",
    "title": "27  Dates and Times",
    "section": "\n27.2 Getting Started",
    "text": "27.2 Getting Started\nLet’s differentiate between three types of data which refer to a point in time:\n\na date\n\na time within a day\na date-time - a specific time on a specific date\n\nNow, let’s think about all of the different ways we can specify dates. The table below has examples along with strptime formats that are used in both R and python for telling the computer which date format is used.\n\n\nTable 27.1: Different ways to specify dates and times.\n\n\n\n\n\n\n\n\n\n\n\nExample\nType\nNotes\n\nstrptime format\n\n\n\n1\nJanuary 12, 2023\ndate\nCommon in US/N. America\n%B %d, %Y\n\n\n2\n12 January 2023\ndate\nCommon in Europe\n%d %B %Y\n\n\n3\n01/12/2023\ndate\nCommon in US\n%m/%d/%Y\n\n\n4\n1/12/23\ndate\nCommon in US\n%m/%d/%y\n\n\n5\n12/01/2023\ndate\nCommon in Europe/Asia\n%d/%m/%Y\n\n\n6\n2023-01-12\ndate\nISO 8601 standard\n(automatically sorts chronologically)\n%Y-%m-%d\nor %F\n\n\n7\n12 2023\ndate\nday of year + year\n%j %Y\n\n\n8\n9:23 PM\ntime\n12h time\n%I:%M %p\n\n\n9\n21:23\ntime\n24h time (military time)\n%H:%M\nor %R\n\n\n10\n21:23:05\ntime\n24h time (with seconds)\n%H:%M:%S\nor %T\n\n\n11\n2023-01-12T21:23:05\ndatetime\nISO 8601 international standard\n%FT%T\n\n\n\n\n\n\nNote that rows 4 and 5 of Table 27.1 are ambiguous if you don’t know what location your data comes from - the dates could refer to December 1, 2023 or January 12, 2023. This only gets worse if you use 2-digit years.\nThere are three main ways that you might want to create a date/time [4]:\n\nFrom a string\nFrom individual date/time components\nFrom an existing date/time object",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "href": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "title": "27  Dates and Times",
    "section": "\n27.3 Creating Dates and Times",
    "text": "27.3 Creating Dates and Times\n\n27.3.1 Creation from Strings\nDates and times are often stored in tabular formats as strings. In some cases, these are read in and automatically formatted as date-times, but in other situations, you have to specify the format yourself.\n\n27.3.1.1 Demo: Datetimes from Strings\nLet’s use some data from the US Geological Service with records of earthquakes with magnitude greater than 6 on the Richter scale that occurred between January 1, 2000 and January 1, 2023. You can pull this data yourself using https://earthquake.usgs.gov/earthquakes/map/, but you can also access a CSV of the data here.\n\n\nR + lubridate\nBase R\nPandas\n\n\n\n\nlibrary(lubridate)\nquake &lt;- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n\nBy default, read.csv reads the time information in as a character variable.\n\nlibrary(readr)\nquake2 &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake2)\n## spc_tbl_ [3,484 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ #EventID         : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : POSIXct[1:3484], format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n##  $ Latitude         : num [1:3484] -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num [1:3484] 171 -124 179 -101 -173 ...\n##  $ Depth/km         : num [1:3484] 10 17.9 73 18 38 ...\n##  $ Author           : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr [1:3484] \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num [1:3484] 6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr [1:3484] \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `#EventID` = col_character(),\n##   ..   Time = col_datetime(format = \"\"),\n##   ..   Latitude = col_double(),\n##   ..   Longitude = col_double(),\n##   ..   `Depth/km` = col_double(),\n##   ..   Author = col_character(),\n##   ..   Catalog = col_character(),\n##   ..   Contributor = col_character(),\n##   ..   ContributorID = col_character(),\n##   ..   MagType = col_character(),\n##   ..   Magnitude = col_double(),\n##   ..   MagAuthor = col_character(),\n##   ..   EventLocationName = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\nHowever, if we use readr::read_csv, the data is correctly read in as a POSIXct format, which is how R indicates that something is a datetime object.\nIf we want to directly convert the Time column in quake to a datetime, we can use the lubridate package, which has helper functions ymd_hms, ymd, and more. Our data is formatted in ISO 8601 standard format, which means we can easily read it in with ymd_hms() .\n\nlibrary(lubridate)\nlibrary(dplyr)\nquake &lt;- quake %&gt;% \n  mutate(dateTime = ymd_hms(Time))\nstr(quake)\n## 'data.frame':    3484 obs. of  14 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  $ dateTime         : POSIXct, format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n\nWe can then test whether quake$dateTime is the same as quake2$Time :\n\nall.equal(quake2$Time, quake$dateTime)\n## [1] TRUE\n\nSo in the case that your data is not automatically read in as a date-time, you can use the helper functions from lubridate (ymd_hms, ymd, mdy, …) to convert strings to date-time data.\n\n\nAs lovely as the lubridate package is, there are some situations where using the tidyverse may not be desirable or even allowed. It is helpful to know how to solve this problem in base R, even if 99% of the time we can use the much easier-to-remember lubridate package.\nIn this case, we would use the as.POSIXct function, and we probably want to have the reference page up (run ?strptime in the R console to pull up the help page).\nWe’ll need to get the codes that tell R what format our datetimes use - you can use Table 27.1, if you like, or read the as.POSIXct help page to see all possible format codes.\n\nquake &lt;- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\nquake$dateTime2 &lt;- as.POSIXct(quake$Time, \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\nSo using as.POSIXct we do not get the convenient handling of time zones that we got using ymd_hms, but we can set the time zone explicitly if we want to do so.\n\nquake$dateTime2 &lt;- as.POSIXct(quake$Time, tz = \"UTC\", \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\n\n\nIn pandas, we can use the to_datetime method. If the format is not specified, pandas will try to guess the date-time format; in this case, the guess works, but if not, you can provide a format = … argument that works the same way as R.\n\nimport pandas as pd\nquake = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nquake.dtypes\n## #EventID              object\n## Time                  object\n## Latitude             float64\n## Longitude            float64\n## Depth/km             float64\n## Author                object\n## Catalog               object\n## Contributor           object\n## ContributorID         object\n## MagType               object\n## Magnitude            float64\n## MagAuthor             object\n## EventLocationName     object\n## dtype: object\n\nquake.Time[0:10]\n## 0    2022-12-28T16:34:20Z\n## 1    2022-12-20T10:34:24Z\n## 2    2022-12-14T18:40:26Z\n## 3    2022-12-11T14:31:29Z\n## 4    2022-12-04T19:24:15Z\n## 5    2022-11-23T01:08:15Z\n## 6    2022-11-22T16:39:05Z\n## 7    2022-11-22T02:37:57Z\n## 8    2022-11-22T02:03:06Z\n## 9    2022-11-18T13:37:08Z\n## Name: Time, dtype: object\n\n# Convert to datetime\nquake['dateTime'] = pd.to_datetime(quake.Time)\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dtype: object\nquake.dateTime[0:10]\n## 0   2022-12-28 16:34:20+00:00\n## 1   2022-12-20 10:34:24+00:00\n## 2   2022-12-14 18:40:26+00:00\n## 3   2022-12-11 14:31:29+00:00\n## 4   2022-12-04 19:24:15+00:00\n## 5   2022-11-23 01:08:15+00:00\n## 6   2022-11-22 16:39:05+00:00\n## 7   2022-11-22 02:37:57+00:00\n## 8   2022-11-22 02:03:06+00:00\n## 9   2022-11-18 13:37:08+00:00\n## Name: dateTime, dtype: datetime64[ns, UTC]\n\n# Convert to datetime\nquake['dateTime2'] = pd.to_datetime(quake.Time, format = \"%Y-%m-%dT%H:%M:%S\")\n## ValueError: unconverted data remains when parsing with format \"%Y-%m-%dT%H:%M:%S\": \"Z\", at position 0. You might want to try:\n##     - passing `format` if your strings have a consistent format;\n##     - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n##     - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dtype: object\nquake.dateTime2[0:10]\n## AttributeError: 'DataFrame' object has no attribute 'dateTime2'\n\n\n\n\n\n\n\n\n\n\nTry it Out - Datetimes from Strings\n\n\n\nIt’s usually important for new parents to keep a log of the new baby’s feeds, to ensure that the baby is getting enough liquids and isn’t getting dehydrated. I used an app to keep track of my daughter’s feeds from birth (though here, we’ll only work with the first 3 months of data), and it used a reasonable, if not standard way to store dates and times.\n\n\nProblem\nR solution\nPython solution\n\n\n\nTake a look at the first month of feeds. Note that these data are from August 7, 2021 to November 4, 2021 – roughly baby’s first 90 days.\n\nConvert Start and End to datetime variables\nCan you plot the feeds somehow?\nCan you do arithmetic with datetimes to see if there are any user entry errors?\nThis data was created by a highly unreliable and error prone couple of individuals – specifically, sleep-deprived new parents.\n\nTo do this, you may need to figure out how to specify a non-standard date format in R and/or python. The parse_date_time function is useful in R, and pd.to_datetime() takes a format argument in python.\n\n\nFirst, let’s read the data in and explore a bit.\n\nlibrary(lubridate)\nlibrary(readr)\nfeeds &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nhead(feeds)\n## # A tibble: 6 × 6\n##      id Start               End       Type  `Quantity (oz)` `Quantity (ml or g)`\n##   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n## 1  1368 20:03:30 11-04-2021 20:45:21… Brea…              NA                   NA\n## 2  1366 18:00:29 11-04-2021 18:18:29… Brea…              NA                   NA\n## 3  1365 16:27:29 11-04-2021 17:03:26… Brea…              NA                   NA\n## 4  1364 14:30:01 11-04-2021 14:42:05… Brea…              NA                   NA\n## 5  1367 12:48:29 11-04-2021 13:50:29… Bott…               3                   88\n## 6  1363 10:59:18 11-04-2021 11:15:18… Bott…               3                   88\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the parse_date_time function in lubridate\n\nfeeds &lt;- feeds %&gt;%\n  mutate(Start = parse_date_time(Start, orders = c(\"%H:%M:%S %m-%d-%Y\")),\n         End = parse_date_time(End, orders = c(\"%H:%M:%S %m-%d-%Y\")))\n\nLet’s then explore how we might plot this data:\n\nlibrary(ggplot2)\nggplot(feeds, aes(xmin = Start, xmax = End, fill = Type)) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"bottom\") + \n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nfeeds %&gt;%\n  mutate(day = floor_date(Start, \"day\"),\n         hour_start = Start - day,\n         hour_end = End - day) %&gt;%\n  mutate(across(starts_with(\"hour\"), ~as.numeric(., units = \"hours\"))) %&gt;%\n  mutate(doy = yday(day)) %&gt;%\nggplot(aes(ymin = day, ymax = day+days(1), xmin = hour_start, xmax = hour_end, fill = Type)) + \n  geom_rect() + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  scale_x_continuous(\"Hour of the day\") + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can also calculate the duration of each feed and look at the distributions for each type of feed.\n\nfeeds &lt;- feeds %&gt;%\n  mutate(duration = End - Start)\n\nggplot(feeds, aes(x = duration, fill = Type)) + geom_histogram(color = \"black\") + \n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"none\") + \n  xlab(\"Feed duration, in seconds\") + facet_wrap(~Type)\n\n\n\n\n\n\n\nWe can see a few suspiciously long feeds - 9000 seconds is 2.5 hours, which is not unheard of for a baby to breastfeed, but would be an exceptionally long bottle feed (unless a parent fell asleep before hitting “stop” on the feed, which is much more likely).\n\n\nFirst, let’s read the data in and explore a bit.\n\nimport pandas as pd\n\nfeeds = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nfeeds.head()\n##      id                Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368  20:03:30 11-04-2021  ...           NaN                NaN\n## 1  1366  18:00:29 11-04-2021  ...           NaN                NaN\n## 2  1365  16:27:29 11-04-2021  ...           NaN                NaN\n## 3  1364  14:30:01 11-04-2021  ...           NaN                NaN\n## 4  1367  12:48:29 11-04-2021  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the format argument to pd.to_datetime to specify this:\n\nfeeds[\"Start\"] = pd.to_datetime(feeds.Start, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds[\"End\"] = pd.to_datetime(feeds.End, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds.head()\n##      id               Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368 2021-11-04 20:03:30  ...           NaN                NaN\n## 1  1366 2021-11-04 18:00:29  ...           NaN                NaN\n## 2  1365 2021-11-04 16:27:29  ...           NaN                NaN\n## 3  1364 2021-11-04 14:30:01  ...           NaN                NaN\n## 4  1367 2021-11-04 12:48:29  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\nIn Python, it is helpful to do a bit of transformation first - this is partly because I’m not as good with Python plotting systems.\n\nimport datetime as dt\nfeeds[\"day\"] = feeds.Start.dt.strftime(\"%Y-%m-%d\")\nfeeds[\"day\"] = pd.to_datetime(feeds.day, format = \"%Y-%m-%d\")\nfeeds[\"day_end\"] = feeds.day + dt.timedelta(days = 1)\n\nfeeds[\"time_start\"] = feeds.Start - feeds.day\nfeeds[\"time_end\"] = feeds.End - feeds.day\nfeeds[\"duration\"] = feeds.time_end - feeds.time_start\n\nNote that as of January 2023, RStudio does not correctly display timedelta data types in python. They show up as NAs in the table, but are printed fine in the console. Don’t spend hours trying to figure out why it isn’t working – it’s bad enough that I did.\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"Start\", xmax = \"End\", fill = \"Type\")) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw() + scale_y_continuous(breaks = [])\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f7dd489b390&gt;\n\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"time_start\", xmax = \"time_end\", ymin = \"day\", ymax = \"day_end\", fill = \"Type\")) + \n  geom_rect() + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw()\n)\n## &lt;plotnine.ggplot.ggplot object at 0x7f7dc67dd150&gt;\n\n\n\n\n\n\n\n27.3.2 Creation from Components\nSometimes, instead of a single string, you’ll have the individual components of the date-time spread across columns. The nycflights13 data is a good example of this.\n\n27.3.2.1 Demo: Datetimes from Components\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the make_date() and make_datetime() functions can be used to create date-times from component pieces.\n\nlibrary(nycflights13)\n\nflights %&gt;%\n  select(year, month, day, hour, minute) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##    year month   day  hour minute\n##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1  2013     1     1     5     15\n## 2  2013     1     1     5     29\n## 3  2013     1     1     5     40\n## 4  2013     1     1     5     45\n## 5  2013     1     1     6      0\n## 6  2013     1     1     5     58\n\nflights &lt;- flights %&gt;%\n  mutate(date = make_date(year, month, day),\n         datetime = make_datetime(year, month, day, hour, minute))\n\nflights %&gt;% select(date, datetime, year, month, day, hour, minute)\n## # A tibble: 336,776 × 7\n##    date       datetime             year month   day  hour minute\n##    &lt;date&gt;     &lt;dttm&gt;              &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n##  1 2013-01-01 2013-01-01 05:15:00  2013     1     1     5     15\n##  2 2013-01-01 2013-01-01 05:29:00  2013     1     1     5     29\n##  3 2013-01-01 2013-01-01 05:40:00  2013     1     1     5     40\n##  4 2013-01-01 2013-01-01 05:45:00  2013     1     1     5     45\n##  5 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  6 2013-01-01 2013-01-01 05:58:00  2013     1     1     5     58\n##  7 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  8 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  9 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## 10 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## # ℹ 336,766 more rows\n\n\n\nIn base R, we can use the ISOdate function to create date times.\n\nflights$datetime_base = with(flights, ISOdatetime(year, month, day, hour, minute, sec= 0, tz=\"UTC\"))\nall.equal(flights$datetime, flights$datetime_base)\n## [1] TRUE\n\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n\nflights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##         year  month  day  hour  minute\n## 0       2013      1    1     5      15\n## 1       2013      1    1     5      29\n## 2       2013      1    1     5      40\n## 3       2013      1    1     5      45\n## 4       2013      1    1     6       0\n## ...      ...    ...  ...   ...     ...\n## 336771  2013      9   30    14      55\n## 336772  2013      9   30    22       0\n## 336773  2013      9   30    12      10\n## 336774  2013      9   30    11      59\n## 336775  2013      9   30     8      40\n## \n## [336776 rows x 5 columns]\n\nflights[\"date\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\"]])\nflights[\"datetime\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]])\n\n\nflights[[\"date\", \"datetime\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##              date            datetime  year  month  day  hour  minute\n## 0      2013-01-01 2013-01-01 05:15:00  2013      1    1     5      15\n## 1      2013-01-01 2013-01-01 05:29:00  2013      1    1     5      29\n## 2      2013-01-01 2013-01-01 05:40:00  2013      1    1     5      40\n## 3      2013-01-01 2013-01-01 05:45:00  2013      1    1     5      45\n## 4      2013-01-01 2013-01-01 06:00:00  2013      1    1     6       0\n## ...           ...                 ...   ...    ...  ...   ...     ...\n## 336771 2013-09-30 2013-09-30 14:55:00  2013      9   30    14      55\n## 336772 2013-09-30 2013-09-30 22:00:00  2013      9   30    22       0\n## 336773 2013-09-30 2013-09-30 12:10:00  2013      9   30    12      10\n## 336774 2013-09-30 2013-09-30 11:59:00  2013      9   30    11      59\n## 336775 2013-09-30 2013-09-30 08:40:00  2013      9   30     8      40\n## \n## [336776 rows x 7 columns]\n\n\n\n\n\n27.3.3 Creation from Other Objects\nSometimes, you may have information in one type of variable (e.g. a datetime) and want to split it into a date and a time, separately.\nSome systems store datetimes as the number of seconds from a specific point (commonly, the Unix Epoch, midnight on 1970-01-01). You may have to convert from seconds since this epoch (or some other epoch [5]) to an actual date-time that is human readable.\nIf you ever have to convert dates and times that were stored in Microsoft Excel, it can be helpful to know that Microsoft stores dates as the number of days since January 1, 1900 [6] (or if the spreadsheet was created on a Mac, January 1, 1904) [7]. Yes, this is as confusing as it sounds. Don’t use MS Excel for handling dates [8], [9] (or really, at all, now that you know better tools). Geneticists have actually renamed genes because Microsoft won’t fix Excel to handle dates properly [10].\n\n27.3.3.1 Demo: Creation from Other Objects\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the as_date() and as_datetime() functions can be used to create date-times from other objects.\n\ntmp &lt;- flights %&gt;%\n  mutate(date2 = as_date(datetime))\n\n# Check that date and date2 are the same\nall.equal(flights$date, flights$date2)\n## [1] \"Modes: numeric, NULL\"                                \n## [2] \"Lengths: 336776, 0\"                                  \n## [3] \"Attributes: &lt; Modes: list, NULL &gt;\"                   \n## [4] \"Attributes: &lt; Lengths: 1, 0 &gt;\"                       \n## [5] \"Attributes: &lt; names for target but not for current &gt;\"\n## [6] \"Attributes: &lt; current is not list-like &gt;\"            \n## [7] \"target is Date, current is NULL\"\n\nHere’s a demonstration of epoch timekeeping.\n\ncurrent_time &lt;- now(tzone = \"UTC\")\n# This converts to the number of seconds since the Unix epoch\nseconds_since_epoch &lt;- current_time %&gt;% seconds()\n# Now let's convert back to a datetime\n(current_time2 &lt;- as_datetime(seconds_since_epoch))\n## [1] \"2025-08-16 15:48:31 UTC\"\n# Check to see that they're equal\nall.equal(current_time, current_time2)\n## [1] TRUE\n\n\n\nIn base R, we can use the as.Date function to create dates from datetimes.\n\nflights$date2 = as.Date(flights$date)\nall.equal(flights$date, flights$date2)\n## [1] TRUE\n\nWe can handle epochs as well:\n\n# Let's see what was 10000 days after the UNIX epoch\nas.Date(1e4, origin = \"1970-01-01\")\n## [1] \"1997-05-19\"\n\n# If we use as.POSIXct, we are counting in seconds from midnight\nas.POSIXct(1e4, origin = as.POSIXct(\"1970-01-01 00:00:00\"))\n## [1] \"1970-01-01 02:46:40 CST\"\n\nBy default, as.POSIXct will use the system’s time zone, which may not be desirable; you can always set the time zone yourself if you would like to do so.\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n\nflights[\"date2\"] = flights.date.dt.date # Convert datetime to date\n\n# They look the same\nflights[[\"date\", \"date2\"]]\n##              date       date2\n## 0      2013-01-01  2013-01-01\n## 1      2013-01-01  2013-01-01\n## 2      2013-01-01  2013-01-01\n## 3      2013-01-01  2013-01-01\n## 4      2013-01-01  2013-01-01\n## ...           ...         ...\n## 336771 2013-09-30  2013-09-30\n## 336772 2013-09-30  2013-09-30\n## 336773 2013-09-30  2013-09-30\n## 336774 2013-09-30  2013-09-30\n## 336775 2013-09-30  2013-09-30\n## \n## [336776 rows x 2 columns]\n\nflights.dtypes\n## year                       int64\n## month                      int64\n## day                        int64\n## dep_time                 float64\n## sched_dep_time             int64\n## dep_delay                float64\n## arr_time                 float64\n## sched_arr_time             int64\n## arr_delay                float64\n## carrier                   object\n## flight                     int64\n## tailnum                   object\n## origin                    object\n## dest                      object\n## air_time                 float64\n## distance                   int64\n## hour                       int64\n## minute                     int64\n## time_hour                 object\n## date              datetime64[ns]\n## datetime          datetime64[ns]\n## date2                     object\n## dtype: object\n# date2 is an object, date is a datetime64.\n\nWe created flights.date using pd.to_datetime(). Given this comparison, it may be better to use to.datetime() and append .dt.date on the end if you do not want to keep the time information that is provided by default.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "href": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "title": "27  Dates and Times",
    "section": "\n27.4 Working with Dates and Times",
    "text": "27.4 Working with Dates and Times\nIn this section, we’ll work with comments by famous Reddit artist Shitty_Watercolour, who responds to people’s comments with a quickly created watercolor-style painting.\n\n\nHere’s one of Shitty Watercolour’s works: \n\n27.4.1 Getting the Data\n\n\nR\nPython\n\n\n\nNote: The textbook caches data, so your results may differ from those shown here because RedditExtractoR only acquires the last ~1000 comments from a user. In addition, in July 2003, reddit removed their API that allowed RedditExtractoR to function, so any future updates are sadly unlikely.\n\n# remotes::install_github(\"ivan-rivera/RedditExtractor\")\nlibrary(RedditExtractoR)\n\ncomment_list &lt;- get_user_content(\"Shitty_Watercolour\")\nwatercolour &lt;- comment_list$Shitty_Watercolour$comments\n\n\n\n\n# Get data from R directly, since redditExtractor package is in R\nwatercolour = r.watercolour\n\n\n\n\n\n27.4.2 Time Zones\nWe often store data in UTC1, but we may want to represent the data in a more familiar time zone for interpretation’s purposes.\n\nWorking with Time Zones. Here, time is a placeholder for whatever variable is being converted.\n\n\n\n\n\n\nTask\nLanguage\nFunction\n\n\n\nSet the time zone\nR\nas_datetime(time, tz = \"GMT\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_localize(\"GMT\"))\n\n\nDisplay the time in a diff TZ\nR\nwith_tz(time, tz = \"America/Chicago\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_convert(\"America/Chicago\"))\n\n\n\n\n\n\n\n\n\nTry it Out - Formatting Dates\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe watercolour dataset above contains 959 comments from Shitty_Watercolour, with the UTC date and timestamp of the comment.\nFigure out how to format these data into a proper date type and timestamp that is user-readable. Make sure you inform R or python that the timestamp is provided in UTC.\n Compare a couple of your timestamps to the timestamps provided by Reddit when you mouse over a comment.\nCan you get R or Python to output the date in your timezone?\n\n\n\nwatercolour &lt;- watercolour %&gt;%\n  mutate(date = ymd(date_utc, tz = \"UTC\"),\n         time = as_datetime(timestamp),\n         time_cst = with_tz(time, tzone_out = \"CST\"))\n\nwatercolour[,c(\"date\", \"time\", \"time_cst\")]\n## # A tibble: 959 × 3\n##    date                time                time_cst           \n##    &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;             \n##  1 2017-12-19 00:00:00 2017-12-19 22:08:19 2017-12-19 16:08:19\n##  2 2017-12-19 00:00:00 2017-12-19 22:07:03 2017-12-19 16:07:03\n##  3 2017-12-19 00:00:00 2017-12-19 20:47:55 2017-12-19 14:47:55\n##  4 2017-12-19 00:00:00 2017-12-19 20:01:19 2017-12-19 14:01:19\n##  5 2017-12-17 00:00:00 2017-12-17 19:58:51 2017-12-17 13:58:51\n##  6 2017-12-15 00:00:00 2017-12-15 02:53:23 2017-12-14 20:53:23\n##  7 2017-12-12 00:00:00 2017-12-12 16:56:23 2017-12-12 10:56:23\n##  8 2017-12-08 00:00:00 2017-12-08 17:48:05 2017-12-08 11:48:05\n##  9 2017-12-08 00:00:00 2017-12-08 03:50:17 2017-12-07 21:50:17\n## 10 2017-12-07 00:00:00 2017-12-07 18:00:58 2017-12-07 12:00:58\n## # ℹ 949 more rows\n\n\n\n\nfrom datetime import datetime\n\nwatercolour[\"date\"] = pd.to_datetime(watercolour.date_utc).dt.date\nwatercolour[\"time\"] = pd.to_datetime(watercolour.timestamp, unit = 's')\n\n# Tell Python the time is in UTC \nwatercolour[\"time\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_localize(\"UTC\"))\n\nwatercolour[\"time_cst\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_convert(\"US/Central\"))\n\nwatercolour[[\"date\", \"time\", \"time_cst\"]]\n##            date                      time                  time_cst\n## 0    2017-12-19 2017-12-19 22:08:19+00:00 2017-12-19 16:08:19-06:00\n## 1    2017-12-19 2017-12-19 22:07:03+00:00 2017-12-19 16:07:03-06:00\n## 2    2017-12-19 2017-12-19 20:47:55+00:00 2017-12-19 14:47:55-06:00\n## 3    2017-12-19 2017-12-19 20:01:19+00:00 2017-12-19 14:01:19-06:00\n## 4    2017-12-17 2017-12-17 19:58:51+00:00 2017-12-17 13:58:51-06:00\n## ..          ...                       ...                       ...\n## 954  2023-02-21 2023-02-21 16:55:03+00:00 2023-02-21 10:55:03-06:00\n## 955  2023-02-21 2023-02-21 15:59:51+00:00 2023-02-21 09:59:51-06:00\n## 956  2023-02-21 2023-02-21 15:09:19+00:00 2023-02-21 09:09:19-06:00\n## 957  2023-02-21 2023-02-21 14:45:10+00:00 2023-02-21 08:45:10-06:00\n## 958  2023-02-21 2023-02-21 11:27:52+00:00 2023-02-21 05:27:52-06:00\n## \n## [959 rows x 3 columns]\n\n\n\n\n\n\n\n27.4.3 Time Spans\nDates and times can be added and subtracted - after all, underneath, they’re usually implemented as a number of XXX from the reference time point, where XXX is usually seconds for datetimes and days for dates.\nIn R, the difference between two timestamps is called a duration and is implemented in the duration class See [11, Ch. 16.4.1] for more info. In Python, a similar class exists and is called a timedelta [12].\n\n\n\n\n\n\nTry it Out - Datetime Math\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nUse the watercolour data and plot the interval between successive Shitty_Watercolour posts in minutes. What can you conclude?\n\n\n\nwatercolour &lt;- watercolour %&gt;%\n  arrange(time) %&gt;%\n  mutate(diff = as.duration(time - lag(time, 1)),\n         diffmin = as.numeric(diff, \"minutes\"))\n\nlibrary(ggplot2)\nggplot(watercolour, aes(x = diffmin)) + \n  geom_histogram() + \n  xlab(\"Time between posts (minutes)\") + \n  ylab(\"# Posts\") + \n  scale_x_log10(breaks = c(1, 15, 30, 60, 1440, 10080))\n\n\n\n\n\n\n\nMost of the time, Shitty_Watercolour takes at least 15 minutes to generate a new comment. There is also a noticable peak just before 1440 minutes, indicating that as with most users, Shitty_Watercolour is active at approximately the same time each day for a few hours. The final break shown, 10080, is the number of minutes in a week, indicating that occasionally, Shitty_Watercolour goes more than a week between posts.\n\n\n\nfrom datetime import datetime\n\nwatercolour = watercolour.sort_values(by = 'time')\nwatercolour[\"diff\"] = watercolour.time.diff().astype('timedelta64')\n## ValueError: Cannot convert from timedelta64[ns] to timedelta64. Supported resolutions are 's', 'ms', 'us', 'ns'\n# This formats in minutes\nwatercolour[\"diffmin\"] = watercolour.time.diff().astype('timedelta64[m]')\n## ValueError: Cannot convert from timedelta64[ns] to timedelta64[m]. Supported resolutions are 's', 'ms', 'us', 'ns'\n# Remove negative minutes - something funky there?\nwatercolour = watercolour.query(\"diffmin &gt; 0\")\n## pandas.errors.UndefinedVariableError: name 'diffmin' is not defined\n\nimport seaborn.objects as so\np = (\n  so.\n  Plot(watercolour, watercolour[\"diffmin\"]).\n  add(so.Bars(width=.95), so.Hist(bins = 30)).\n  scale(x = so.Continuous(trans = \"log\").\n    tick(at = [1, 15, 30, 60, 1440, 10080]).\n    label(like=\"{x:d}\")).\n  label(x = \"Time between posts (minutes)\", y = \"# Posts\")\n)\n## KeyError: 'diffmin'\np.show()\n## NameError: name 'p' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#references",
    "href": "part-wrangling/07-datetime.html#references",
    "title": "27  Dates and Times",
    "section": "\n27.5 References",
    "text": "27.5 References\n\n\n\n\n[1] \nFreewind, “Why is subtracting these two times (in 1927) giving a strange result? Stack overflow,” May 22, 2021. [Online]. Available: https://stackoverflow.com/q/6841333/2859168. [Accessed: Jan. 21, 2023]\n\n\n[2] \nJ. Skeet, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841479/2859168. [Accessed: Jan. 21, 2023]\n\n\n[3] \nM. Borgwardt, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841572/2859168. [Accessed: Jan. 21, 2023]\n\n\n[4] \nH. W. {and}. G. Grolemund, “Dates and times,” in R for data science, 1st ed., O’Reilly Media, p. 518 [Online]. Available: https://r4ds.had.co.nz/dates-and-times.html. [Accessed: Jan. 23, 2023]\n\n\n[5] \nWikipedia contributors, “Epoch (computing),” Wikipedia. Mar. 23, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Epoch_(computing)&oldid=1146251518. [Accessed: Apr. 04, 2023]\n\n\n[6] \nMicrosoft Support, “DATEVALUE function - Microsoft Support,” 2023. [Online]. Available: https://support.microsoft.com/en-us/office/datevalue-function-df8b07d4-7761-4a93-bc33-b7471bbff252. [Accessed: Apr. 04, 2023]\n\n\n[7] \nElizabeth Mott, “Why Do Dates Come in Different in Excel From a Mac to a PC?” Jun. 12, 2013. [Online]. Available: https://smallbusiness.chron.com/dates-come-different-excel-mac-pc-68917.html. [Accessed: Apr. 04, 2023]\n\n\n[8] \nH. Caudill, “Excel Hell: A cautionary tale,” Dec. 19, 2018. [Online]. Available: https://medium.com/all-the-things/a-single-infinitely-customizable-app-for-everything-else-9abed7c5b5e7. [Accessed: Apr. 04, 2023]\n\n\n[9] \nChris88888888, “Excel Still Sucks at Recognizing Dates.” Apr. 15, 2020. [Online]. Available: https://answers.microsoft.com/en-us/msoffice/forum/all/excel-still-sucks-at-recognizing-dates/5305f6db-8211-49d5-932d-c4871df27fc7. [Accessed: Apr. 04, 2023]\n\n\n[10] \nJ. Vincent, “Scientists rename human genes to stop Microsoft Excel from misreading them as dates,” Aug. 06, 2020. [Online]. Available: https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates. [Accessed: Apr. 04, 2023]\n\n\n[11] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[12] \nPython Foundation, “Datetime Basic date and time types,” Apr. 05, 2023. [Online]. Available: https://docs.python.org/3/library/datetime.html. [Accessed: Apr. 05, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#footnotes",
    "href": "part-wrangling/07-datetime.html#footnotes",
    "title": "27  Dates and Times",
    "section": "",
    "text": "Coordinated Universal Time, but the acronym is in a different language and the words are thus in a different order.↩︎",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html",
    "href": "part-wrangling/08-functional-prog.html",
    "title": "28  Functional Programming",
    "section": "",
    "text": "Objectives\nIn this section, we’re going to change focus slightly from learning specific functions to learning programming patterns. We’re going to start this process by talking about functional programming and its connection to lists. While this topic may be a bit advanced if you’re just starting to learn how to program, it may help to skim through the deeper explanation so that you can at least recognize some of these words if you encounter them later.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#objectives",
    "href": "part-wrangling/08-functional-prog.html#objectives",
    "title": "28  Functional Programming",
    "section": "",
    "text": "Use functional programming to replace for loops\nArticulate why functional programming can be preferable to using for loops\nUse functional programming to clean data, model data subsets, and assemble hierarchical data.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "href": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "title": "28  Functional Programming",
    "section": "\n28.1 Programming Philosophies",
    "text": "28.1 Programming Philosophies\n\nThis section is intended for everyone, but I do not expect that people who are just learning to program for the first time will fully absorb everything in this section. Get what you can out of this, use it to improve how you write code, and come back to it later if it’s too confusing.\n\nJust as spoken languages fall into families, like Indo-European or Sino-Tibetan, programming languages also have broad classifications. Here are a few “families” or classifications of programming languages [1]:\n\nMany languages are procedural: a program provides a list of instructions that tell the computer what to do with provided input. C, Pascal, Fortran, and UNIX shells are naturally procedural. JavaScript is also a fairly natural procedural language. Many R analysis scripts are also naturally written in a procedural style; SAS code is almost always procedural.\n\nDeclarative languages use code to describe the problem that needs to be solved, and the language figures out how to solve it. SQL is the most common declarative language you’ll encounter for data-related tasks.\n\nObject oriented languages (sometimes abbreviated OOP, for object-oriented programming) manipulate collections of objects or classes. Data is stored in classes that have associated functions, which are often called methods. Java is explicitly object-oriented; C++ and Python support object-oriented programming but don’t force you to use those features.\n\nFunctional programming languages describe a problem using a set of functions, which only take inputs and produce outputs. Functions don’t have any internal tracking of state - purely functional languages move from input to output without storing variables or even printing output to the command line, but it is common to adopt a functional approach to programming without requiring strict adherence to all principles of a fully functional approach. Haskell and Rust are fairly standard functional programming languages.\n\n\n\nHadley’s talk on The Joy of Functional Programming for Data Science\n\n\nFunctional programming languages have a goal of writing pure functions - functions that do not change the global state (stuff stored in objects, memory, parameters, or files) of the program and thus have no side effects. The return value of a pure function is based solely on the inputs to the function. Not all functions can be pure functions - for instance, there’s no pure way to do file IO operations. But it is a nice goal to be able to move parameters into functions and have the correct object returned from that function, so that you can pipe multiple operations together into a pipeline.\nMost general-purpose languages like C++ and Python and even some domain languages like R support multiple different programming paradigms. While preparing to write this chapter, I saw functional programming books with examples in Java [2], JavaScript [3], and C# [4] - all languages that I would associate with OOP or procedural styles. I also found books teaching object oriented programming using Fortran 90-95 [5], which is something I wouldn’t have considered possible.\nAll of this is to say that while certain languages are built around principles like OOP or functional programming, almost every language has users who rely more heavily on one approach than the other. There are very few “pure” programming languages, which reminds me of one of my favorite quotes about English:\n\n“The problem with defending the purity of the English language is that English is about as pure as a cribhouse whore. We don’t just borrow words; on occasion, English has pursued other languages down alleyways to beat them unconscious and rifle their pockets for new vocabulary.” ― James D. Nicoll\n\n\n28.1.1 Object Oriented Philosophy in R and Python\n\n\nPython\nR\n\n\n\nWhen you call df.size() in Python, you are calling the size method that is part of the df object, which is a DataFrame. This suggests that Pandas, at least, is programmed using an object-oriented paradigm.\n\n\n\n\n\nAn easy example of R’s object oriented nature is that when you fit different models or perform different tests, the default output is different.\n\ndata(mtcars)\n\nr1 &lt;- t.test(mtcars$mpg~mtcars$vs)\nprint(r1)\n## \n##  Welch Two Sample t-test\n## \n## data:  mtcars$mpg by mtcars$vs\n## t = -4.6671, df = 22.716, p-value = 0.0001098\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -11.462508  -4.418445\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        16.61667        24.55714\n\nr2 &lt;- lm(mtcars$mpg ~ mtcars$vs)\nprint(r2)\n## \n## Call:\n## lm(formula = mtcars$mpg ~ mtcars$vs)\n## \n## Coefficients:\n## (Intercept)    mtcars$vs  \n##       16.62         7.94\n\nThe output is different because each test/regression model object has a different print method, which allows R to create different output for each type of object.\n\n\n\nFunctional programming allows us to write programs that are more modular, and thus, are easier to test and debug. In addition, functional programming encourages you to think about data pipelines: a sequence of steps that are reproducible and reusable for different data sets and analyses. Functional programming is convenient for another (more esoteric, but important) reason - it allows you to prove that a function or series of functions is actually correct (rather than just testing input/output combinations).\n\n\nA pipeline of functional data analysis. Each function (station) modifies the data in some way and the returned result is passed into the next function (station) as input. This allows a sequence of functions to format data, visualize it, model it, and then package the results. While this paradigm doesn’t require a functional approach, functional programming does make it simpler. Modified from Allison Horst’s work\n\nIf you have been using the R pipe (|&gt; or %&gt;%), you didn’t realize it, but you were already using functional programming. Piping results from one function to another in a chain is a prime example of the “pure function” idea - it allows us to chain each step of a sequence together to create a sequence that is modular and testable.\n\n28.1.2 A simple Functional Example\nA functional is a function that takes another function as input and returns a vector as output.\nOne simple example of a functional that is found in both R and Python is the apply function (or variants in R like lapply, sapply, tapply). In Python, .apply is a method in Pandas, but we can find an even more low-level equivalent in the ideas of list comprehensions and map functions.\nOne additional concept that is helpful before we start is the idea of a lambda function - a small anonymous function (that is, a function that is not named or stored in a variable). Lambda functions are great for filling in default arguments, but they have many other uses as well.\nCan you identify the lambda functions in each of the following examples?\n\n\nR\nPython\n\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nlapply(1:5, function(x) rnorm(5, mean = x, sd = 1))\n## [[1]]\n## [1] 1.7376750 0.9774437 0.6612776 2.9410737 0.5466997\n## \n## [[2]]\n## [1] 0.3904605 0.5207098 2.8580246 3.0270216 0.5384761\n## \n## [[3]]\n## [1] 2.958824 2.874217 3.038163 4.217607 3.940978\n## \n## [[4]]\n## [1] 2.982685 3.616670 3.874052 3.903398 3.742444\n## \n## [[5]]\n## [1] 5.143394 7.120080 4.683096 4.506871 5.575555\n\nOr, if you have R 4.1.0 or above, you can use a shorthand version:\n\nlapply(1:5, \\(x) rnorm(5, mean = x))\n## [[1]]\n## [1] 1.4836091 0.5830340 0.6267771 0.4006560 0.4358748\n## \n## [[2]]\n## [1] 3.783781 1.787605 2.371306 3.236497 1.248459\n## \n## [[3]]\n## [1] 2.751271 4.127867 3.581806 2.891846 2.011299\n## \n## [[4]]\n## [1] 5.015525 4.688117 4.985439 4.668918 3.094503\n## \n## [[5]]\n## [1] 5.519588 4.991453 7.075959 6.043888 5.554968\n\nThe \\(x) is shorthand for function(x) and allows you to quickly and easily define anonymous functions in R.\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nimport numpy as np\n\n# List comprehension approach\nr1 = [np.random.normal(i, size = 5) for i in range(1, 6)]\nprint(r1) \n## [array([ 1.10404619,  3.12807274, -0.67679321,  1.15481007,  1.59400011]), array([0.86087157, 1.29108991, 1.591428  , 2.86399395, 2.83616899]), array([1.92030115, 4.51750118, 3.13449761, 2.17667854, 3.01451633]), array([3.00370919, 3.60764535, 6.03856048, 4.34374203, 5.78558691]), array([4.33726451, 4.15296522, 4.73413082, 5.99119168, 4.5818202 ])]\n\n# Functional approach\n# Defining a lambda function allows us to fill in non-default options\nr2 = map(lambda a: np.random.normal(a, size = 5), range(1, 6))\n\n# This is what map spits out by default\nprint(r2)\n## &lt;map object at 0x7f1a17723eb0&gt;\n# get your results back out with list()\nr2b = list(r2) \nprint(r2b)\n## [array([3.8107501 , 1.90888951, 1.54949946, 1.08190564, 0.13496985]), array([1.43409284, 2.28462998, 2.62845498, 2.36771706, 0.33939605]), array([2.39968194, 3.22489351, 3.31801959, 3.67289761, 4.64338556]), array([4.2876777 , 5.03315384, 4.85789056, 4.32852604, 5.03758276]), array([3.40877172, 4.71761725, 5.32781321, 6.42700535, 5.65195079])]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "href": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "title": "28  Functional Programming",
    "section": "\n28.2 Replacing Loops with Functional Programming",
    "text": "28.2 Replacing Loops with Functional Programming\nOne really convenient application of functional programming is to replace loops. As Hadley Wickham says in [6],\n\nthe real downside of for loops is that they’re very flexible: a loop conveys that you’re iterating, but not what should be done with the results\n\nThat is, in many cases when programming with data, what we want is to iterate over a vector and return a vector of results. This is a perfect use case for functional programming, since we’re specifying both that we’re iterating AND more explicitly collecting the results into a form that makes sense.\nIf we work with this definition of functional programming, then python list comprehensions are also a functional approach: they specify how the results are collected (usually by putting [] around the statement) and how the iteration will occur [7].\n\n\nThere is an excellent vignette comparing Base R functional programming approaches to the purrr package that is worth a look if you’ve used one and want to try the other [8].\nLet’s look at a few examples.\n\nSuppose we want to look at the Lego data and create a decade variable that describes the decade a set was first released.\n\n\nbase R\nR: purrr\nPython\n\n\n\n\nlego &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego$decade &lt;- sapply(lego$year, \\(x) floor(x/10)*10)\nhead(lego[,c(\"set_num\", \"name\", \"year\", \"decade\")])\n##   set_num                       name year decade\n## 1   001-1                      Gears 1965   1960\n## 2  0011-2          Town Mini-Figures 1979   1970\n## 3  0011-3 Castle 2 for 1 Bonus Offer 1987   1980\n## 4  0012-1         Space Mini-Figures 1979   1970\n## 5  0013-1         Space Mini-Figures 1979   1970\n## 6  0014-1         Space Mini-Figures 1979   1970\n\nStrictly speaking, this use of sapply isn’t necessary - because R is vectorized by default, we could also have used lego$decade &lt;- floor(lego$year/10)*10. However, there are functions in R that are not fully vectorized, and it is useful to know this approach for those use-cases as well, and it’s easier to demonstrate this approach with a relatively simple use case.\n\n\nIn purrr, you can create anonymous functions using ~ with . as a placeholder. If you need more parameters, you can use .x, .y and map2 (for now) or .1, .2, .3, ... with pmap.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\n\nlego &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego &lt;- lego |&gt; # Either pipe will work here\n  mutate(decade = purrr::map_int(year, ~floor(./10)*10))\n\nlego |&gt; \n  select(set_num, name, year, decade) |&gt; \n  head()\n## # A tibble: 6 × 4\n##   set_num name                        year decade\n##   &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;  &lt;int&gt;\n## 1 001-1   Gears                       1965   1960\n## 2 0011-2  Town Mini-Figures           1979   1970\n## 3 0011-3  Castle 2 for 1 Bonus Offer  1987   1980\n## 4 0012-1  Space Mini-Figures          1979   1970\n## 5 0013-1  Space Mini-Figures          1979   1970\n## 6 0014-1  Space Mini-Figures          1979   1970\n\n\n\n\nimport pandas as pd\nimport math\n\nlego = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\nlego['decade'] = [math.floor(i/10)*10 for i in lego.year]\nlego[['set_num', 'name', 'year', 'decade']].head()\n##   set_num                        name  year  decade\n## 0   001-1                       Gears  1965    1960\n## 1  0011-2           Town Mini-Figures  1979    1970\n## 2  0011-3  Castle 2 for 1 Bonus Offer  1987    1980\n## 3  0012-1          Space Mini-Figures  1979    1970\n## 4  0013-1          Space Mini-Figures  1979    1970\n\n\n\n\n\nFor a more interesting example, though, let’s consider fitting a different linear regression for each generation of Pokemon, describing the relationship between HP (hit points) and CP (combat power, aka total in this dataset).\n\n\nI am sure that the python code I’ve written here is a bit kludgy, so if you are more fluent in python/pandas than I am, please feel free to submit a pull request if you know a better or more “pretty” way to do this.\n\n\n\n\n\n\nExample: Pokemon modeling\n\n\n\n\n\nbase R\nTidy R\nPython\n\n\n\n\npoke &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n# Get rid of mega pokemon - they're a different thing\npoke &lt;- subset(poke, !grepl(\"Mega\", poke$variant)) # step 1\n\n# Split into a list of data frames from each gen\npoke_gens &lt;- split(poke, poke$gen) # step 2\n\n# Fit linear regressions for each generation of pokemon\nmodels &lt;- lapply(poke_gens, \\(df) lm(total ~ hp, data = df)) # step 3\n\n# Pull out coefficients and r-squared values\nresults &lt;- lapply(models, \\(res) data.frame(coef1 = coef(res)[1], coef2 = coef(res)[2], rsq = summary(res)$r.squared))  # step 4\n\n# Join the results back into a data.frame\nresults &lt;- do.call(\"rbind\", results) # step 5\n\nresults\n##      coef1    coef2       rsq\n## 1 262.4730 2.394032 0.3895255\n## 2 258.5815 2.133868 0.3165292\n## 3 245.3375 2.753912 0.2686807\n## 4 268.5792 2.792350 0.3500187\n## 5 189.1339 3.506458 0.5578802\n## 6 252.7434 2.798942 0.5278159\n## 7 234.9329 3.293730 0.4382885\n## 8 205.2726 3.500811 0.6042219\n## 9 236.8656 2.757408 0.4757028\n\n\nData in data frame\nData split into a list of data frames\nModels in a list corresponding to data in step 2\nResults in a list of data frames corresponding to models in step 3\nBind results in step 4 back into a data frame\n\nIn each step, we specify not only what the iterative action should be, but also what form the results will take.\n\n\nIn the tidyverse, we use tidyr::nest() to accomplish a similar thing to split in base R.\nThis approach is designed to work entirely within a single data frame, which keeps the environment relatively clean and ensures that each step’s results are stored in a convenient, easy-to-find place.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\n\nres &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\") %&gt;%\n  # str_detect doesn't play nice with NAs, so replace NA with \"\"\n  mutate(variant = replace_na(variant, \"\")) %&gt;%\n  # Remove mega pokemon\n  filter(str_detect(variant, \"Mega\", negate = T)) %&gt;% # step 1\n  # Sub-data-frames\n  nest(.by = gen) %&gt;% # step 2\n  # Fit model\n  mutate(model = map(data, ~lm(total ~ hp, data = .))) %&gt;% # step 3\n  # Extract coefficients\n  mutate(res = map(model, ~data.frame(coef1 = coef(.)[1], \n                                      coef2 = coef(.)[2], \n                                      rsq = summary(.)$r.squared))) %&gt;% # step 4\n  # Bind together\n  unnest(c(res)) # step 5\nres\n## # A tibble: 9 × 6\n##     gen data                model  coef1 coef2   rsq\n##   &lt;dbl&gt; &lt;list&gt;              &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 &lt;tibble [269 × 15]&gt; &lt;lm&gt;    262.  2.39 0.390\n## 2     2 &lt;tibble [118 × 15]&gt; &lt;lm&gt;    259.  2.13 0.317\n## 3     3 &lt;tibble [173 × 15]&gt; &lt;lm&gt;    245.  2.75 0.269\n## 4     4 &lt;tibble [173 × 15]&gt; &lt;lm&gt;    269.  2.79 0.350\n## 5     5 &lt;tibble [236 × 15]&gt; &lt;lm&gt;    189.  3.51 0.558\n## 6     6 &lt;tibble [118 × 15]&gt; &lt;lm&gt;    253.  2.80 0.528\n## 7     7 &lt;tibble [133 × 15]&gt; &lt;lm&gt;    235.  3.29 0.438\n## 8     8 &lt;tibble [134 × 15]&gt; &lt;lm&gt;    205.  3.50 0.604\n## 9     9 &lt;tibble [123 × 15]&gt; &lt;lm&gt;    237.  2.76 0.476\n\nOur data takes the form:\n\nAn ungrouped data frame\nA data frame with 9 rows, one for each generation, with a list-column data that contains the full data for each generation\nWe fit our model and store the model results into another list-column named model that contains the fitted model object\nWe define some summary information and store it into a list-column containing each 1-row data frame\nWe “unnest” the summary information, which is equivalent to bringing the columns we defined up to the primary level and binding the rows together.\n\nAt each step, we’re specifying the form of the results along with the contents.\n\n\nThis construct of storing everything inside a single data frame isn’t as common in Python, but we can make it work with only a little extra effort.\nYou will need to pip install statsmodels to get the statsmodels [9] package that implements many basic statistical models. The scikit-learn package [10] is another commonly used package [11], but it does not have the easy accessor functions to pull out e.g. coefficients and r-squared values, so we’ll use statsmodels here.\n\nimport pandas as pd\nfrom statsmodels.formula.api import ols\n\n# Create a function to fit a linear regression\n# There is probably a better way to do this flexibly,\n# but this approach is simple and useful for illustrative purposes\ndef pokereg(data):\n  x = data[[\"hp\"]].values\n  y = data[[\"total\"]].values\n  model = ols('total ~ hp', data)\n  results = model.fit()\n  return results\n\nres = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n\n# Replace NAs with \"\"\nres[\"variant\"] = [\"\" if pd.isna(i) else i for i in res.variant]\n# Remove mega pokemon\nres = res.query('~(variant.str.contains(\"(?:^[mM]ega)\"))')\n\n# Group data frames and apply regression function to each group\nres_reg = (\n  res # step 1\n    .groupby(\"gen\") # step 2\n    .apply(pokereg) # step 3\n)\n\n# Make results into a dataframe and rename the column as 'results'\nres_reg = pd.DataFrame(res_reg).rename(columns = {0:'results'}) # step 4\n\n# Get values of interest and store in new columns # step 5\nres_reg = res_reg.reset_index() # store gen in its own column\nres_reg['coef1'] = res_reg.results.map(lambda x: x.params[0])\nres_reg['coef2'] = res_reg.results.map(lambda x: x.params[1])\nres_reg['rsq'] = res_reg.results.map(lambda x: x.rsquared)\n\nres_reg[['gen', 'coef1', 'coef2', 'rsq']]\n##    gen       coef1     coef2       rsq\n## 0    1  262.472956  2.394032  0.389526\n## 1    2  258.581538  2.133868  0.316529\n## 2    3  245.337496  2.753912  0.268681\n## 3    4  268.579237  2.792350  0.350019\n## 4    5  189.133898  3.506458  0.557880\n## 5    6  252.743437  2.798942  0.527816\n## 6    7  234.932868  3.293730  0.438289\n## 7    8  205.272637  3.500811  0.604222\n## 8    9  236.865627  2.757408  0.475703\n\nWhile this doesn’t store our data in the same DataFrame as the model results, we do have a key that links the two: the gen variable is present in both res and res_reg and can be used to join the data to the regression results, if necessary.\n\nData in an ungrouped data frame\nWe group by gen (generation)\nWe apply the function pokereg to fit a linear regression, and the results are stored in an indexed Series where the index corresponds to gen.\nWe make the results into a DataFrame so that we can add extra columns, and rename the automatically created Series to results to be more descriptive\nWe create summary information and store the summaries in columns in the data frame.\n\nWhile the grouping and binding operations are in a different order in Python than in R, the basic specification of the structure of the output each time we iterate is similar.\n\n\n\n\n\n\n\nHere’s another demonstration of the use of the tidymodels package and purrr to fit multiple regression models to data subsets.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "href": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "title": "28  Functional Programming",
    "section": "\n28.3 Complex Data Structures",
    "text": "28.3 Complex Data Structures\nNot all datasets are strictly tabular. One of the most common situations where we get data that can’t be made into a completely tabular structure is when we’re dealing with hierarchical data: tree structures, (network) graph structures, and even most webpages contain data that isn’t strictly tabular in nature. Sometimes, we can get that data into a tabular structure, but it generally depends on the data itself.\nOne of the most common structures for storing data on the web is JSON: JavaScript Object Notation[12].\n\n\n(JSON is pronounced “Jason”, like the person’s name).\n In this section we’ll work with some data gathered from TMDB (the movie database). I submitted a query for all movies that Patrick Stewart was involved with, and you can find the resulting JSON file here.\n\n28.3.1 JSON File Parsing\n\n\nR\nPython\n\n\n\nWe’ll use the jsonlite package to read the data in, but invariably this package still requires us to do some post-processing ourselves.\n\nlibrary(jsonlite)\ndata_url &lt;- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\nps_json &lt;- fromJSON(data_url)\n\n\nExploring the output structure\n\n# head(ps_json) # This output is too long\nps_json |&gt; str(max.level = 1)\n## List of 3\n##  $ cast:'data.frame':    150 obs. of  17 variables:\n##  $ crew:'data.frame':    6 obs. of  17 variables:\n##  $ id  : int 2387\nmap(ps_json, head) # show the first 6 rows of each element in the list\n## $cast\n##   adult                    backdrop_path       genre_ids    id\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg 878, 28, 12, 53   193\n## 2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg 878, 28, 12, 53   199\n## 3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg 878, 28, 12, 53   200\n## 4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg 878, 28, 12, 53   201\n## 5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg              99 10946\n## 6 FALSE                             &lt;NA&gt;              99 21746\n##   original_language              original_title\n## 1                en      Star Trek: Generations\n## 2                en    Star Trek: First Contact\n## 3                en     Star Trek: Insurrection\n## 4                en          Star Trek: Nemesis\n## 5                en                       Earth\n## 6                en The Secret of Life on Earth\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                overview\n## 1                                                                                                                                                                                                                                                                                                              Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2                                                                                                                                                                                                                                                    The Borg, a relentless race of cyborgs, are on a direct course for Earth. Violating orders to stay away from the battle, Captain Picard and the crew of the newly-commissioned USS Enterprise E pursue the Borg back in time to prevent the invaders from changing Federation history and assimilating the galaxy.\n## 3                                                                                                                                                                                                                                                                                 When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 4 En route to the honeymoon of William Riker to Deanna Troi on her home planet of Betazed, Captain Jean-Luc Picard and the crew of the U.S.S. Enterprise receives word from Starfleet that a coup has resulted in the installation of a new Romulan political leader, Shinzon, who claims to seek peace with the human-backed United Federation of Planets. Once in enemy territory, the captain and his crew make a startling discovery: Shinzon is human, a slave from the Romulan sister planet of Remus, and has a secret, shocking relationship to Picard himself.\n## 5                                                                                                                                                                                                                                                                                                 From the acclaimed team that brought you BBC's visual feast \"Planet Earth,\" this feature length film incorporates some of the same footage from the series with all new scenes following three remarkable, yet sadly endangered, families of animal across the globe.\n## 6                                                                                                                                                                                                                                                                          A breathtaking adventure across five continents and through time to reveal nature's most vital secret. Watch a flying fox gorge itself on a midnight snack of figs. Climb into the prickly jaws of insect-eating plants. Witness a mantis disguised as a flower petal lure its prey to doom.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     29.750 /vrC1lkTktFQ4AqBfqf4PXoDDLcw.jpg   1996-11-22\n## 3     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 4     33.614 /cldAwhvBmOv9jrd3bXWuqRHoXyq.jpg   2002-12-13\n## 5      9.188 /xybnXW6E28W9agiwUeGLbTYS454.jpg   2007-04-22\n## 6      1.831 /baa6T6noxiFUZcb6Jz8TjjlOoCH.jpg   1993-10-14\n##                         title video vote_average vote_count\n## 1      Star Trek: Generations FALSE        6.526       1126\n## 2    Star Trek: First Contact FALSE        7.305       1519\n## 3     Star Trek: Insurrection FALSE        6.425       1025\n## 4          Star Trek: Nemesis FALSE        6.293       1218\n## 5                       Earth FALSE        7.600        311\n## 6 The Secret of Life on Earth FALSE        6.000          1\n##                 character                credit_id order\n## 1 Captain Jean-Luc Picard 52fe4225c3a36847f80076d9     0\n## 2 Captain Jean-Luc Picard 52fe4226c3a36847f8007ba3     0\n## 3 Captain Jean-Luc Picard 52fe4226c3a36847f8007c27     0\n## 4 Captain Jean-Luc Picard 52fe4226c3a36847f8007cf1     0\n## 5                Narrator 52fe43d79251416c75020267     0\n## 6        Narrator (voice) 52fe4425c3a368484e01220f     0\n## \n## $crew\n##   adult                    backdrop_path            genre_ids      id\n## 1 FALSE                             &lt;NA&gt;               99, 35 1093380\n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg      878, 28, 12, 53     200\n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg 10770, 14, 18, 10751   48358\n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg                   37   47913\n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg 18, 36, 10770, 10749   37945\n## 6 FALSE /g5CMQPz5cqUHro9pNLBRW7cT8cY.jpg               18, 14   16716\n##   original_language          original_title\n## 1                en           Red Dwarf A-Z\n## 2                en Star Trek: Insurrection\n## 3                en   The Canterville Ghost\n## 4                en           King of Texas\n## 5                en      The Lion in Winter\n## 6                en       A Christmas Carol\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 overview\n## 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                       When a teenaged girl moves to England, with her brothers and parents into the ancient Canterville Hall, she's not at all happy. Especially as there's a ghost and a mysterious re-appearing bloodstain on the hearth. She campaigns to go back home, and her dad, believing the ghost's pranks are Ginny's, is ready to send her back. But then Ginny actually meets the elusive 17th-century Sir Simon de Canterville (not to mention the cute teenaged duke next door), and she sets her hand to the task of freeing Sir Simon from his curse.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this re-imagining of Shakespear's King Lear, Patrick Stewart stars as John Lear, a Texas cattle baron, who, after dividing his wealth among his three daughters, is rejected by them.\n## 5 King Henry II (Patrick Stewart) keeps his wife, Eleanor (Glenn Close) locked away in the towers because of her frequent attempts to overthrow him. With Eleanor out of the way he can have his dalliances with his young mistress (Yuliya Vysotskaya). Needless to say the queen is not pleased, although she still has affection for the king. Working through her sons, she plots the king's demise and the rise of her second and preferred son, Richard (Andrew Howard), to the throne. The youngest son, John (Rafe Spall), an overweight buffoon and the only son holding his father's affection is the king's choice after the death of his first son, young Henry. But John is also overly eager for power and is willing to plot his father's demise with middle brother, Geoffrey (John Light) and the young king of France, Phillip (Jonathan Rhys Meyers). Geoffrey, of course sees his younger brother's weakness and sees that route as his path to power. Obviously political and court intrigue ensues\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Scrooge is a miserly old businessman in 1840s London. One Christmas Eve he is visited by the ghost of Marley, his dead business partner. Marley foretells that Scrooge will be visited by three spirits, each of whom will attempt to show Scrooge the error of his ways. Will Scrooge reform his ways in time to celebrate Christmas?\n##   popularity                      poster_path release_date\n## 1      0.841                             &lt;NA&gt;   2003-02-10\n## 2     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 3      9.267 /m71l7oGGKLxdQaUceVTndg2qjJJ.jpg   1996-01-27\n## 4      3.951 /jFvDJsgnLRVuRpsbm3YHIn0dHxI.jpg   2002-06-02\n## 5      7.097 /f6yEfCBBMNp6jdny9AD4ZTG9tgi.jpg   2003-12-26\n## 6     13.427 /oi1NcVDXlFEsdpLp37BJmFbVlg9.jpg   1999-12-05\n##                     title video vote_average vote_count\n## 1           Red Dwarf A-Z FALSE        0.000          0\n## 2 Star Trek: Insurrection FALSE        6.425       1025\n## 3   The Canterville Ghost FALSE        6.042         48\n## 4           King of Texas FALSE        5.100         11\n## 5      The Lion in Winter FALSE        6.100         17\n## 6       A Christmas Carol FALSE        6.800        161\n##                  credit_id department                job\n## 1 63fead85699fb70096ff260e       Crew             Thanks\n## 2 52fe4226c3a36847f8007c1d Production           Producer\n## 3 5962f6d292514122510c57a0 Production        Co-Producer\n## 4 59807f88925141491d0113a0 Production Executive Producer\n## 5 5f72da29e4b5760039f36206 Production Executive Producer\n## 6 63c31ac8d46537007dbd999a Production Executive Producer\n## \n## $id\n## [1] 2387\n\nBy default, fromJSON does a LOT of heavy lifting for us:\n\nIdentifying the structure of the top-level data: cast, crew, and id information\nParses cast information into a data frame with list-columns\nParses crew information into a data frame with list-columns\n\nIt’s hard to explain how nice this is to someone who hasn’t had to parse this type of information by hand before… so let’s briefly explore that process.\n\nlibrary(jsonlite)\n\nps_messy &lt;- fromJSON(data_url, simplifyVector = T, simplifyDataFrame = F)\n\n\nExploring the output structure (long version)\n\n# Top-level objects (show the first object in the list)\nps_messy$cast[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] \"/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg\"\n## \n## $genre_ids\n## [1] 878  28  12  53\n## \n## $id\n## [1] 193\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Star Trek: Generations\"\n## \n## $overview\n## [1] \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\"\n## \n## $popularity\n## [1] 19.783\n## \n## $poster_path\n## [1] \"/rHsCYDGHFUarGh5k987b0EFU6kC.jpg\"\n## \n## $release_date\n## [1] \"1994-11-18\"\n## \n## $title\n## [1] \"Star Trek: Generations\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 6.526\n## \n## $vote_count\n## [1] 1126\n## \n## $character\n## [1] \"Captain Jean-Luc Picard\"\n## \n## $credit_id\n## [1] \"52fe4225c3a36847f80076d9\"\n## \n## $order\n## [1] 0\nps_messy$crew[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99 35\n## \n## $id\n## [1] 1093380\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Red Dwarf A-Z\"\n## \n## $overview\n## [1] \"A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\"\n## \n## $popularity\n## [1] 0.841\n## \n## $poster_path\n## NULL\n## \n## $release_date\n## [1] \"2003-02-10\"\n## \n## $title\n## [1] \"Red Dwarf A-Z\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 0\n## \n## $vote_count\n## [1] 0\n## \n## $credit_id\n## [1] \"63fead85699fb70096ff260e\"\n## \n## $department\n## [1] \"Crew\"\n## \n## $job\n## [1] \"Thanks\"\nps_messy$id\n## [1] 2387\n\nLet’s start with the cast list. Most objects seem to be single entries; the only thing that isn’t is the genre_ids field. So let’s see whether we can just convert each list entry to a data frame, and then deal with the genre_ids column afterwards.\n\ncast_list &lt;- ps_messy$cast\n\n\nData frame conversion\n\nas.data.frame(cast_list[[1]])\n##   adult                    backdrop_path genre_ids  id original_language\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n##           original_title\n## 1 Star Trek: Generations\n## 2 Star Trek: Generations\n## 3 Star Trek: Generations\n## 4 Star Trek: Generations\n##                                                                                                                                                                                                                                                   overview\n## 1 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 3 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 4 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 3     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 4     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n##                    title video vote_average vote_count               character\n## 1 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 2 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 3 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 4 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n##                  credit_id order\n## 1 52fe4225c3a36847f80076d9     0\n## 2 52fe4225c3a36847f80076d9     0\n## 3 52fe4225c3a36847f80076d9     0\n## 4 52fe4225c3a36847f80076d9     0\n\n\nmap(cast_list, as.data.frame)\n## Error in `map()`:\n## ℹ In index: 6.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\nWell, that didn’t work, but the error message at least tells us what index is causing the problem: 6. Let’s look at that data:\n\nData frame conversion errors\n\ncast_list[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nOk, so backdrop_path is NULL, and as.data.frame can’t handle the fact that some fields are defined (length 1) and others are NULL (length 0). We could possibly replace the NULL with NA first?\n\nfix_nulls &lt;- function(x) {\n  lapply(x, \\(y) if (is.null(y)) NA else y)\n}\n\ncast_list_fix &lt;- map(cast_list, fix_nulls)\n\ncast_list_fix[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nmap(cast_list_fix, as.data.frame)\n## Error in `map()`:\n## ℹ In index: 8.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## list()\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\nOk, well, this time, we have an issue with position 8, and we have an empty list of genre_ids.\nAn empty list and NULL both have length 0, so let’s alter our fix_nulls function to test for things of length 0 instead of testing for nulls. That should fix both problems using the same code, and we’re trying to directly test for the issue which was causing problems, which is perhaps a better approach anyways.\n\nfix_nulls &lt;- function(x) {\n  lapply(x, \\(y) if (length(y) == 0) NA else y)\n}\n\ncast_list_fix &lt;- map(cast_list, fix_nulls)\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] NA\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\ncast_list_df &lt;- map_df(cast_list_fix, as.data.frame)\ncast_list_df[1:10, 1:5]\n##    adult                    backdrop_path genre_ids  id original_language\n## 1  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n## 5  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg       878 199                en\n## 6  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        28 199                en\n## 7  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        12 199                en\n## 8  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        53 199                en\n## 9  FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg       878 200                en\n## 10 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg        28 200                en\n\nWe still have too many rows for each entry because of the multiple genre_ids. But we can fix that with the nest command.\n\ncast_list &lt;- nest(cast_list_df, genre_ids = genre_ids )\ncast_list[1:10,c(1:4, 17)]\n## # A tibble: 10 × 5\n##    adult backdrop_path                       id original_language genre_ids\n##    &lt;lgl&gt; &lt;chr&gt;                            &lt;int&gt; &lt;chr&gt;             &lt;list&gt;   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg   193 en                &lt;tibble&gt; \n##  2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg   199 en                &lt;tibble&gt; \n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg   200 en                &lt;tibble&gt; \n##  4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg   201 en                &lt;tibble&gt; \n##  5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg 10946 en                &lt;tibble&gt; \n##  6 FALSE &lt;NA&gt;                             21746 en                &lt;tibble&gt; \n##  7 FALSE /cN4qq4B8JR4ekuKAIKGVa4bBssl.jpg 25224 en                &lt;tibble&gt; \n##  8 FALSE &lt;NA&gt;                             33335 en                &lt;tibble&gt; \n##  9 FALSE /89hVgLIH55PVE7wwLCVZF1j3ZGL.jpg 26950 en                &lt;tibble&gt; \n## 10 FALSE /x6f4Axjvr5Ybi2mfdpVSWvASdxX.jpg 28123 en                &lt;tibble&gt;\n\nThen, we’d have to apply this whole process to the crew list as well. Let’s see how robust our process actually is!\n\ncrew_list &lt;- ps_messy$crew\ncrew_list_fix &lt;- map(crew_list, fix_nulls)\ncrew_list_df &lt;- map_df(crew_list_fix, as.data.frame)\ncrew_list &lt;- nest(crew_list_df, genre_ids = genre_ids )\ncrew_list[1:5,c(1:4, 17)]\n## # A tibble: 5 × 5\n##   adult backdrop_path                         id original_language genre_ids\n##   &lt;lgl&gt; &lt;chr&gt;                              &lt;int&gt; &lt;chr&gt;             &lt;list&gt;   \n## 1 FALSE &lt;NA&gt;                             1093380 en                &lt;tibble&gt; \n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg     200 en                &lt;tibble&gt; \n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg   48358 en                &lt;tibble&gt; \n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg   47913 en                &lt;tibble&gt; \n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg   37945 en                &lt;tibble&gt;\n\nOk, so that actually worked, but only because the structure of the crew data is the same as the structure of the cast data.\nIt’s good to see what we’d have to do manually if fromJSON() failed on us. It’s also an excellent example of functional programming in a practical setting.\nLet’s finish this up by converting our cast and crew data frames into a single data frame with a variable indicating which source DF is relevant.\n\npatrick_stewart_movies &lt;- bind_rows(\n  mutate(cast_list, role = \"cast\"),\n  mutate(crew_list, role = \"crew\")\n)\npatrick_stewart_movies %&gt;%\n  arrange(id)\n## # A tibble: 156 × 20\n##    adult backdrop_path              id original_language original_title overview\n##    &lt;lgl&gt; &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;          &lt;chr&gt;   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW…   193 en                Star Trek: Ge… \"Captai…\n##  2 FALSE /wygUDDRNpeKUnkekRGeLC…   199 en                Star Trek: Fi… \"The Bo…\n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3m…   200 en                Star Trek: In… \"When a…\n##  4 FALSE /vsjuHP9RQZJgYUvvSlO3m…   200 en                Star Trek: In… \"When a…\n##  5 FALSE /6z9w8eidKWDDXwZNSVNaR…   201 en                Star Trek: Ne… \"En rou…\n##  6 FALSE /2mEXtIjgsoe5uqH70CLps…   815 en                Animal Farm    \"An ani…\n##  7 FALSE /5wJ2tckpvwcxGCAgZicco…   841 en                Dune           \"In the…\n##  8 FALSE /92mpNNg6v2PN2HN2C2Z4g…  1273 en                TMNT           \"After …\n##  9 FALSE /wvqdJLVh0mSblly7UnYFP…  2080 en                X-Men Origins… \"After …\n## 10 FALSE /hPDv0O8tvbEvcVVphIieS…  2107 en                L.A. Story     \"With t…\n## # ℹ 146 more rows\n## # ℹ 14 more variables: popularity &lt;dbl&gt;, poster_path &lt;chr&gt;, release_date &lt;chr&gt;,\n## #   title &lt;chr&gt;, video &lt;lgl&gt;, vote_average &lt;dbl&gt;, vote_count &lt;int&gt;,\n## #   character &lt;chr&gt;, credit_id &lt;chr&gt;, order &lt;int&gt;, genre_ids &lt;list&gt;,\n## #   role &lt;chr&gt;, department &lt;chr&gt;, job &lt;chr&gt;\n\nWe could theoretically clean this up so that movies where Patrick Stewart was in both the cast and crew are on a single row, but I think this is “good enough” for now.\n\n\nPandas includes a read_json function, so let’s try that and see if it works as well as fromJSON() did in R:\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\npd.read_json(data_url)\n## ValueError: All arrays must be of the same length\n\nIf we read the documentation for read_json, we can see that we have a few different options - maybe playing around with some of those options will help? Our top-level structure is a list with 3 values: cast, crew, and id. So let’s see if we can read things in as a series instead of a DataFrame first, and hopefully we can use that to get some traction on the situation.\n\npatrick_stewart = pd.read_json(data_url, typ='series', orient = 'records')\n\n# List the objects\npatrick_stewart.index\n## Index(['cast', 'crew', 'id'], dtype='object')\n\n# First item in the cast list\npatrick_stewart.cast[0]\n## {'adult': False, 'backdrop_path': '/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg', 'genre_ids': [878, 28, 12, 53], 'id': 193, 'original_language': 'en', 'original_title': 'Star Trek: Generations', 'overview': \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\", 'popularity': 19.783, 'poster_path': '/rHsCYDGHFUarGh5k987b0EFU6kC.jpg', 'release_date': '1994-11-18', 'title': 'Star Trek: Generations', 'video': False, 'vote_average': 6.526, 'vote_count': 1126, 'character': 'Captain Jean-Luc Picard', 'credit_id': '52fe4225c3a36847f80076d9', 'order': 0}\n\nSo now how do we get our data into a proper form? If we read the documentation a bit further, we can see a “See also” section that has a json_normalize function which promises to “Normalize semi-structured JSON data into a flat table”. That sounds pretty good, let’s try it!\n\nps_cast = pd.json_normalize(patrick_stewart.cast)\nps_cast.head()\n##    adult                     backdrop_path  ...                 credit_id  order\n## 0  False  /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg  ...  52fe4225c3a36847f80076d9      0\n## 1  False  /wygUDDRNpeKUnkekRGeLCZM93tA.jpg  ...  52fe4226c3a36847f8007ba3      0\n## 2  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  52fe4226c3a36847f8007c27      0\n## 3  False  /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg  ...  52fe4226c3a36847f8007cf1      0\n## 4  False  /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg  ...  52fe43d79251416c75020267      0\n## \n## [5 rows x 17 columns]\n\nHuh, that actually worked! (I’m not used to this type of thing working on the first try).\n\nps_crew = pd.json_normalize(patrick_stewart.crew)\nps_crew.head()\n##    adult                     backdrop_path  ...  department                 job\n## 0  False                              None  ...        Crew              Thanks\n## 1  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  Production            Producer\n## 2  False  /ccsLztuF4cKlfnriitwdxs0coBa.jpg  ...  Production         Co-Producer\n## 3  False  /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg  ...  Production  Executive Producer\n## 4  False  /li27iYcGbSp89YTlVRmwujteykw.jpg  ...  Production  Executive Producer\n## \n## [5 rows x 17 columns]\n\nWe can combine these as we did in R into a single data frame, and sort by movie ID to simplify the list.\n\nps_cast['role'] = 'cast'\nps_crew['role'] = 'crew'\nps_movies = pd.concat([ps_cast, ps_crew])\nps_movies[['id', 'original_title', 'character', 'job']].sort_values(['id'])\n##           id               original_title                character       job\n## 0        193       Star Trek: Generations  Captain Jean-Luc Picard       NaN\n## 1        199     Star Trek: First Contact  Captain Jean-Luc Picard       NaN\n## 2        200      Star Trek: Insurrection  Captain Jean-Luc Picard       NaN\n## 1        200      Star Trek: Insurrection                      NaN  Producer\n## 3        201           Star Trek: Nemesis  Captain Jean-Luc Picard       NaN\n## ..       ...                          ...                      ...       ...\n## 46   1088162  The Elves and the Shoemaker         Narrator (voice)       NaN\n## 140  1093380                Red Dwarf A-Z                     Self       NaN\n## 0    1093380                Red Dwarf A-Z                      NaN    Thanks\n## 126  1095754           John Clare: \"I Am\"            Cyrus Redding       NaN\n## 47   1104829     In the Company of Whales                 Narrator       NaN\n## \n## [156 rows x 4 columns]\n\n\n\n\n\n\n\n\n\n\nTry It Out: JSON File Parsing\n\n\n\n\n\nThe Movie Database\n\nI used TMDB to find all movies resulting from the query “Star Trek” and stored the resulting JSON file here.\n\n\nProblem\nR solution\nPython solution\n\n\n\nCreate a data frame using the Star Trek query results. Because there were 6 pages of query results, the JSON file looks a bit different than the format used in the example above. Can you create a plot of the release date and rating of each movie?\n\n\n\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(dplyr)\n\nfile_loc &lt;- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\n\nstartrek &lt;- fromJSON(file_loc) |&gt;\n  unnest(results)\n\nstartrek |&gt;\n  select(title, release_date, popularity, vote_average, vote_count) |&gt;\n  head()\n## # A tibble: 6 × 5\n##   title                    release_date popularity vote_average vote_count\n##   &lt;chr&gt;                    &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;      &lt;int&gt;\n## 1 Star Trek                2009-05-06         51.2         7.43       9075\n## 2 Star Trek: Nemesis       2002-12-13         33.6         6.29       1218\n## 3 Star Trek Beyond         2016-07-07         40.3         6.78       6037\n## 4 Star Trek: Insurrection  1998-12-11         28.8         6.42       1025\n## 5 Star Trek Into Darkness  2013-05-05         41.3         7.33       8370\n## 6 Star Trek: First Contact 1996-11-22         29.8         7.30       1519\n\n# convert release_date to datetime\nlibrary(lubridate)\nstartrek &lt;- startrek |&gt;\n  mutate(rel_date = ymd(release_date))\n\nstartrek |&gt;\n  arrange(rel_date) |&gt;\n  select(title, rel_date, popularity, vote_average, vote_count) |&gt;\n  head()\n## # A tibble: 6 × 5\n##   title                            rel_date   popularity vote_average vote_count\n##   &lt;chr&gt;                            &lt;date&gt;          &lt;dbl&gt;        &lt;dbl&gt;      &lt;int&gt;\n## 1 Jr. Star Trek                    1969-01-01       0.6          0             0\n## 2 Ömer the Tourist in Star Trek    1973-01-01       1.90         6.36         38\n## 3 Star Trek: The Motion Picture    1979-12-07      26.5          6.50       1480\n## 4 Star Trek II: The Wrath of Khan  1982-06-04      21.6          7.47       1655\n## 5 Leonard Nimoy: Star Trek Memori… 1983-01-01       0.6          7             1\n## 6 Star Trek III: The Search for S… 1984-06-01      17.0          6.62       1159\n\nlibrary(ggplot2)\nggplot(startrek, aes(x = rel_date, y = popularity)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\n\n\n\nggplot(startrek, aes(x = rel_date, y = vote_average)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfile_loc = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\ntrek = pd.read_json(file_loc)\n\ntrek['results2'] = trek.results.map(pd.json_normalize)\n\n# This doesn't actually keep the page info but I don't think we need that\ntrek_tidy = pd.concat(trek.results2.to_list())\ntrek_tidy['rel_date'] = pd.to_datetime(trek_tidy.release_date)\n\nimport matplotlib.pyplot as plt\np1 = trek_tidy.plot.scatter('rel_date', 'popularity')\nplt.show()\n\n\n\n\n\n\n\np2 = trek_tidy.plot.scatter('rel_date', 'vote_average')\nplt.show()",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "href": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "title": "28  Functional Programming",
    "section": "\n28.4 Assembling Hierarchical Data",
    "text": "28.4 Assembling Hierarchical Data\nAnother common situation we find ourselves in as analysts is to have multiple levels of data.\nLet’s start with a totally absurd hypothetical situation: Suppose I watched the documentary “Chicken People” and became interested in the different breeds of chicken. As a data scientist, I want to assemble a dataset on chicken breeds that I might use to decide what breed(s) to raise.\nA site such as Cackle Hatchery has an overall summary table as well as pages for each individual breed. I’m not going to show you how to web scrape here - it’s not relevant to this chapter - but we can at least outline the process:\n\nAcquire the overall table\nUse the links to each breed in the overall table to get more specific information for each breed\n\nThis will require a function to scrape that individual data\nWe can use map to apply that function to acquire individual data from each breed\n\n\n\nI’ve used this approach to generate two files:\n\n\nchicken-breeds.csv - the original table of breed information\n\nchicken-breed-details.json, which is a JSON file assembled by scraping information off of each breed’s individual page.\n\n\n\n\n\n\n\nTry It Out: Chicken Breed Data Assembly\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nCan you create a nested data frame that has all of the information from both the CSV and JSON file in a single tabular structure?\n\n\n\nlibrary(readr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(stringr)\n\noverall &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails &lt;- fromJSON(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\nhead(overall)\n## # A tibble: 6 × 9\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Austra White            220-280 eggs pe… Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         &lt;NA&gt;        \n## 3 Barnevelder             150-200 eggs pe… Dark Brown  Poor         &lt;NA&gt;        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         &lt;NA&gt;        \n## 5 Barred Cochin Standard  110-160 eggs pe… Brown       Very         &lt;NA&gt;        \n## 6 Barred Old English Ban… 120 eggs per ye… Cream       Good         &lt;NA&gt;        \n## # ℹ 4 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;\nhead(details)\n##                             name\n## 1           Austra White Chicken\n## 2            Ayam Cemani Chicken\n## 3            Barnevelder Chicken\n## 4           Barred Cochin Bantam\n## 5 Barred Cochin Standard Chicken\n## 6 Barred Old English Game Bantam\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              description\n## 1 Our Austra Whites are a cross between one of our best Cackle HatcheryÂ® production/bloodlines of Black Australorp rooster and one of our best Cackle HatcheryÂ® production/bloodlines of White Leghorn hens (parent stock). They were first developed in the early 1900â\\u0080\\u0099s. This cross produces offspring that are very good layers throughout the year and lay a very light brown to off white egg shell color egg. The Austra White pullet will be white with little black specks in some feathers. This cross is considered to be a heavier breed and their disposition is calmer than the pure Leghorn chicken breed. There are many benefits to raising baby chickens from this cross. These hens will lay a lot of large eggs; have good feed to egg production ratios and good for chickens for backyards. Raising chickens for eggs at home can be fun and relaxing. Free range chicken eggs are great tasting eggs and this hybrid chicken breed has good reflexes for predator avoidance, however, they are white and easily seen.\\nWe also offer at limited times of the year Austra White Fertile Hatching Eggs\n## 2                                    The breed originated from the island of Java, Indonesia and has probably been used for centuries in religious and mystical purposes. The breed was first described by Dutch colonial settlers and first imported to Europe in 1998 by Dutch breeder Jan Steverink. Their beak and tongue, comb and wattles, even their meat bones and organs appear black. The blood of the Ayam Cemani is normally colored. The birdâ\\u0080\\u0099s black color occurs as a result of excess pigmentation of the tissues, caused by a genetic condition known as fibro melanosis. This gene is also found in some other black fowl breeds. Roosters can get some mulberry upon maturity due to testosterone and other influences. The hens lay cream-colored eggs, although they are poor setters and rarely hatch their own brood. Our Ayam Cemaniâ\\u0080\\u0099s bloodline includes Raven and some Greenfire.\\n30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks.\\nWe now have Ayam Cemani Fertile Hatching Eggs for sale click here!\n## 3                                                                                                                                                                                                                                                                                                                                                                          The Barnevelder chicken originates from the Barneveld region of Holland and known for laying a dark brown egg. This beautiful bird has a single comb, is hardy and quiet and doesnâ\\u0080\\u0099t mind being confined. The breed was first recognized by the American Standard of Perfection in 1991. Cackle HatcheryÂ®â\\u0080\\u0099s Barnevelders breeding stock will produce feathering of partridge single laced andÂ  double laced feather pattern. Each year breeding season our objective is to breed more for the double laced pattern. The Barnevelder chicken is rare to find in the USA but becoming more popular each year. To buy Barnevelder chickens, please select your quantity under 50 above.\\nAlso may likeÂ Dark Brown Egg Female Surplus.\n## 4                                                                                                                                                                                                                                                                                                                                                                                          At Cackle HatcheryÂ®, we offer several different types of the Cochin Bantam, including the Barred Cochin Bantam. If youâ\\u0080\\u0099re not familiar with this chicken breed, it is a miniature version of the Standard Cochin. For more than a century these chickens were admitted to the American Poultry Standard of Perfection, and they make great pets and mothers for chicks. The standard version of the breed, the Barred Cochin Bantam, is an excellent choice, so place your order today. For more details about the Barred Cochin Chicken, please contact us!\\nMany people who like this breed of chicken also like the standard version of the breed, theÂ Barred Cochin Chicken.\\nAlso may likeÂ Cochin Bantam Special Surplus.\n## 5                                                                                                                                          When it comes to rare breed chickens, the professionals at Cackle HatcheryÂ® have a lot to offer. We have more than 200 breeds to choose from, including the Barred Cochin Standard. This bird is one of the many color types of Cochins that we have available and it is notable for several reasons. This is a very large chicken with a lot of unique feathering and feathered legs. This chicken is also great around children, making it a perfect pet for the family farm. This is a very hard color to find of the standard cochin with very few breeders in the USA. We further improved our flock by adding some of Roland Doerr bloodline into our flock in 2009. Make a great show and exhibition type chicken. You can place your order today or you can call us for more information.\\nMany people who like this breed of chicken also like the miniature version of the breed (bantam), theÂ Barred Cochin Bantam Chicken.\\nAlso, may likeÂ Cochin Standard Surplus Special.\\nÂ\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                             Cackle HatcheryÂ® offers several varieties of the Old English Bantam, a miniature version of the Standard Old English Game chicken. The Barred Old English Game Bantam is just one of many high-quality chicken breeds we have available, and this variety is notable for its black and white spotted coloring. Because these chickens require little space and feed they make perfect pets, and they are generally well behaved. In fact, some Barred Old English Game Bantams can even become so tame that they will sit on your arm. Get started today by placing your order for baby chicks, and contact us if you have questions!\\nAlso may likeÂ Old English Bantam Surplus Special.\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    breed_facts\n## 1                                                                                                                                                                                                                                                                                                                                                                : Not applicable, Â Henâ\\u0080\\u0094â\\u0080\\u00935 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u00946 1/2 lbs\\nPulletâ\\u0080\\u00944 1/2 lbs\\nCockerelâ\\u0080\\u0094-5 1/2 lbs, Â Primary production, Egg Laying & Pet/Secondary meat source, Very light brown to off white egg shell color, Â Â 220-280 eggs per year (estimates only, seeÂ FAQ), Â Large, Â Active, Â 80-85%, Fertility Percentage:Â 65-80%, Non Setter, Â 12 Females to 1 Males, Â 4 to 8 feet, Â Hybrid, Â No, Â No, Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline or strain of Austra White since 1939.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                         Â Not applicable, Â Hen â\\u0080\\u0094â\\u0080\\u0094-4.1/2 lbs\\nRoosterâ\\u0080\\u0094-7 lb\\nPulletâ\\u0080\\u0094â\\u0080\\u00944Â lbs\\nCockerelâ\\u0080\\u00945 lbs, Â Ornamental/ Meat and Egg, Â Cream, Light tan, Â 80-120 per yearÂ (estimates only, seeÂ FAQ), Docile, Â 8 Females to 1 Male, Â 4+ feet, Yes sometimes, Â Java, Indonesia, Â No, Â Not listed, Breeder Farm Source:Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline or strain of pure Ayam Cemani since 2018\n## 3                                                                                                                                                                                                                                                                                                Â Continental Class, Weights:Â Henâ\\u0080\\u0094â\\u0080\\u00936 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u00947 lbs\\nPulletâ\\u0080\\u0094-5 lbs\\nCockerelâ\\u0080\\u0094â\\u0080\\u00936 lbs, Â Egg Laying; Exhibition, Â Dark Brown, Â 150-200 eggs per year (estimates only, see FAQ), Â Large, Â Active, Â 80-85%, Fertility Percentage:Â 65-80%, Broody:Â Non Setter, Â 7 Females to 1 Male, Â 2 to 4 feet, Country of Origin:Â Holland, APA:Â Yes, Recognized by the American Standard of Perfection, TLC:Â Not Listed, BREEDER FARM SOURCE:Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline/strain of pure Barnevelder chickens since 2008.\n## 4                                                                                                                                                                                                                                                                                                                                                                           Feather Legged Bantams, Hen â\\u0080\\u0094â\\u0080\\u0094-26 oz Â Â Â Â Â Â Roosterâ\\u0080\\u0094â\\u0080\\u009330 oz, Purpose and Type:Â Pets,Very Broody, Ornamental; Exhibition, Egg Shell Color:Â Brown Bantam Sized Egg, Egg Production:Â Fair, Egg Size:Â Small, Â Docile, 75-80%, Â 40-55%, Broody:Â Setters, Â 6 Females to 1 Male, Â 0 to 2 feet, Country of Origin:Â Asia, Yes, Recognized by the Standard of Perfection in 1965, No, â\\u0080\\u009c Cackle HatcheryÂ® Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure Cochin Bantams since 1971.\n## 5 Â Asiatic Class, Weights â\\u0080\\u0093Â Henâ\\u0080\\u0094â\\u0080\\u00938 1/2 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u009411 lbs\\nPulletâ\\u0080\\u00947 lbs\\nCockerelâ\\u0080\\u0094-9 lbs, Purpose and Type â\\u0080\\u0093Â OrnamentalÂ and meat; Exhibition, Â Brown, Egg ProductionÂ â\\u0080\\u0093 110-160 eggs per year (*estimates only, seeÂ FAQ), Egg Size:Â Medium-Large, Â Docile, Â 40-55%, Broody:Â Setter, Â 6 Females to 1 Male, Â 0 to 2 feet, Asia, Yes, Recognized by the American Standard of Perfection in 1982., Recovering Status, Considered a sustainable heritage chicken breed, Â â\\u0080\\u009cCackle HatcheryÂ®Â Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975., Breeder Farm Source:Â Cackle HatcheryÂ®Â Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975.\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Game Bantams, â\\u0080\\u0093Â Hen â\\u0080\\u0094â\\u0080\\u0094-22 ozÂ Â Â Â Â Â Â Â Roosterâ\\u0080\\u0094-24 oz\\nPulletâ\\u0080\\u0094â\\u0080\\u009420 ozÂ Â Â Â Â Â Â Â Cockerelâ\\u0080\\u009422 oz, Purpose and TypeÂ â\\u0080\\u0093 Ornamental; Exhibition, Egg Shell ColorÂ â\\u0080\\u0093 Cream or Tinted Bantam Sized Egg, Â â\\u0080\\u0093Â Poor, Egg Size:Â Small, Â Active, Â 40-55%, Broody:Â Setters, Â 9 Females to 1 Male, Â 3+ feet, â\\u0080\\u0093Â Europe, : No, No\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    availability\n## 1                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 2 04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, low-availability, unavailable, available, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, low-availability, low-availability, unavailable, low-availability, low-availability, low-availability, low-availability\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 5                     04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , low-availability, low-availability, unavailable, low-availability, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, available, unavailable, unavailable, unavailable, low-availability, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 6                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n##                                                                                                                            videos\n## 1                                                                                       https://www.youtube.com/embed/fnKRESJcpuY\n## 2                                            https://www.youtube.com/embed/AapH24ImQBo, https://www.youtube.com/embed/Y00jzCR7b-s\n## 3 https://www.youtube.com/embed/J7hIxdTgOPc, https://www.youtube.com/embed/sqhz7Rdc_0U, https://www.youtube.com/embed/T7UeVZe3j10\n## 4 https://www.youtube.com/embed/kr7huXt_-Fk, https://www.youtube.com/embed/dwsRHSr5scc, https://www.youtube.com/embed/k82eEH913Y8\n## 5                                            https://www.youtube.com/embed/iNwIbI3xbHA, https://www.youtube.com/embed/HbdlI_AEVQM\n## 6                                            https://www.youtube.com/embed/3tN2xqkoxLc, https://www.youtube.com/embed/dpLJK0Sc2xk\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            reviews\n## 1 364845, 326510, 326419, 172038, 171864, 171778, 171750, 171713, 171601, 171490, 171424, 171417, 171391, 171259, 171219, 170835, 170494, 170468, 170254, 170205, 170136, 5, 5, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, Andrea , Millermcnutt , Vinny , Sharon , Chad , Alexa , Lori , Mike , mmeyer , Kasen , Raymond , rhodyray , Sonia  , Chickens R Us , Darrell , Dorothy , Jill, Indiana April 2013 , Michelle, New York May 2013 , Beth Oklahoma February 2015 , Michael Georgia February 2014 , Janie, Missouri May 2014 , 2020-07-17T16:18:08-05:00, 2020-04-18T01:05:55-05:00, 2020-03-13T03:22:27-05:00, 2018-08-08T21:29:59-05:00, 2018-03-08T20:42:45-05:00, 2017-10-30T22:18:53-05:00, 2017-09-08T01:32:20-05:00, 2017-07-29T13:21:09-05:00, 2017-05-15T06:48:03-05:00, 2017-03-08T17:41:26-05:00, 2016-12-06T03:08:46-05:00, 2016-12-03T23:16:45-05:00, 2016-11-06T05:53:01-05:00, 2016-06-16T20:08:04-05:00, 2016-05-19T10:45:33-05:00, 2015-12-25T06:23:42-05:00, 2015-11-30T14:06:01-05:00, 2015-11-27T21:42:12-05:00, 2015-11-12T15:56:18-05:00, 2015-11-10T21:37:58-05:00, 2015-11-09T15:29:51-05:00, Very good, LOOOOVE my Austra White Hen, Good, Thank You, Austra Duds, Got my Chicks!, Nice birds!, Wonderful Chicks, Motherâ\\u0080\\u0099s day surprise , Good, Like this bird, Good Hens, Adventurous birds! , Not your ordinary personalities, BEST\\n, Yum! We used to get , Cackle Hatchery, Cackle Hatchery, Testimonal, Cackle Hatchery, Cackle Hatchery, Very happy to see my 25 hens get here on time. All of them seem, so far happy and healthy., Purchased Austra White pullet last year. I LOOOVE her, she went broody late summer, hatched 2 eggs (which I had ordered) such a good Mommy bird.  Henny Penny is very friendly, she lets me pick her up and she comes running to me when I walk out the door.  She is a precious bird!, Want to say two years later they are great birds hardy and great free rangers. 5 eggs a week from my birds. Will get this breed again. , Got my chicks, all alive and doing well. I had 2 setters and they adopted them as their own. So precious. Thank you, a very pleased customer., I ordered a handful of these last spring and have to admit I am disappointed with them. When they do lay the eggs are super large and a beautiful cream color but mine do not lay often. Two of them are broody every couple months and never seem to lay and the other two only give me eggs once in awhile. My Easter Eggers, Ameraucanas, and Welsummers are by far better layers., Got my baby chicks this morning! All healthy, all doing fantastic! The tracking for my package was spot on with much appreciated detail. I got my chicks shipped to Hawaii which has additional fees and you have to have a higher minimum of birds and it was totally 100% worth it. I will be using your service in the future and will be recommending it to others. Thank you so very much!, I have 3 of these pullets and at about 21 weeks they are just starting to lay.  They are large, handsome and very friendly birds. As youngsters they were always flying up on my shoulders and are very adventurous.  Their eggs are still smallish and are light cream in color.  Iâ\\u0080\\u0099m very happy with them., I received 4 Austra White pullets as part of an order of 16 chicks on March 1st and they have made beautiful hens.  One began laying at 16 weeks of age which is the earliest I have ever heard of.  I would highly recommend this breed and Cackle Hatchery as this flock of hens that also contains 4 Buff Orpingtons, 3 Turkens, 3 Cuckoo Marans and 2 Black Golden Laced Wyandottes is the nicest flock I have ever had in my 50+ years of raising chickens., I ordered 15 chicks at the end of November 2016.  10 EEâ\\u0080\\u0099s and 5\\n Austra Whites.  All 5 of the Whites survived, but I lost 5 EE in January.  The white chicks were more aggrisive then my Easter eggers and separated them for a month until bigger.  I kept them in a large brooder box in an enclosed porch area with an electric chicken heater until April. The whites were very hungry chicks.  Iâ\\u0080\\u0099ve never had a leg horn breed type before.  They are certainly more assertive than my Easter egger, RIR,  and Cinnamon Queens (they lay large to jumbo eggs, but are kind of dumb birds).  Today, Motherâ\\u0080\\u0099s Day, one of my Austra Whites layed a small pretty cream egg!  Looking forward to seeing how big  and how many they  can lay., Good, Love this bird.  Not as skittish or as flighty as many suggest.  One was bullied by a Black Sex Link however I solved the issue by removing the Black Sex Link for three days and now everything is fine.  Lay some of the biggest eggs Iâ\\u0080\\u0099ve ever seen.  Two within the last two weeks measuring 3â\\u0080³ X 2â\\u0080³ (thatâ\\u0080\\u0099s big my friend).  Friendly at times and will eat from my hand., Received chicks in the middle of March and as they age have become more friendly.  Guess it takes time for them to trust humans.  Anyhow they have been consistent layers and one bird in particular has layed two eggs over the last month that measured 3â\\u0080³ X 2â\\u0080³.  Now they are humongeous eggs.  The Austra Whites share a coop with Red and Black Sex link hens are all are doing fine., We ordered 6 Austra Whites and received 7 on time and in excellent, healthy condition. We gave them water with electrolytes right away, and they were so much fun to handle as tiny chicks. They are now about 5 months old and we still have all 7. They havenâ\\u0080\\u0099t started laying yet, but they are very adventurous and have been from the time they were baby chicks. They were the first of all 5 breeds we have to fly out of the trough we kept the babies in, they were the first to scale our fence, they were the first to find their way onto the roof of our house, and they are the only breed we have that will wander far from the house (which kinda worries us because we have had hawks get a couple other breeds, and Austra Whites areâ\\u0080¦wellâ\\u0080¦white.) They definitely keep us on our toes! , Out of 4:  One was very mean at one month, pecking all her siblings so I had to get rid of her.  Another one escaped three weeks ago and never was seen again.  Another is an escape artists and ALWAYS climbs up over the chicken wire.  I have one ordinary one.  Phew!  This breed keeps me hopping., I am now on week number 2 with the 22 Chicks Cackle has sent me. They are all very healthy and beautiful birds. They are growing very nice and canâ\\u0080\\u0099t wait for them to be able to move to the Big coop so they can roam around freely. More orders to come for sure. Friendly FAST Service for sure. Already made a pre-order for some Buff Orpingtons. Can not wait to get them., Yum! We used to get green eggs all the time but my green egg layers have been on strkie lately so all we have now is rose and brown ð\\u009f\\u0099\\u0082 I Love green eggs!, Just wanted to tell you that we received our chicks midday on the 17th..and all were perfectly perfect and adorable! Thanks soooooo much for working with us on a quicker ship date:), They all arrived yesterday in great health and lots of vigor. By far the best order we have ever received from any hatchery. Thanks, Last year I purchased a dozen chickens six barred rocks and six Austra Whites. i am pleased with them. All of these girls are healthy and ornery at a year old. They are just now at nearly a year old starting to lay eggs. All of the eggs i have found have been quite large, surprising for pullets. They are all loaded with personality. With the exception of a few chickens I obtained from neighbors and friends, at least 3/4 of my flock came from Cackle Hatchery. Even the mixed breeds that I hatched out indirectly came from your hatchery since the parent birds were hatched in your facility. We have 23 chickens in all and I am always getting compliments on how good they look, as well as how friendly they are. I just wanted to say thank you for these fine quality birds that I get to enjoy having around. Thank you, Just a quick thank you. Once again your company delivered a box full of live healthy and vibrant chickens. All arrived doing fine. We ordered 60 and weâ\\u0080\\u0099re not disappointed with the chicks in the least. Thanks to Cackle sending a few more chicks for warmth than we ordered.\\nWe ordered Egyptian for the first time. They are the most active chicks I have ever seen. The special heavy assorted was a great bargain. The chicks were even marked as requested!!!. You have earned my business once again. I have shared your catalog with friends and relatives and will continue to do so. Thanks again and keep up the excellent service., I received my chicks today and found all but 1 made it alive not bad odds for ordering 70 chicks. Thank you for the chicks and I am looking forward to ordering more. \n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       607315, 607316, Cackle Hatchery , Cackle Hatchery , 2022-10-05T11:35:27-05:00, 2022-10-05T11:36:06-05:00, Oh no so sorry to hear that. As stated on website possible white on wings, though rare,  is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks., Oh no so sorry to hear that. As stated on website possible white on wings, though rare, is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes. We cannot guarantee the distribution of black pigment on chicks.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  172155, 171588, 171517, 170545, 170528, 5, NA, NA, NA, NA, Mtviewranch , AnaMaria , T Rathjen , Jerry, Arizona March 2012 , Bill, Missouri February 2013 , 2019-01-07T02:35:08-05:00, 2017-05-05T20:35:00-05:00, 2017-03-20T19:00:11-05:00, 2015-12-02T21:11:34-05:00, 2015-11-30T17:33:10-05:00, Love them!!, Beautiful Birds, Beautiful Birds, Chicken order, Thank You, Iâ\\u0080\\u0099ve ordered from you guys twice now and I have not lost one chick. In the two orders you sent extra girls for warmth I think 7 all together bonus! Beautiful birds my barnevelders just started laying large dark speckled eggs. They have great personalities I highly recommend this breed., My 4 Barnevelders are 3 weeks old. Theyâ\\u0080\\u0099ve arrived in 24 hours to Atlanta, in great health. They seem to grow by the second, but weâ\\u0080\\u0099ve had no problems since their arrival. They are beutiful, full of personality, friendly and well behaved. Im very happy. Thank you. I will try to post a follow ip when they are older., I purchased my chicks in Sept of 2016.  Everyone of them were healthy and lived.  I purchased a combination of Sussex and Wyandotteâ\\u0080\\u0099s.  They were all hand raised and are very friendly and love to â\\u0080\\u009ctalkâ\\u0080\\u009d to you.  We had no issues with health or poop and they have grown into beautiful, large birds, and produce plenty of eggs.  We will definitely buy from you again when we are ready to expand our flock., I just wanted to send you some feedback on an order you sent me. They arrived all healthy and are doing great!!! My wife and I wanted to thank you for the way you handled the order and the quality of the chicks you sent us., Iâ\\u0080\\u0099m just wanting to express my thanks to you for the five different breeds of chicks that I got from your hatchery Feb 12th. They are doing quite well, one or two of the chicks has a soft poop but still seem to be full of energy. Being I never experienced chicks before, I was really surprised at how fast they grow. On the fifth day I had to extend my wall of the brooding area because they would jump and or fly over my twelve inch high pen. I did lose one of the barred rocks around the third or fourth day. I actually expected to lose two or three but I am blessed to have lost only one. Thanks for good service, information and kindly taking the time to answer my many questions.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     325390, 325352, 325322, 325320, 325106, 170959, 170836, 5, 5, 3, 5, 2, NA, NA, farmr john , Susannah C , mudman , Crystal , Grace , Sylvia June 2008 , Fares , 2019-10-31T16:58:50-05:00, 2019-10-30T16:33:57-05:00, 2019-10-29T21:24:13-05:00, 2019-10-29T21:00:55-05:00, 2019-07-01T14:24:36-05:00, 2016-01-11T16:37:32-05:00, 2015-12-25T06:46:45-05:00, Awesome birds, Beautiful, sweet birds / consistent layers, review, Beautiful chickens and fun pets !, Not my favs, Cackle Hatchery, We had a great year , These birds are awesome . They are lively from the start unlike to golden laced cochins. The show type bred into these birds is incredible. I have 6 pullets and 3 cockerels that could stand lots of competition. They are also heavy egg layers. So very impressed with them. They should be priced at double the cost!, We brought in barred cochin bantams last spring, and with thoughtful, gentle interaction, they have grown up to be beautiful, friendly, personable birds with very sweet personalities. I have many breeds in my flock, all with their own charm, but I do think these two Barred Cochins are perhaps the sweetest, cuddliest birds Iâ\\u0080\\u0099ve ever had.  Highly recommend for a gentle flock. (We have Polish, Silkies, Seramas, Brahmas, Wyandottes, Ameraucanas.) These birds arenâ\\u0080\\u0099t bullies and would not do well with super aggressive flockmates., nice breed doing well, We absolutely love our Barred Cochin Bantams!  I bought them for my children to raise as beloved pets.  They are so beautiful and sweet tempered.  They love to be held and fed worms.  , We bought a few different kinds of bantams and these were definitely my least favorite. One died in transit, one died two days after we got them, one was a rooster, and one is a nice little pullet. We bought sexed females. All-in-all I like the one that we ended up with, but did not like how sickly the rooster and the one that died were. Our other bantams that we received, golden laced Cochin, Rhode island red, and barred Rock, have all been healthy and seem much stronger., Just wanted you to know how pleased I am with my chickens from your company. They arrived in great health and have grown into these beauties. Many Thanks!, We had a great year at county fair.  This was our fifth year doing 4-H, but our first year being on the hoetaemsd with 4-H.  We have done static (the cooking, rockets, sewing, etc) exhibits before and last year we took 3 meat goats, but we had to keep them a t a friends house for 2 months prior to fair.  Last year se moved to a rental house 1 week before fair and didnâ\\u0080\\u0099t get our statics turned in.  So this year, now that we have a few acres we got a little carried away and took 38 statics   between 3 kids, and we took 3 dairy goats, 5 meat goats, 3 sheep, 6 chickens and 1 rabbit. Whoo!  that was probably a bit much!  We had a great time and the kids received 100 ribbons, plaques and awards in 4-H and our family got another 15 in open class.  We tried a lot of new things and the kids were pleased that for our first year in poultry they got either champion or reserve in showmanship.  We found out we donâ\\u0080\\u0099t like sheep, and we like the chickens more than we thought we would.  We love 4-H and will be incorporating 4-H projects into our homeschool so that most of the projects are done by June and we donâ\\u0080\\u0099t have the summer scramble to finish and then we can just concentrate on the livestock.\n## 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      326461, 326272, 326143, 171438, 170950, 5, 5, 5, NA, NA, Garcia  , PonyGirl , Ellen , Debbie North Carolina Jan 2017 , Larry December 2015 , 2020-04-02T06:12:50-05:00, 2020-01-11T22:50:46-05:00, 2019-12-10T23:38:08-05:00, 2017-01-04T15:23:10-05:00, 2016-01-11T15:51:18-05:00, Great Birds , Love my cacklehatchery chickens, Beautiful Birds, Cust Response, Cackle Hatchery, Great, beautiful, and wonderful mothers. My hen was only about 7 months old and she became broody and hatched out 7 chicks. They are great hens and letâ\\u0080\\u0099s not forget the roosters! He is very protective of his hens but at the same time, he is very nice and gentle and likes to be held by humans. Great bred for first time chicken owners or if you live in cold areas like colorado , Our chickens arrived in excellent condition and have all been incredibly healthy.  The are now 6 months old and absolutely beautiful. I get compliments on how pretty my cochins are all the time.  I highly recommend buying all your chickens from Cacklehatchery.com!!! , Ordered Barred Cochins. I love these birds. Big, sturdy birds that are beautiful, fully feathered down to their feet, and so sweet. Hands down my favorite breed!, Just wanted to say that the Standard Golden Laced Cochins and the Standard Barred Cochins I ordered and received back in April are looking just beautiful! Very beautiful chickens, with great feathering! I am pleased with both breeds, and am especially excited about the standard sized Barred Cochins because it was only when I saw them on your list of cochins did I realize that the Barred even existed in the standard size! Thank you again, I would and do recommended your hatchery to my friends and acquaintances., Liked your youtube videos!\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          170939, Emily Ohio April 2008 , 2016-01-11T15:27:50-05:00, Thanks You Cackle Hatchery, I received my order this morning and could not be more pleased! I ordered 30 chicks and received 57. They are beautiful! Also I want to thank you for offering a discount to 4-Hers! I really appreciate it. I will be showing my birds at the fair this year. You folks also have AWESOME customer service, so keep up the good work! You have won my business!\n\nbreed_details &lt;- details |&gt;\nunnest(c(\"name\", \"description\")) |&gt;\n  mutate(name = str_remove_all(name, \" Chicken\") |&gt;\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |&gt; \n           str_remove_all(\"Standard|Game\") |&gt;\n           str_replace_all(\"D’\", \"d\") |&gt;\n           str_to_title() |&gt;\n           str_squish())\n\noverall &lt;- overall |&gt;\n  mutate(name = `Chicken Breed Name` |&gt;\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |&gt;\n           str_remove_all(\"Standard|Game\") |&gt;\n           str_replace_all(\"D’\", \"d\") |&gt;\n           str_to_title() |&gt;\n           str_squish())\n\n# Names aren't exactly the same, but close enough after some minor string manipulation\nanti_join(overall, breed_details)\n## # A tibble: 7 × 10\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Buff Ameraucanas        180-200 eggs pe… Blue        Very         &lt;NA&gt;        \n## 2 Red Broiler             Poor             Brown       Good         Good        \n## 3 French Cuckoo Marans    180+ eggs per y… Dark Brown  Moderate     Moderate    \n## 4 Saipan Jungle Fowl      Poor             Light Brown Poor         &lt;NA&gt;        \n## 5 Splash Old English Ban… Poor             Cream       Good         &lt;NA&gt;        \n## 6 White Crested Blue Pol… Good             White       Poor         &lt;NA&gt;        \n## 7 White Rock              200 – 280 eggs … Brown       Very         &lt;NA&gt;        \n## # ℹ 5 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;, name &lt;chr&gt;\nanti_join(breed_details, overall)\n## # A tibble: 5 × 6\n##   name                description        breed_facts availability videos reviews\n##   &lt;chr&gt;               &lt;chr&gt;              &lt;list&gt;      &lt;list&gt;       &lt;list&gt; &lt;list&gt; \n## 1 Blue Polish         \"When it comes to… &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;   \n## 2 Buff Ameraucana     \"Buff Ameraucana … &lt;df&gt;        &lt;named list&gt; &lt;chr&gt;  &lt;df&gt;   \n## 3 Cackles Red Broiler \"A great alternat… &lt;df&gt;        &lt;named list&gt; &lt;chr&gt;  &lt;df&gt;   \n## 4 Saipan              \"Saipan chickens … &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;   \n## 5 White Plymouth Rock \"The White Plymou… &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;\n\nchickens &lt;- full_join(overall, breed_details)\nhead(chickens)\n## # A tibble: 6 × 15\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Austra White            220-280 eggs pe… Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         &lt;NA&gt;        \n## 3 Barnevelder             150-200 eggs pe… Dark Brown  Poor         &lt;NA&gt;        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         &lt;NA&gt;        \n## 5 Barred Cochin Standard  110-160 eggs pe… Brown       Very         &lt;NA&gt;        \n## 6 Barred Old English Ban… 120 eggs per ye… Cream       Good         &lt;NA&gt;        \n## # ℹ 10 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;, name &lt;chr&gt;, description &lt;chr&gt;, breed_facts &lt;list&gt;,\n## #   availability &lt;list&gt;, videos &lt;list&gt;, reviews &lt;list&gt;\n\n\n\n\nimport pandas as pd\noverall = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails = pd.read_json(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\noverall['name'] = overall['Chicken Breed Name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D’\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n\ndetails = details.explode(['name', 'description'])\n\ndetails['name'] = details['name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D’\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n  \nchickens = overall.merge(details, how='outer', on = 'name',  indicator=True)\nchickens[(chickens._merge!='both')][['name', '_merge']]\n##                           name      _merge\n## 44                 Blue Polish  right_only\n## 53             Buff Ameraucana  right_only\n## 54            Buff Ameraucanas   left_only\n## 63         Cackles Red Broiler  right_only\n## 92        French Cuckoo Marans   left_only\n## 128                Red Broiler   left_only\n## 138                     Saipan  right_only\n## 139         Saipan Jungle Fowl   left_only\n## 162  Splash Old English Bantam   left_only\n## 176  White Crested Blue Polish   left_only\n## 185        White Plymouth Rock  right_only\n## 187                 White Rock   left_only\n\nchickens.head()\n##        Chicken Breed Name  ... _merge\n## 0            Austra White  ...   both\n## 1             Ayam Cemani  ...   both\n## 2             Barnevelder  ...   both\n## 3  Barred Cochin Standard  ...   both\n## 4    Barred Cochin Bantam  ...   both\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Cleaning Chicken Data\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nUnnest the chicken breed facts data, cleaning the responses. Which jobs are most suitable for a functional programming approach?\n\n\n\n# Column names in breed_facts are too different\n# chickens_exp &lt;- chickens |&gt; unnest('breed_facts', names_sep='facts')\n\nfix_names &lt;- function(df) {\n  if (!is.null(df)) {\n    names(df) &lt;- names(df) |&gt;\n      str_to_title() |&gt;\n      str_remove_all(\"[^A-z]\") |&gt; # Remove anything that isn't A-z, including spaces.\n      str_replace_all(c(\"CountryOfOrigin?\" = \"Origin\", \"Weights\" = \"Weight\", \"Tlc\" = \"TLC\", \"Albc\" = \"ALBC\", \"Apa\" = \"APA\", \"BroodyS\" = \"Broody\", \"Temperment\" = \"Temperament\", \"Broody\" = \"Broody_facts\", \"Purpose\" = \"Purpose_facts\")) |&gt;\n      str_remove_all(\"Shell|FarmSource|SourceFarm|Small|PoultryShow\") |&gt;\n      str_replace_all(\"^$\", \"xxx\") # replace blank names with xxx\n    df\n  } else {\n    return(NULL) \n  }\n}\nchickens_fix &lt;- chickens |&gt; \n  mutate(breed_facts = map(breed_facts, fix_names))\n\n# Test names\nchickens_fix$breed_facts %&gt;% map(names) |&gt; unlist() |&gt; unique()\n##  [1] \"Class\"                \"Weight\"               \"Purpose_factsAndType\"\n##  [4] \"EggColor\"             \"EggProduction\"        \"EggSize\"             \n##  [7] \"Temperament\"          \"GenderAccuracy\"       \"FertilityPercentage\" \n## [10] \"Broody_facts\"         \"MatingRatio\"          \"RoostHeight\"         \n## [13] \"Origin\"               \"APA\"                  \"TLC\"                 \n## [16] \"Breeder\"              \"Purpose_facts\"        \"xxx\"                 \n## [19] \"ALBC\"                 \"Rooster\"              \"Pullet\"              \n## [22] \"Cockerel\"             \"Exhibition\"\n\nWe’ve fixed some of the misspellings and duplications. Rooster, Pullet, and Cockerel are all likely to be parsing issues stemming from Weight, but that’s the reality of working with data that is gathered from the internet.\n\nchickens_exp &lt;- chickens_fix |&gt; unnest(\"breed_facts\")\n\nhead(chickens_exp[,c(1, 16:37)])\n## # A tibble: 6 × 23\n##   `Chicken Breed Name`      EggProduction     EggSize Temperament GenderAccuracy\n##   &lt;chr&gt;                     &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;         \n## 1 Austra White              \"Â Â 220-280 egg… Â Large Â Active    Â 80-85%      \n## 2 Ayam Cemani               \"Â 80-120 per ye… &lt;NA&gt;    Docile      &lt;NA&gt;          \n## 3 Barnevelder               \"Â 150-200 eggs … Â Large Â Active    Â 80-85%      \n## 4 Barred Cochin Bantam      \"Egg Production:… Egg Si… Â Docile    75-80%        \n## 5 Barred Cochin Standard    \"Egg ProductionÂ… Egg Si… Â Docile    &lt;NA&gt;          \n## 6 Barred Old English Bantam \"Â â\\u0080\\u0093… Egg Si… Â Active    &lt;NA&gt;          \n## # ℹ 18 more variables: FertilityPercentage &lt;chr&gt;, Broody_facts &lt;chr&gt;,\n## #   MatingRatio &lt;chr&gt;, RoostHeight &lt;chr&gt;, Origin &lt;chr&gt;, APA &lt;chr&gt;, TLC &lt;chr&gt;,\n## #   Breeder &lt;chr&gt;, Purpose_facts &lt;chr&gt;, xxx &lt;chr&gt;, ALBC &lt;chr&gt;, Rooster &lt;chr&gt;,\n## #   Pullet &lt;chr&gt;, Cockerel &lt;chr&gt;, Exhibition &lt;chr&gt;, availability &lt;list&gt;,\n## #   videos &lt;list&gt;, reviews &lt;list&gt;\n\nThere’s still quite a bit of cleaning left to do to get this data to be “pretty”.\n\ntidy_col &lt;- function(x, text = \"(?:\\\\(estimates only, see FAQ\\\\))|(?:^APA)|(?:^TLC)|EggSize|(?:Fertility Percentage)|(?:Purpose and Type)\") {\n  str_remove_all(x, \"[\\u0600-\\u06FF]\") |&gt; # Remove non-ascii characters\n    str_remove_all(\"[Â®â¢Ââ]\") |&gt;\n    str_remove_all(text) |&gt;\n    str_remove_all(\"[:\\\\.\\\\?!\\\\*]\") |&gt;\n    str_replace_all(\"\\u0094\", \"-\") |&gt;\n    str_replace_all(\"-{1,}\", \"-\") |&gt;\n    str_squish()\n}\n\ntmp &lt;- mutate(chickens_exp, across(Class:Purpose_facts, tidy_col))\n\nhead(select(tmp, 1, Class:Purpose_facts))\n## # A tibble: 6 × 18\n##   `Chicken Breed Name`  Class Weight Purpose_factsAndType EggColor EggProduction\n##   &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;        \n## 1 Austra White          Not … Hen-5… Primary production,… Very li… 220-280 eggs…\n## 2 Ayam Cemani           Not … Hen -… &lt;NA&gt;                 Cream, … 80-120 per y…\n## 3 Barnevelder           Cont… Weigh… Egg Laying; Exhibit… Dark Br… 150-200 eggs…\n## 4 Barred Cochin Bantam  Feat… Hen -… Pets,Very Broody, O… Egg She… Egg Producti…\n## 5 Barred Cochin Standa… Asia… Weigh… Ornamental and meat… Brown    Egg Producti…\n## 6 Barred Old English B… Game… Hen -… Ornamental; Exhibit… Egg She… Poor         \n## # ℹ 12 more variables: EggSize &lt;chr&gt;, Temperament &lt;chr&gt;, GenderAccuracy &lt;chr&gt;,\n## #   FertilityPercentage &lt;chr&gt;, Broody_facts &lt;chr&gt;, MatingRatio &lt;chr&gt;,\n## #   RoostHeight &lt;chr&gt;, Origin &lt;chr&gt;, APA &lt;chr&gt;, TLC &lt;chr&gt;, Breeder &lt;chr&gt;,\n## #   Purpose_facts &lt;chr&gt;\n\nIf we consider the use of across() as a functional programming technique (which it is), then it is much easier to create a generic tidy_col function than to tidy each column individually. There are probably a few things we’ve missed, but the data looks decent for the amount of time we put in.\n\n\n\nimport pandas as pd\n\nXXX TODO",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#sec-functional-prog-refs",
    "href": "part-wrangling/08-functional-prog.html#sec-functional-prog-refs",
    "title": "28  Functional Programming",
    "section": "\n28.5 References",
    "text": "28.5 References\n\n\n\n\n[1] \nA. M. Kuchling, “Functional programming HOWTO. cPython documentation,” Oct. 31, 2022. [Online]. Available: https://docs.python.org/3/howto/functional.html. [Accessed: Mar. 20, 2023]\n\n\n[2] \nR.-G. Urma, M. Fusco, and A. Mycroft, Modern java in action: Lambdas, streams, functional and reactive programming, 2nd edition. Shelter Island: Manning, 2018. \n\n\n[3] \nM. Fogus, Functional JavaScript: Introducing functional programming with underscore.js, 1st edition. Sebastopol, CA: O’Reilly Media, 2013. \n\n\n[4] \nE. Buonanno, Functional programming in c#: How to write better c# code, 1st edition. Shelter Island, NY: Manning, 2017. \n\n\n[5] \nE. Akin, Object-oriented programming via fortran 90-95. Cambridge University Press, 2003. \n\n\n[6] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[7] \nrituraj_jain, “Nested list comprehensions in python. GeeksforGeeks,” Nov. 07, 2018. [Online]. Available: https://www.geeksforgeeks.org/nested-list-comprehensions-in-python/. [Accessed: Mar. 20, 2023]\n\n\n[8] \nHadley Wickham, Lionel Henry, and RStudio, “Purrr &lt;-&gt; base r. Purrr base r,” Oct. 10, 2022. [Online]. Available: https://purrr.tidyverse.org/articles/base.html. [Accessed: Apr. 10, 2023]\n\n\n[9] \nS. Seabold and J. Perktold, “Statsmodels: Econometric and statistical modeling with python,” in 9th python in science conference, 2010 [Online]. Available: https://proceedings.scipy.org/articles/Majora-92bf1922-011\n\n\n\n[10] \nF. Pedregosa et al., “Scikit-learn: Machine learning in python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011. \n\n\n[11] \nA. Menon, “Linear Regression in 6 lines of Python,” Medium. Oct. 2018 [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[12] \nWikipedia contributors, “JSON,” Wikipedia. Apr. 05, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=JSON&oldid=1148380721. [Accessed: Apr. 10, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html",
    "href": "part-advanced-topics/01-simulation.html",
    "title": "29  Simulation",
    "section": "",
    "text": "29.1  Objectives\nSimulation is an extremely important part of computational statistics. Bayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models. In this module, we’re going to touch on a few foundational pieces of simulation in computing. Hopefully, you will get more exposure to simulation in both theory and methods courses down the line.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#objectives",
    "href": "part-advanced-topics/01-simulation.html#objectives",
    "title": "29  Simulation",
    "section": "",
    "text": "Understand the limitations of pseudorandom number generation\nUnderstand the connection between sampling methods such as inverse probability sampling and rejection sampling and theoretical statistics\nImplement sampling methods as required for distributions that do not have closed form approximations\nUse Monte Carlo methods for integration and other simulation tasks\nDetermine the appropriate data structure to use for storing simulation task results",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#pseudorandom-number-generation",
    "href": "part-advanced-topics/01-simulation.html#pseudorandom-number-generation",
    "title": "29  Simulation",
    "section": "\n29.2 Pseudorandom Number Generation",
    "text": "29.2 Pseudorandom Number Generation\nComputers are almost entirely deterministic, which makes it very difficult to come up with “random” numbers. In addition to the deterministic nature of computing, it’s also somewhat important to be able to run the same code and get the same results every time, which isn’t possible if you rely on truly random numbers.\nHistorically, pseudorandom numbers were generated using linear congruential generators (LCGs) [1]. These algorithms aren’t typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem “random” but are actually deterministic. LCGs use modular arithmetic: \\[X_{n+1} = (aX_n + c) \\mod m\\] where \\(X_0\\) is the start value (the seed), \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. When using a LCG, the user generally specifies only the seed.\n\n\nLCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia)\n\nThe important thing to note here is that if you specify the same generator values (\\(a\\), \\(c\\), \\(m\\), and \\(X_0\\)), you will always get the same series of numbers. Since \\(a\\), \\(c\\), \\(m\\) are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time.\n\n\n\n\n\n\nWarning\n\n\n\nIt is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.\n\n\nOnce you set your seed, the remaining results will only be reproducible if you generate the same amount of random numbers every time.\n\n\nI once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds). We got brilliant results, but couldn’t ever reproduce them, because I hadn’t set the seed first and we never could figure out what the original seed was. 😭\n\n\n\n\n\n\nExample: Setting Seeds for Reproducibility\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(50))\n## [1] -0.1095366\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  4 57 69 10 76 15 67  1  3 91\n\n\n\n\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 500 std normal draws and take the mean\nnp.mean(rng.standard_normal(500))\n## np.float64(-0.008197259441979758)\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([33, 38,  3, 95,  3, 58, 79,  3, 77, 23])\n\n\n\n\nCompare the results above to these results:\n\n\nR\nPython\n\n\n\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(30))\n## [1] -0.1936645\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  49  37   6  34   9   3 100  43   7  29\n\n\n\n\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 30 std normal draws and take the mean\nnp.mean(rng.standard_normal(30))\n## np.float64(0.3016849078747997)\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([21, 49, 21, 99, 45,  1, 56, 70, 15, 82])\n\n\n\n\nNotice how the results have changed?\n\n\n\nTo make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I’ve already set the seed earlier. This introduces certain “fixed points” where results won’t change immediately after I’ve re-set the seed. This is particularly important when I’m generating bootstrap estimates, fitting models, or simulating data for graphics experiments.\nPick your seed in any way you want. I tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people’s birthdays, the current time in hhmmss format… basically, you can use anything, so long as it’s a valid integer.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#simulating-from-probability-distributions",
    "href": "part-advanced-topics/01-simulation.html#simulating-from-probability-distributions",
    "title": "29  Simulation",
    "section": "\n29.3 Simulating from Probability Distributions",
    "text": "29.3 Simulating from Probability Distributions\n\n29.3.1 Using Built-in Simulation Functions\nOften, we can get away with simulating data from a known distribution. In these cases, there is absolutely no point in DIY – use the implementation that is available in R or Python, as it will be more numerically stable and much faster due to someone else having optimized the underlying C code.\n\n\nR\nPython\n\n\n\nYou can see the various distribution options using ?Distributions. In general, dxxx is the PDF/PMF, pxxx is the CDF, qxxx is the quantile function, and rxxx gives you random numbers generated from the distribution. (xxx, obviously, is whatever distribution you’re looking to use.)\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nset.seed(109025879)\n\ntibble(\n  norm = rnorm(500),\n  gamma = rgamma(500, shape = 3, scale = 1),\n  exp = rexp(500, rate = 1), # R uses a exp(-ax) \n  t = rt(500, df = 5),\n  chisq = rchisq(500, 5)\n) %&gt;%\n  pivot_longer(1:5, names_to = \"dist\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = \"free\", nrow = 1)\n\n\n\n\n\n\nFigure 29.1: Density curves created from 500 samples from each of the Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions.\n\n\n\n\n\n\n\nimport random\nrandom.seed(109025879)\n\nimport pandas as pd\nimport numpy as np\n\nwide_df = pd.DataFrame({\n  \"norm\": np.random.normal(size=500),\n  \"gamma\": np.random.gamma(size=500, shape = 3, scale = 1),\n  \"exp\": np.random.exponential(size = 500, scale = 1),\n  \"t\": np.random.standard_t(df = 5, size = 500),\n  \"chisq\": np.random.chisquare(df = 5, size = 500)\n})\n\nlong_df = pd.melt(wide_df, id_vars = None, var_name = \"dist\", value_name = \"value\")\n\nfrom plotnine import *\n\np = (ggplot(long_df, aes(x = \"value\")) + geom_density() + facet_wrap(\"dist\", scales=\"free\", nrow = 1) + theme(figure_size=(10,2)))\np.show()\n\n\n\n\n\n\nFigure 29.2: Density curves created from 500 samples from each of the Chi-Sq(5), Exponential(1), Gamma(3, 1), Normal(0,1), and t(5) distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nGenerate variables x and y, where x is a sequence from -10 to 10 and y is equal to \\(x + \\epsilon\\), \\(\\epsilon \\sim N(0, 1)\\). Fit a linear regression to your simulated data (in R, lm, in Python, sklearn.linear_model’s LinearRegression).\nHint: Sample code for regression using sklearn [2].\n\n\n\nset.seed(20572983)\ndata &lt;- tibble(x = seq(-10, 10, .1), \n               y = x + rnorm(length(x)))\nregression &lt;- lm(y ~ x, data = data)\nsummary(regression)\n## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.14575 -0.70986  0.03186  0.65429  2.40305 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.01876    0.06869  -0.273    0.785    \n## x            0.99230    0.01184  83.823   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9738 on 199 degrees of freedom\n## Multiple R-squared:  0.9725, Adjusted R-squared:  0.9723 \n## F-statistic:  7026 on 1 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nrandom.seed(20572983)\n\ndata = pd.DataFrame({'x': np.arange(-10, 10, .1)})\ndata['y'] = data.x + np.random.normal(size = data.x.size)\n\n# Fitting the regression and predictions\n# scikit-learn requires that we reshape everything into\n# nparrays before we pass them into the model.fit() function.\nmodel = LinearRegression().\\\n  fit(data.x.values.reshape(-1, 1),\\\n      data.y.values.reshape(-1, 1))\ndata['pred'] = model.predict(data.x.values.reshape(-1, 1))\n\n# Plotting the results\nimport matplotlib.pyplot as plt\nplt.clf()\n\nplt.scatter(data.x, data.y)\nplt.plot(data.x, data.pred, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation from different distributions can be used to determine which estimators are most appropriate for a given scenario, to determine how likely it is to observe a specific value in a sample of size \\(n\\), and for many other applications.\n\n\n\n\n\n\nExample: Mean vs. Median\n\n\n\n\n\nSuppose you want to determine which estimator is better - mean or median - for \\(t\\)-distributions with different degrees of freedom and different numbers of observations.\nYou consider degrees of freedom \\(\\nu = 2, 5, 10, 30\\) and samples of size \\(n = 10, 20, 30, 50\\). For each sample, you calculate the mean and median, \\(\\hat\\theta\\), and you repeat this process \\(N=1000\\) times. You then calculate the bias of the sample estimator \\(\\hat\\theta\\) (mean, median) and the MSE of that estimator. The bias is \\(\\text{Bias}(\\hat\\theta, \\theta) = \\text{Bias}_\\theta\\left[\\hat\\theta\\right] = E_{x|\\theta}\\left[\\hat\\theta\\right]- \\theta = E_{x|\\theta} \\left[\\hat\\theta - \\theta\\right]\\). The MSE (mean squared error) of \\(\\hat\\theta\\) is \\(\\text{MSE}(\\hat\\theta) = E\\left[(\\hat\\theta-\\theta)^2\\right]\\). We know that the MSE is related to the bias and the variance of \\(\\hat\\theta\\): \\(\\text{MSE}(\\hat\\theta) = \\left(\\text{Bias}(\\hat\\theta,\\theta)\\right)^2+\\text{Var}(\\hat\\theta)\\) (the Bias-Variance tradeoff).\nHow can you use this information to determine which estimator is preferable in each \\(\\nu, n\\) situation?\n\n\nR\nPython\n\n\n\nFirst, let’s conduct the simulation. I find it helpful to make a function to simulate the data and calculate the necessary quantities, and then to use the purrr package to run my function \\(N\\) times with each parameter set.\nI always save the original simulated data, even when it’s not explicitly necessary, because I want to know exactly what results contributed, and this is the easiest way to get it. This is not the most memory efficient way to go, so if you are concerned about overflowing your computer’s memory, you might omit saving the data once you’re confident the simulation function works as planned.\n\nlibrary(tibble)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Write a function to draw a single sample\ndraw_sample &lt;- function(n = 10, v = 2) {\n  my_samp &lt;- rt(n, v)\n  tibble(data = list(my_samp), mean = mean(my_samp), median = median(my_samp))\n}\n\npars &lt;- expand_grid(n = c(10, 20, 30, 50), v = c(2, 5, 10, 30), id = 1:1000)\n\n# Only recompute results if they're not present already\nif (!file.exists(\"data/t-sim-results.RDS\")) {\n  results &lt;- pars |&gt;\n  rowwise() |&gt;\n  mutate(res = map2(n, v, draw_sample))\n  \n  results &lt;- unnest(results, \"res\")\n  saveRDS(results, file = \"data/t-sim-results.RDS\")\n} else {\n  results &lt;- readRDS(\"data/t-sim-results.RDS\")\n}\n\n\nresult_sum &lt;- results |&gt;\n  pivot_longer(mean:median, names_to = \"estimator\", values_to = \"value\") |&gt;\n  group_by(n, v, estimator) |&gt;\n  summarize(bias = mean(value), variance = var(value), mse = mean(value^2))\n\nNow that I have the simulation summaries, I can think about plotting the results to focus on the difference in Bias/MSE between estimators under different parameter sets. As our goal is to determine whether mean or median is preferable, we should focus on making that comparison as easy as possible by plotting mean and median lines on the same plot.\n\nlibrary(ggplot2)\nresult_sum |&gt; \n  pivot_longer(bias:mse, names_to = \"quantity\", values_to = \"value\") |&gt;\n  mutate(quantity = factor(quantity, levels = c(\"bias\", \"variance\", \"mse\"), ordered = T)) |&gt;\n  ggplot(aes(x = n, y = value, linetype = estimator, color = factor(v))) + \n  geom_line() + \n  guides(color = 'none') + \n  facet_grid(v~quantity, scales = \"free_y\", space = \"free_y\") + \n  xlab(\"Sample Size\") + ylab(\"Expected Value\") +\n  theme_bw()\n\n\n\nSummary chart of t-distribution estimator simulation results. Across \\(n\\) and $ u$, we see that the bias is around zero - that is, both mean and median are unbiased estimators (which is sensible given that the \\(t\\)-distribution is symmetrical). We see that when $ u=2$, the variance component is much higher than $ u=5,10,30$, and as MSE is the sum of the bias and variance, the MSE is also higher in this case. At $ u=5$, the mean and median perform fairly similarly; but for $ u=2$, the median has lower variance and thus lower MSE than the mean, which is due to the very heavy tails of the \\(t\\)-distribution with low degrees of freedom; the mean is influenced by the higher chances of extreme observations. Thus, when $ u=2$, we would prefer the median as an estimator. When $ u=10,30$, however, the mean has lower variance and thus MSE than the median across all values of \\(n\\), indicating that when there are sufficient degrees of freedom, we should prefer the mean as an estimator.\n\n\n\n\n\n\nfrom scipy.stats import t # t distribution class\nimport numpy as np # mean, median\nimport pandas as pd\n\nrv = t(2)\nn_opts = [10, 20, 30, 50]\nv_opts = [2, 5, 10, 30]\n\npars = [(n, v, i) for n in n_opts for v in v_opts for i in range(1000)]\n# Simulate t-data for each parameter combination and iteration\n# IMO, it's easier to use list comprehension for this, \n# but you could absolutely write a function instead\ndata = [t(x[1]).rvs(x[0]) for x in pars]\nmedian= [np.median(dat) for dat in data]\nmean = [np.mean(dat) for dat in data]\n\n# Construct a data structure to hold all results together\nresults = pd.DataFrame(pars, columns = ['n', 'df', 'i'])\nresults['data'] = data\nresults['median'] = median\nresults['mean'] = mean\n\ndef mse(x, x0=0):\n  return np.mean((x - x0)**2)\n\n# summarize\nres_sum = results.groupby(['n', 'df']).agg({'median': ['mean', 'var', mse], 'mean': ['mean', 'var', mse]})\n# Merge column indices together\nres_sum.columns = ['_'.join(col) for col in res_sum.columns]\n# \"Ungroup\" python style\nres_sum = res_sum.reset_index()\n# Melt to long form\nres_sum_long = pd.melt(res_sum, id_vars=['n', 'df'], var_name = 'est', value_name = 'value')\nres_sum_long[['estimator', 'measure']] = res_sum_long.est.str.split(\"_\", expand = True)\nres_sum_long['measure'] = res_sum_long['measure'].str.replace('mean', 'bias', regex = True)\nres_sum_long = res_sum_long.drop('est', axis = 1)\n\nNow that I have the simulation summaries, I can think about plotting the results to focus on the difference in Bias/MSE between estimators under different parameter sets. As our goal is to determine whether mean or median is preferable, we should focus on making that comparison as easy as possible by plotting mean and median lines on the same plot.\n\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt\n(\n  so.Plot(res_sum_long, x = \"n\", y = \"value\", linestyle = 'estimator')\n  .add(so.Line())\n  .facet('measure', 'df')\n  .share(y=\"row\")\n  .label(x = \"Sample Size\", y = \"Value\")\n  .show()\n)\n\n\n\nSummary chart of t-distribution estimator simulation results. Across \\(n\\) and $ u$, we see that the bias is around zero - that is, both mean and median are unbiased estimators (which is sensible given that the \\(t\\)-distribution is symmetrical). We see that when $ u=2$, the variance component is much higher than $ u=5,10,30$, and as MSE is the sum of the bias and variance, the MSE is also higher in this case. At $ u=5$, the mean and median perform fairly similarly; but for $ u=2$, the median has lower variance and thus lower MSE than the mean, which is due to the very heavy tails of the \\(t\\)-distribution with low degrees of freedom; the mean is influenced by the higher chances of extreme observations. Thus, when $ u=2$, we would prefer the median as an estimator. When $ u=10,30$, however, the mean has lower variance and thus MSE than the median across all values of \\(n\\), indicating that when there are sufficient degrees of freedom, we should prefer the mean as an estimator.\n\n\n\n\n\n\nConsidering the results we obtained, it seems clear that when we have a very low (\\(\\nu&lt;=5\\)) degrees of freedom, the median is a preferable estimator because it has lower variance; when we have a higher number of degrees of freedom, the mean is a preferable estimator on the basis of variance. Both estimators are asymptotically unbiased, and are unbiased even for small sample sizes when the distribution is symmetric.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#simulating-from-non-standard-distributions",
    "href": "part-advanced-topics/01-simulation.html#simulating-from-non-standard-distributions",
    "title": "29  Simulation",
    "section": "\n29.4 Simulating from Non-standard distributions",
    "text": "29.4 Simulating from Non-standard distributions\nIn some cases, you may want to simulate from a probability distribution that isn’t already implemented in your software of choice. To decide how to do this efficiently requires that you first take stock of what you actually have describing your distribution.\nDo you have a:\n\nmethod for getting from a standard distribution to your distribution via e.g. censoring, combining variables, etc.?\nprobability density/mass function?\ncumulative density function?\n\nIf you have a PDF/PMF, can it be integrated to get a CDF, either analytically or computationally?\nIf your PDF/PMF cannot be easily integrated, can you come up with a more standard distribution with a similar shape that you can use to get samples from your distribution?\nIf you have a method of getting from a standard distribution via censoring or combinations of variables, this is often the most intuitive way of generating a random sample from a complex distribution. In addition, this can be a good mechanism for generating distributions that mimic real-world processes that are not always natural to translate into functional probability distributions.\nIf you have a CDF, and the CDF can be easily inverted, it is often easiest to try inverse probability sampling first. In some cases, however, it is difficult to get an analytical form for the CDF (or the inverse CDF); in these cases, it may be preferable to work with the PDF/PMF instead. When working with a probability density or mass function, it is often natural to try rejection sampling before moving on to other methods.\n\n29.4.1 Inverse Probability Sampling\nIf you have a cumulative distribution function \\(F\\) that is nondecreasing, you can use that \\(F\\) to get samples from your distribution by generating uniform random variables \\(u \\sim U[0,1]\\) and computing \\(x = F^{-1}(u)\\). This method relies on the probability integral transform.\nLet’s consider how this might work in the case of a distribution with the following CDF: \\[F(x\\leq X) = x^2, \\ \\ 0 &lt; x &lt; 1.\\]\nSteps:\n\nGenerate a random number \\(u \\sim U[0,1]\\)\n\nFind the inverse of the desired CDF, \\(x^\\ast = F^{-1}_X(u)\\)\n\n\nThis procedure can often be done in a vectorized manner, if the CDF and inverse CDF (or quantile function) are written with vectorization in mind.\n\n\n\n\n\n\n\nFigure 29.3: Steps in Inverse Probability Sampling. First, we consider the CDF, which has a range of [0,1] over the domain of \\(x\\). A uniform[0,1] random variable can be used to select a random position along the CDF’s range, corresponding to the first step of the inverse probability sampling algorithm. Inverting the CDF produces a value \\(X^st\\), which is a random sample from the distribution specified by the CDF \\(F(x)\\).\n\n\n\n\n\n29.4.2 Rejection Sampling\nIn some cases, you have a PDF but not an easily obtainable or invert-able CDF. You could numerically integrate your PDF and use inverse probability sampling, but it may also be easier to use rejection sampling.\nRejection sampling, which is also sometimes called ‘acceptance-rejection’ sampling works for any distribution with a density function. Rejection sampling operates by generating random samples from a proposal distribution, and accepting those proposals with a certain probability determined by the relationship between the proposal and target distribution at that point.\nIt is easiest to illustrate how this works by showing a probability distribution with a defined domain (say, \\([0,10]\\)). In this case, I’ve created a nonstandard probability distribution by sketching a bimodal distribution on the interval and converting it to a polynomial.\n\n\n\n\n\n\n\nFigure 29.4: Intuition for Rejection Sampling. First, we consider the density function. If we sample points from the rectangle enclosing the density function, we can keep only points below the density, and those points would have \\(x\\) values corresponding to the probability distribution \\(f(x)\\).\n\n\n\n\nThink about throwing darts at this PDF (who hasn’t wanted to do that?). Any darts that land below the line describing \\(f(x)\\) would be distributed uniformly within the area under the curve, and their \\(x\\) values would be a random sample from \\(f(x)\\). Rejection sampling is essentially a formalization of this idea.\nIn the crudest case, rejection sampling is essentially a Monte Carlo sampling method – we generate random proposals from \\(\\mathbb{R}^n\\) and reject anything outside of our probability density. But, we can usually improve over generating over the full \\(\\mathbb{R}^n\\) – in most cases, we can find a better proposal space. In Figure 29.4, we generated 2000 points and rejected 1034 of those points, for an acceptance rate of 48.3%.\nBut, what if we were a bit smarter about how we proposed points? Bimodal distributions are irritating, because there’s always that space in the middle between the modes, but, what if we could at least not “throw darts” at the top corners?\nThe idea here is to get an “envelope distribution” - a function that is greater than or equal to \\(f(x)\\) at every point where \\(f(x) &gt; 0\\), and use that function to generate a proposal value. Then, we can accept or reject that proposal value based on the relationship between our envelope distribution and \\(f(x)\\). This is essentially the same thing that we did before, but our “envelope” was very rectangular.\nWhat is interesting is that we can generate an “envelope distribution” from a distribution that isn’t, strictly speaking, greater than \\(f(x)\\), by using some fancy scaling.\nAn envelope distribution \\(g(x)\\) can be used for rejection sampling if \\(\\displaystyle \\frac{f(x)}{g(x)} \\leq M\\) for some constant \\(M&lt;\\infty\\).\nSteps:\n\nGenerate \\(u\\sim U[0,1]\\)\n\nGenerate \\(x\\sim g(x)\\), a proposal value\nAccept \\(x\\) if \\(u &lt; \\displaystyle\\frac{f(x)}{M g(x)}\\); otherwise, start over.\n\n\n\n\n\n\n\n\nFigure 29.5: Rejection Sampling with a Better Envelope. First, we consider the density function f(x) and an envelope function g(x) such that M g(x) &gt; f(x) for a constant M &lt; infinity. We can determine \\(M\\) by the maximum of the ratio between f(x) and g(x) on the full domain \\(x\\). Then, if we sample points from g(x) and accept those points when a uniform(0,1) RV u &lt; f(x)/(M g(x)), this gives us a sample from f(x). Essentially, we keep the sample g(x) with probability scaled by the likelihood ratio between f and g at x. This is more adaptive than the rectangular proposal envelope but functionally works the same way.\n\n\n\n\nIn Figure 29.5, where the proposal density is a mixture of two truncated normal densities, we generated 2000 points and rejected 390 of those points, for an acceptance rate of 80.5%. I’m entirely sure I could get the proposal density much closer to \\(f(x)\\) if necessary, but it’s better to be able to illustrate how the process works. It is possible to use any given \\(M\\) for the proposal density, so long as the \\(M\\) you use meets the condition of \\(M \\geq \\frac{f(x)}{g(x)}\\) for all \\(x\\) with \\(P(x) &gt; 0\\). Using a larger \\(M\\) will simply decrease the acceptance probability, which will be less efficient but will still produce a valid sample.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#simulation-to-test-model-assumptions",
    "href": "part-advanced-topics/01-simulation.html#simulation-to-test-model-assumptions",
    "title": "29  Simulation",
    "section": "\n29.5 Simulation to test model assumptions",
    "text": "29.5 Simulation to test model assumptions\nOne of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model. Suppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors. How bad will your results be? Where can you trust the results, and where should you be cautious?\n\n\nThe purrr::map notation specifies that we are using the map function from the purrr package. When functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function.\npurrr::map takes an argument and for each “group” calls the compute_interval function, storing the results in res. So each row in res is a 1x2 tibble with columns lb and ub.\nThis pattern is very useful in all sorts of applications. You can read more about purrr in Chapter 28.\n\n\n\n\n\n\nExample: Confidence Interval coverage rates\n\n\n\n\n\n\n\nProblem\nSimulate Data\nFunction\nApply to Data\nResults\n\n\n\nSuppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our 25 observations.\nYou want to assess the coverage probability of a confidence interval computed under two different modeling scenarios:\n\nWorking with the log-transformed values, ln(x), and then transform the computed interval back\nWorking with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal.\n\nScenario 1:\n\nthe expected value of the standard normal deviates is 0\nthe variance of the data is 1\nthe SE(\\(\\overline x\\)) is \\(\\sqrt\\frac{1}{25} = \\frac{1}{5}\\)\n\n\nOur theoretical interval should be \\((\\exp(-1.96/5), \\exp(1.96/5)) = (0.6757, 1.4799)\\).\nScenario 2\n\nThe expected value of the lognormal distribution is \\(\\exp(1/2) = 1.6487213\\)\n\nThe variance of the data is \\((\\exp(1) - 1)(\\exp(1)) = 4.6707743\\)\n\nThe SE(\\(\\overline x\\)) is thus \\(\\sqrt{\\frac{(e^1 - 1)e^1}{25}} = \\frac{\\sqrt{(e^1 - 1)e^1}}{5} = 0.4322\\)\n\n\nOur theoretical interval should be \\((0.8015, 2.4959)\\). This interval could, if the circumstances were slightly different, contain 0, which is implausible for lognormally distributed data.\nOur expected values are different under scenario 1 and scenario 2:\n\nIn scenario 1 we are computing an interval for \\(\\mu\\)\n\nIn scenario 2, we are computing an interval for the population mean, which is \\(\\exp(\\mu + .5\\sigma^2)\\)\n\n\nBoth are valid quantities we might be interested in, but they do not mean the same thing.\n\n\n\nset.seed(40295023)\n\nsim &lt;- tibble(\n  id = rep(1:100, each = 25), # generate 100 samples of 25 points each\n  ln_x = rnorm(25*100), # generate 25 normal deviates for each sample\n  x = exp(ln_x), # transform into lognormal deviates\n) %&gt;%\n  # this creates a 100-row data frame, with one row for each id. \n  # the columns x, ln_x are stored in the data list-column as a tibble.\n  nest(data = c(x, ln_x))\n  \nhead(sim)\n## # A tibble: 6 × 2\n##      id data             \n##   &lt;int&gt; &lt;list&gt;           \n## 1     1 &lt;tibble [25 × 2]&gt;\n## 2     2 &lt;tibble [25 × 2]&gt;\n## 3     3 &lt;tibble [25 × 2]&gt;\n## 4     4 &lt;tibble [25 × 2]&gt;\n## 5     5 &lt;tibble [25 × 2]&gt;\n## 6     6 &lt;tibble [25 × 2]&gt;\nsim$data[[1]]\n## # A tibble: 25 × 2\n##        x    ln_x\n##    &lt;dbl&gt;   &lt;dbl&gt;\n##  1 0.310 -1.17  \n##  2 0.622 -0.475 \n##  3 0.303 -1.19  \n##  4 1.05   0.0525\n##  5 0.529 -0.636 \n##  6 1.09   0.0891\n##  7 1.97   0.676 \n##  8 8.94   2.19  \n##  9 0.598 -0.514 \n## 10 0.183 -1.70  \n## # ℹ 15 more rows\n\n\n\n\ncompute_interval &lt;- function(x) {\n  s1 &lt;- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x)))\n  s2 &lt;- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x))\n  tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n         mean = c(1, exp(1/2)),\n         lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]),\n         in_interval = (lb &lt; mean) & (ub &gt; mean))\n}\n\n\n\n\n\nsim_long &lt;- sim %&gt;%\n  # This line takes each data entry and computes an interval for x.\n  # .$x is code for take the argument you passed in to map and get the x column\n  mutate(res = purrr::map(data, ~compute_interval(.$x))) %&gt;%\n  # this \"frees\" res and we end up with two columns: lb and ub, for each scenario\n  unnest(res)\n  \n\nci_df &lt;- tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n                mu = c(1, exp(1/2)),\n                lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5),\n                ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5))\n\n\n\n\nggplot() + \n  geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), \n            data = ci_df,\n            fill = \"grey\", alpha = .5, color = NA) + \n  geom_vline(aes(xintercept = mu), data = ci_df) + \n  geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval),\n               data = sim_long) + \n  scale_color_manual(values = c(\"red\", \"black\")) + \n  theme_bw() + \n  facet_wrap(~scenario)\n\n\n\n\n\n\n\n\n\n\nFrom this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain.\n\n\n\n\n\nHere is a similar example worked through in SAS with IML. Note the use of BY-group processing to analyze each group at once - this is very similar to the use of purrr::map() in the R code.\n\n\n\n\n\n\nExample: Multilevel Regression and Post Stratification simulation\n\n\n\n\n\nMultilevel regression and post-stratification simulation with toddler bedtimes [3]\nThis example talks about how to take a biased sample and then recover the original unbiased estimates – which is something you have to test using simulation to be sure it works, because you never actually know what the true population features are when you are working with real world data. When reading this example, you may not be all that interested with the specific model - but focus on the process of simulating data for your analysis so that you understand how and why you would want to simulate data in order to test a computational method.\n\n\n\n\n\n\n\n\n\nExample: Regression and high-leverage points\n\n\n\n\n\nWhat happens if we have one high-leverage point (e.g. a point which is an outlier in both x and y)? How pathological do our regression coefficient estimates get?\nThe challenging part here is to design a data generating mechanism.\n\n\nData Generation\nData Checking\nModel Fitting\nResults\n\n\n\n\ngen_data &lt;- function(o = 1, n = 30, error_sd = 2) {\n  # generate the main part of the regression data\n  data &lt;- tibble(x = rnorm(n = n - o, \n                           mean = seq(-10, 10, length.out = n - o), \n                           sd = .1),\n                 y = x + rnorm(length(x), \n                               mean = 0, \n                               sd = error_sd))\n  # generate the outlier - make it at ~(-10, 5)\n  outdata &lt;- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd))\n  bind_rows(data, outdata)\n}\n\nsim_data &lt;- crossing(id = 1:100, outliers = 0:2) %&gt;%\n  mutate(\n  # call gen_data for each row in sim_data, \n  # but don't use id as a parameter.\n  data = purrr::map(outliers, gen_data) \n)\n\n\n\n\nhead(sim_data)\n## # A tibble: 6 × 3\n##      id outliers data             \n##   &lt;int&gt;    &lt;int&gt; &lt;list&gt;           \n## 1     1        0 &lt;tibble [30 × 2]&gt;\n## 2     1        1 &lt;tibble [30 × 2]&gt;\n## 3     1        2 &lt;tibble [30 × 2]&gt;\n## 4     2        0 &lt;tibble [30 × 2]&gt;\n## 5     2        1 &lt;tibble [30 × 2]&gt;\n## 6     2        2 &lt;tibble [30 × 2]&gt;\n\n# plot a few datasets just to check they look like we expect:\nsim_data %&gt;%\n  filter(id %% 100 &lt; 3) %&gt;%\n  unnest(data) %&gt;%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  facet_grid(id ~ outliers, labeller = label_both)\n\n\n\n\n\n\n\n\n\n\nlibrary(broom) # the broom package cleans up model objects to tidy form\n\nsim_data &lt;- sim_data %&gt;%\n  # fit linear regression\n  mutate(model = purrr::map(data, ~lm(y ~ x, data = .)))  %&gt;%\n  mutate(tidy_model = purrr::map(model, tidy))\n\n\n\n\n# Get the coefficients out\ntidy_coefs &lt;- select(sim_data, id, outliers, tidy_model) %&gt;%\n  unnest(tidy_model) %&gt;%\n  mutate(group = case_when(outliers == 0 ~ \"No HLPs\",\n                           outliers == 1 ~ \"1 HLP\",\n                           outliers == 2 ~ \"2 HLPs\") %&gt;%\n           factor(levels = c(\"No HLPs\", \"1 HLP\", \"2 HLPs\")))\n\nggplot(tidy_coefs, aes(x = estimate, color = group)) + \n  facet_grid(term ~ .) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\nObviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0).\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\nProblem\nHint\nGeneral Solution\nR Code\nPython Code\n\n\n\nLet’s explore what happens to estimates when certain observations are censored.\nSuppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102\\(^\\circ F\\); for these temperatures, the thermometer will record a value of 102.0.\nIt is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness. Given that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer’s censoring.\n\n\nIf most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution? What if you assume that 101 and 102.5 are the 2SD bounds?\n\n\nIf 101 and 102.5 are the anchor points we have, let’s assume that 95% of normal pet temperatures fall in that range. So our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375.\nWe can simulate 1000 observations from \\(N(101.75, 0.375)\\), create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are.\n\n\n\nset.seed(204209527)\ndogtemp &lt;- tibble(\n  actual = rnorm(1000, 101.75, 0.375),\n  read = pmin(actual, 102)\n) \ndogtemp %&gt;%\n  summarize_all(mean) %&gt;%\n  diff()\n## Error in r[i1] - r[-length(r):-(length(r) - lag + 1L)]: non-numeric argument to binary operator\n\nThe effect of the thermometer’s censoring is around 0.06 degrees F for animals that are not ill.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(204209527)\ndogtemp = pd.DataFrame({\n  \"actual\": np.random.normal(size = 1000, loc = 101.75, scale = 0.375)\n})\ndogtemp['read'] = np.minimum(dogtemp.actual, 102)\n\nnp.diff(dogtemp.mean())\n## array([-0.05279924])\n\nThe effect of the thermometer’s censoring is around 0.06 degrees F for animals that are not ill.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#monte-carlo-methods",
    "href": "part-advanced-topics/01-simulation.html#monte-carlo-methods",
    "title": "29  Simulation",
    "section": "\n29.6 Monte Carlo methods",
    "text": "29.6 Monte Carlo methods\nMonte carlo methods [4] are methods which rely on repeated random sampling in order to solve numerical problems. Often, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically.\nIn general, a MC problem involves these steps:\n\nDefine the input domain\nGenerate inputs randomly from an appropriate probability distribution\nPerform a computation using those inputs\nAggregate the results.\n\n\n\n\n\n\n\nExample: Sum of Uniform Random Variables\n\n\n\n\n\n\n\nProblem\nDefining Steps\nR Code\nPython Code\nLearn More\n\n\n\nLet’s try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1.\nMore precisely, if \\(u_i \\sim U(0,1)\\), where _{i=1}^k u_i &gt; 1, what is the expected value of \\(k\\)?\n\n\n\nIn this simulation, our input domain is [0,1].\nOur input is \\(u_i \\sim U(0,1)\\)\n\nWe generate new \\(u_i\\) until \\(\\sum_{i=1}^k &gt; 1\\) and save the value of \\(k\\)\n\nWe average the result of \\(N\\) such simulations.\n\n\n\n\n# It's easier to think through the code if we write it inefficiently first\nsim_fcn &lt;- function() {\n  usum &lt;- 0\n  k &lt;- 0\n  # prevent infinite loops by monitoring the value of k as well\n  while (usum &lt; 1 & k &lt; 15) {\n    usum &lt;- runif(1) + usum\n    k &lt;- k + 1\n  }\n  return(k)\n}\n\nset.seed(302497852)\nres &lt;- tibble(k = replicate(1000, sim_fcn(), simplify = T))\n\nmean(res$k)\n## [1] 2.717\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nset.seed(20417023)\n\nsim_res &lt;- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) \n\nsim_res &lt;- sim_res %&gt;%\n  mutate(running_avg_est = cummean(samp),\n         N = row_number())\n\nggplot(aes(x = N, y = running_avg_est), data = sim_res) + \n  geom_hline(yintercept = exp(1), color = \"red\") + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport random\nimport pandas as pd\n\n\ndef sim_fcn():\n  usum = 0\n  k = 0\n  # prevent infinite loops by monitoring the value of k as well\n  while usum &lt; 1 and k &lt; 15:\n    # print(\"k = \", k)\n    usum = np.random.uniform(size=1) + usum\n    k += 1\n  return k\n\nrandom.seed(302497852)\nres = pd.DataFrame({\"k\": [sim_fcn() for _ in range(1000)]})\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nrandom.seed(20417023)\n\nsim_res = pd.DataFrame({\"k\": [sim_fcn() for _ in range(250000)]})\nsim_res['running_avg_est'] = sim_res.k.expanding().mean()\nsim_res['N'] = np.arange(len(sim_res))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = sim_res, x = 'N', y = 'running_avg_est', color = \"black\")\ngraph.axhline(y = np.exp(1), xmin = 0, xmax = 1, color = \"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe expected number of uniform RV draws required to sum to 1 is \\(e\\)!\nExplanation of why this works\n\n\n\n\n\n\nMonte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods).\n\n\n\n\n\n\nExample: Integration\n\n\n\n\n\n\n\nProblem\nR Code\nPython Code\nRiemann R Code\n\n\n\nSuppose you want to integrate \\[\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) dx\\]\n\n\n\n\nf(x) over the interval [0,1].\n\n\n\nYou could set up Riemann integration and evaluate the integral using a sum over \\(K\\) points, but that approach only converges for smooth functions (and besides, that’s boring Calc 2 stuff, right?).\nInstead, let’s observe that this is equivalent to \\(\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) \\cdot 1 dx\\), where \\(p(x) = 1\\) for a uniform random variable. That is, this integral can be written as the expected value of the function over the interval \\([0,1]\\). What if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at those points, and average the result?\nYou can use the law of large numbers to prove that this approach will converge. [5]\n\n\n\nset.seed(20491720)\nfn &lt;- function(x)  x^2 * sin(1/x)\n\nsim_data &lt;- tibble(x = runif(100000),\n                   y = fn(x))\nmean(sim_data$y)\n## [1] 0.28607461\n\n\n\n\nrandom.seed(20491720)\n\ndef fn(x):\n  return x**2 * np.sin(1/x)\n\nsim_data = pd.DataFrame({\"x\": np.random.uniform(size = 100000)})\nsim_data['y'] = fn(sim_data.x)\n\nsim_data.y.mean()\n## np.float64(0.28663674657707916)\n\n\n\n\nfn &lt;- function(x)  x^2 * sin(1/x)\n\nriemann &lt;- tibble(x = seq(0, 1, length.out = 10000)[-1],\n                  y = fn(x))\nmean(riemann$y)\n## [1] 0.28657161\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Integration in 2d\n\n\n\n\n\n\n\nProblem\nR Code\nPython Code\nNumeric integration\n\n\n\nLet’s say that you want to find an estimate for \\(\\pi\\), and you know that a circle with radius 1 has an area of exactly that. You also know, that all of the points on this circle can be written as \\(x^2 + y^2 \\le 1\\).\n\n\n\n\nThe unit circle.\n\n\n\nEvaluating the area of the circle mathematically, would need us to either change to polar-coordinates or separate the graph into suitable functions (half-circles), and evaluate the integral between the top and the bottom: \\[\n\\int_{-1}^1 2 \\sqrt{1-x^2} dx\n\\] Instead, we note that the circle is encapsulated in a square with side length 2. We can reach all points in that square by using two independent uniform random random variables over the interval \\([-1,1]\\), i.e. when we generate two random values from U[-1,1], and use one as the \\(x\\) coordinate and one as the \\(y\\) coordinate, we get a point in the square. If the sum of the squares of the coordinates are less than 1, the point will also fall inside the circle. If not, the point falls in one of the four corners of the square that are outside the circle.\n\n\n\n\nThe unit circle is encapsulated by a square and overlaid with uniform points from U[-1,1] x U[-1,1].\n\n\n\nHow do we get to an estimate of \\(\\pi\\) from there? We know that the area of the square is simply \\(2^2 = 4\\). The area of the circle is then directly proportional to the rate at which points fall into the circle, ie.\n\\[\n\\hat{\\pi} = 4 \\times \\frac{\\text{Number of points with } x^2+y^2 \\le 1}{\\text{Number of points generated}}.\n\\] The more points we generate, the closer our estimate will be to the real value.\nThis problem is an example for Monte-Carlo Integration using an Acceptance-Rejection approach: we can slightly re-write the simulation and think of the generation of a new point in the circle as a two step process, where we first generate a value for \\(x\\) from U[-1,1], and in second step generate a candidate \\(c\\) for \\(y\\) from U[-1, 1], which we will only accept as \\(y\\), if \\(|c| \\le \\sqrt{1-x^2}\\). Acceptance-Rejection sampling is the basis of a lot of Markov-Chain Monte-Carlo (MCMC) methods, such as e.g. the Metropolis-Hastings algorithm.\n\n\n\nset.seed(20491720)\n\ncalculate_pi &lt;- function(R) {\n  x = runif(R, min=-1, max=1)\n  y = runif(R, min=-1, max=1)\n  in_circle = x^2+y^2&lt;1\n  \n  4 * sum(in_circle) / R\n}\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## [1] 3.16\ncalculate_pi(100)\n## [1] 3.2\ncalculate_pi(100)\n## [1] 2.96\n\n# Better with 10,000\ncalculate_pi(10000)\n## [1] 3.126\ncalculate_pi(10000)\n## [1] 3.1392\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## [1] 3.14344\n\npi\n## [1] 3.1415927\n\n\n\n\nrandom.seed(20491720)\n\ndef calculate_pi(R):\n  x = np.random.uniform(size = R)\n  y = np.random.uniform(size = R)\n  in_circle = x**2+y**2&lt;1\n  \n  return 4 * sum(in_circle) / R\n\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## np.float64(3.24)\ncalculate_pi(100)\n## np.float64(3.16)\ncalculate_pi(100)\n## np.float64(3.08)\n\n# Better with 10,000\ncalculate_pi(10000)\n## np.float64(3.1636)\ncalculate_pi(10000)\n## np.float64(3.1568)\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## np.float64(3.14214)\n\nnp.pi\n## 3.141592653589793\n\n\n\nWe can compare our results to the results we would get using a more typical numerical integration function, such as integrate in R.\n\nset.seed(20491720)\nfn &lt;- function(x)  2*sqrt(1-x^2)\n\nintegrate(fn, lower=-1, upper=1)\n## 3.1415927 with absolute error &lt; 2e-09\n\npi\n## [1] 3.1415927\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\nProblem\nR code\nPython Code\n\n\n\nBuffon’s needle is a mathematical problem which can be boiled down to a simple physical simulation. Read this science friday description of the problem and develop a monte carlo simulation method which estimates \\(\\pi\\) using the Buffon’s needle method. Your method should be a function which\n\nallows the user to specify how many sticks are dropped\nplots the result of the physical simulation\nprints out a numerical estimate of pi.\n\n\n\nLet’s start out with horizontal lines at 0 and 1, and set our stick length to 1. We need to randomly generate a position (of one end of the stick) and an angle. The position in \\(x\\) doesn’t actually make much of a difference (since what we care about is the \\(y\\) coordinates), but we can draw a picture if we generate \\(x\\) as well.\n\nneedle_sim &lt;- function(sticks = 100) {\n  df &lt;- tibble(xstart = runif(sticks, 0, 10), \n         ystart = runif(sticks, 0, 1), \n         angle = runif(sticks, 0, 360),\n         xend = xstart + cos(angle/180*pi), \n         yend = ystart + sin(angle/180*pi)\n  ) %&gt;%\n    # We can see if a stick crosses a line if the floor() function of ystart is \n    # different than floor(yend). \n    # Note this only works for integer line values.\n  mutate(crosses_line = floor(ystart) != floor(yend)) \n  \n  \n  gg &lt;- ggplot() + \n  geom_hline(yintercept = c(0, 1)) + \n  geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend,\n                   color = crosses_line), data = df) + \n  coord_fixed()\n  \n  return(list(est = 2 * sticks / sum(df$crosses_line), plot = gg))\n}\n\nneedle_sim(10)\n## $est\n## [1] 3.3333333\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(100)\n## $est\n## [1] 3.125\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(1000)\n## $est\n## [1] 3.1446541\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(10000)\n## $est\n## [1] 3.1730922\n## \n## $plot\n\n\n\n\n\n\n\n\n\n\ndef needle_sim(sticks = 100):\n  df = pd.DataFrame({\n    \"xstart\": np.random.uniform(0, 10, size = sticks),\n    \"ystart\": np.random.uniform(0, 1, size = sticks),\n    \"angle\": np.random.uniform(0, 360, size = sticks)\n  })\n  \n  df['xend'] = df.xstart + np.cos(df.angle/180*np.pi)\n  df['yend'] = df.ystart + np.sin(df.angle/180*np.pi)\n  df['crosses_line'] = np.floor(df.ystart) != np.floor(df.yend)\n  \n  return df\n\ndata = needle_sim(100000)\ndata['N'] = np.arange(len(data)) + 1\ndata['cum_est'] = 2*data.N / data.crosses_line.expanding().sum()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = data, x = \"N\", y = \"cum_est\", color = \"black\")\ngraph.axhline(y = np.pi, xmin = 0, xmax = 1, color = \"red\")\nplt.show()",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#other-resources",
    "href": "part-advanced-topics/01-simulation.html#other-resources",
    "title": "29  Simulation",
    "section": "\n29.7 Other Resources",
    "text": "29.7 Other Resources\n\nSimulation (R programming for Data Science chapter)\nSimulation - R Studio lesson\nSimulation, focusing on statistical modeling (R)\nSimulating Data with SAS (Excerpt)\nSimulating a Drunkard’s Walk in 2D in SAS\nSimulation from a triangle distribution (SAS)\nSimulating the Monty Hall problem (SAS)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#references",
    "href": "part-advanced-topics/01-simulation.html#references",
    "title": "29  Simulation",
    "section": "\n29.8 References",
    "text": "29.8 References\n\n\n\n\n[1] \nWikipedia contributors, “Linear congruential generator.” Sep. 09, 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Linear_congruential_generator&oldid=1109285432\n\n\n\n[2] \nA. Menon, “Linear Regression in 6 lines of Python,” Medium. Oct. 2018 [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[3] \nR. Alexander, “Telling stories with data,” Jul. 27, 2023. [Online]. Available: https://tellingstorieswithdata.com/. [Accessed: Aug. 01, 2023]\n\n\n[4] \nWikipedia contributors, “Monte Carlo method,” Wikipedia. Oct. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Monte_Carlo_method&oldid=1116159451. [Accessed: Oct. 17, 2022]\n\n\n[5] \nY.-C. Chen, “Lecture 2: Monte Carlo Simulation,” Monte Carlo Simulation. 2017 [Online]. Available: http://faculty.washington.edu/yenchic/17Sp_403/Lec2_MonteCarlo.pdf",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html",
    "href": "part-advanced-topics/02-data-doc.html",
    "title": "30  Data Documentation",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#objectives",
    "href": "part-advanced-topics/02-data-doc.html#objectives",
    "title": "30  Data Documentation",
    "section": "",
    "text": "Understand the importance of data documentation for reproducibility and scientific validity\nLeverage existing data documentation to provide accurate citations, identify necessary data cleaning steps, and inform modeling decisions\nWrite data documentation that supports data re-use and analysis reproducibility",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#understanding-data-about-data",
    "href": "part-advanced-topics/02-data-doc.html#understanding-data-about-data",
    "title": "30  Data Documentation",
    "section": "30.1 Understanding Data About Data",
    "text": "30.1 Understanding Data About Data\nMetadata is a word that breaks down “into data about data”. More specifically, metadata is information about how the data came to exist, how it is formatted, how it may be used, what has been done to modify it from the raw form, and so on.\nAs statisticians, metadata is critical - without understanding the provenance of the data, we cannot select an appropriate model, identify data cleaning strategies, and define the limitations of our analysis and conclusions. Without appropriate metadata, statisticians can’t really ethically do inference – you can’t generalize to a population if you don’t know how the sample was collected and what decisions were made between data collection and the cleaned data! Sometimes, data comes to us with only informal, verbally communicated metadata from a collaborator, but it is still metadata - it would be much better to write the metadata down during the verbal communication to ensure that the documentation exists and can be referenced by others.\nIn its simplest form, metadata can be a README file that accompanies the data. There are several slapdash, informal metadata files that accompany datasets in this textbook - short text files that exist mostly to remind me where I got the dataset and (if they’re more recent), how the data was collected and maybe some additional context - variable descriptions (if I had time and they were easily accessible), citation information, and so on. There absolutely should be more examples in the data folder of this textbook’s repository, but I haven’t found that many, which is perhaps evidence that metadata is always something we (I) can improve on, at both an individual dataset level and in generating metadata for all datasets.\nBasic metadata includes elements like: - the source of the data - the data format - is it a CSV? JSON file? Available as either? - what variables are present? What are the units?\nThe next section will explore the full list of different elements of metadata, but it’s far from exhaustive. There are many types of data, and each type might have some additional useful metadata which will be of interest to someone using that dataset.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#elements-of-data-documentation",
    "href": "part-advanced-topics/02-data-doc.html#elements-of-data-documentation",
    "title": "30  Data Documentation",
    "section": "30.2 Elements of Data Documentation",
    "text": "30.2 Elements of Data Documentation\nWhen you are writing metadata for a data file in your custody (or if you’re a statistician or data scientist trying to impose order on your collaborator’s data), it can be important to consider what information you should include in the data documentation file. Ideally, each data table should be documented with a different file, though if there are only a few highly related tables, it might be appropriate to use a single file to document them all. In addition, the collection of data tables should include an overall documentation file describing the data set as a whole as well and providing links to the sub-documentation files. Ideally, there is a balance between too many README files and a massive README file that the reader has to scroll for hours through to find the necessary information.\nThe next sections describe important components of data documentation; most of these components are essential for at least one hierarchical level of data documentation if there are multiple documentation files, but for now, let us assume that there is only a single README file documenting perhaps one or two data files.\n\n30.2.1 Descriptive Metadata\n\nThe first information to provide is the metadata describing the collection – the essentials, including a title, author(s), creation date, and purpose.\nThe title of the metadata should be descriptive – if your data are physiological measurements of captured jackalopes, you might want to title the data as “Morphometry of Live-Trapped Jackalopes (Lepus temperamentalus) in Southern USA”, providing the technical term for physiological measurements (“morphometry”), the data collection method (“live-trapped”), the species (“jackalope (Lepus temperamentalus)” – both the common and scientific names), and a location (southern USA) gives a level of detail that helps potential users determine whether the data are fit for purpose.\n# Morphometry of Live-Trapped Jackalopes in the Southern Appalachian Region, USA\n\nAuthors:\n- Brer Rabbitson\n- Fred Adanko\n\nData collection locations:\n- Westminister, SC\n- Cataloochee, NC\n- Mouth of Wilson, VA\n- Hiawassee, GA\n- Wautauga, TN\n\nData collected between 2023-04-06 and 2024-08-23 using live traps baited with brandy-soaked carrots.\nIn the initial description of the data, this level of detail is sufficient – it might be necessary to add additional data collection protocols in a different section of the file or in a different file altogether.\nThe next section of the file should contain information about the files in the dataset and the variables contained within each file. Typically, this involves a list containing the column/field name, observation type (character, numeric, integer, logical), and units, if applicable.\nIf there are multiple datasets, then this information may be specified as a nested list, with each data file containing a list of columns or fields. In more complicated instances where there are multiple relational tables, it may be helpful to provide a data schema showing how different tables and variables relate to each other.\n\n## Files\n- `jackalope-morphometry.csv`\n  - Columns\n    - `id` (4-digit integer, WXYZ) where W indicates the collection site (1-5, in order as listed above), and XYZ indicates the observation from that collection site. \n    - `sex` (M or F) Sex of the rabbit, as identified from examination of external genitalia. \n    - `antler_length_left` (double) Length of the left antler, in cm, measured from the base where it protrudes from the skin to the tip, following the thickest branch at each junction. The tape should be held as closely as possible to the antler along the curved path. \n    - `antler_length_right` (double) Length of the right antler, in cm, measured from the base where it protrudes from the skin to the tip, following the thickest branch at each junction. The tape should be held as closely as possible to the antler along the curved path. \n    - `ear_length_left` (double) Length of the left ear, in cm, measured from the center of the base of the ear to the tip of the ear.  \n    - `ear_length_right` (double) Length of the right ear, in cm, measured from the center of the base of the ear to the tip of the ear.  \n    - `hind_foot_length_left` (double) Length of the left hind foot, in cm, measured from hock to tip of the longest toe.\n    - `hind_foot_length_right` (double) Length of the right hind foot, in cm, measured from hock to tip of the longest toe.\n    - `fore_foot_length_left` (double) Length of the left fore foot, in cm, measured from carpus to tip of the longest toe.\n    - `fore_foot_length_right` (double) Length of the right fore foot, in cm, measured from carpus to tip of the longest toe.\n    - `body_length` (double) Length of the rabbit's body, in cm, measured from bottom of the spine just above the tail to base of the skull.\n    - `muzzle_length` (double) Length of the rabbit's face, in cm, measured from between the base of the front of the ears to the tip of the nose. \n    - `weight` (double) Weight of the rabbit, in kg, measured using a spring scale with a fabric container holding the rabbit. \n  - Notes\n    - In some cases, sex determination can be difficult to determine without causing additional distress to the animal. In these cases, sex is coded as NA. \n\n\n\n\n\n\n\n\nFigure 30.1: It can be helpful to include diagrams showing how measurements are to be taken. In this image, it is clear that the antler measurements are not taken linearly, but are measured along the curved path of the antler. Source, CC-By-SA, Modified to add measurements.\n\n\n\nIn some cases, it can be useful to include one or more images showing the specifics of data collection. Figure 30.1 provides a visual aid to communicate exactly how the antler measurements are taken. When images need to be interspersed with text, it may be preferable to use a markdown file for the README, rather than a plain-text file, so that HTML or PDF README files can be generated.\nAdditional notes might specify the precision of the data collection tool(s), standards for rounding, and more.\nIf materials were used or consumed during data collection, the README may specify batch numbers, sources of materials, and other information necessary to replicate the experiment.\nData should be provided in a plain-text format where possible, and an open, well-documented format when plain-text data is infeasible or impractical. Storing data in proprietary formats should be avoided, as software support is not guaranteed over a long time period (or even a relatively short period of 5 years!).\n\n\n30.2.2 Administrative Metadata\n\nWhen creating data documentation, it is important to consider licensing and use specifications. Consider whether the data should be usable for commercial purposes, and whether attribution should be required if the data is reused. [1] describes some of the licensing options available for data.\n\n\nClearly specifying who may use the data, for what purposes, and under what terms is an important part of ensuring that other researchers have the information to be able to reuse your data subject to restrictions that may be imposed by corporate partners, funding agencies, or collaborators.\n\n\n30.2.3 Provenance Metadata\nIt is important to provide information about any transformations or preprocessing the data has been through. Providing the raw data, along with intermediate steps and code to enact the transformations, is extremely helpful, as it allows someone to replicate your analysis, but also allows for the possibility that they will make different decisions about how to work with the raw data. It is also important to document any changes which are made to the raw data. One way to do this is to check plain-text raw data into a version control repository and use commit messages to document changes and the reason for the change. It may also be useful to create a changelog, where changes to the raw data are documented in a specific text file, along with the date and the reason for the change.\n\n\n\n30.2.4 Data Collection Procedure Metadata\n\n\n\n\nAs with anything, the art of data documentation is knowing what to include and what to exclude. When in doubt, it may be useful to document the details in separate files and allow the potential user of the data to determine what aspects of data collection might be relevant. Psychology studies used to document the exact brand of computer and monitor used to present the stimuli, along with screen resolution, size, driver information, and more. This list of information was probably overkill, but in some experiments, the screen frequency might have been important. When documenting the collection protocol for a dataset, it’s useful to make lists of factors which were controlled across trials, factors which were manipulated across trials, and incidental factors. For the jackalope morphology study, we could list these factors out as follows:\n\n\nControlled factors\n\nmeasurement protocol\n\nmodel of scale used, margin of error, etc. and calibration procedure\nprocedure for measuring jackalope weight\nprocedure(s) for measuring jackalope dimensions\nprocedure for baiting and placing trap\n\n\n\nManipulated factors\n\nLocation\n\nTrap placements for each location\n\n\n\nIncidental factors\n\nEnvironment\n\nAssociated weather information during data collection period\n\n\n\n\nObviously, documenting all of this can be a ton of work and may involve e.g. videos showing procedures and even additional datasets.\nWhile I’ve obviously made up the jackalope example, there is a very nice sea otter morphometry dataset [2] that has excellent documentation.\n\n\n\n\n\n\n30.3 Example: Horse Measurements\n\n\n\n\n\nI reached out to a paper author about finding some morphometric data similar to palmerpenguins, because using the same dataset all the time can get tedious. In addition to the data I asked for, I got some bonus horse morphology data that was documented incredibly well, without needing to use a formal schema such as the one provided by DataCite.\nDr. Sutton (the paper author) provided 3 files: a fields.txt metadata file, the data itself, and the PDF of the paper describing the data.\nConsider an excerpt of a few lines of the metadata file:\nbreed   \"Breed names, nomenclature typically follows breed clubs\"\nsex \"M=mare, G=gelding, S=stallion\"\nAGE age in years at time of phenotyping\nbody_condition_score    \"larger values = heavier, judged subjectively by measurement-taker\"\nfore_shod   yes/no\nhind_shod   yes/no\nfactor_dish_roman   \"See paper for details, factor scale 1-5 judged subjectively by measurement-taker\"\nfactor_feather  \"See paper for details, factor scale 1-5 judged subjectively by measurement-taker\"\nWhile not formally structured or delimited, this file is incredibly clear, and points to the paper for variables which cannot be adequately described in the text file.\n\n\n\nA screenshot of some of the supplemental information in the PDF paper, describing the factor_dish_roman variable in more detail.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#tools-and-standards",
    "href": "part-advanced-topics/02-data-doc.html#tools-and-standards",
    "title": "30  Data Documentation",
    "section": "30.4 Tools and Standards",
    "text": "30.4 Tools and Standards\nDataCite standards Data dictionaries README files codebooks\nautomated documentation systems??",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#best-practices",
    "href": "part-advanced-topics/02-data-doc.html#best-practices",
    "title": "30  Data Documentation",
    "section": "30.5 Best Practices",
    "text": "30.5 Best Practices\n\nVersion Control\nDocumentation of processing and cleaning steps (and software, versions, environments)\nLogs of parameter settings and analysis code\nLinks between data documentation and code documentation",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#case-studies-and-examples",
    "href": "part-advanced-topics/02-data-doc.html#case-studies-and-examples",
    "title": "30  Data Documentation",
    "section": "30.6 Case Studies and Examples",
    "text": "30.6 Case Studies and Examples\n\nLevels of documentation\nFailed reproducibility efforts\nSuccessful reproducibility efforts",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/02-data-doc.html#data-documentation-checklist",
    "href": "part-advanced-topics/02-data-doc.html#data-documentation-checklist",
    "title": "30  Data Documentation",
    "section": "30.7 Data Documentation Checklist",
    "text": "30.7 Data Documentation Checklist\n\n\n\n\n[1] I. Labastida and T. Margoni, “Licensing FAIR Data for Reuse,” Data Intelligence, vol. 2, no. 1–2, pp. 199–207, Jan. 2020, doi: 10.1162/dint_a_00042. [Online]. Available: https://doi.org/10.1162/dint_a_00042. [Accessed: Aug. 06, 2025]\n\n\n[2] D. H. Monson, K. Kloecker, and G. G. Esslinger, “Morphometric and Reproductive Status Data for Sea Otters Collected or Captured in Alaska,” United States Geological Survey. 2021 [Online]. Available: https://www.usgs.gov/data/morphometric-and-reproductive-status-data-sea-otters-collected-or-captured-alaska. [Accessed: Aug. 09, 2025]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Data Documentation</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html",
    "href": "part-advanced-topics/03-web-scraping.html",
    "title": "31  Web Scraping",
    "section": "",
    "text": "31.1 Objectives",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#objectives",
    "href": "part-advanced-topics/03-web-scraping.html#objectives",
    "title": "31  Web Scraping",
    "section": "",
    "text": "Understand the structure of XML and HTML files\nUse developer tools to locate nodes of interest and CSS or XPATH to precisely identify those nodes\nIdentify web pages that cannot be scraped using simple methods because of content injection\nScrape data from web pages\n\nby reading in HTML tables directly using R or Python\nby writing custom functions to pull data from individual fields",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#html-and-xml-language-specification",
    "href": "part-advanced-topics/03-web-scraping.html#html-and-xml-language-specification",
    "title": "31  Web Scraping",
    "section": "31.2 HTML and XML Language Specification",
    "text": "31.2 HTML and XML Language Specification\nLet’s start at the beginning. eXtensible Markup Language (XML) was developed in the late 1990s to provide a flexible specification for documents, data files, and various other documents. You’ve (likely) been using XML-based documents for most of your life, either in Microsoft Office (.docx, .xlsx, .pptx - the x is for XML) or in web pages. HyperText Markup Language (HTML) is a specific flavor of XML [1] that uses a defined set of fields to specify the composition of a web page1.\nIf you right click in your browser on this page, and “Inspect Source”, you can see the HTML code that generates this book.\n\n\n\n\n\n\n\n\nChromium right click menu\n\n\n\n\n\n\n\nFirefox right click menu\n\n\n\n\n\n\nFigure 31.1: Right click menu options that allow the user to see the HTML code for a webpage.\n\n\n\n\n31.2.1 XML\nXML documents are composed of a series of tags that look like this:\n\n&lt;tag-type attribute=\"value\"&gt;Tag contents&lt;/tag-type&gt;\n\nEach tag name, tag, is enclosed in angle brackets, &lt;tag&gt;, and has a closing tag that starts with a slash but has the same name, &lt;/tag&gt;. Tags can include additional information that is typically encoded in key=\"value\" pairs - above, there is an attribute field (the part before the =) that has value \"value\" (the part after the =). A set of matching tags is called an element, and the information within the start and end tags is called the content.\nThe XML 1.0 (5th edition) specification requires that\n\nThe document contains only one element, called the root or document element, at the top level. No part of the root element appears in the content of any other element.\nAny other elements\n\nare defined by start and end tags, and\nnest properly within each other (are well formed)\n\nAn element contains text, a sequence of characters. Legal characters are tab, carriage return, line feed, and the legal characters of Unicode and ISO/IEC 10646 (PDF warning)2.\nComments:\n\n\n&lt;!-- This is a valid comment --&gt;\n&lt;!-- This is not a valid comment ---&gt; (The end must have exactly 2 dashes)\n&lt;!-- This is not a valid comment -- \n     two dashes cannot be included in the string --&gt;\n\n\n\n\n\n\n\nAdvanced: Additional XML metadata\n\n\n\n\n\nA valid XML document also contains some additional metadata at the start of the file. First, there should be an XML declaration that specifies the version of XML being used: &lt;?xml version=\"1.0\"?&gt; (Note the ? immediately before and after the angle brackets).\nIn addition, valid XML documents contain a Document Type Definition (DTD) that provides some information about valid tags used within the document.\n\n&lt;!DOCTYPE Name (ExternalID)? '[' intSubset ']' &gt;\n\nDocument Type Definition requirements:\n\nName in the DTD has the same value as the root node tag name\nAn Internal or External Subset of valid entity names (tag names) is provided. It is also possible to specify valid tag attributes, types, and other information within these listings.\n\n\n1&lt;?xml version=\"1.0\"?&gt;\n2&lt;!DOCTYPE greeting SYSTEM \"hello.dtd\"&gt;\n3&lt;greeting&gt;Hello, world!&lt;/greeting&gt;\n\n\n1\n\nXML version declaration\n\n2\n\nDocument Type Definition. greeting is the name, and the root node matches that name. hello.dtd is the system identifier and provides an address leading to an external document type declaration file\n\n3\n\nRoot node. greeting is the tag name. Hello, world! is the tag value. The tag is closed with a valid, matching end tag.\n\n\n\n\n\n1&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n2&lt;!DOCTYPE greeting [\n  &lt;!ELEMENT greeting (#PCDATA)&gt;\n]&gt;\n3&lt;greeting&gt;Hello, world!&lt;/greeting&gt;\n\n\n1\n\nXML version declaration, specifying the document’s text encoding\n\n2\n\nAn internally defined DTD that specifies valid elements (tags) (the only valid element is greeting). Additional tags could be specified within the [ ] space, if desired.\n\n3\n\nThe root node with value Hello, world!\n\n\n\n\n\n\n\nElements are typically nested in an XML document.\nConsider the following set of elements that describe the main characters in the children’s TV show Bluey. Note that the same information about the family could be described in a number of different hierarchical ways.\n\nBlueyBluey Alt 1Bluey Alt 2Bluey Alt 3\n\n\n\n&lt;family&gt;\n  &lt;adults&gt;\n    &lt;person sex=\"M\" coat=\"blue\"&gt;Bandit Heeler&lt;/person&gt;\n    &lt;person sex=\"F\" coat=\"orange\"&gt;Chili Heeler&lt;/person&gt;\n  &lt;/adults&gt;\n  &lt;kids&gt;\n    &lt;person sex=\"F\" coat=\"blue\"&gt;Bluey Heeler&lt;/person&gt;\n    &lt;person sex=\"F\" coat=\"orange\"&gt;Bingo Heeler&lt;/person&gt;\n  &lt;/kids&gt;\n&lt;/family&gt;\n\n\nfamily contains two child nodes - ‘adults’, and ‘kids’. Collectively, family has two direct children.\n‘adults’ contains two child nodes - persons - that describe the adults in the family, Chili and Bandit Heeler\n‘kids’ contains two child nodes - persons - that describe the kids in the family, Bluey and Bingo Heeler.\nThe parent of the element describing Bingo Heeler is ‘kids’\nThe parent of the ‘kids’ element is ‘family’\n‘adults’ and ‘kids’ are sibling elements\n\n\n\n\n&lt;family&gt;\n    &lt;person status=\"adult\" sex=\"M\" coat=\"blue\"   &gt; Bandit Heeler  &lt;/person&gt;\n    &lt;person status=\"adult\" sex=\"F\" coat=\"orange\" &gt; Chili  Heeler  &lt;/person&gt;\n    &lt;person status=\"kid\"   sex=\"F\" coat=\"blue\"   &gt; Bluey  Heeler  &lt;/person&gt;\n    &lt;person status=\"kid\"   sex=\"F\" coat=\"orange\" &gt; Bingo  Heeler  &lt;/person&gt;\n&lt;/family&gt;\n\n\n\n\n&lt;family&gt;\n    &lt;person status=\"adult\" sex=\"M\" coat=\"blue\"   &gt; \n      &lt;name&gt;&lt;first&gt;Bandit&lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n    &lt;/person&gt;\n    &lt;person status=\"adult\" sex=\"F\" coat=\"orange\" &gt; \n      &lt;name&gt;&lt;first&gt;Chili &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n    &lt;/person&gt;\n    &lt;person status=\"kid\"   sex=\"F\" coat=\"blue\"   &gt; \n      &lt;name&gt;&lt;first&gt;Bluey &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n    &lt;/person&gt;\n    &lt;person status=\"kid\"   sex=\"F\" coat=\"orange\" &gt; \n      &lt;name&gt;&lt;first&gt;Bingo &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n    &lt;/person&gt;\n&lt;/family&gt;\n\n\n\n\n&lt;family&gt;\n    &lt;person&gt;\n      &lt;name&gt;&lt;first&gt;Bandit&lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n      &lt;status&gt;adult&lt;/status&gt; \n      &lt;sex&gt;male&lt;/sex&gt;   \n      &lt;coat&gt;blue&lt;/coat&gt;\n    &lt;/person&gt;\n    &lt;person&gt;\n      &lt;name&gt;&lt;first&gt;Chili &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n      &lt;status&gt;adult&lt;/status&gt; \n      &lt;sex&gt;female&lt;/sex&gt; \n      &lt;coat&gt;orange&lt;/coat&gt;\n    &lt;/person&gt;\n    &lt;person&gt;\n      &lt;name&gt;&lt;first&gt;Bluey &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n      &lt;status&gt;child&lt;/status&gt; \n      &lt;sex&gt;female&lt;/sex&gt; \n      &lt;coat&gt;blue&lt;/coat&gt;\n    &lt;/person&gt;\n    &lt;person&gt;\n      &lt;name&gt;&lt;first&gt;Bingo &lt;/first&gt; &lt;last&gt;Heeler&lt;/last&gt;&lt;/name&gt; \n      &lt;status&gt;child&lt;/status&gt; \n      &lt;sex&gt;female&lt;/sex&gt; \n      &lt;coat&gt;orange&lt;/coat&gt;\n    &lt;/person&gt;\n&lt;/family&gt;\n\n\n\n\nWhen working with XML (and HTML), it is important to understand how the data are represented structurally, so that you can get the components of the data back out of this hierarchical format.\n\n\n\n\n\n\nDemo: Exploring an XML file\n\n\n\nThe District of Columbia (Washington, DC) local government provides a dataset of grocery store locations throughout the district using a definition of full service grocery store that requires:\n\nSales of six of the following categories of food:\n\nFresh fruits and vegetables\nFresh and uncooked meats, poultry and seafood\nDairy products\nCanned foods\nFrozen foods\nDry groceries and baked goods\nNon-alcoholic beverages\n\nEither 50% of the store’s total square footage, or 6000 square feet must be dedicated to selling the food products above.\nAt least 5% of the selling area must be dedicated to each food category.\n\nThe city then included some small grocery stores that are very close to meeting the full-service grocery store definition based on Appendix D of a city-wide food system assessment (PDF warning).\nWe can download the KML file and change the extension to XML so that the file opens in a standard web browser, or we can directly download the KML file as an XML file as in the code chunk below.\n\nurl &lt;- \"https://opendata.dc.gov/api/download/v1/items/1d7c9d0e3aac49c1aa88d377a3bae430/kml?layers=4\"\nfilename &lt;- \"../data/DC_Grocery_Stores_2025.xml\"\n\nif(!file.exists(filename)) {\n  download.file(url, destfile = filename, mode = \"wb\")\n}\n\nxml_text &lt;- readLines(filename)\nxml_tbl &lt;- data.frame(line = 1:length(xml_text), \n                      xml = xml_text)\n\n\n\n\n\n\n\n\nThe first line contains the document type definition and encoding.\nThe second line contains a link to the KML specification, which is itself another XML document. Note that it is also a valid tag, so at the end of the file, we should find .\nThe third line is another tag that indicates that this is the start of the document.\nLines 4-40 contain the dataset schema – a list of all of the fields present for each store, along with their data type.\nLine 41 defines a folder, which contains Placemarks (Line 42), which contain ExtendedData (Line 43). The \\t characters are a text representation of tabs and indicate some indentation.\nEach grocery store appears to be described by a Placemark node that contains ExtendedData nodes containing variables, along with a Point node that contains two coordinates (latitude and longitude).\n\n\n\n\n\n\n\n\n(Click to Enlarge) Screenshot of DC_Grocery_Stores_2025.xml showing the nodes corresponding to the Schema definition and to one individual grocery store.\n\n\n\n\n31.2.2 HTML\nHTML is a markup language that appears very similar to XML. Technically, HTML predates XML by a few years (1993 vs 1996) [2], but they’ve been developed in parallel and there are obvious influences in both directions.\nImportant differences between HTML and XML [2]:\n\nHTML tags display information. XML tags describe information.\nHTML uses pre-defined tags instead of XML’s user-defined tags.\nHTML doesn’t always require closing tags, while XML does.\nHTML is more robust than XML, in that it will ignore small errors.\nHTML is not case sensitive, while XML is.\nHTML ignores white space, but XML doesn’t necessarily ignore white space.\n\n\n\n\n\n\n\nDemo: HTML Document Structure\n\n\n\nLike XML, HTML documents have a basic structure:\n\n1&lt;!DOCTYPE html&gt;\n2&lt;html&gt;\n3&lt;head&gt;\n4&lt;title&gt;Page Title&lt;/title&gt;\n&lt;/head&gt;\n\n5&lt;body&gt;\n&lt;h1&gt;Level 1 Heading&lt;/h1&gt;\n&lt;p&gt;This paragraph provides information about the topic in the L1 heading.&lt;/p&gt;\n&lt;h2&gt;Level 2 Heading&lt;/h2&gt;\n&lt;p&gt;This paragraph provides even more information relating the L2 heading to L1.&lt;/p&gt;\n6&lt;br&gt;\n&lt;/body&gt;\n\n&lt;/html&gt; \n\n\n1\n\nThis indicates that the document is an HTML5 document\n\n2\n\nThe root element of an HTML page\n\n3\n\nThe head element contains meta information about the page. Most scripts and formatting information (CSS) are also loaded in this element.\n\n4\n\nThe page title is what will show up in the browser tab.\n\n5\n\nThe body element contains the actual information rendered on the page.\n\n6\n\nThis element creates a line break, but has no content. Empty elements (elements with no content) do not have to have a closing tag (but you can add one if it makes you happy).\n\n\n\n\n\n\nYou can easily see the HTML code that creates any website by right clicking on the web page in your browser and selecting some variant of “Inspect” or “Inspect Source” or “View Source”, as shown in Figure 31.1. This is an incredibly helpful tool when you want to figure out how to pull data out of a webpage, getting only the parts you want without the rest of the clutter.\nThe best way to think about a HTML page is that it consists of a series of elements that are best thought of as boxes. Elements like &lt;h1&gt; &lt;/h1&gt; define a box with specific contents. These boxes are then styled and arranged via the “magic” of Cascading Style Sheets (CSS). When I first learned HTML (circa 2001?), there were a reasonable number of tags and CSS was not really a thing, so it was easier to understand how web pages were laid out, and we could spend our time decking out webpages with neon colors, animated images, and annoying music.\n\n\n\n\n\n\n\n\nFigure 31.2: CSS is “magic” … finicky and annoying magic.\n\n\n\nNow, there are too many valid HTML tags for to cover in an introduction, but first let’s review the anatomy of an XML or HTML element, and then we can look at some of the most important HTML tags.\n\nHTML Element Anatomy\n\\[\\underbrace{\\ \\ \\overbrace{&lt; \\text{p}\\ \\  \\underbrace{\\text{class}}_{\\text{Attribute}}=\\underbrace{\\text{'important'}}_{\\text{Value}} &gt;}^{\\text{Start tag}} \\ \\ \\ \\underbrace{\\text{This is a paragraph}}_{\\text{Content}} \\ \\ \\ \\overbrace{&lt;/ \\text{p} &gt;}^{\\text{End tag}}\\ \\ \\ }_{\\text{Element}} \\]\n\n\nImportant HTML Tags\n\n&lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;h4&gt;, &lt;h5&gt;, &lt;h6&gt;: Headings.\nHeadings should be nested, so you should never have content in &lt;h2&gt;This is wrong&lt;/h2&gt; unless there is an &lt;h1&gt; &lt;/h1&gt; element above it in the same block of content.\nCorresponds to #, …, ###### in markdown.\n&lt;p&gt;: Paragraphs. All text that isn’t part of some other tag should be contained in paragraph tags.\n&lt;a&gt;: links. &lt;a href=\"https://google.com\"&gt;This links to google&lt;/a&gt;\nhref is a tag attribute, and https://google.com is the attribute’s value.\n&lt;img&gt;: Images. &lt;img src=\"path/to/picture.jpg\" alt=\"Alt-text description of the picture\"&gt;. As with line breaks, you do not have to have a &lt;/img&gt; tag unless having unclosed tags bothers you.\n&lt;table&gt;: Tables. In HTML, tables are constructed by row. Here is a minimal table that has a header:\n\n\n\nHTML:\n\n1&lt;table&gt;\n2  &lt;tr&gt;\n3    &lt;th&gt;Heading Col 1&lt;/th&gt;\n    &lt;th&gt;Heading Col 2&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n4    &lt;td&gt;Value 1&lt;/td&gt;\n    &lt;td&gt;Value 2&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n\n\n1\n\nThe table element contains all table contents\n\n2\n\nThe tr element defines a new row of the table\n\n3\n\nThe th element defines a header cell\n\n4\n\nThe td element defines a normal (body) cell\n\n\n\n\n\n\n\nRendered:\n\n\n\n\n\nHeading Col 1\nHeading Col 2\n\n\nValue 1\nValue 2\n\n\n\n\n\n\n\n\n&lt;ul&gt; and &lt;ol&gt;: Unordered (bulleted) and ordered lists. List elements are included as child elements and start/end with &lt;li&gt;&lt;/li&gt; (for both types of lists).\n&lt;span&gt;: a container for inline content – that is, content that is part of another piece of text. Spans are often used to highlight specific text using color or other formatting attributes (font, boldness, size).\n&lt;div&gt;: a container for other content. These containers are often used to help style and lay out content using CSS.\n\n\n\nCommon HTML Attributes\nSome attributes in HTML are special and are used across a number of different tag types.\n\nID: A shorthand name for the element. For instance, the ID for this level 4 section heading is common-html-attributes\nClass: The category the element belongs to. Often, this is used to help style the element with appropriate CSS - for instance, to ensure that all level 4 headings have the same font size and color. There can be multiple values for the class attribute, separated by spaces. The class value for this level 4 section heading is level4 unnumbered, indicating both the heading level and that I’ve switched off numbering for this sub-sub-sub-sub-section.\nStyle: Any CSS items which apply only to the specific element. For this section, only the link to this section added by quarto/pandoc has a style attribute specified (because it is applied automatically when the quarto document is compiled)\n\n\n\n\n\n\n\nDemo: HTML for the Common HTML Attributes subsection\n\n\n\n\n\nI have added line-breaks and indentation for readability, but the rest of the code is copied from my browser.\n&lt;section id=\"common-html-attributes\" class=\"level4 unnumbered\"&gt;\n  &lt;h4 class=\"unnumbered anchored\" data-anchor-id=\"common-html-attributes\"&gt;\n    Common HTML Attributes\n    &lt;a class=\"anchorjs-link \" aria-label=\"Anchor\" data-anchorjs-icon=\"\" \n       href=\"#common-html-attributes\" \n       style=\"font: 1em / 1 anchorjs-icons; margin-left: 0.1875em; \n              padding-right: 0.1875em; padding-left: 0.1875em;\"&gt;&lt;/a&gt;\n  &lt;/h4&gt;\n  &lt;p&gt;Some attributes in HTML are special and are used across a number of \n     different tag types.&lt;/p&gt;\n  &lt;ul&gt;\n    &lt;li&gt;ID: A shorthand name for the element. \n        For instance, the ID for this level 4 section heading is \n        &lt;code&gt;common-html-attributes&lt;/code&gt;&lt;/li&gt;\n    &lt;li&gt;Class: The category the element belongs to. \n        Often, this is used to help style the element with appropriate CSS - \n        for instance, to ensure that all level 4 headings have the same font \n        size and color. \n        There can be multiple values for the class attribute, separated by spaces. \n        The class value for this level 4 section heading is \n        &lt;code&gt;level4 unnumbered&lt;/code&gt;, \n        indicating both the heading level and that I’ve switched off numbering \n        for this sub-sub-sub-sub-section.&lt;/li&gt;\n    &lt;li&gt;Style: Any CSS items which apply only to the specific element. \n        For this section, only the link to this section added by quarto/pandoc \n        has a style attribute specified (because it is applied automatically \n        when the quarto document is compiled)&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/section&gt;\n\n\n\n\n\n\n31.2.3 Reading HTML and XML files\n\n\n\n\n\n\nTip: When searching for help, be sure to specify packages in your query!\n\n\n\n\n\nThere are many different packages and approaches in both R and Python to working with HTML and XML. Some people write their own custom parsers with regular expressions (masochists!), and there are multiple versions of packages (xml vs xml2, httr and httr2) as well as packages that have fairly similar syntax (rvest, polite) and packages that operate at very different technical levels (curl, httr2, rvest). In the python ecosystem, there are multiple packages that have different ways to handle files in addition to different mechanisms and syntax for sifting through the HTML/XML nodes.\n\n\n\n\n\n\n\n\n\n31.2.3.1 Demo: Reading HTML files from the web\n\n\n\nHere is the basic syntax for reading in an XML or HTML file in R and python using commonly recommended packages for the job.\n\nRPython\n\n\n\nlibrary(xml2)\n\nurl &lt;- \"https://en.wikipedia.org/wiki/HTML\"\n\ndoc &lt;- read_html(url)\n\nxml_children(doc)\n## {xml_nodeset (2)}\n## [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] &lt;body class=\"skin--responsive skin-vector skin-vector-search-vue mediawik ...\n\nWe use read_html() to read in the document (the syntax is the same whether you’re using a URL or a file path) and then can use xml_children() and other parsing functions to make some sense of the structure of the document – in this case, confirming that it’s a valid HTML document with a head and body node.\n\n\n\nimport requests # Gets files from the web\n\nurl = \"https://en.wikipedia.org/wiki/HTML\"\nresponse = requests.get(url)\n# response.text # This outputs a ton of stuff\n\nfrom bs4 import BeautifulSoup # Parses HTML/XML\nsoup = BeautifulSoup(response.text)\nchildren = soup.html.find_all(recursive=False) # get direct children of the HTML tag\n\nfor child in children:\n  print(child.name) # content is too long to print everything\n## body\n\nWe have to request the HTML document using requests.get(url) and then parse the HTML document with bs4.BeautifulSoup.\nThe remaining code just serves to confirm that we have both a head and a body node.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#filtering-data-from-htmlxml-css-and-xpath-selectors",
    "href": "part-advanced-topics/03-web-scraping.html#filtering-data-from-htmlxml-css-and-xpath-selectors",
    "title": "31  Web Scraping",
    "section": "31.3 Filtering Data from HTML/XML: CSS and XPath Selectors",
    "text": "31.3 Filtering Data from HTML/XML: CSS and XPath Selectors\nOften, when pulling data from HTML or XML documents, it is useful to be able to navigate to one or more specific elements of the document and extract only those elements. There are two different “languages” for doing this - CSS selectors, which primarily are used with HTML documents, and XPath selectors, which can be used with both HTML and XML.\n\n\n\n\n\n\nLearn More\n\n\n\n\n\nYou may find it helpful to reference these guides to XPath and CSS selectors directly:\n\nXPath syntax\nCSS selectors\n\n\n\n\n\n31.3.1 Selecting by element type\n\n\n\n\n\n\n\n\nTask\nCSS\nXPath\n\n\n\n\nSelect all p elements from the document\np\n//p\n\n\nSelect all p elements that are direct descendants of an h1 element\nh1 \\&gt; p\nh1/p\n\n\nSelect all p elements that are descendants of an h1 element, no matter where they are under h1\nh1 p\nh1//p\n\n\n\nCSS elements can be combined using combinators - specific combination operators. There are also XPath axes that provide some useful ways to combine selectors to get specific results.\n\n\n31.3.2 Selecting by element ID/Class\n\n\n\n\n\n\n\n\nTask\nCSS\nXPath\n\n\n\n\nSelect the element with ID “objectives”\n#objectives\n//[@id='objectives']\n\n\nSelect the &lt;p&gt; element with ID “objectives”\np#objectives\n//p[@id='objectives']\n\n\nSelect the element with class ‘level2’\n.level2\n//[@class='level2']\n\n\nSelect the &lt;p&gt; element with class ‘level2’\np.level2\n//p[@class='level2']\n\n\n\nOf course, you can chain these selectors together just as in the previous section.\n\n\n31.3.3 Selecting by element attribute/value\n\n\n\n\n\n\n\n\nTask\nCSS\nXPath\n\n\n\n\nSelect all elements with a “dataID” attribute\n[dataID]\n//@dataID\n\n\nSelect all &lt;p&gt; elements with any attribute\nnot supported\n//p[@*]\n\n\nSelect all &lt;p&gt; elements with a “dataID” attribute\np[dataID]\n//p[@dataID]\n\n\nSelect all elements with a “dataID” attribute equal to “mydata”\n[dataID='mydata']\n//*[@dataID='mydata']\n\n\nSelect all elements with a “dataID” attribute containing “my”\n[dataID~='my'] or [dataID*='my']\n//*[contains(@dataID, 'mydata')]\n\n\n\n\n\n\n\n\n\n31.3.4 Demo: Reading in Olympics Medals from HTML\n\n\n\nThe IOC (International Olympic Committee) publishes medal counts for each Olympics, including the 2024 Paris games. If the code below doesn’t work, try using the archived page instead. A screenshot of the rendered page is available in Figure 31.3. Though the page appears to have a table, in fact, the table does not make use of an HTML table element; instead, each row is a series of div elements\n\nRPython\n\n\nIf you’re curious about why I’m reading the HTML from a saved version of the webpage, see the explanation in Section 31.4.2.\n\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\n\ndoc &lt;- read_html(\"../data/Paris 2024 Olympic Medal Table - Gold, Silver & Bronze.html\")\n\n1html_element(doc, \"table\")\n## {xml_missing}\n## &lt;NA&gt;\n\n\n1\n\nSearch the HTML for &lt;table&gt; – no results\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n1from bs4 import BeautifulSoup, SoupStrainer\n\n2file = open(\"../data/Paris 2024 Olympic Medal Table - Gold, Silver & Bronze.html\")\n3index = file.read()\n\n\n1\n\nbs4 is a library for web scraping. BeautifulSoup is its primary function.\n\n2\n\nThe HTML page is saved to disk for this example, so we can open that file using open()\n\n3\n\nWe can read the HTML using file.read()\n\n\n\n\nPandas has a function that will read in an HTML table from an HTML page and convert it directly to a data frame. Let’s try that first. Pandas expects an HTML file, not the literal HTML, so we pass in file instead of index.\n\npd.read_html(file)\n##   File \"&lt;string&gt;\", line 0\n## lxml.etree.XMLSyntaxError: no text parsed from document\n\nWell, that didn’t work so well, so let’s look for an actual table element:\n\nsoup = BeautifulSoup(index)\ntables = soup.find_all(\"table\")\ntables\n## []\n\n\n\n\nNext, we look at the HTML and see that the table itself is wrapped in an element:\n\n&lt;div class=\"Table-styles__Wrapper-sc-7c5c517a-0 cqlSCS\"&gt;&lt;/div&gt;\n\nThat’s too much to type, so search for elements with a class that contains “Table”.\n\nRPython\n\n\n\ntable_nodes &lt;- html_element(doc, css = \"[class*='Table']\")\ntable_nodes\n## {html_node}\n## &lt;div class=\"Table-styles__Wrapper-sc-7c5c517a-0 cqlSCS\"&gt;\n## [1] &lt;div data-cy=\"order-by\" class=\"styles__OrderBy-sc-2eade904-0 kaHofu\"&gt;\\n&lt;s ...\n## [2] &lt;div data-cy=\"table-header\" class=\"Table-styles__CommonGrid-sc-7c5c517a-1 ...\n## [3] &lt;div data-cy=\"table-content\" class=\"Table-styles__CommonGrid-sc-7c5c517a- ...\n\n\n\n\n1table_nodes = soup.select(\"*[class*=Table]\")\n2len(table_nodes)\n\n3[i.__str__()[0:100] for i in table_nodes]\n## 3\n## ['&lt;div class=\"Table-styles__Wrapper-sc-7c5c517a-0 cqlSCS\"&gt;&lt;div class=\"styles__OrderBy-sc-2eade904-0 ka', '&lt;div class=\"Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Header-sc-7c5c517a-2 dTTMvn bBWhIS\"', '&lt;div class=\"Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Content-sc-7c5c517a-3 dTTMvn jfNiTo']\n\n\n1\n\nGet all nodes that have class that starts with Table\n\n2\n\nHow many nodes did we find?\n\n3\n\nPrint out the first 100 characters of each element to see what we found without drowning in text.\n\n\n\n\n\n\n\nIt can be hard to get the hang of “reading” HTML to make sense of it – what I usually do is try to find the narrowest set of nodes I can that have the content I want. To identify this, I’ll use a process like that shown in Figure 31.4, where I’ll right click on the web page in the browser, select “Inspect element”, and then interactively hover over various HTML elements until I get a sense of what each component of the rendered page looks like in HTML.\nIn this case, the ‘minimal node’ is:\n\n&lt;div data-cy=\"table-content\" class=\"Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Content-sc-7c5c517a-3 dTTMvn jfNiTo\"&gt;&lt;/div&gt;\n\nIt has just the content, without the sortable headers and other stuff that we don’t particularly care about. That is, the headers and such are important for context, but can be added in to the table relatively quickly (and thus aren’t worth the time to scrape).\nI also noticed that in each “column” of a single row, there are div elements with attributes that contain the row number (data-row-id or data-medal-id), and that each div element has one or the other of these attributes, but not both. That means that if we can get the row numbers from the attributes and the column numbers based on the relative position of the elements, we can reconstruct the table structure from just a list of disjoint div elements.\n\n\n\n\n\n\nAnother Approach: Text Editor HTML Magic\n\n\n\n\n\nSometimes, if I’m not feeling the interactive inspection in the browser, or if things are complicated and/or the inspection isn’t going well, I’ll take a more direct approach and copy a subset of the HTML out into a text editor. Then, I do a little bit of find-and-replace magic:\n\nSearch for &lt; and replace it with \\n&lt;, to get each tag on its own line in the text document. HTML may not care much about whitespace between text nodes, but it makes it a lot more human-readable.\nSearch for \\n&lt;/ and replace it with &lt;/ to get the end tags back on the same line as the corresponding start tag (at least for elements with no children).\n\nYou can see the result of this find-and-replace magic in this file.\nIn my simplified view, I start to notice a pattern of div-span-span. That is, each div element contains a couple of span elements, and there are several div- elements that show up for each row in the table. Some of these elements have attribute data-row-id and some have attribute data-medal-id, but the values of each of these attributes have row-## where ## is the row number in the table. As a result, it seems like we might want to focus primarily on the div elements and only work with the span elements if we absolutely have to.\n\n\n\nThere is always a “prettier” way to do HTML parsing, but in most cases it’s faster and more straightforward to take a quick-and-dirty approach and only make things more complicated if necessary.\nBased on the basic structure I’ve described above, let’s try to write code that\n\nLooks for the div elements that have attributes data-row-id and data-medal-id\nGets the text from these elements\nRearranges the text from (2) into rows and columns using a pivot operation\nConverts medal counts to numeric variables, if they are characters, replacing ‘-’ with NA\n\nEach step in this outline may take more than one line of code.\n\nRPython\n\n\nThe CSS selector we use here is a compound selector - it looks for div elements that have data-row-id or data-medal-id as an attribute.\n\n1table_data &lt;- html_elements(doc, css = \"div[data-row-id],div[data-medal-id]\")\n\n2header_names &lt;- c(\"row\", \"flag\", \"country_abb\",\n                  \"gold\", \"silver\", \"bronze\", \"total\", \"country\")\n\nmedal_data &lt;- tibble(\n3  idx = 1:length(table_data),\n4  text = map_chr(table_data, html_text),\n5  medal_attributes = map_chr(table_data, ~html_attr(., \"data-medal-id\")),\n6  row_attributes = map_chr(table_data, ~html_attr(., \"data-row-id\"))\n) |&gt;\n7  mutate(row = if_else(is.na(medal_attributes),\n                       str_extract(row_attributes, \"\\\\d{1,}\"),\n                       str_extract(medal_attributes, \"\\\\d{1,}\"))) |&gt;\n8  group_by(row) |&gt;\n  mutate(column = 1:n())\n\n\n1\n\nSearch for div nodes that have either data-row-id or data-medal-id attributes.\n\n2\n\nDefine a header row that corresponds to the 8 div elements in each row.\n\n3\n\nCreate a table with the index of the nodes in the list created in step 1.\n\n4\n\nConvert each HTML node to text (quick and dirty option)\n\n5\n\nGet the data-row-id if it exists\n\n6\n\nGet the data-medal-id if it exists\n\n7\n\nGet the row number from data-row-id if medal_attributes is NA, and otherwise, get it from data-medal-id.\n\n8\n\nGroup by row number and create column number, assuming the nodes are in row, column order.\n\n\n\n\nThese steps account for the first two steps listed in the outline above. Next, we need to pivot the data to get the table as it is shown visually on the webpage, and then we need to convert the medal totals from character to numeric variables.\n\nmedal_data_wide &lt;- medal_data |&gt;\n1  select(-idx, -medal_attributes, -row_attributes) |&gt;\n2  pivot_wider(names_from = column, values_from = text) |&gt;\n3  set_names(header_names) |&gt;\n4  mutate(across(gold:total, as.numeric))\n\n\n1\n\nRemove columns that are no longer necessary so they don’t interfere with pivot operation\n\n2\n\nPivot to replicate structure of original table, using the column values as column names for now.\n\n3\n\nSet column names to descriptive labels\n\n4\n\nConvert gold, silver, bronze, and total columns to numeric variables, which causes NA to replace ‘-’ in the original table.\n\n\n\n\n\n\n\n\n\nidx\ntext\nmedal_attributes\nrow_attributes\nrow\ncolumn\n\n\n\n\n1\n\nNA\ncountry-medal-row-1\n1\n1\n\n\n2\nMIX AIN\nNA\ncountry-medal-row-1\n1\n2\n\n\n3\n1\ngold-medals-row-1\nNA\n1\n3\n\n\n4\n3\nsilver-medals-row-1\nNA\n1\n4\n\n\n5\n1\nbronze-medals-row-1\nNA\n1\n5\n\n\n6\n5\ntotal-medals-row-1\nNA\n1\n6\n\n\n\n\n\n\n\n\n1table_nodes = soup.select(\"div[data-row-id],div[data-medal-id]\")\n\n2header_names = [\"flag\", \"country_abb\", \"gold\", \"silver\", \"bronze\", \"total\", \"country\"]\n\n\nmedal_data = pd.DataFrame({\n3  \"i\": range(0, len(table_nodes)),\n4  \"chr\": [i.get_text() for i in table_nodes],\n5  \"attr\": [dict(i.attrs) for i in table_nodes]\n  })\n  \n6medal_data[\"row\"] = [medal_data.attr[i]['data-row-id']\n                      if 'data-row-id' in medal_data.attr[i]\n                      else medal_data.attr[i]['data-medal-id']\n                      for i in medal_data.i]\n\n\n1\n\nThis gets all of the div elements with a data-row-id or data-medal-id attribute\n\n2\n\nCreate a list of column names (for later)\n\n3\n\nCreate an index along table_nodes to match the row number\n\n4\n\nGet the text from table_nodes as it would be rendered in HTML\n\n5\n\nCreate a dictionary of attributes for each of the entries in table_nodes. This will allow us to pull values out by attribute name.\n\n6\n\nThis is a fairly long list comprehension (sorry). For each of the rows, we’re going to check and see if attr has an entry data-row-id, and if so, we’re going to pull the value out. If not, we’ll pull out the value from data-medal-id.\n\n\n\n\nNext, we need to pivot the data to get the table as it is shown visually on the webpage, and then we need to convert the medal totals from character to numeric variables.\n\n1import re\n\n2medal_data['rownum'] = [re.search(r'\\d+', i).group() for i in medal_data.row]\n3medal_data['colnum'] = medal_data.groupby('rownum').cumcount() + 1\n4medal_data_wide = medal_data.loc[:,['chr', 'rownum', 'colnum']].pivot(columns='colnum', index = 'rownum')\n5medal_data_wide.columns = header_names\n\n6tmp = [pd.to_numeric(medal_data_wide[i], errors='coerce') for i in ['gold', 'silver', 'bronze', 'total']]\n\n7tmpfix = pd.DataFrame(tmp).transpose()\n8medal_data_wide[['gold', 'silver', 'bronze', 'total']] = tmpfix\n\n\n1\n\nLoad regular expression library\n\n2\n\nSearch for one or more consecutive digits in the row name\n\n3\n\ngroup by the row number and count the rows in each group to get columns\n\n4\n\nSelect only the columns we need - row, col, and the text shown on the HTML page, and then pivot so that rows are rows and cols are cols\n\n5\n\nSet names to be sensible/meaningful.\n\n6\n\nFor each of the four columns with medal totals, convert to numeric (coercing ‘-’ to NaN). The result is a list of length \\(4\\), and each element in the list is a list with \\(92\\) entries.\n\n7\n\nConvert the list of lists to a pandas DataFrame with dimensions \\(4\\times 92\\) and then transpose it so that it will fit back in our data frame.\n\n8\n\nStore the numeric variables back into the wide data frame in the correct columns.\n\n\n\n\n\n\n\n\n\nflag\ncountry_abb\ngold\nsilver\nbronze\ntotal\ncountry\n\n\n\n\n\nMIX AIN\n1\n3\n1\n5\nAIN\n\n\n\nBEL Belgium\n3\n1\n6\n10\nBelgium\n\n\n\nBOT Botswana\n1\n1\nNaN\n2\nBotswana\n\n\n\nBRA Brazil\n3\n7\n10\n20\nBrazil\n\n\n\nBUL Bulgaria\n3\n1\n3\n7\nBulgaria\n\n\n\nCPV Cabo Verde\nNaN\nNaN\n1\n1\nCabo Verde\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 31.3: (Click to enlarge) Screenshot of the 2024 Paris Olympics Medal “Table”\n\n\n\n\n\n\n\n\n\nFigure 31.4: (Click to enlarge) Screen recording: Using the Developer Console to map web page elements to HTML code in order to identify attributes and document structure that can be parsed.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#ethical-web-scraping",
    "href": "part-advanced-topics/03-web-scraping.html#ethical-web-scraping",
    "title": "31  Web Scraping",
    "section": "31.4 Ethical Web Scraping",
    "text": "31.4 Ethical Web Scraping\nScraping information off of the web can quickly push you into a zone where you have to worry about legality, privacy, and copyright issues. In addition, not all websites want to be scraped, and scraping a website (even if the website is not actively blocking you) can create unacceptable load on the server hosting the site, if you are not careful and ethical about what you are doing. This section describes a couple of important components of scraping websites ethically, but these components also provide some important technical background that can be useful for debugging why your web scraping script isn’t working the way you expect.\n\n31.4.1 robots.txt\nThe robots.txt file is found at the root level of the website, and gives instructions to web crawlers/scrapers as to what areas of the site are allowed to be scraped, and who is allowed to scrape them. Websites identify who you are using a User-agent that is transmitted any time your computer requests information from the internet.\nWhen we look at what we’re allowed to scrape, we want to find the User-agent: * section of the robots.txt file that describes what someone with any user agent not otherwise specified is allowed to scrape.\nCheck out some robots.txt files to see what things they allow and block.\n\nhttps://www.google.com/robots.txt\nIn google’s robots.txt file, they explicitly block scraping search, groups, and other products (patents, citations, scholar, maps), but they allow scraping some books content (related, editions, subject, about).\nhttps://github.com/robots.txt\nGithub’s robots.txt file provides some information about their API (which is more convenient than scraping, generally - see Chapter 33), but allows all user-agents and disallows most of the “extra” information about repositories - pulse, projects, forks, issues, milestones, commits, etc. - so you can access the main repositories but not the details via scraping, if you choose to scrape. Github also provides a contact email if you want to crawl the entire site – likely not relevant unless you’re writing a search engine, but it’s helpful to know who to contact in any case.\nhttps://en.wikipedia.org/robots.txt\nWikipedia’s robots.txt file allows some bots and disallows others. They mention paying attention to 429 rate limit response codes and not copying the entire site. Further down, they mention a reasonable request rate of 20 requests per second.\nhttps://www.olympics.com/robots.txt\nThis site doesn’t allow scraping /athletes/library/*, search (which just prevents infinite recursive scraping – that’s actually helping bots out), test directories, or syndicated videos. No big deal.\nhttps://www.juniorrollerderby.org/robots.txt\nThis site doesn’t allow crawling /users/ or /event sections of the site, but is otherwise fine with being scraped.\n\nrobots.txt files will often use a local path - the base site is https://wikipedia.org/ or https://google.com/ or https://github.com/, so they will list /search/ as shorthand for https://google.com/search/. It is fairly common for links to be specified relative to the base site as well, so you will often have to paste the front part of the URL onto the local path to get a valid URL.\nThe polite R package will ensure that your requests accommodate anything specified in the robots.txt file [3], [4]. The reppy python package [5], [6] has similar functions to facilitate ethical web scraping.\nAs with any ethical system, there can be reasons to justify not complying with robots.txt [7], [8]. Journalists often want to archive web pages exactly as they were to ensure that their sources are unimpeachable. More broadly, the Internet Archive announced in 2017 that it would no longer observe robots.txt rules because its archival mission had a different goal than bots that crawl the internet for search indexing purposes. During the 2025 Trump administration directed purge of DEI content from military and government websites [9], internet activists often ignored robots.txt files in order to ensure that content was archived and not entirely erased from historical records. However, even under these scenarios, ensuring that scraping does not overload the web server is still important, as overloaded servers don’t respond to requests, and too many requests may get your IP address blocked by the server entirely.\nrobots.txt is also a primary defense against content being stolen and fed into AI applications; as a result, it is important to be aware of the file both as a consumer of web-based information and as someone who may one day publish content on the web.\n\n\n\n\n\n\nDemo: Checking robots.txt programmatically\n\n\n\n\nRPython\n\n\n\nlibrary(polite)\n\nbow(\"https://wikipedia.org\")\n## &lt;polite session&gt; https://wikipedia.org\n##     User-agent: polite R package\n##     robots.txt: 465 rules are defined for 34 bots\n##    Crawl delay: 5 sec\n##   The path is scrapable for this user-agent\nbow(\"https://google.com/search\")\n## &lt;polite session&gt; https://google.com/search\n##     User-agent: polite R package\n##     robots.txt: 525 rules are defined for 5 bots\n##    Crawl delay: 5 sec\n##   The path is not scrapable for this user-agent\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nimport urllib.robotparser\n\ndef is_allowed(url):\n  # Setup robots.txt parser\n  rp = urllib.robotparser.RobotFileParser()\n  rp.set_url(urljoin(url, '/robots.txt'))\n  rp.read()\n  return rp.can_fetch(\"*\", url)\n\n# Check if you're allowed to scrape the site\nis_allowed(\"https://wikipedia.org\")\n## False\nis_allowed(\"https://google.com/search\")\n## False\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Can you scrape these sites?\n\n\n\n\nProblemSolution (manual)Solution (R)Solution (Python)\n\n\nDetermine (using any method you like) whether you’re allowed to scrape the following sites, and whether there are any custom restrictions on the interval between requests:\n\nhttps://linkedin.com\nhttps://lincoln.craigslist.org/mis/\nhttps://reddit.com/r/Aww/\n\nAlso determine whether the site has a contact point for individual requests, if the data you want is not scrape-able.\n\n\n\nLinkedIn: robots.txt\n\nUser-agent: *\nDisallow: /\n\n# Notice: If you would like to crawl LinkedIn,\n# please email whitelist-crawl@linkedin.com to apply\n# for white listing.\nScraping LinkedIn is not allowed.\n\nCraigslist Missed Connections robots.txt\n\nUser-agent: *\nDisallow: /reply\nDisallow: /fb/\nDisallow: /suggest\nDisallow: /flag\nDisallow: /mf\nDisallow: /mailflag\nDisallow: /eaf\nScraping Craigslist missed connections is technically allowed. Even so, Craigslist is notorious for banning users for too many requests, even when they’re spaced out over a significant period of time and handled politely.\n\nReddit /r/Aww: robots.txt\n\n# Welcome to Reddit's robots.txt\n# Reddit believes in an open internet, but not the misuse of public content.\n# See https://support.reddithelp.com/hc/en-us/articles/26410290525844-Public-Content-Policy Reddit's Public Content Policy for access and use restrictions to Reddit content.\n# See https://www.reddit.com/r/reddit4researchers/ for details on how Reddit continues to support research and non-commercial use.\n# policy: https://support.reddithelp.com/hc/en-us/articles/26410290525844-Public-Content-Policy\n\nUser-agent: *\nDisallow: /\nScraping Reddit is not allowed. Some posts may be accessible via the API.\n\n\n\nlibrary(polite)\n\nbow(\"https://linkedin.com\")\n## &lt;polite session&gt; https://linkedin.com\n##     User-agent: polite R package\n##     robots.txt: 4352 rules are defined for 75 bots\n##    Crawl delay: 5 sec\n##   The path is not scrapable for this user-agent\nbow(\"https://lincoln.craigslist.org/mis/\")\n## &lt;polite session&gt; https://lincoln.craigslist.org/mis/\n##     User-agent: polite R package\n##     robots.txt: 8 rules are defined for 14 bots\n##    Crawl delay: 5 sec\n##   The path is scrapable for this user-agent\nbow(\"https://reddit.com/r/Aww/\")\n## &lt;polite session&gt; https://reddit.com/r/Aww/\n##     User-agent: polite R package\n##     robots.txt: 1 rules are defined for 1 bots\n##    Crawl delay: 5 sec\n##   The path is not scrapable for this user-agent\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nimport urllib.robotparser\n\ndef is_allowed(url):\n  # Setup robots.txt parser\n  rp = urllib.robotparser.RobotFileParser()\n  rp.set_url(urljoin(url, '/robots.txt'))\n  rp.read()\n  return rp.can_fetch(\"*\", url)\n\n# Check if you're allowed to scrape the site\nis_allowed(\"https://linkedin.com\")\n## False\nis_allowed(\"https://lincoln.craigslist.org/mis/\")\n## True\nis_allowed(\"https://reddit.com/r/Aww/\")\n## False\n\n\n\n\n\n\n\n\n31.4.2 User Agents\nWeb traffic logs on servers include your user agent, which provides information about your operating system and browser. This information can help server administrators understand who is accessing their site, and can lead to decisions to e.g. upgrade the web page to no longer accommodate Internet Explorer users, since they make up a tiny fraction of all web traffic. From a user perspective, some sites will prevent e.g. Linux users from downloading unsupported software, or increase prices if your user agent indicates you’re using a Mac [10], so changing your user agent is not just something you might want to do when scraping the web.\n\n\n\n\n\nYou can use the site https://whatmyuseragent.com to find out what information you’re providing to websites. Don’t depend on the accuracy of the location – it is often where your internet provider is headquartered, rather than where you’re actually located.\n\n\n\n\n\n\n\n\nFrom the Other Side: Examining Access Logs and User Agents\n\n\n\n\n\nIt’s sometimes helpful to look at what a server administrator might see if you’re scraping the web improperly. To demonstrate, I decided to pull the access logs from my personal website just to see who had been snooping around and what their user agents were. I’ve censored the IP addresses and will primarily focus on the browser type, if it is properly defined.\n\n\nQuick and dirty R code to look at the most common User-agents accessing my home website.\nlibrary(stringr)\nreadLines(\"../data/server-access-log.txt\") |&gt;\n  str_extract(\".*\\\"(.*)\\\"$\", group = 1) |&gt;\n  table() |&gt;\n  sort(decreasing=T) |&gt;\n  head(10) |&gt;\n  as.data.frame() |&gt;\n  set_names(c(\"User-agent\", \"Freq\")) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nUser-agent\nFreq\n\n\n\n\nScrapy/2.11.2 (+https://scrapy.org)\n238\n\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\n144\n\n\nMozilla/5.0 (compatible; AhrefsBot/7.0; +http://ahrefs.com/robot/)\n40\n\n\n-\n39\n\n\nMozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1\n30\n\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\n24\n\n\nMozilla/5.0 (Linux; Android 10; LIO-AN00 Build/HUAWEILIO-AN00; wv) MicroMessenger Weixin QQ AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/78.0.3904.62 XWEB/2692 MMWEBSDK/200901 Mobile Safari/537.36\n20\n\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.43\n20\n\n\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36 Edg/90.0.818.46\n17\n\n\n‘Mozilla/5.0 (compatible; GenomeCrawlerd/1.0; +https://www.nokia.com/genomecrawler)’\n16\n\n\n\n\n\nIt looks like the most common user agent is scrapy, a python library for web scraping, and the second most common is some variant of Mozilla/5.0 on various operating systems and platforms. As a result of this investigation, I blocked scrapy using robots.txt, because I don’t know what that’s being used for but there is literally nothing of interest to the wider internet, data-wise, on my personal website. I also blocked a bunch of different AI bots because I don’t want them indexing my personal site either.\nIt’s important to note from a server administrator side that robots.txt doesn’t provide any additional security, as malicious actors can and will ignore ethical/polite web scraping conventions.\n\n\n\nConvention dictates that your user agent when scraping should provide some contact information and reasoning, such as “Graduate student scraping for research on underwater basketweaving. Contact me at name@myuni.edu for more info.” However, some sites will block user agents that do not appear to be a normal browser, even if scraping is explicitly allowed. In these cases, you will want to set a header that mimics a browser (Consult this up-to-date list of common user agents for a few good choices).\n\n\n\n\n\n\nSetting a User Agent\n\n\n\n\nR: rvestR: politePython\n\n\nYou may have noticed in Section 31.3.4 that the R code read the file in from a saved version instead of from the web directly. When we try to read from the web directly, we get this message:\n\nlibrary(rvest)\ndoc &lt;- read_html(\"https://www.olympics.com/en/olympic-games/paris-2024/medals\")\n\nBy default, rvest uses the default user agent specified in the curl package3, which is NULL. So, let’s try setting an informative user agent, instead of NULL.\n\n1library(httr)\n\n# Set the UA to describe what we're doing\nua &lt;- \"Demonstrating web scraping for educational purposes. Contact svanderplas2@unl.edu for more info.\"\nolympics &lt;- GET(\"https://www.olympics.com/en/olympic-games/paris-2024/medals\", \n2                user_agent(ua))\n## Error in curl::curl_fetch_memory(url, handle = handle): Stream error in the HTTP/2 framing layer [www.olympics.com]:\n## HTTP/2 stream 1 was not closed cleanly: INTERNAL_ERROR (err 2)\n\n\n1\n\nhttr is used to specify a user agent directly\n\n2\n\nUse a GET request to acquire the HTML page code from the server, instead of reading the HTML directly\n\n\n\n\nOk, well, that doesn’t appear to work - we’re still getting blocked by the server and can’t even read the HTML file properly. The robots.txt file for this site allows scraping as long as we’re not looking at /athletes/library/ or /search/, so we’re in compliance with the rules they’ve set… but maybe we just need to set a user agent that makes us look like a browser?\n\n1library(httr)\n\n# Set the user agent to a super common one\nua &lt;- \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\"\nolympics &lt;- GET(\"https://www.olympics.com/en/olympic-games/paris-2024/medals\", \n2                user_agent(ua))\n3doc &lt;- read_html(olympics)\n4html_element(doc, \"[data-cy=\\\"table-content\\\"]\")\n## {html_node}\n## &lt;div data-cy=\"table-content\" class=\"Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Content-sc-7c5c517a-3 dTTMvn jfNiTo\"&gt;\n##  [1] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n##  [2] &lt;div data-row-id=\"country-medal-row-1\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n##  [3] &lt;div data-row-id=\"country-medal-row-1\" class=\"sc-26c0a561-4 kaSJtG\"&gt;\\n&lt;d ...\n##  [4] &lt;div data-cy=\"medal-module\" data-medal-id=\"gold-medals-row-1\" title=\"Gol ...\n##  [5] &lt;div data-cy=\"medal-module\" data-medal-id=\"silver-medals-row-1\" title=\"S ...\n##  [6] &lt;div data-cy=\"medal-module\" data-medal-id=\"bronze-medals-row-1\" title=\"B ...\n##  [7] &lt;span class=\"mobile-hidden\"&gt;&lt;/span&gt;\n##  [8] &lt;div data-cy=\"medal-module\" data-medal-id=\"total-medals-row-1\" title=\"\"  ...\n##  [9] &lt;div data-cy=\"country-complete-name\" data-row-id=\"country-medal-row-1\" c ...\n## [10] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n## [11] &lt;div data-row-id=\"country-medal-row-2\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n## [12] &lt;div data-row-id=\"country-medal-row-2\" class=\"sc-26c0a561-4 kaSJtG\"&gt;\\n&lt;d ...\n## [13] &lt;div data-cy=\"medal-module\" data-medal-id=\"gold-medals-row-2\" title=\"Gol ...\n## [14] &lt;div data-cy=\"medal-module\" data-medal-id=\"silver-medals-row-2\" title=\"S ...\n## [15] &lt;div data-cy=\"medal-module\" data-medal-id=\"bronze-medals-row-2\" title=\"B ...\n## [16] &lt;span class=\"mobile-hidden\"&gt;&lt;/span&gt;\n## [17] &lt;div data-cy=\"medal-module\" data-medal-id=\"total-medals-row-2\" title=\"\"  ...\n## [18] &lt;div data-cy=\"country-complete-name\" data-row-id=\"country-medal-row-2\" c ...\n## [19] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n## [20] &lt;div data-row-id=\"country-medal-row-3\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n## ...\n\n\n1\n\nhttr is used to specify a user agent directly\n\n2\n\nUse a GET request to acquire the HTML page code from the server, instead of reading the HTML directly\n\n3\n\nUse read_html to parse the HTML code that the server provided\n\n4\n\nCheck to make sure the information we wanted is present.\n\n\n\n\nAnd, lo and behold, we can scrape the site properly once our user agent is set.\n\n\n\n1library(polite)\n\n# Set the user agent to a super common one\nua &lt;- \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\"\nurl &lt;- \"https://www.olympics.com/en/olympic-games/paris-2024/medals\"\n2olympics &lt;- bow(url, user_agent = ua)\n3doc &lt;- scrape(olympics)\n4html_element(doc, \"[data-cy=\\\"table-content\\\"]\")\n## {html_node}\n## &lt;div data-cy=\"table-content\" class=\"Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Content-sc-7c5c517a-3 dTTMvn jfNiTo\"&gt;\n##  [1] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n##  [2] &lt;div data-row-id=\"country-medal-row-1\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n##  [3] &lt;div data-row-id=\"country-medal-row-1\" class=\"sc-26c0a561-4 kaSJtG\"&gt;\\n&lt;d ...\n##  [4] &lt;div data-cy=\"medal-module\" data-medal-id=\"gold-medals-row-1\" title=\"Gol ...\n##  [5] &lt;div data-cy=\"medal-module\" data-medal-id=\"silver-medals-row-1\" title=\"S ...\n##  [6] &lt;div data-cy=\"medal-module\" data-medal-id=\"bronze-medals-row-1\" title=\"B ...\n##  [7] &lt;span class=\"mobile-hidden\"&gt;&lt;/span&gt;\n##  [8] &lt;div data-cy=\"medal-module\" data-medal-id=\"total-medals-row-1\" title=\"\"  ...\n##  [9] &lt;div data-cy=\"country-complete-name\" data-row-id=\"country-medal-row-1\" c ...\n## [10] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n## [11] &lt;div data-row-id=\"country-medal-row-2\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n## [12] &lt;div data-row-id=\"country-medal-row-2\" class=\"sc-26c0a561-4 kaSJtG\"&gt;\\n&lt;d ...\n## [13] &lt;div data-cy=\"medal-module\" data-medal-id=\"gold-medals-row-2\" title=\"Gol ...\n## [14] &lt;div data-cy=\"medal-module\" data-medal-id=\"silver-medals-row-2\" title=\"S ...\n## [15] &lt;div data-cy=\"medal-module\" data-medal-id=\"bronze-medals-row-2\" title=\"B ...\n## [16] &lt;span class=\"mobile-hidden\"&gt;&lt;/span&gt;\n## [17] &lt;div data-cy=\"medal-module\" data-medal-id=\"total-medals-row-2\" title=\"\"  ...\n## [18] &lt;div data-cy=\"country-complete-name\" data-row-id=\"country-medal-row-2\" c ...\n## [19] &lt;div class=\"line\"&gt;&lt;/div&gt;\\n\n## [20] &lt;div data-row-id=\"country-medal-row-3\" class=\"sc-26c0a561-2 bNqscu\"&gt;&lt;pic ...\n## ...\n\n\n1\n\npolite allows you to specify a user agent directly\n\n2\n\nbow acquires the HTML page code from the server\n\n3\n\nscrape parses the HTML code that the server provided, at which point we can use various rvest and xml2 functions to pull out relevant information.\n\n4\n\nCheck to make sure the information we wanted is present.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n1from bs4 import BeautifulSoup, SoupStrainer\n2import urllib.request\n\n3req = urllib.request.Request(\n  \"https://www.olympics.com/en/olympic-games/paris-2024/medals\",\n  data = None,\n  headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0'}\n)\n\n4page_bytearray = urllib.request.urlopen(req)\npage = page_bytearray.read()\npage_bytearray.close()\n\n5soup = BeautifulSoup(page)\ntable_nodes = soup.select(\"div[data-row-id],div[data-medal-id]\")\n\n\n1\n\nbs4 is a library for web scraping. BeautifulSoup is its primary function.\n\n2\n\nurllib helps with web calls, and request allows us to set browser headers (so it looks like we’re using e.g. Firefox instead of a programming language to access the web).\n\n3\n\nThis requests the URL using headers that suggest we’re using an older version of Firefox on Windows.\n\n4\n\nThese lines open the page, read it, and close the open page in memory (which is important because open files in memory can cause memory leaks and other badness)\n\n5\n\nCheck to make sure the information we wanted is present.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#scraping-static-webpages",
    "href": "part-advanced-topics/03-web-scraping.html#scraping-static-webpages",
    "title": "31  Web Scraping",
    "section": "31.5 Scraping Static Webpages",
    "text": "31.5 Scraping Static Webpages\nStatic web pages have HTML that stays the same as the web page loads – all of the content is specified in the HTML, and it doesn’t change (much). Any scripting changes are cosmetic, in that they might change the appearance of the page somewhat but they don’t e.g. insert a data table during the page loading process. These pages can be scraped using rvest and xml2 in R or bs4 (BeautifulSoup) in Python.\n\n31.5.1 Reading in HTML Tables\nIn the simplest possible scenario, the data is already well-organized in a tabular structure (with actual &lt;table&gt; tags, unlike the example in Section 31.3.4).\n\n\n\n\n\n\nDemo: Reading HTML Tables Directly\n\n\n\nConsider the Wikipedia page containing a List of American Revolutionary War Battles.\n\nRPython\n\n\nWe would use the html_table() function from rvest to read each of the tables on this webpage into a tibble object.\n\nlibrary(rvest)\nrev_war_tables &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles\") |&gt;\n  html_table()\n\nlength(rev_war_tables)\n## [1] 33\n\n33 tables is a few too many to deal with!\nLooking at the page source, we can see that actual data tables (as opposed to tables used for formatting and arranging content4) are sortable and have class class=\"sortable wikitable jquery-tablesorter\". This is a good example of a static page that uses JavaScript (JQuery, to be specific) to enhance the page (e.g. making the table sortable) without injecting large amounts of content into the page. We only need the dynamic scraping solutions if the content is injected – in this case, the JavaScript is only making modifications to the function/appearance of the page.\n\nlibrary(rvest)\nrev_war_battles &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles\") |&gt;\n  html_element(\".wikitable\") |&gt;\n  html_table()\n\nhead(rev_war_battles)\n## # A tibble: 6 × 5\n##   Battle                             Date                 Location Outcome Ref  \n##   &lt;chr&gt;                              &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;\n## 1 Powder Alarm*                      September 1, 1774    Massach… Britis… [1]  \n## 2 Storming of Fort William and Mary* December 14, 1774    New Ham… Americ… [2]  \n## 3 Battles of Lexington and Concord   April 19, 1775       Massach… Americ… [3]  \n## 4 Battle of Meriam's Corner          April 19, 1775       Massach… Americ… &lt;NA&gt; \n## 5 Siege of Boston                    April 19, 1775 –Mar… Massach… Americ… [4]  \n## 6 Gunpowder Incident*                April 20, 1775       Virginia Virgin… [1]\n\n\n\n\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles\"\nrev_war_battles = pd.read_html(url, attrs= {\"class\": \"sortable wikitable\"})[0]\n## urllib.error.HTTPError: HTTP Error 403: Forbidden\n\nrev_war_battles.iloc[:,0:3]\n## NameError: name 'rev_war_battles' is not defined\n\n\n\n\nIf all we wanted was the date of the battle and the state, we could stop here. However, if we want any supplemental information, such as links to the battles or a more specific location that would be suitable for plotting on a map, we will need to use a more customized approach… continue on to Section 31.5.2 to augment this data some more!\n\n\n\n\n31.5.2 Scraping HTML with Custom Functions\nMuch of the time, the information we want from a website is not directly in tabular form, even if it may have a common structure. Consider, for instance, a school directory listing the names, email addresses, office numbers, and phone numbers of the faculty and staff. This could be formatted as a table (and we probably want the end result to be tabular), but more commonly it’s structured on the web as a set of “Cards” formatted like the one shown in Figure 31.5.\n\n\n\n\n\n\n\n\nFigure 31.5: “Cards” are often used to show a combination of image and text information populated from structured data. Image modified from a sample image by Artguru AI\n\n\n\nOften, in addition to the text, you will want to acquire image data, links, or data from linked pages along with the information presented on the landing page. Or, you may want to acquire information from several pages with similar structure – for instance, you may want to get all departments at an institution, or information from all of the animal shelters in your state. Even when data is not in a “cards” style format, but is instead structured in a table, or something that is visually similar to a table but is not actually contained within &lt;table&gt;&lt;/table&gt; tags, it is typically easy enough to get the data out of the HTML markup systematically.\nThe most effective way to write code for web scraping tasks more complicated than just reading in a single table is to write custom functions to extract the components you need (and put in appropriate empty/NA values if those components are missing), and then call those custom functions from a main function that can put everything back together). You may need functions for multiple levels of pages, if you have to follow links from one main page to individual, more specific pages. These functions will need to contain error handling, because it is incredibly common for information to be missing, at which point your searches for specific nodes will come back empty. It is very annoying to have your code fail on the 99th of 100 pages and have to re-scrape everything (and, in fact, you generally don’t want to do this – it is more polite to work in a way that ensures you are saving intermediate results and don’t have to rescrape everything each time).\n\n\n\n\n\n\nDemo: Revolutionary War Battles\n\n\n\nIf we want to get information from the table of revolutionary war battles that isn’t just text (for instance, the link locations to the pages with more detailed information), then we need to write a bit more custom code – first, to get the links from the table, and second to get additional information from the specific pages for each battle.\nIn this case, it’s easiest to think of the table as a set of rows with columns that have a specific order. We can use CSS pseudo-classes (things you can append to elements): :first-child, :nth-child(2), :nth-child(3) to get the information from the first 3 columns – the fourth is text that we don’t need right now, and the fifth is a citation.\n\n\n\n\n\n\nIn order to reduce the load on wikipedia and the compile time for the textbook, I have saved the artifacts from this example after running the code once. I am invisibly loading those saved objects rather than re-scraping everything each time this book is re-compiled.\n\n\n\n\nRPython\n\n\n\nlibrary(rvest)\nlibrary(dplyr)\ndoc &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles\") \n\nrev_war_rows &lt;- html_elements(doc, \".wikitable tr\")\nhead(rev_war_rows)\n## {xml_nodeset (6)}\n## [1] &lt;tr&gt;\\n&lt;th&gt;Battle&lt;/th&gt;\\n&lt;th&gt;Date&lt;/th&gt;\\n&lt;th&gt;Location&lt;/th&gt;\\n&lt;th&gt;Outcome\\n&lt;/t ...\n## [2] &lt;tr&gt;\\n&lt;td&gt;\\n&lt;a href=\"/wiki/Powder_Alarm\" title=\"Powder Alarm\"&gt;Powder Alar ...\n## [3] &lt;tr&gt;\\n&lt;td&gt;\\n&lt;a href=\"/wiki/Capture_of_Fort_William_and_Mary#Raid\" title=\" ...\n## [4] &lt;tr&gt;\\n&lt;td&gt;&lt;a href=\"/wiki/Battles_of_Lexington_and_Concord\" title=\"Battles ...\n## [5] &lt;tr&gt;\\n&lt;td&gt;&lt;a href=\"/w/index.php?title=Battle_of_Meriam%27s_Corner&amp;act ...\n## [6] &lt;tr&gt;\\n&lt;td&gt;&lt;a href=\"/wiki/Siege_of_Boston\" title=\"Siege of Boston\"&gt;Siege o ...\n\nOur operational node here will be each row of the table. To keep the data together (in case, for instance, some battles don’t have their own pages, and thus don’t have a link), we need to get a node list of the rows first, and then use selectors on that list to get our data in a consistent order.\n\nrev_war_battles &lt;- tibble(\n  name = html_elements(rev_war_rows, \"td:first-child\") |&gt; html_text(),\n  link = html_elements(rev_war_rows, \"td:first-child &gt; a\") |&gt; html_attr(\"href\"),\n  date = html_elements(rev_war_rows, \"td:nth-child(2)\") |&gt; html_text(),\n  location = html_elements(rev_war_rows, \"td:nth-child(3)\") |&gt; html_text()\n)\n## Error in `tibble()`:\n## ! Tibble columns must have compatible sizes.\n## • Size 271: Existing data.\n## • Size 270: Column `link`.\n## ℹ Only values of size one are recycled.\n\nUnfortunately, that doesn’t work either! One of the battles doesn’t have a unique page, so there’s a row mismatch. We need to process each row separately and then rbind them together, or figure out how to fix the fact that if an element doesn’t have a child element a, then it won’t have a href attribute, either, and yet we don’t want to lose track of the spot where there isn’t a corresponding link.\nLet’s fix the missing-child issue first, and then I will demonstrate the other approach. In this case, they’re approximately the same level of difficulty, but often one approach is easier.\n\nsafe_href &lt;- function(i) {\n  # Safely get a child link href from node i, returning NA if no such link exists\n1  j &lt;- html_element(i, \"a\")\n2  if(length(j) == 0){\n    return(NA)\n  } else {\n3    res &lt;- try(html_attr(j, \"href\"))\n    if (\"try-error\" %in% class(res)) return(NA) else return(res)\n  }\n}\n\nrev_war_battles2 &lt;- tibble(\n  name = html_elements(rev_war_rows, \"td:first-child\") |&gt; html_text(),\n  link = html_elements(rev_war_rows, \"td:first-child\") |&gt; map_chr(safe_href),\n  date = html_elements(rev_war_rows, \"td:nth-child(2)\") |&gt; html_text(),\n  location = html_elements(rev_war_rows, \"td:nth-child(3)\") |&gt; html_text()\n)\n\n\n1\n\nFirst get any child of the node that is a\n\n2\n\nCheck if any children exist - if not, return NA\n\n3\n\nTry to get the href attribute from a, and if that works, return it; if not, return NA. In reality, every a element should have a href attribute, but this is also a good example of how to ensure your code doesn’t fail unexpectedly.\n\n\n\n\nWhen writing custom functions for web scrapign, it’s usually helpful to give yourself an idea of what the function does in a comment – you can often reuse these functions, but that only works if you’re kind to future you and remind yourself what they actually do.\n\nhead(rev_war_battles2)\n## # A tibble: 6 × 4\n##   name                               link                         date  location\n##   &lt;chr&gt;                              &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;   \n## 1 &lt;NA&gt;                               &lt;NA&gt;                         &lt;NA&gt;  &lt;NA&gt;    \n## 2 Powder Alarm*                      /wiki/Powder_Alarm           Sept… Massach…\n## 3 Storming of Fort William and Mary* /wiki/Capture_of_Fort_Willi… Dece… New Ham…\n## 4 Battles of Lexington and Concord   /wiki/Battles_of_Lexington_… Apri… Massach…\n## 5 Battle of Meriam's Corner          /wiki/Battle_of_Meriam%27s_… Apri… Massach…\n## 6 Siege of Boston                    /wiki/Siege_of_Boston        Apri… Massach…\n\n\nlibrary(purrr)\n\n1process_battle_row &lt;- function(node){\n2  link = html_element(node, \"td:first-child a\") |&gt; html_attr(\"href\")\n  tibble(\n    name = html_element(node, \"td:first-child\") |&gt; html_text(),\n3    link = if_else(length(link) == 0, NA, link),\n    date = html_element(node, \"td:nth-child(2)\") |&gt; html_text(),\n    location = html_element(node, \"td:nth-child(3)\") |&gt; html_text()\n  )\n}\n\n4rev_war_battles3 &lt;- map(rev_war_rows, process_battle_row) |&gt; bind_rows()\n\n\n1\n\nThe function is called on a single row of the table, so it’s much harder to “lose” a value.\n\n2\n\nFirst, get all first td elements with a children, and get the href attribute from those children. This will be NULL or c() (a list of length 0) if there is no a child of the first td element in the node.\n\n3\n\nThe biggest modification here is to use if_else() to handle the missing values and replace them with NA instead.\n\n4\n\nCall process_battle_row on each row and bind the rows together.\n\n\n\n\n\nhead(rev_war_battles3)\n## # A tibble: 6 × 4\n##   name                               link                         date  location\n##   &lt;chr&gt;                              &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;   \n## 1 &lt;NA&gt;                               &lt;NA&gt;                         &lt;NA&gt;  &lt;NA&gt;    \n## 2 Powder Alarm*                      /wiki/Powder_Alarm           Sept… Massach…\n## 3 Storming of Fort William and Mary* /wiki/Capture_of_Fort_Willi… Dece… New Ham…\n## 4 Battles of Lexington and Concord   /wiki/Battles_of_Lexington_… Apri… Massach…\n## 5 Battle of Meriam's Corner          /w/index.php?title=Battle_o… Apri… Massach…\n## 6 Siege of Boston                    /wiki/Siege_of_Boston        Apri… Massach…\n\n\n\n\nfrom bs4 import BeautifulSoup, SoupStrainer\nimport urllib.request\n\nheader={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0'}\nurl = \"https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles\"\nreq = urllib.request.Request(url, headers=header)\n\npage_bytearray = urllib.request.urlopen(req)\npage = page_bytearray.read()\npage_bytearray.close()\n\nOur operational node here will be each row of the table. To keep the data together (in case, for instance, some battle doesn’t have its own page, and thus doesn’t have a link), we need to get a node list of the rows first, and then use selectors on that list to get our data in a consistent order.\n\n1soup = BeautifulSoup(page)\n\n2rev_war_rows = soup.select(\"table.sortable tr\")\n\n3def getText(x):\n  try:\n    return x.text\n  except:\n    return \"\"\n\n4def getLink(x):\n  try:\n    return x[0].find_all(\"a\")[0].attrs[\"href\"]\n  except:\n    return pd.NA\n\n5battleCol = [i.select(\"td:first-child\") for i in rev_war_rows][1:]\ndateCol = [i.select_one(\":nth-child(2)\") for i in rev_war_rows][1:]\nlocationCol = [i.select_one(\":nth-child(3)\") for i in rev_war_rows][1:]\n\n6name = [getText(i) for i in battleCol]\nlink = [getLink(i) for i in battleCol]\ndate = [getText(i) for i in dateCol]\nlocation = [getText(i) for i in locationCol]\n\n7rev_war_battles = pd.DataFrame({\"name\": name, \"link\": link, \"date\": date, \"location\": location})\n\n\n1\n\nRead the HTML for the full page\n\n2\n\nSelect the rows of the table that’s sortable (e.g. the main table)\n\n3\n\nDefine a helper function that will return the text of a node if there is any, and “” otherwise. This makes list comprehensions much easier to read in step 6.\n\n4\n\nDefine a helper function that will return the href attribute of the link of a child node, if one exists, and NA otherwise. This makes list comprehensions much easier to read in step 6.\n\n5\n\nSelect the columns of interest - battle, date, and location. Battle will create two columns in our result - one for the name of the battle, one for the link.\n\n6\n\nGet the actual columns we intend to have in our data frame, using the helper functions created in steps 3-4.\n\n7\n\nCreate a DataFrame by creating a dict of lists and then converting it.\n\n\n\n\n\n\n\n\nNow that we have links to the individual pages (where they exist), we can supplement the data in the original table with data from the more detailed battle-level pages. One way to display this data might be to show an animated map that shows simultaneous actions in different locations. To do this, we need latitude and longitude that are at least approximate (and ideally, at a higher resolution than state-level). If we click on the Powder Alarm battle page, we can see a structured table on the right that contains the location of the battle and links to Suffolk County, Massachusetts. The page for Suffolk County, MA has a latitude and longitude.\n\n\n\n\n\n\n\n\n(Click to enlarge) Each battle has a formatted table on the right with a sub-table that has a location field.\n\n\n\n\n \n\n\n\n\n\n(Click to enlarge) Each location also has a formatted table on the right. GPS location is located within a span element with class geo-dms.\n\n\n\n\n\n\nFigure 31.6: Two steps are necessary in order to get geographic information from the information provided in the table of revolutionary war battles.\n\n\n\nSo, our function needs to:\n\nIdentify the structured table on the battle page\nFind the location row in that table\nLook for a link in the location row and follow it, if it exists, to the linked location page\nLook for a latitude/longitude entry in the location link page\n\nIf the structured table doesn’t exist, the location row doesn’t exist, or there is no location link, the function should return a data frame row with lat=NA, long=NA.\n\nRPython\n\n\n\nlibrary(tidyr)\nlibrary(tibble)\n\nget_battle_location &lt;- function(battle_link) {\n  if(is.na(battle_link)) return(tibble(latitude = NA, longitude = NA, battle_link = NA))\n  battle_url &lt;- paste0(\"https://en.wikipedia.org/\", battle_link)\n  battle_doc &lt;- try(read_html(battle_url))\n  if(\"try-error\" %in% class(battle_doc)) {\n    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = NA))\n  }\n  \n  # Get first (hopefully most specific) location link\n  location_url &lt;- html_node(battle_doc, \".infobox .location a\") |&gt; html_attr(\"href\")\n  if(is.na(location_url)) {\n    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = NA))\n  }\n  location_doc &lt;- read_html(paste0(\"https://en.wikipedia.org/\", location_url))\n  if(\"try-error\" %in% class(location_doc)) {\n    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = location_url))\n  }\n  \n  # Sleep for one second to limit load on servers - \n  # this limits us to at most 2 requests per second, \n  # well below Wikipedia's 20rps limit\n  Sys.sleep(1)\n  \n  # Get location coords\n  location_node &lt;- html_node(location_doc, \".geo-dms\")\n  tibble(\n    type = location_node |&gt; html_children() |&gt; html_attr(\"class\"),\n    value = location_node |&gt; html_children() |&gt; html_text()\n  ) |&gt; pivot_wider(names_from = \"type\") |&gt;\n    mutate(battle_link = battle_link, location_link = location_url) # makes merging easier\n}\n\nbattle_locations &lt;- purrr::map(rev_war_battles2$link, get_battle_location)\nbattle_locations &lt;- bind_rows(battle_locations)\n\n\nbattle_locations &lt;- battle_locations |&gt; unique() # filter out NA values that don't have any links at all\n\nrev_war_battles3 &lt;- left_join(rev_war_battles2, battle_locations, by = c(\"link\" = \"battle_link\"))\nhead(rev_war_battles3)\n## # A tibble: 6 × 7\n##   name                     link  date  location latitude longitude location_link\n##   &lt;chr&gt;                    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;        \n## 1 &lt;NA&gt;                     &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;      &lt;NA&gt;         \n## 2 Powder Alarm*            /wik… Sept… Massach… 42°21′3… 71°03′28… /wiki/Suffol…\n## 3 Storming of Fort Willia… /wik… Dece… New Ham… 43°04′2… 70°42′58… /wiki/New_Ca…\n## 4 Battles of Lexington an… /wik… Apri… Massach… 42°29′N  71°23′W   /wiki/Middle…\n## 5 Battle of Meriam's Corn… /wik… Apri… Massach… 42°27′3… 71°19′27… /wiki/Meriam…\n## 6 Siege of Boston          /wik… Apri… Massach… 42°21′3… 71°3′28″W /wiki/Boston\n\n\n\n\nfrom bs4 import BeautifulSoup,SoupStrainer\nimport pandas as pd\nimport urllib.request\n1import time\n\n\n2def req_page(url):\n  req = urllib.request.Request(url)\n  page_bytearray = urllib.request.urlopen(req)\n  page = page_bytearray.read()\n  page_bytearray.close()\n  return page\n\n\n3def get_battle_location(battle_link):\n\n4  try:\n    page=req_page(\"https://en.wikipedia.org\" + battle_link)\n    soup = BeautifulSoup(page)\n    location = soup.select_one(\".infobox .location a\")\n    if len(location) &gt; 0:\n      location_link = location.attrs['href']\n    else:\n      raise Exception(\"No location links found\")\n  except:\n    return {\"latitude\":pd.NA, \"longitude\":pd.NA,\n            \"battle_link\":battle_link, \"location_link\":pd.NA}\n  \n5  try:\n    time.sleep(1)\n    page2 = req_page(\"https://en.wikipedia.org\" + location_link)\n    soup2 = BeautifulSoup(page2)\n    location_node = soup2.select_one(\".geo-dms\").find_all(\"span\")\n    if len(location_node) == 2:\n      coords = [i.string for i in location_node]\n    else:\n      raise Exception(\"Insufficient geo-dms coordinates found\")\n  except:\n    return {\"latitude\":pd.NA, \"longitude\":pd.NA,\n            \"battle_link\":battle_link, \"location_link\":location_link}\n    \n6  return {\"latitude\":coords[0], \"longitude\":coords[1],\n          \"battle_link\":battle_link, \"location_link\":location_link}\n\n7battle_locations = [get_battle_location(i) for i in rev_war_battles.link]\nbattle_locations = pd.DataFrame(battle_locations)\n\n\n1\n\nUse the time library to sleep for 1 second before making a 2nd request within the same function.\n\n2\n\nWe’ll be making multiple requests in this function, so streamline it so that we don’t have to type 4 lines each time by making a short custom function.\n\n3\n\nDefine a function that takes the local link to the battle page and (1) gets the most specific location link provided for the battle, and (2) follows the link to the location page to get the latitude and longitude of that location. Return a dict containing lat/long, battle_link, and location_link.\n\n4\n\nUse a try/except statement to handle any errors that arise when a link doesn’t resolve or a specific element isn’t found. Get the battle page and select the first location infobox link, finding the href attribute within the link if there is a location (if not, raise an error that should trigger the except part of the statement). If anything fails, return a dict that has NAs for everything but the battle link.\n\n5\n\nUse a try/except statement to get the location page (after sleeping for 1 sec). Look for a .geo-dms node and get its children. If there are exactly two children (lat/long), then extract the coords from the text of the element. Otherwise, raise an exception and return a dict with the battle and location link, but with NA values for latitude and longitude.\n\n6\n\nIf everything works, then return a dict with all of the relevant information.\n\n7\n\nCall the function for every link in the rev_war_battles object, and then convert the result (a list of dicts) into a DataFrame.\n\n\n\n\n\nrev_war_battles_full = rev_war_battles.join(battle_locations.loc[:,[\"latitude\", \"longitude\", \"location_link\"]])\n\nWe can use the .join method because both DataFrames have the same index; thus, we don’t have to do a merge operation like we did in R.\n\n\n\n\nA spatial package could be used to transform the latitude and longitude into numerical values for plotting, but we will stop here, as this example has demonstrated the utility of using custom functions to extract components of a webpage without cataloging all of the information.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#scraping-dynamic-webpages",
    "href": "part-advanced-topics/03-web-scraping.html#scraping-dynamic-webpages",
    "title": "31  Web Scraping",
    "section": "31.6 Scraping Dynamic Webpages",
    "text": "31.6 Scraping Dynamic Webpages\nMany modern webpages load page content dynamically using JavaScript. This can have some advantages - for instance, the page loads quickly, and as you scroll, additional content is added, saving the cost of transferring additional data if you decide not to read the full page. However, it’s very annoying when your goal is to scrape content from the page, because tools like rvest and BeautifulSoup only get the initial HTML, not the HTML after JavaScript has modified the page.\nSometimes, there are clever ways around this problem - you may be able to submit a second request to the page and get a response with the content you want. But more often, the best solution is just to emulate a browser and read the dynamic content out of the browser once JavaScript has changed the page.\n\n\n\n\n\n\nDemo: Roller Derby Skater Names\n\n\n\nRoller Derby is a team sport played on a roller skating rink, and most leagues seem to be female-dominated. The Junior Roller Derby Association (JRDA) web page contains a list of US leagues, and each league contains a list of registered organizations. Organizations can have multiple teams that are either open (anyone can join) or female-only, and each team has a roster on the organization page. Suppose that our goal is to examine the names that players register with (examples: “Hitty Stardust”, “Collide-a-scope”, “Malice in Wonderland”, “The Curly Dervish”, “Rainbow Bite”) that are often puns with slight hints of violence, gendered humor, or personal characteristics. In addition to players, we want to keep the league, organization, team, and team type (open/female-only).\nIn order to scrape this data, we have to consider multiple levels because of the hierarchical nature of both the website and the organization. In these cases, it is often useful to write separate functions which scrape each level of the hierarchy, and then write different functions to clean different components of the data. This provides modularity, but it also keeps your code from getting too long and complex for you to keep track of how each job is getting done.\n\n\n\n\n\n\n\n\n\nRoller Derby leagues - 5 tabs, and each tab has a list of organizations. Organization “blocks” of information are contained in divs that have class textBlockElement – hopefully, that’s enough of a selector to get the information.\n\n\n\n\n\n\n\nRoller Derby organizations are specified within the tab content div. There are several divs, only one of which is displayed (the others correspond to tabs which aren’t selected). Organization “blocks” of information are contained in divs that have class textBlockElement – hopefully, that’s enough of a selector to get the information.\n\n\n\n\n\nWhat’s interesting is that the first time you click on a new tab, you see it “fill” with data. That is, the data isn’t present when the page is loaded. This page is a dynamic webpage – it is loading the data with scripts, and thus has to be treated more carefully.\nWe can do this in one of two ways\n\nMirror the entire site using a command-line tool like wget, and then trawl through the downloaded information to see if there’s a spreadsheet, table, CSV, or a bunch of HTML files that have the data present.\nLaunch a remote-controlled browser, click on things using commands, and then read the HTML after the script has run.\n\nThe first approach is usually faster, though many sites will block wget using robots.txt5.\nThe second approach tends to be more successful assuming you can get the infrastructure set up and configured (this has gotten much easier recently, at least in R), and has the bonus effect of making you feel like a very powerful programmer (or mad scientist) as you sit back and watch your computer navigate the web according to your instructions, if you decide to launch the browser window to see the magic happen.\nYou will need to install the chromote package in R and the selenium package in python.\n\nRPython\n\n\n\nlibrary(rvest)\n1library(chromote)\n\nlibrary(xml2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\n\nurl &lt;- \"https://www.juniorrollerderby.org/us-leagues\"\n\n2rmt &lt;- read_html_live(url)\n3rmt$click(\".pageElement .contentTabs li:first-child\")\nrmt$click(\".pageElement .contentTabs li:nth-child(2)\")\nrmt$click(\".pageElement .contentTabs li:nth-child(3)\")\nrmt$click(\".pageElement .contentTabs li:nth-child(4)\")\nrmt$click(\".pageElement .contentTabs li:last-child\")\n\n4# rmt$view()\n\n5teams &lt;- rmt |&gt;\n  html_elements(\".tabContainer .pageEl .textBlockElement\")\n\nis_team_node &lt;- map_int(teams, ~length(xml_find_first(., \"div\"))) &gt; 0\nteams &lt;- teams[is_team_node]\n\n6rmt$close()\n\n\n1\n\nThe chromote package allows you to remote control a browser, though by default it will be headless - that is, you won’t be able to see it.\n\n2\n\nrvest includes a read_html_live() function that implicitly sets up a chromote session. the rmt object here is the browser – you can interact with it using a bunch of different methods, usually called with $&lt;function&gt;. rmt is a special type of object called a class, and works very similarly to many python objects, except that instead of using .. to access the class methods, you will use $.\n\n3\n\nWe remotely click on each of the 5 regional tabs in order to load all of the content for each region\n\n4\n\nThis line is commented out because it doesn’t work when the book is compiled, but it will work interactively if you uncomment it – it allows you to see the browser that you’re remote controlling.\n\n5\n\nYou can interact with rmt using other rvest functions just like you would when using read_html, looking for elements and extracting content.\n\n6\n\nIt’s good practice to close the session when you’re done with it rather than leaving it open. This saves computer memory, but also doesn’t keep a connection to the remote server alive, which is polite.\n\n\n\n\nOnce we’ve extracted the content from the main page, we won’t necessarily need anything else read “live” - it depends on how the content on the team pages is loaded. Our next task is to write a function to extract the info from the main page.\n\nget_team_summary &lt;- function(node) {\n  team_img &lt;- html_node(node, \".leftTextImage img\") |&gt; html_attr(\"src\")\n  team_link &lt;- html_node(node, \".text p:nth-child(2) a\") |&gt; html_attr(\"href\")\n  team_name &lt;- html_node(node, \".text p:nth-child(2) a\") |&gt; html_text()\n  team_location &lt;- html_node(node, \".text p:nth-child(3)\") |&gt; html_text()\n  \n  tibble(name = team_name, link = team_link, img = team_img, location = team_location)\n}\n\nteam_df &lt;- map(teams, get_team_summary) |&gt; bind_rows()\n## Error in `map()` at rlang/R/dots.R:91:3:\n## ℹ In index: 1.\n## Caused by error in `xml_find_first.xml_node()`:\n## ! external pointer is not valid\n\n\n\n\nimport pandas as pd\nfrom io import StringIO\nfrom bs4 import BeautifulSoup,SoupStrainer\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\nfrom selenium.webdriver.common.by import By\nimport time # to ensure page loads before we get source\n1options = FirefoxOptions()\noptions.add_argument(\"-headless\")\nbrowser = webdriver.Firefox(options = options)\n\n2browser.get(\"https://www.juniorrollerderby.org/us-leagues\")\ntime.sleep(2)\n\n3tab = browser.find_element(By.CSS_SELECTOR, \".pageElement .contentTabs li:first-child\")\ntab.click()\ntab = browser.find_element(By.CSS_SELECTOR, \".pageElement .contentTabs li:nth-child(2)\")\ntab.click()\ntab = browser.find_element(By.CSS_SELECTOR, \".pageElement .contentTabs li:nth-child(3)\")\ntab.click()\ntab = browser.find_element(By.CSS_SELECTOR, \".pageElement .contentTabs li:nth-child(4)\")\ntab.click()\ntab = browser.find_element(By.CSS_SELECTOR, \".pageElement .contentTabs li:last-child\")\ntab.click()\n\n4soup = BeautifulSoup(StringIO(browser.page_source))\n\n5browser.close()\n\n6teams = soup.select(\".tabContainer .pageEl .textBlockElement\")\n7teams = [i for i in teams if len(i.find_all('div')) &gt; 0]\n\n\n1\n\nOpen a Firefox window (you could also use webdriver.Chrome() to get Chrome/Chromium if you prefer). The options() statements ensure this driver is “headless” - that is, it doesn’t actually pop open a window (this allows it to run on a server without a graphical display, like the one that builds the book automatically). You can just replace all three lines with browser = webdriver.Firefox() if you want to see the website.\n\n2\n\nOpen the target URL and sleep 2 seconds to let the page load\n\n3\n\nFind each tab and click on it.\n\n4\n\nGet the HTML code from the browser after JavaScript has had fun with it (browser.page_source), and tell BeautifulSoup it’s HTML in string form rather than a request format (StringIO).\n\n5\n\nClose the browser - we don’t need it now that we’ve gotten the HTML.\n\n6\n\nSelect the team div nodes (this selector will include some headings, like “National” and “Regional”)\n\n7\n\nRemove the extra headings that don’t have div children.\n\n\n\n\n\ndef get_team_summary(node):\n  list(list(node.children)[1].children)[1].attrs['src']\n\n\n\n\n\nNext, we need to figure out how to get the team roster information from the individual team pages. A look at the HTML for the Renegades team shows this interesting node:\n\n&lt;iframe src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSZPNZu2hhBDjnxWH58quhER6anponm1YCl512tHLTcURYUDzCXWqT7sNy7S_wcDKy8Wnoj4VNaAYyh/pubhtml?gid=1152027622&amp;single=true&amp;widget=false&amp;headers=false&amp;chrome=false&amp;range=a1:i25\" width=\"100%\" height=\"600\"&gt;&lt;/iframe&gt;\n\nAn iframe allows you to include another web page inside the current webpage. In this case, we can see that this iframe is showing us a google sheet, rendered as a table. In theory, we could use the googlesheets4 package to read in the sheet, but html_table should work just as well.\nIt can be helpful to know a bit about how URLs are constructed in order to simplify this URL a bit. Additional key-value information is often added to the end of a URL, separated from the main part of the URL by a ?. So, all of the information after the question mark in the iframe src attribute is not necessary for our purposes.\n\nRPython\n\n\n\nplayers &lt;- read_html(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSZPNZu2hhBDjnxWH58quhER6anponm1YCl512tHLTcURYUDzCXWqT7sNy7S_wcDKy8Wnoj4VNaAYyh/pubhtml\") |&gt;\nhtml_table()\n\n\nlength(players)\n## [1] 64\nplayers[[2]]\n## # A tibble: 48 × 9\n##       `` ``       `` ``                            ``    ``    ``    ``    ``   \n##    &lt;int&gt; &lt;lgl&gt; &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;\n##  1     1 NA       NA \"\"                            \"\"    \"\"    NA    \"\"    \"\"   \n##  2     2 NA       NA \"CURRENT ACTIVE CHARTER ROST… \"CUR… \"CUR… NA    \"LEA… \"Ad …\n##  3     3 NA       NA \"CURRENT ACTIVE CHARTER ROST… \"CUR… \"CUR… NA    \"TEA… \"Ad …\n##  4     4 NA       NA \"Skater Number\"               \"Ska… \"Act… NA    \"DIV… \"FEM…\n##  5     5 NA        1 \"011\"                         \"Wic… \"4/1… NA    \"\"    \"\"   \n##  6     6 NA        2 \"02\"                          \"Ken… \"4/1… NA    \"UNI… \"WHI…\n##  7     7 NA        3 \"09\"                          \"Sas… \"4/1… NA    \"\"    \"BLU…\n##  8     8 NA        4 \"092\"                         \"Poc… \"4/1… NA    \"\"    \"\"   \n##  9     9 NA        5 \"101\"                         \"Slu… \"4/1… NA    \"COA… \"COA…\n## 10    10 NA        6 \"112\"                         \"Gna… \"4/1… NA    \"Leg… \"Der…\n## # ℹ 38 more rows\n\n\n\n\nplayers = pd.read_html(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSZPNZu2hhBDjnxWH58quhER6anponm1YCl512tHLTcURYUDzCXWqT7sNy7S_wcDKy8Wnoj4VNaAYyh/pubhtml\")\n## ValueError: No tables found\n\n\nlen(players)\n## 64\nplayers[1]\n##     Unnamed: 0  ...                    Unnamed: 8\n## 0            1  ...                           NaN\n## 1            2  ...  Ad Astra Junior Roller Derby\n## 2            3  ...                  Ad Astra (F)\n## 3            4  ...                        FEMALE\n## 4            5  ...                           NaN\n## 5            6  ...                         WHITE\n## 6            7  ...                          BLUE\n## 7            8  ...                           NaN\n## 8            9  ...                COACHING STAFF\n## 9           10  ...                    Derby Name\n## 10          11  ...                           NaN\n## 11          12  ...                           NaN\n## 12          13  ...                           NaN\n## 13          14  ...                           NaN\n## 14          15  ...                           NaN\n## 15          16  ...                           NaN\n## 16          17  ...                           NaN\n## 17          18  ...                           NaN\n## 18          19  ...                           NaN\n## 19          20  ...                           NaN\n## 20          21  ...                           NaN\n## 21          22  ...                           NaN\n## 22          23  ...                           NaN\n## 23          24  ...                           NaN\n## 24          25  ...                           NaN\n## 25          26  ...  Ad Astra Junior Roller Derby\n## 26          27  ...                  Ad Astra (O)\n## 27          28  ...                          OPEN\n## 28          29  ...                           NaN\n## 29          30  ...                         WHITE\n## 30          31  ...                          BLUE\n## 31          32  ...                           NaN\n## 32          33  ...                           NaN\n## 33          34  ...                           NaN\n## 34          35  ...                           NaN\n## 35          36  ...                           NaN\n## 36          37  ...                           NaN\n## 37          38  ...                           NaN\n## 38          39  ...                           NaN\n## 39          40  ...                           NaN\n## 40          41  ...                           NaN\n## 41          42  ...                           NaN\n## 42          43  ...                           NaN\n## 43          44  ...                           NaN\n## 44          45  ...                           NaN\n## 45          46  ...                           NaN\n## 46          47  ...                           NaN\n## 47          48  ...                           NaN\n## \n## [48 rows x 9 columns]\n\n\n\n\nInteresting - we can get all of the player names for all teams from the single sheet! Some organizations have multiple teams, but they all use the same basic structure for data - there are 23 rows of structured information, and there are blank lines at the top and bottom of each team.\nSo, to clean this up, we have to (roughly):\n\nIdentify rows that are either primarily blank or primarily NA\nRemove any empty columns that aren’t particularly needed\nFind “runs” of the same value indicating that a row is empty-ish\nCompute start/end of run sequences, assuming that the run is longer than 1 value\nGet only data frame rows corresponding to the run, separating the organization tables into separate teams.\n\nNote that for organizations with multiple teams, there are usually only one set of coaches, so by separating this information out we (somewhat) lose the ability to keep coach information with the team. On the other hand, coaches weren’t on our list of critical information.\n\nRPython\n\n\n\nteam_data &lt;- map(players, split_team_df) |&gt; \n  bind_rows() |&gt;\n  mutate(nrow = end-start) |&gt;\n  filter(nrow &gt; 15) |&gt;\n  magrittr::extract2(\"data\")\n\nteam_data[[1]]\n## # A tibble: 19 × 7\n##       ``    `` ``                            ``                ``    ``    ``   \n##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n##  1     2    NA CURRENT ACTIVE CHARTER ROSTER CURRENT ACTIVE C… CURR… \"LEA… \"509…\n##  2     3    NA CURRENT ACTIVE CHARTER ROSTER CURRENT ACTIVE C… CURR… \"TEA… \"The…\n##  3     4    NA Skater Number                 Skater Name       Acti… \"DIV… \"OPE…\n##  4     5     1 03                            Rumble Guppy      1/14… \"\"    \"\"   \n##  5     6     2 10                            Ruby Roundhouse   1/10… \"UNI… \"WHI…\n##  6     7     3 106                           Ferdinand         1/13… \"\"    \"BLA…\n##  7     8     4 187                           Belle of the Bra… 1/10… \"\"    \"\"   \n##  8     9     5 22                            Badadytude        1/10… \"COA… \"COA…\n##  9    10     6 236                           Hell-no Kitty     1/13… \"Leg… \"Der…\n## 10    11     7 247                           Snot Rocket       1/13… \"Abb… \"\"   \n## 11    12     8 3                             Chicken Nugget    PEND… \"Vir… \"Cle…\n## 12    13     9 312                           Slimy Booger      1/14… \"\"    \"\"   \n## 13    14    10 34                            The Postman       1/10… \"\"    \"\"   \n## 14    15    11 404                           Fatal Error       1/10… \"\"    \"\"   \n## 15    16    12 68                            Graycon The Dest… 1/10… \"\"    \"\"   \n## 16    17    13 710                           Boo Boo           1/10… \"\"    \"\"   \n## 17    18    14 77                            Hurricane         1/10… \"\"    \"\"   \n## 18    19    15 82                            Blackout          1/10… \"\"    \"\"   \n## 19    20    16 911                           Chaos             1/18… \"\"    \"\"\n\n\nsplit_team_df &lt;- function(df) {\n  is_emptyish &lt;- rowSums(is.na(df) | df == \"\") &gt; 4\n  \n  emptycols &lt;- which(colSums(is.na(df))==nrow(df))\n  df2 &lt;- df[,-emptycols]\n  \n  run_stats &lt;- rle(is_emptyish)\n  run_stats$end &lt;- cumsum(run_stats$lengths)\n  run_stats$start &lt;- c(1, lag(run_stats$end)[-1] + 1)\n  team_dfs &lt;- tibble(start = run_stats$start, end = run_stats$end, value = run_stats$values) |&gt;\n    filter(!value) |&gt;\n    mutate(data = map2(start, end, ~df2[.x:.y,]))\n}\n\n\n\n\nimport numpy as np\n\ndef split_team_df(df):\n  is_emptyish = pd.isna(df).sum(axis = 1) &gt; 4\n  fullcols = pd.isna(df).sum(axis = 0)!=df.shape[0]\n  \n  # https://stackoverflow.com/questions/66633109/aggregating-row-repeats-in-pandas-run-lengths\n  run_vals = (is_emptyish != is_emptyish.shift()).cumsum()\n  run_vals_tab = run_vals.value_counts()\n  \n  run_vals_full_idx = run_vals[~is_emptyish].value_counts().index\n  run_vals_idx = run_vals_tab.index[run_vals_tab!=1]\n  data_idx = np.intersect1d(run_vals_full_idx.values, run_vals_idx.values)\n  return [df.loc[run_vals==i, fullcols] for i in data_idx]\n\nfrom itertools import chain\n# https://stackoverflow.com/questions/11860476/how-to-unnest-a-nested-list\nteam_data = [split_team_df(i) for i in players]\nteam_data = list(chain(*team_data))\n\nteam_data[0]\n##     Unnamed: 0  Unnamed: 2  ...        Unnamed: 7               Unnamed: 8\n## 1            2         NaN  ...       LEAGUE NAME  509 Junior Roller Derby\n## 2            3         NaN  ...         TEAM NAME      The Killer Bees (O)\n## 3            4         NaN  ...          DIVISION                     OPEN\n## 4            5         1.0  ...               NaN                      NaN\n## 5            6         2.0  ...    UNIFORM COLORS                    WHITE\n## 6            7         3.0  ...               NaN                    BLACK\n## 7            8         4.0  ...               NaN                      NaN\n## 8            9         5.0  ...    COACHING STAFF           COACHING STAFF\n## 9           10         6.0  ...        Legal Name               Derby Name\n## 10          11         7.0  ...      Abby Damerow                      NaN\n## 11          12         8.0  ...  Virginia Thommes             Clean Willie\n## 12          13         9.0  ...               NaN                      NaN\n## 13          14        10.0  ...               NaN                      NaN\n## 14          15        11.0  ...               NaN                      NaN\n## 15          16        12.0  ...               NaN                      NaN\n## 16          17        13.0  ...               NaN                      NaN\n## 17          18        14.0  ...               NaN                      NaN\n## 18          19        15.0  ...               NaN                      NaN\n## 19          20        16.0  ...               NaN                      NaN\n## \n## [19 rows x 7 columns]\n\n\n\n\nThen, we need to write another set of functions to clean up the multiple tables in the spreadsheet so that they’re actually tidy data instead of a set of tables that are laid out in a format that is definitely not tidy. In this case, it seemed easier to handle the player data and then the team data with separate functions, then combine that information.\n\n\n\n\n\n\nFigure 31.7: Sketch of which components of the spreadsheet need to go where.\n\n\n\n\nRPython\n\n\n\nget_players &lt;- function(sub_df) {\n  # Identify row where player table starts\n  header_row &lt;- which(sub_df[,1] == \"Skater Number\")\n  \n  if (length(header_row) == 0) return(NA)\n  \n  # Get player info\n  player_info &lt;- sub_df[(header_row+1):(header_row+21),] |&gt;\n    set_names(c(\"Skater_Number\", \"Skater_Name\", \"Skater_Active_Date\")) |&gt;\n    na.omit()\n  \n  player_info\n}\n\nget_team_info &lt;- function(sub_df) {\n  leagueidx &lt;- sub_df[,1] == \"LEAGUE NAME\"\n  league &lt;- if_else(length(leagueidx) == 0, NA, unlist(sub_df[leagueidx,2]))\n  \n  teamidx &lt;- sub_df[,1] == \"TEAM NAME\"\n  team &lt;- if_else(length(teamidx) == 0, NA, unlist(sub_df[teamidx,2]))\n  \n  divisionidx &lt;- sub_df[,1] == \"DIVISION\"\n  division &lt;- if_else(length(divisionidx) == 0, NA, unlist(sub_df[divisionidx,2]))\n  \n  coloridx &lt;- which(sub_df[,1] == \"UNIFORM COLORS\")\n  if (length(coloridx) &gt; 0) {\n    colors &lt;- sub_df[coloridx:(coloridx+1),1:2] |&gt;\n      unlist()\n    colors &lt;- colors[colors != \"UNIFORM COLORS\"]\n    colors &lt;- colors[colors != \"\"]\n    colors &lt;- paste(na.omit(colors), collapse = \",\")\n  } else {\n    colors &lt;- NA\n  }\n  \n  tibble(league = league, team = team, division = division, colors = colors)\n}\n\nclean_team_df &lt;- function(df) {\n  if(nrow(df) &lt;= 10) {\n    warning(\"Insufficient number of rows, returning NULL\")\n    return(NULL)\n  }\n  if(sum(df == \"\", na.rm = T) &gt; 50) {\n    warning(\"Insufficient number of rows, returning NULL\")\n    return(NULL)\n  }\n  \n  players &lt;- get_players(df[,3:5])\n  \n  team_info &lt;- get_team_info(df[,6:7]) |&gt;\n    mutate(players = list(players))\n  return(team_info)\n}\n\nAfter all of that, we’re finally ready to use our cleaning functions!\n\nteam_data_clean &lt;- map(team_data, clean_team_df) |&gt; \n  bind_rows()\n\n\n\n\nfrom warnings import warn\n\ndef get_players(sub_df):\n  sub_df = sub_df.reset_index().drop(\"index\", axis = 1)\n  x = sub_df.iloc[:,0]==\"Skater Number\"\n  header_row = x.index[x][0]\n  \n  if pd.isnull(header_row):\n    return pd.NA\n  \n  stopidx = min(header_row+21, sub_df.shape[0])\n  playerdf = sub_df.iloc[(header_row+1):(stopidx+1),:].reset_index().drop(\"index\", axis = 1)\n  playerdf.columns = [\"skater_number\", \"skater_name\", \"skater_active_date\"]\n  return {\"players\": playerdf.dropna()}\n\ndef get_if_exists(idx, sub_df_col, replace=pd.NA):\n  header_row = idx.index[idx][0]\n  if not pd.isnull(header_row):\n    return sub_df_col[header_row]\n  return replace\n\ndef get_team_info(sub_df):\n  league = get_if_exists(sub_df.iloc[:,0]==\"LEAGUE NAME\", sub_df.iloc[:,1])\n  team = get_if_exists(sub_df.iloc[:,0]==\"TEAM NAME\", sub_df.iloc[:,1])\n  division = get_if_exists(sub_df.iloc[:,0]==\"DIVISION\", sub_df.iloc[:,1])\n  \n  return {\"league\": league, \"team\": team, \"division\": division}\n\ndef clean_team_df(df):\n  if (df.shape[0] &lt; 8):\n    warn(\"fewer than 8 rows in data frame, skipping\")\n    return {}\n  players = get_players(df.iloc[:,2:5])\n  team_info = get_team_info(df.iloc[:,5:7])\n  team_info = [team_info | players]\n  return team_info\n\n\nteam_data_clean = [clean_team_df(i) for i in team_data] \nlen(team_data_clean)\n## 99\nteam_data_clean[0]\n## [{'league': '509 Junior Roller Derby', 'team': 'The Killer Bees (O)', 'division': 'OPEN', 'players':    skater_number            skater_name skater_active_date\n## 0             03           Rumble Guppy            1/14/25\n## 1             10        Ruby Roundhouse            1/10/25\n## 2            106              Ferdinand            1/13/25\n## 3            187     Belle of the Brawl            1/10/25\n## 4             22             Badadytude            1/10/25\n## 5            236          Hell-no Kitty            1/13/25\n## 6            247            Snot Rocket            1/13/25\n## 7              3         Chicken Nugget            PENDING\n## 8            312           Slimy Booger            1/14/25\n## 9             34            The Postman            1/10/25\n## 10           404            Fatal Error            1/10/25\n## 11            68  Graycon The Destroyer            1/10/25\n## 12           710                Boo Boo            1/10/25\n## 13            77              Hurricane            1/10/25\n## 14            82               Blackout            1/10/25\n## 15           911                  Chaos            1/18/25}]\n\n\n\n\nLet’s see if there are any common skater names, after all of that hard work.\n\nRPython\n\n\n\nteam_data_clean |&gt;\n  unnest(players) |&gt;\n  group_by(Skater_Name) |&gt;\n  count() |&gt;\n  arrange(desc(n)) |&gt;\n  head(10)\n## # A tibble: 10 × 2\n## # Groups:   Skater_Name [10]\n##    Skater_Name          n\n##    &lt;chr&gt;            &lt;int&gt;\n##  1 Cherry Bomb         13\n##  2 Killer Queen         6\n##  3 Lunatic              6\n##  4 Stitches             6\n##  5 Ghost                5\n##  6 Jawbreaker           5\n##  7 Sugar Rush           5\n##  8 Frightening Bolt     4\n##  9 Gnarly Quinn         4\n## 10 Hurricane            4\n\n\n\n\nplayer_names = pd.concat([i[0]['players'] for i in team_data_clean if len(i) &gt; 0])\nplayer_names.skater_name.value_counts()[0:10]\n## skater_name\n## Cherry Bomb     13\n## Stitches         6\n## Lunatic          6\n## Killer Queen     6\n## Sugar Rush       5\n## Firecracker      5\n## Jawbreaker       5\n## Ghost            5\n## Hurricane        4\n## Gnarly Quinn     4\n## Name: count, dtype: int64\n\n\n\n\n\n\n\nThere are a LOT of steps when you’re cleaning data from the internet. It can be very easy to get overwhelmed. I find it’s helpful to look at the data as it exists on the web first, and imagine what form you’d like the data to be in at the end. Then, sketching out the transformations step by step, as in Figure 31.7 can help with maintaining focus on one job at a time without getting overwhelmed.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#sec-webscraping-refs",
    "href": "part-advanced-topics/03-web-scraping.html#sec-webscraping-refs",
    "title": "31  Web Scraping",
    "section": "31.7 References",
    "text": "31.7 References\n\n\n\n\n[1] Wikimedia contributors, “XML,” Wikipedia. Wikimedia Foundation, Jun. 03, 2025 [Online]. Available: https://en.wikipedia.org/w/index.php?title=XML&oldid=1293665596. [Accessed: Jun. 04, 2025]\n\n\n[2] “HTML vs XML. GeeksforGeeks,” Jan. 2024. [Online]. Available: https://www.geeksforgeeks.org/html-vs-xml/. [Accessed: Jun. 09, 2025]\n\n\n[3] J. Struck, Introduction to web scraping with r. [Online]. Available: https://sscc.wisc.edu/sscc/pubs/webscraping-r/index.html. [Accessed: Jun. 12, 2025]\n\n\n[4] D. Perepolkin, Polite: Be nice on the web. 2023 [Online]. Available: https://CRAN.R-project.org/package=polite\n\n\n[5] B. Kemp, “Respecting robots.txt in your python web crawler: What, why, and how. Text meaning,” Mar. 22, 2025. [Online]. Available: https://textmeaning.com/2025/03/22/respecting-robots-txt-in-your-python-web-crawler-what-why-and-how/. [Accessed: Jun. 12, 2025]\n\n\n[6] b4hand, E. Battaglia, D. Lecocq, and L. Reno, “Reppy: Replacement robots.txt parser.” Sep. 16, 2019 [Online]. Available: http://github.com/seomoz/reppy. [Accessed: Jun. 12, 2025]\n\n\n[7] “Ethics of robots.txt. Stack overflow,” Aug. 12, 2013. [Online]. Available: https://stackoverflow.com/q/999056. [Accessed: Jun. 12, 2025]\n\n\n[8] D. Pierce, “The rise and fall of robots.txt. The verge,” Feb. 14, 2024. [Online]. Available: https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders. [Accessed: Jun. 12, 2025]\n\n\n[9] L. Baldor, “The pentagon’s DEI purge: Officials describe a scramble to remove and then restore online content. AP news,” Mar. 22, 2025. [Online]. Available: https://apnews.com/article/pentagon-dei-diversity-social-media-purge-fb15996733408a8122a97acd3baa6820. [Accessed: Jun. 12, 2025]\n\n\n[10] T. Klosowski, “How web sites vary prices based on your information (and what you can do about it). Lifehacker,” Jan. 07, 2013. [Online]. Available: https://lifehacker.com/how-web-sites-vary-prices-based-on-your-information-an-5973689. [Accessed: Jun. 12, 2025]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/03-web-scraping.html#footnotes",
    "href": "part-advanced-topics/03-web-scraping.html#footnotes",
    "title": "31  Web Scraping",
    "section": "",
    "text": "Technically, HTML was developed first, but it does have many of the same components as XML, and we use similar terminology to describe elements, content, attributes, and values, so it’s easiest (imo) to think of HTML as a subset of XML with additional rules and with certain rules relaxed.↩︎\nThis is a real rabbit hole if you are curious about the very technical details of what makes up a character in text.↩︎\ncurl is a very low-level way to interface with the web… be glad we don’t have to use it directly.↩︎\nYou aren’t supposed to do this because it’s hard to make sense of the information with a screen reader - users who are low vision might think the information was actually related structurally when it is not and the table is just for arrangement purposes. As there are other ways to arrange content that are arguably better in other ways, tables should be reserved for situations where the content is actually structurally tabular.↩︎\nTechnically, there is a flag for wget to ignore the robots rules, but it’s probably better to use an approach that doesn’t break the implicit rules set by the server.↩︎",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html",
    "href": "part-advanced-topics/04-xml-json-list-processing.html",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "",
    "text": "Prerequisites\nChapter 31 introduces how XML and HTML documents are constructed and demonstrates different techniques for scraping data from the web.\nChapter 33 introduces Application Programming Interfaces (APIs) to get data from the web in a cleaner, more efficient way. Web-based data often uses different formats, like JSON (JavaScript Object Notation), to provide data from requests in a structured way. Before we can effectively use APIs, it helps to review some basic patterns and methods for working with record based data and converting it into the rectangular data that most statistical analyses are built around.\nThis chapter will assume that you’ve used (or at least seen) techniques like split-apply-combine or map-reduce, anonymous functions, and table joins (full, left, right, inner) and similar techniques before. Here, we will focus on how these strategies apply specifically to record-based, hierarchically formatted data that is often found in XML and JSON files.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#prerequisites",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#prerequisites",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "",
    "text": "Working knowledge of data wrangling techniques (Chapter 22, Chapter 23, Chapter 24)\nFamiliarity with table joins (Chapter 26)\nFamiliarity with functional programming (Chapter 28)\nFamiliarity with XML file structures (Chapter 31)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#objectives",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#objectives",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "Objectives",
    "text": "Objectives\n\nDifferentiate between tabular and record-based data structures\n\nDevelop strategies to transform record-based data into tabular data\nRecognize situations where multiple linked tables or nested list-columns may be required to represent the data effectively in a tabular format\n\nTransform data in record-based formats such as XML and JSON into single or multiple linked rectangular data tables.\nImplement data cleaning and quality control measures to ensure that data is read in and transformed correctly and important information is not lost.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#data-models",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#data-models",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "32.1 Data Models",
    "text": "32.1 Data Models\nIf you are reading this book, chances are you’re approaching programming from a more statistical or data-sciency point of view, rather than as a computer scientist. As a result, you probably have a general expectation that data will be laid out in a rectangular form, with rows representing observations or individuals and columns representating variables, measurements, or other dimensions of data which are applicable to each observation or individual.\nThis is an assumption which is much more common (at least in my experience) in statistics than in computer science more generally, though of course there are statisticians working on all sorts of different data types, including those we will discuss here.\n\n32.1.1 Relational Data\nRelational data is a particular type of data model that assumes table-based data storage. That is, when we access data in spreadsheets, CSVs, and so on, we are working with relational data. In computer science terms, a relation consists of a heading and a body.\n\nThe heading defines a set of attributes that have a name and a data type (mathematically, a domain).\nThe body is a set of tuples (a tuple is a collection of \\(n\\) values, and is a formal data type in Python, but not in R), where there are as many values as are defined in the heading.\n\nThis is all an abstract way of describing the composition of a Data Frame, as this book did in Section 11.6, where a DataFrame is a heterogeneous list of columns where:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\nIn a relational model, a record typically corresponds to a row of the data – in statistical terms, an observation (especially if our relational table is in tidy form).\nNot every record-based system is relational, however. Let’s examine a few other structures.\n\n\n32.1.2 Record-based Data Models\nBefore the relational data model became popular, however, there was the hierarchical data model. In the 1960s, computers began to be utilized for data storage, and this naturally led to record-based data models.\nIn a record-based data model, data are stored as records that are a collection of fields, where each field is a single value with an associated (usually fixed length/size) type. The fields in a record determine the record’s type.\n\n32.1.2.1 Hierarchical Data Models\nA generic entity or class can be defined as a collection of fields in a more formal object-oriented hierarchical representation. Links connect records with associated records, forming a tree.\nThis type of data structure is incredibly common, but it does not always (easily) reduce to tabular data. In many cases, though, it is possible to represent hierarchical data as a set of tables that relate to each other through keys.\n\n\n\n\n\n\nDemo: Hierarchical Employee Data\n\n\n\nWhen a company hires an employee, many different records may be generated:\n\nemployee information (name, address, phone number, ssn)\ninitial paperwork (background check, tax information)\ntraining history\nemployment agreement details (position type - permanent/contract/intern, start date, benefits, pay amount, which position the employee reports to)\n\nIn the pre-computer days, you can imagine that each set of records might be kept alphabetized by the most important field (employee name, in many cases, position in others) in separate filing cabinets. When computers entered the picture, the most direct translation was to build a hierarchical set of records with a structure much like the file cabinets - each set of information was kept with other records of its type, and these records could be linked together – usually, by following direct relational links between different forms. SAP, which is a very common enterprise data management system, still works this way - you pull up a record, and then click on linked records to navigate between different forms in the system.\n\n\n\n\n\n\nerDiagram\n  EMPLOYEE {\n    int ID PK\n    string first\n    string last\n    string middle\n    string address\n    string phone\n    int ssn\n  }\n  BACKGROUND_CHECK {\n    int ID PK\n    int EMPLOYEE_ID FK\n    date date\n    string results\n  }\n  TAX_INFO {\n    int ID PK\n    int EMPLOYEE_ID FK\n    int year\n    float withholding\n  }\n  TRAINING {\n    int ID PK\n    int TRAINING_TYPE\n    int EMPLOYEE_ID FK\n    float score\n  }\n  EMP_AGREE {\n    int ID PK\n    int EMPLOYEE_ID FK\n    int POSITION_ID FK\n    int POSITION_TYPE\n    date START_DATE\n    date END_DATE\n    int benefits_class\n    float pay_hourly_equiv\n    float hours_wk\n    int pay_period\n  }\n  ORG_REL {\n    int ID PK\n    int EMPLOYEE_ID FK\n    int SUPERVISOR_ID FK\n  }\n  EMPLOYEE }|--|| EMP_AGREE : has\n  EMPLOYEE }|--|| ORG_REL : has\n  EMPLOYEE ||--|{ TAX_INFO : has\n  EMPLOYEE ||--|{ TRAINING : has\n  EMPLOYEE ||--|{ BACKGROUND_CHECK : has\n\n\n  accTitle: Sample Company Employee-related Records\n  accDescr {\n    This diagram contains information about corporate records relating to a single employee, such as employee details, supervisory relationships, employment agreement details, training records, tax information, and background checks.\n  }\n\n\n\nFigure 32.1: This entity-relationship diagram contains information about corporate records relating to a single employee, such as employee details, supervisory relationships, employment agreement details, training records, tax information, and background checks.\n\n\n\n\n\nIn record-based data models, it can be complicated to actually do anything with the set of records. It might be relatively easy to e.g. list out all employees, but other related tasks, like determining how many people one individual is supervising, may require sifting through every ORG_REL record and could be complicated by how the records are stored (are they all text files in folders by employees?). Record-based data models, whether hierarchical or not, were originally a digital extension of physical records (think rows of file cabinets in old movies).\nNote that it is a relatively simple step between a hierarchical data model and a relational data model with separate tables for each record type. This isn’t shocking, if only because the relational data model where tables are joined together is a direct descendant of the hierarchical form-based data model described here.\n\n\n\n\n\n\n\n\nExample: Hierarchical Employee Data\n\n\n\n\nProblem DescriptionR (purrr)Python\n\n\nRead in this XML file of sample employee data and\n\nAssemble a table of all of the employee information in Figure 32.1 (that is, ID, first, middle, and last name, address, phone number, and social security number).\n\nCan you do this using data processing functions like map_dfr and as_list in R or read_xml in pandas (you’ll have to use chained operations in R and custom arguments in python)?\nIdentify any employees with an invalid social security number using your tabular data representation.\n\nIdentify the supervisor who has the most people reporting to them, without converting the data to tabular format, and then retrieve a list of all of that person’s direct reports as employee IDs.\nIdentify whether there are any employees who took the same training twice, without converting the data to tabular format.\n\nDo you prefer to work with tabular data or hierarchical data? Why?\n\n\n\nlibrary(xml2)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(stringr)\n\ninfo &lt;- read_xml(\"../data/sample_employee_data.xml\")\nrecords &lt;- xml_find_all(info, \"//*/EMPLOYEE\")\ndf &lt;- records |&gt; \n  as_list() |&gt;\n  map_dfr(~.x |&gt; \n            unlist(recursive = T) |&gt; \n            t() |&gt; \n            as.data.frame() |&gt; \n            set_names(\"id\", \"first\", \"last\", \"middle\", \n                      \"address\", \"phone\", \"ssn\")\n  ) |&gt;\n  mutate(valid_ssn = str_count(ssn, \"\\\\d\")==9)\nhead(df)\n##     id   first      last middle\n## 1 2824 Michael  Phillips      E\n## 2 1409 William  Gonzalez      B\n## 3 5506  Donald     Watts      L\n## 4 5012   Erica   Johnson      C\n## 5 4657 William  Townsend      D\n## 6 3286    Mark Fernandez      J\n##                                                    address               phone\n## 1          341 Bonilla Extensions\\nLake Jacktown, VA 72058 +1-701-028-0259x700\n## 2       85759 Danielle Lights\\nLake Anthonymouth, SD 72346   540.044.8808x1629\n## 3                68711 Janet Wall\\nMcdonaldmouth, MT 77414     +1-059-051-0485\n## 4      755 Brandon Mill Suite 800\\nNorth Phillip, LA 12123  (680)650-9821x6860\n## 5 00624 Johnson Harbor Apt. 211\\nWoodwardchester, AK 03498        026-982-2613\n## 6       910 Mathew Mall Suite 805\\nSchwartzmouth, OK 23331    630.559.6490x275\n##         ssn valid_ssn\n## 1 821097324      TRUE\n## 2 394613566      TRUE\n## 3 367179644      TRUE\n## 4 330054170      TRUE\n## 5 164054528      TRUE\n## 6 297177301      TRUE\nfilter(df, !valid_ssn)\n##     id     first   last middle\n## 1 7873     Susan  Mason      K\n## 2 5552   Cameron Miller      C\n## 3 6635 Alexander  Marsh      A\n## 4 5333     Kayla Parker      M\n##                                                address                  phone\n## 1                     PSC 4488, Box 1248\\nAPO AE 96883      152.806.9336x6388\n## 2 0819 Douglas Drives Suite 857\\nDicksonfort, WY 67373 001-727-389-3454x96815\n## 3              94716 Karen Square\\nSmithside, WA 74182    +1-126-214-2102x641\n## 4               1921 David Spur\\nNorth Sarah, IA 01256     549-226-0959x23219\n##        ssn valid_ssn\n## 1 12078633     FALSE\n## 2  9361578     FALSE\n## 3 39197635     FALSE\n## 4 69647139     FALSE\n\nThe purrr::pluck() function is a good way to pull out the information we need, once we convert the xml file to a list structure (which is still not a tabular form).\n\nsupervisors &lt;- xml_find_all(info, \"//*/ORG_REL/*\")|&gt; \n  as_list() |&gt;\n  map_chr(~purrr::pluck(., \"SUPERVISOR_ID\", 1)) \nemployees &lt;- xml_find_all(info, \"//*/ORG_REL/*\")|&gt; \n  as_list() |&gt;\n  map_chr(~purrr::pluck(., \"EMPLOYEE_ID\", 1)) \n\nsupervisor_reports &lt;- supervisors |&gt;\n  table() |&gt; sort(decreasing = T)\n\nemployees[which(supervisors == names(supervisor_reports)[1])]\n## [1] \"1434\" \"4611\" \"3547\" \"6925\"\n\n# Just for context\nfilter(df, id%in%employees[which(supervisors == names(supervisor_reports)[1])])\n##     id       first   last middle\n## 1 1434   Elizabeth Harris      B\n## 2 4611      Deanna  Doyle      C\n## 3 3547      Sherri Warner      T\n## 4 6925 Christopher  Jones      P\n##                                                      address\n## 1               720 James Passage\\nPort Justinfort, NC 55779\n## 2 328 Kelley Junctions Suite 782\\nNorth Jacqueline, PA 71278\n## 3           280 Allen Highway Apt. 700\\nSummerview, TX 62208\n## 4         26004 Kelly Rest Apt. 898\\nNew Katherine, OH 96504\n##                   phone       ssn valid_ssn\n## 1   +1-448-259-7632x679 531909398      TRUE\n## 2 001-272-144-5412x0950 226062294      TRUE\n## 3  001-011-455-7532x304 228640945      TRUE\n## 4     (195)612-9014x134 533899921      TRUE\n\n\nids &lt;- xml_find_all(info, \"//*/TRAINING/TRAININ/EMPLOYEE_ID\")|&gt; xml_text()\n\nget_employee_training_type &lt;- function(id) {\n  employee_training_xpath &lt;- sprintf(\"//*/TRAINING/TRAININ/EMPLOYEE_ID[text()='%s']\", id)\n  employee_training_nodes &lt;- xml_find_all(info, employee_training_xpath)\n  training_type &lt;- employee_training_nodes |&gt; \n    xml_parent() |&gt;\n    xml_child(\"TRAINING_TYPE\") |&gt; \n    xml_text() |&gt; \n    unlist()\n}\n\nduplicates &lt;- function(list) {\n  length(list) != length(unique(list))\n}\n\ndupe_employees &lt;- map(unique(ids), get_employee_training_type) |&gt;\n  map_lgl(duplicates)\n\nunique(ids)[dupe_employees] |&gt; as.numeric() |&gt; sort()\n## [1] 1106 2519 2584 2674 5506\n\n\n# This is how easy it is in tabular form... \ntraining_str &lt;- xml_find_all(info, \"//*/TRAINING/TRAININ\")|&gt; \n  as_list() |&gt;\n  map_dfr(~.x |&gt; \n            unlist(recursive = T) |&gt; \n            t() |&gt; \n            as.data.frame() |&gt; \n            set_names(\"id\", \"type\", \"employee_id\", \"score\")\n  ) |&gt; group_by(employee_id) |&gt;\n  summarize(n = n(), n_unique = length(unique(type))) |&gt;\n  filter(n != n_unique)\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndf = pd.read_xml(\"../data/sample_employee_data.xml\", \n                 iterparse={\"EMPLOYEE\": [\"id\", \"first\", \"last\", \"middle\", \n                                         \"address\", \"phone\", \"ssn\"]})\ndf.head()\n##      first      last  ...                phone        ssn\n## 0  Michael  Phillips  ...  +1-701-028-0259x700  821097324\n## 1  William  Gonzalez  ...    540.044.8808x1629  394613566\n## 2   Donald     Watts  ...      +1-059-051-0485  367179644\n## 3    Erica   Johnson  ...   (680)650-9821x6860  330054170\n## 4  William  Townsend  ...         026-982-2613  164054528\n## \n## [5 rows x 6 columns]\n\n# Invalid SSNs\ndf.query(\"~(ssn&lt;=999999999 & ssn &gt;= 100000000)\")\n##         first    last  ...                   phone       ssn\n## 19      Susan   Mason  ...       152.806.9336x6388  12078633\n## 27    Cameron  Miller  ...  001-727-389-3454x96815   9361578\n## 36  Alexander   Marsh  ...     +1-126-214-2102x641  39197635\n## 37      Kayla  Parker  ...      549-226-0959x23219  69647139\n## \n## [4 rows x 6 columns]\n\nIn order to not convert things to a tabular format, we have to use the xml library directly. This is annoying and a good vote in favor of using tabular formats instead of hierarchical stuff, particularly when pandas makes it easy to get the tabular format we need back out.\n\nimport xml.etree.ElementTree as ET\ndoc = ET.parse(r\"../data/sample_employee_data.xml\")\nroot = doc.getroot()\nrelationships = root.find(\"ORG_REL\")\nrelationships = relationships.findall(\"ORG_RE\")\n\n# These get the employee ID and supervisor ID for every relationship listed\nemployees = pd.Series([e[1].text for e in relationships])\nsupervisors = pd.Series([e[2].text for e in relationships])\n\n# Count # times supervisor ID appears in list\nsupervisors.groupby(supervisors).count().head()\n## 1409    1\n## 1434    1\n## 1520    3\n## 2139    1\n## 2291    2\n## dtype: int64\n\n# Supervisor with most employees\nbusiest_sup = supervisors.groupby(supervisors).count().sort_values(ascending=False).index[0]\nbusiest_sup\n## '3615'\n\n# List of those employees\nbusiest_sup_employees = employees.loc[supervisors==busiest_sup]\nbusiest_sup_employees\n## 16    1434\n## 20    4611\n## 28    3547\n## 45    6925\n## dtype: object\n\nNow we tackle the training problem in a similar way, making use of the fact that we can group one series (training types) by another (employee ID) and the index of the grouped series will have the employee ID we want. This is right on the border of tabular data, and yes, I’m intentionally blurring the lines even further. Ultimately, what we want is to solve the problem.\n\ntrainings = root.find(\"TRAINING\")\ntrainings = trainings.findall(\"TRAININ\")\n\ntraining_types = pd.Series([e[1].text for e in trainings])\ntraining_empl = pd.Series([e[2].text for e in trainings])\nmultiple_trainings = training_types.groupby(training_empl).agg(lambda x: len(x) != x.nunique())\n# Employees who took the same training multiple times\nmultiple_trainings.index[multiple_trainings.values]\n## Index(['1106', '2519', '2584', '2674', '5506'], dtype='object')\n\n\n\n\n\n\n\n\n32.1.2.2 Network Data Models\nIn a hierarchical data model, each record has only one parent. This is, as it turns out, a fairly restrictive constraint, as in the real world, there can be many-to-many relationships that are not strictly hierarchical - imagine trying to represent genealogical data with the restriction that each node can have only one parent!\nAnother form of record-based data model is a network. This model allows many-to-many relationships between records and even reciprocal links between two or more records (a “cycle” in network terms).\nOften, it is convenient to separately model the individual entities (“nodes”) in one table and the edges in another, when converting network data to something rectangular. In the case that edges are not directional (and thus cycles are not possible), it is often useful to impose a constraint to ensure that there is only one proper link between two nodes. One common constraint is a variant of “the ID of the first node is greater than the ID of the second”. When edges are directional, the order of the two nodes is useful, and no such constraints are required.\nIn both cases, extra information about the link between the two nodes may be included in additional columns in the ‘edge’ table.\n\n\n\n\n\n\n32.1.2.2.1 Demo: Relationship Data Networks\n\n\n\nIn an attempt to demonstrate how complex a network data model can get, I asked ChatGPT to generate a data set of romantic relationships between Grey’s Anatomy characters over the show’s 21 seasons.\n\n\n\n\n\n\n\ngraph TD\n\n%% Meredith Grey\nMeredithGrey -- romantic --&gt; DerekShepherd\nMeredithGrey -- romantic --&gt; NathanRiggs\nMeredithGrey -- romantic --&gt; AndrewDeLuca\nMeredithGrey -- romantic --&gt; NickMarsh\nMeredithGrey -- one-night --&gt; GeorgeOMalley\nMeredithGrey -- one-night --&gt; SteveMurphy\nMeredithGrey -- one-night --&gt; WilliamThorpe\nMeredithGrey -- one-night --&gt; FinnDandridge\n\n%% Cristina Yang\nCristinaYang -- romantic --&gt; OwenHunt\nCristinaYang -- romantic --&gt; PrestonBurke\nCristinaYang -- one-night --&gt; ShaneRoss\n\n%% Owen Hunt\nOwenHunt -- romantic --&gt; CristinaYang\nOwenHunt -- romantic --&gt; AmeliaShepherd\nOwenHunt -- romantic --&gt; TeddyAltman\nOwenHunt -- flirtation --&gt; EmmaMarling\n\n%% Alex Karev\nAlexKarev -- romantic --&gt; IzzieStevens\nAlexKarev -- romantic --&gt; JoWilson\nAlexKarev -- one-night --&gt; LexieGrey\nAlexKarev -- one-night --&gt; OliviaHarper\nAlexKarev -- one-night --&gt; RebeccaPope\n\n%% Izzie Stevens\nIzzieStevens -- romantic --&gt; AlexKarev\nIzzieStevens -- romantic --&gt; DennyDuquette\nIzzieStevens -- romantic --&gt; GeorgeOMalley\n\n%% George O'Malley\nGeorgeOMalley -- romantic --&gt; CallieTorres\nGeorgeOMalley -- one-night --&gt; MeredithGrey\nGeorgeOMalley -- romantic --&gt; IzzieStevens\nGeorgeOMalley -- one-night --&gt; OliviaHarper\n\n%% Callie Torres\nCallieTorres -- romantic --&gt; GeorgeOMalley\nCallieTorres -- romantic --&gt; EricaHahn\nCallieTorres -- romantic --&gt; ArizonaRobbins\nCallieTorres -- one-night --&gt; MarkSloan\nCallieTorres -- one-night --&gt; PennyBlake\n\n%% Arizona Robbins\nArizonaRobbins -- romantic --&gt; CallieTorres\nArizonaRobbins -- flirtation --&gt; LaurenBoswell\nArizonaRobbins -- flirtation --&gt; CarinaDeLuca\n\n%% Mark Sloan\nMarkSloan -- romantic --&gt; LexieGrey\nMarkSloan -- flirtation --&gt; AddisonMontgomery\nMarkSloan -- one-night --&gt; CallieTorres\nMarkSloan -- one-night --&gt; TeddyAltman\nMarkSloan -- one-night --&gt; ReedAdamson\n\n%% Lexie Grey\nLexieGrey -- romantic --&gt; MarkSloan\nLexieGrey -- romantic --&gt; JacksonAvery\nLexieGrey -- one-night --&gt; AlexKarev\n\n%% Jackson Avery\nJacksonAvery -- romantic --&gt; LexieGrey\nJacksonAvery -- romantic --&gt; AprilKepner\nJacksonAvery -- romantic --&gt; MaggiePierce\nJacksonAvery -- one-night --&gt; JoWilson\n\n%% April Kepner\nAprilKepner -- romantic --&gt; JacksonAvery\nAprilKepner -- romantic --&gt; MatthewTaylor\n\n%% Jo Wilson\nJoWilson -- romantic --&gt; AlexKarev\nJoWilson -- romantic --&gt; JasonMyers\nJoWilson -- one-night --&gt; JacksonAvery\n\n%% Andrew DeLuca\nAndrewDeLuca -- romantic --&gt; MaggiePierce\nAndrewDeLuca -- romantic --&gt; MeredithGrey\n\n%% Amelia Shepherd\nAmeliaShepherd -- romantic --&gt; OwenHunt\nAmeliaShepherd -- romantic --&gt; AtticusLincoln\nAmeliaShepherd -- romantic --&gt; KaiBartley\n\n%% Maggie Pierce\nMaggiePierce -- romantic --&gt; AndrewDeLuca\nMaggiePierce -- romantic --&gt; JacksonAvery\nMaggiePierce -- romantic --&gt; WinstonNdugu\n\n%% Teddy Altman\nTeddyAltman -- romantic --&gt; OwenHunt\nTeddyAltman -- romantic --&gt; TomKoracick\nTeddyAltman -- one-night --&gt; MarkSloan\n\n%% Erica Hahn\nEricaHahn -- romantic --&gt; CallieTorres\n\n%% Preston Burke\nPrestonBurke -- romantic --&gt; CristinaYang\n\n%% Nathan Riggs\nNathanRiggs -- romantic --&gt; MeredithGrey\nNathanRiggs -- romantic --&gt; MeganHunt\n\n%% Levi Schmitt\nLeviSchmitt -- romantic --&gt; NicoKim\n\n%% Nico Kim\nNicoKim -- romantic --&gt; LeviSchmitt\n\n%% Miranda Bailey\nMirandaBailey -- romantic --&gt; BenWarren\n\n%% Ben Warren\nBenWarren -- romantic --&gt; MirandaBailey\n\n%% Richard Webber\nRichardWebber -- romantic --&gt; EllisGrey\nRichardWebber -- romantic --&gt; CatherineFox\n\n%% Ellis Grey\nEllisGrey -- romantic --&gt; RichardWebber\n\n%% Catherine Fox\nCatherineFox -- romantic --&gt; RichardWebber\n\n%% Tom Koracick\nTomKoracick -- romantic --&gt; TeddyAltman\n\n%% Atticus Lincoln\nAtticusLincoln -- romantic --&gt; AmeliaShepherd\n\n%% Nick Marsh\nNickMarsh -- romantic --&gt; MeredithGrey\n\n\n\n\nFigure 32.2: Romantic relationships between characters in Grey’s Anatomy, aggregated over all 21 seasons.\n\n\n\n\n\nThis is a directed graph – the edges are arrows, starting from the person initiating the relationship (presumably) and ending at the person who is the target of the relationship. There is an additional value on each edge that describes the type of relationship.\nWe can imagine describing this data in tabular form as follows, showing some of the nodes involving Meredith Grey to save space:\n\n\n\nPerson1\nPerson2\ntype\n\n\n\n\nMeredithGrey\nDerekShepherd\nromantic\n\n\nMeredithGrey\nNathanRiggs\nromantic\n\n\nMeredithGrey\nAndrewDeLuca\nromantic\n\n\nMeredithGrey\nNickMarsh\nromantic\n\n\nMeredithGrey\nGeorgeOMalley\none-night\n\n\nGeorgeOMalley\nMeredithGrey\none-night\n\n\n…\n…\n…\n\n\n\nThen, if we have additional data on each person, such as an astrological sign, we could also have a person table containing that information. We can then reshape and join these tables to create different tables that are suitable for different analyses.\nIf you’d like to play around with this data, there are two JSON files that contain all of the information you would need: relationships, and astrological signs (not all characters have known or imputed astrological signs, so there will be some missing data).\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(jsonlite)\n\nsign_order &lt;- c(\"Aries\", \"Taurus\", \"Gemini\", \"Cancer\", \"Leo\", \"Virgo\", \"Libra\", \"Scorpio\", \"Sagittarius\", \"Capricorn\", \"Aquarius\", \"Pisces\")\n\nrelationships &lt;- read_json(\"../data/greys-anatomy-data.json\") |&gt;\n  map_df(as_tibble)\nastrology &lt;- read_json(\"../data/greys-anatomy-astrology.json\")\nastrology &lt;- tibble(Person = names(astrology), sign = unlist(astrology)) |&gt;\n  mutate(sign = factor(sign, levels = sign_order, ordered = T))\n\nSince I don’t know anything about astrology, let’s start by examining whether certain astrological signs are more likely to have a certain type of relationship.\n\nlong_rels &lt;- relationships |&gt; \n1  mutate(id = 1:n()) |&gt;\n  pivot_longer(cols = 1:2)\n\nrels_signs &lt;- long_rels |&gt;\n2  left_join(astrology, by = c(\"value\" = \"Person\"))\n\n# Data that is left out\n3anti_join(long_rels, astrology, by = c(\"value\" = \"Person\"))\nanti_join(astrology, long_rels, by = c(\"Person\" = \"value\"))\n\n4contingency_table &lt;- table(long_rels$relationship, long_rels$sign)\n## Error in table(long_rels$relationship, long_rels$sign): all arguments must have the same length\nknitr::kable(contingency_table, caption = \"Types of relationships, by astrological signs of the participants.\", label = \"tbl-grey-relationship-type-sign\")\n## Error: object 'contingency_table' not found\n## # A tibble: 16 × 4\n##    relationship    id name   value         \n##    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;         \n##  1 one-night        6 target Steve Murphy  \n##  2 one-night        7 target William Thorpe\n##  3 one-night        8 target Finn Dandridge\n##  4 flirtation      14 target Emma Marling  \n##  5 one-night       18 target Olivia Harper \n##  6 one-night       19 target Rebecca Pope  \n##  7 one-night       23 target Olivia Harper \n##  8 one-night       27 target Penny Blake   \n##  9 flirtation      28 target Lauren Boswell\n## 10 flirtation      29 target Carina DeLuca \n## 11 one-night       33 target Reed Adamson  \n## 12 romantic        38 target Matthew Taylor\n## 13 romantic        39 target Jason Myers   \n## 14 romantic        42 target Kai Bartley   \n## 15 romantic        43 target Winston Ndugu \n## 16 romantic        45 target Nico Kim      \n## # A tibble: 0 × 2\n## # ℹ 2 variables: Person &lt;chr&gt;, sign &lt;ord&gt;\n\n\n1\n\nKeep relationships together by adding an ID variable (for tracking data backwards after the pivot operation), and then pivot the first two variables into a “name” (source or target) and “value” (Person’s name) column, keeping relationship type and id as originally specified.\n\n2\n\nAdd in the person’s astrological sign. This is expected to be a many-to-one relationship – each person should have one astrological sign, but there may be many relationships specified for a single person in the table, as the show went on for 21 years.\n\n3\n\nWe do a sanity check to see what data was left out of the join step in 2. This requires checking both the forward and backwards joins – the first shows the rows in long_rels that aren’t in astrology (because the characters don’t have astrological signs that are known), while the second shows the rows in astrology that don’t have an equivalent in long_rels (there aren’t any, in this case). Checking this ensures that you don’t accidentally lose data.\n\n4\n\nCreate the contingency table showing relationship type in rows and astrological sign in columns.\n\n\n\n\nThis table has a lot of zeros, so I don’t think we’re going to be able to pull off a statistical analysis because the data is too sparse. Perhaps we could ask AI for similar data sets on the characters of House, ER, and other medical dramas, and then combine the data, if we wanted to proceed with a formal analysis. [1] provides demonstrations for how to analyze network data, if you are interested.\nWe can also just join both the source and the target in the same dataset to combine the information, as long as we are careful about renaming columns – the resulting contingency table is shown under the code chunk below. The sparsity of this data (we have 144 possible astrological combinations, and only 48 relationships) all but guarantees that we won’t be able to do any meaningful data analysis – Grey’s Anatomy just didn’t have enough relationships over 21 years for statistical purposes.\n\nrelationship_signs &lt;- relationships |&gt; \n  left_join(astrology, by = c(\"source\" = \"Person\")) |&gt;\n  rename(source_sign = sign) |&gt;\n  left_join(astrology, by = c(\"target\" = \"Person\")) |&gt;\n  rename(target_sign = sign)\n\ntable(Source = relationship_signs$source_sign, Target = relationship_signs$target_sign) |&gt;\n  knitr::kable(caption = \"Relationships in Grey's Anatomy, by astrological sign of the initiating individual (rows) and the target (columns). Not all relationships are included as not all characters have known or imputed (by the actor's sign) astrological sign.\")\n\n\nRelationships in Grey’s Anatomy, by astrological sign of the initiating individual (rows) and the target (columns). Not all relationships are included as not all characters have known or imputed (by the actor’s sign) astrological sign.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAries\nTaurus\nGemini\nCancer\nLeo\nVirgo\nLibra\nScorpio\nSagittarius\nCapricorn\nAquarius\nPisces\n\n\n\n\nAries\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTaurus\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nGemini\n1\n1\n0\n0\n1\n0\n0\n0\n2\n0\n0\n0\n\n\nCancer\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nLeo\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\nVirgo\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n1\n\n\nLibra\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nScorpio\n0\n0\n0\n1\n0\n2\n0\n1\n0\n0\n0\n1\n\n\nSagittarius\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nCapricorn\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nAquarius\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n\n\nPisces\n0\n0\n2\n0\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n\nI have no desire to watch all 21 seasons to check the bot’s work, so we’re going to assume that AI is the authority here. Feel free to submit a pull request if you have corrections to offer.\n\n\nBefore converting relational tables describing a network into a joint tabular structure assessing what we want to know, it is important to think carefully about the size of the data you are working with, as well as the types of joins you plan to execute. Thinking carefully about what dimension you expect your output to be will help you ensure that your join was executed as expected, and will also hopefully prevent you from doing a multiple-to-multiple join that outputs a behemoth of a dataset and potentially crashes your computer. Every example is different, so think carefully about which columns you want to use to join, whether you expect a many-to-many relationship between tables, and what the output dimension should be.\n\n\n\n\n\n\nExample: Computer Language Influences\n\n\n\nNew computer languages are often influenced by existing languages – for instance, R is an open-source clone of S, which was a proprietary language used at IBM – the function names are the same, but the methods are not necessarily identical underneath the hood.\nI worked with ChatGPT to develop a “family tree” of programming languages, focusing primarily on those used for data analysis tasks. ChatGPT was categorically wrong several times, so while I can vouch for the R/Julia/Python portions of the tree, it’s harder to guarantee that the other portions of the tree are 100% correct, though they seem to mostly match up with my prior knowledge. In this case, the goal is to provide you with a dataset that can be manipulated, so that’s the critical part of this exercise.\n\nProblem DescriptionRPython\n\n\nThe json file Language-Inheritance.json provides data on each programming language in the relationship diagram, including the release date and creator(s) credited with the language.\n\n\n\n\n\n\n---\nconfig:\n  layout: elk\n---\nflowchart TD\n    Lisp[\"Lisp\"] --&gt; Smalltalk[\"Smalltalk\"]\n    Algol[\"Algol\"] --&gt; C[\"C\"] & Pascal[\"Pascal\"]\n    C --&gt; C++[\"C++\"] & ObjectiveC[\"ObjectiveC\"] & Go[\"Go\"]\n    C++ --&gt; Java[\"Java\"]\n    Java --&gt; C#[\"C#\"] & Scala[\"Scala\"] & Kotlin[\"Kotlin\"]\n    Smalltalk --&gt; ObjectiveC\n    Perl[\"Perl\"] --&gt; Ruby[\"Ruby\"]\n    ML[\"ML\"] --&gt; Haskell[\"Haskell\"]\n    Scheme[\"Scheme\"] --&gt; JavaScript[\"JavaScript\"]\n    ObjectiveC --&gt; Swift[\"Swift\"]\n    S[\"S\"] --&gt; R[\"R\"]\n    Fortran[\"Fortran\"] -.-&gt; Algol & SAS[\"SAS\"] & S\n    Smalltalk -.-&gt; C++ & Java & Ruby\n    Perl -.-&gt; Python[\"Python\"]\n    Lisp -.-&gt; Python & R & Julia[\"Julia\"] & S\n    C -.-&gt; Python\n    Haskell -.-&gt; Scala\n    C++ -.-&gt; Rust[\"Rust\"]\n    ML -.-&gt; Rust\n    Rust -.-&gt; Swift\n    Java -.-&gt; JavaScript\n    SAS -.-&gt; R\n    Python -.-&gt; Julia\n    R -.-&gt; Julia\n    UnixShell[\"UnixShell\"] -.-&gt; S\n    GLIMSPSS[\"GLIMSPSS\"] -.-&gt; S\n\n\n\n\nFigure 32.3: Relationships between computer languages. Direct influences are shown with solid lines, while indirect influences based on the larger ecosystem of programming languages at the time are shown with dotted lines.\n\n\n\n\n\nRead this JSON data in, and convert the information to two tables - a language table and a relationship table. Ensure that ChatGPT at least did not claim that a language (A) inherits from a language (B) released after the release of language A. (One draft of this data claimed that Matlab inherited from Julia, which was released well after Matlab).\nIn the case of equal release years, this would indicate rapid language development. Otherwise, this would indicate one of several possibilities: time travel, evolution of the language to incorporate new influences, or ChatGPT is wrong. In any of these cases, more research would be necessary to determine what happened and whether the link is reasonably included in the dataset.\n\n\n\nlibrary(jsonlite)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n1proglang &lt;- read_json(\"../data/Language-Inheritance.json\")\n# str(proglang) # long set of output\n\nproglang$nodes[[1]]\n\n2fix_nodes &lt;- function(i) {\n  tibble(id = i$id, year = i$year, creators = list(i$creators))\n}\n\n3proglangnodes &lt;- map_dfr(proglang$nodes, fix_nodes)\nhead(proglangnodes)\n\nproglang$edges[[1]]\n\n4proglangedges &lt;- map_dfr(proglang$edges, as_tibble)\nhead(proglangedges)\n\n5proglangrels &lt;- left_join(proglangedges, proglangnodes, by = c(\"source\"=\"id\")) |&gt;\n6  rename(year_source=year) |&gt;\n  select(-creators) |&gt;\n7  left_join(proglangnodes, by = c(\"target\" = \"id\")) |&gt;\n8  rename(year_target = year) |&gt;\n  select(-creators)\n\n9filter(proglangrels, year_source &gt;= year_target)\n## $id\n## [1] \"Fortran\"\n## \n## $year\n## [1] 1957\n## \n## $creators\n## $creators[[1]]\n## [1] \"John Backus\"\n## \n## $creators[[2]]\n## [1] \"IBM\"\n## \n## \n## # A tibble: 6 × 3\n##   id         year creators  \n##   &lt;chr&gt;     &lt;int&gt; &lt;list&gt;    \n## 1 Fortran    1957 &lt;list [2]&gt;\n## 2 Lisp       1958 &lt;list [1]&gt;\n## 3 Algol      1958 &lt;list [1]&gt;\n## 4 C          1972 &lt;list [1]&gt;\n## 5 Smalltalk  1972 &lt;list [3]&gt;\n## 6 C++        1983 &lt;list [1]&gt;\n## $source\n## [1] \"Fortran\"\n## \n## $target\n## [1] \"Algol\"\n## \n## $relationship\n## [1] \"ecosystem\"\n## \n## # A tibble: 6 × 3\n##   source    target    relationship\n##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       \n## 1 Fortran   Algol     ecosystem   \n## 2 Lisp      Smalltalk inheritance \n## 3 Algol     C         inheritance \n## 4 Algol     Pascal    inheritance \n## 5 C         C++       inheritance \n## 6 Smalltalk C++       ecosystem   \n## # A tibble: 1 × 5\n##   source target     relationship year_source year_target\n##   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;              &lt;int&gt;       &lt;int&gt;\n## 1 Java   JavaScript ecosystem           1995        1995\n\n\n1\n\nRead in the data\n\n2\n\nCreate a tibble row from the data, using a list-column for the creators. This approach is a direct result of seeing the structure of the first node in the nodes sub-list, because it can’t be easily rectangularized without list-columns.\n\n3\n\nFor each language, apply the fix_nodes function, combining results rowwise into a tibble.\n\n4\n\nCreate a tibble row from the edge data. We can use as_tibble here because the data only has 3 components and there aren’t any list-columns needed.\n\n5\n\nLeft join the edges and nodes to get information on the source language.\n\n6\n\nRename the year to be clearer that this is the year of the source language, and remove creators because we don’t need them right now.\n\n7\n\nLeft join the edges and nodes again, adding information on the target language.\n\n8\n\nRename the year to be clearer - target language, and remove creators again because they’re not important to answer our quesiton.\n\n9\n\nDetermine whether any source languages have a year that’s greater than or equal to the target language.\n\n\n\n\n\n\n\nimport json\nimport pandas as pd\n\n1with open('../data/Language-Inheritance.json') as f:\n  data = json.load(f)\n\n2proglangnodes = pd.DataFrame(data['nodes'])\n3proglangyears = proglangnodes.drop('creators', axis = 1).set_index('id')\n\n4proglangedges = pd.DataFrame(data['edges'])\n\n5res = proglangedges.join(proglangyears, on = \"source\", how = 'left')\n6res = res.rename(columns = {'year':'year_source'})\nres.head()\n\n7res = res.join(proglangyears, on = \"target\", how = 'left')\n8res = res.rename(columns = {'year':'year_target'})\nres.head()\n\n9res.query('year_source&gt;=year_target')\n##     source     target relationship  year_source\n## 0  Fortran      Algol    ecosystem         1957\n## 1     Lisp  Smalltalk  inheritance         1958\n## 2    Algol          C  inheritance         1958\n## 3    Algol     Pascal  inheritance         1958\n## 4        C        C++  inheritance         1972\n##     source     target relationship  year_source  year_target\n## 0  Fortran      Algol    ecosystem         1957       1958.0\n## 1     Lisp  Smalltalk  inheritance         1958       1972.0\n## 2    Algol          C  inheritance         1958       1972.0\n## 3    Algol     Pascal  inheritance         1958          NaN\n## 4        C        C++  inheritance         1972       1983.0\n##    source      target relationship  year_source  year_target\n## 25   Java  JavaScript    ecosystem         1995       1995.0\n\n\n1\n\nRead in the data\n\n2\n\nCreate a DataFrame from the nodes (pandas automatically uses list-columns here)\n\n3\n\nDefine a DataFrame with just the language and year, and set the language (‘id’) as the index to prepare for joining this to the edge dataset.\n\n4\n\nCreate a DataFrame from the edge data.\n\n5\n\nLeft join the edges and nodes to get information on the source language.\n\n6\n\nRename the year to be clearer that this is the year of the source language.\n\n7\n\nLeft join the edges and nodes again, adding information on the target language.\n\n8\n\nRename the year to be clearer - target language.\n\n9\n\nDetermine whether any source languages have a year that’s greater than or equal to the target language.\n\n\n\n\n\n\n\nThe Wikipedia page for JavaScript suggests that JavaScript was influenced by Java, and that JavaScript appeared in December 1995 while Java appeared in May 1995. So, I suppose we can conclude at this point that the stated link is reasonable.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#developing-a-conversion-strategy",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#developing-a-conversion-strategy",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "32.2 Developing A Conversion Strategy",
    "text": "32.2 Developing A Conversion Strategy\nWhen you have an XML or JSON data file that is record based, it can be tricky to determine the best way to convert this into a tabular data structure.\nA general strategy will look something like this:\n\nWhat types of records do you have? Are there multiple types? How do they relate to each other?\n\nIdentify linking variables (keys)\nIdentify problematic attributes – things that may be missing in some records, or may need to be list-columns or even nested data frames.\n\nWhat data do you need? Sometimes, you don’t have to convert the entire dataset into a tabular structure – you can be selective about it, and save yourself a ton of work.\nWhat do you plan to do with the data? Sketch out the form of your target data (rows, columns, data types) so that you can plan a strategy of how to get from A to B.\nWrite functions to convert records into rows of a table (if step 1 has problematic attributes) or use a list-to-table conversion function (if step 1 doesn’t have problematic attributes).\nDouble-check several records to ensure all data is accounted for and that keys match as expected.\n\n\n\n\n\n\n\n32.2.1 Demo: Book/TV Show Characters\n\n\n\nI have assembled a JSON file of all of the characters in A Song of Ice and Fire by George R. R. Martin using An API of Ice and Fire (I’ll show how to do this in the next chapter). The series has been used to create several TV Shows (Game of Thrones, House of the Dragon, and A Knight of the Seven Kingdoms).\nThe JSON file has more than 50k rows, but it’s clear that there is a structure to the data, along with a lot of missing information.\nThe tabs below work through the five steps listed immediately above this section.\n\n1. Examine the record structure2. Identify Needed Data3. Sketch Form of the Data4. Write Functions to Format Data5. Double-check Records for Accuracy\n\n\nThere is only one type of record in the file – data corresponding to each character in the series.\n\n\n\n\n\n\nSelection of 4 records\n\n\n\n\n\n{\n        \"url\": \"https://anapioficeandfire.com/api/characters/1\",\n        \"name\": \"\",\n        \"gender\": \"Female\",\n        \"culture\": \"Braavosi\",\n        \"born\": \"\",\n        \"died\": \"\",\n        \"titles\": [],\n        \"aliases\": [\n            \"The Daughter of the Dusk\"\n        ],\n        \"father\": \"\",\n        \"mother\": \"\",\n        \"spouse\": \"\",\n        \"allegiances\": [],\n        \"books\": [\n            \"https://anapioficeandfire.com/api/books/5\"\n        ],\n        \"povBooks\": [],\n        \"tvSeries\": [],\n        \"playedBy\": []\n    },  \n    {\n        \"url\": \"https://anapioficeandfire.com/api/characters/238\",\n        \"name\": \"Cersei Lannister\",\n        \"gender\": \"Female\",\n        \"culture\": \"Westerman\",\n        \"born\": \"In 266 AC, at Casterly Rock\",\n        \"died\": \"\",\n        \"titles\": [\n            \"Light of the West\",\n            \"Queen Dowager\",\n            \"Protector of the Realm\",\n            \"Lady of Casterly Rock\",\n            \"Queen Regent\"\n        ],\n        \"aliases\": [\n            \"Brotherfucker\",\n            \"The bitch queen\"\n        ],\n        \"father\": \"\",\n        \"mother\": \"\",\n        \"spouse\": \"https://anapioficeandfire.com/api/characters/901\",\n        \"allegiances\": [\n            \"https://anapioficeandfire.com/api/houses/229\"\n        ],\n        \"books\": [\n            \"https://anapioficeandfire.com/api/books/1\",\n            \"https://anapioficeandfire.com/api/books/2\",\n            \"https://anapioficeandfire.com/api/books/3\"\n        ],\n        \"povBooks\": [\n            \"https://anapioficeandfire.com/api/books/5\",\n            \"https://anapioficeandfire.com/api/books/8\"\n        ],\n        \"tvSeries\": [\n            \"Season 1\",\n            \"Season 2\",\n            \"Season 3\",\n            \"Season 4\",\n            \"Season 5\",\n            \"Season 6\"\n        ],\n        \"playedBy\": [\n            \"Lena Headey\"\n        ]\n    },\n    {\n        \"url\": \"https://anapioficeandfire.com/api/characters/2\",\n        \"name\": \"Walder\",\n        \"gender\": \"Male\",\n        \"culture\": \"\",\n        \"born\": \"\",\n        \"died\": \"\",\n        \"titles\": [],\n        \"aliases\": [\n            \"Hodor\"\n        ],\n        \"father\": \"\",\n        \"mother\": \"\",\n        \"spouse\": \"\",\n        \"allegiances\": [\n            \"https://anapioficeandfire.com/api/houses/362\"\n        ],\n        \"books\": [\n            \"https://anapioficeandfire.com/api/books/1\",\n            \"https://anapioficeandfire.com/api/books/2\",\n            \"https://anapioficeandfire.com/api/books/3\",\n            \"https://anapioficeandfire.com/api/books/5\",\n            \"https://anapioficeandfire.com/api/books/8\"\n        ],\n        \"povBooks\": [],\n        \"tvSeries\": [\n            \"Season 1\",\n            \"Season 2\",\n            \"Season 3\",\n            \"Season 4\",\n            \"Season 6\"\n        ],\n        \"playedBy\": [\n            \"Kristian Nairn\"\n        ]\n    },\n  {\n        \"url\": \"https://anapioficeandfire.com/api/characters/208\",\n        \"name\": \"Brandon Stark\",\n        \"gender\": \"Male\",\n        \"culture\": \"Northmen\",\n        \"born\": \"In 290 AC, at Winterfell\",\n        \"died\": \"\",\n        \"titles\": [\n            \"Prince of Winterfell\"\n        ],\n        \"aliases\": [\n            \"Bran\",\n            \"Bran the Broken\",\n            \"The Winged Wolf\"\n        ],\n        \"father\": \"\",\n        \"mother\": \"\",\n        \"spouse\": \"\",\n        \"allegiances\": [\n            \"https://anapioficeandfire.com/api/houses/362\"\n        ],\n        \"books\": [\n            \"https://anapioficeandfire.com/api/books/5\"\n        ],\n        \"povBooks\": [\n            \"https://anapioficeandfire.com/api/books/1\",\n            \"https://anapioficeandfire.com/api/books/2\",\n            \"https://anapioficeandfire.com/api/books/3\",\n            \"https://anapioficeandfire.com/api/books/8\"\n        ],\n        \"tvSeries\": [\n            \"Season 1\",\n            \"Season 2\",\n            \"Season 3\",\n            \"Season 4\",\n            \"Season 6\"\n        ],\n        \"playedBy\": [\n            \"Isaac Hempstead-Wright\"\n        ]\n    }\n\n\n\nAny entry that includes a URL is a linking variable (by definition, as well as in practice – it is not this easy to figure out linking variables in every dataset).\nWe can also see that ‘books’ does not include ‘povBooks’ - that is, if the character has a point-of-view chapter in a book, it is included in ‘povBooks’ instead of ‘books’. We might want to do a bit of cleaning here, and create a table that has books and a logical variable indicating if the character has a POV chapter in that book.\nWhat isn’t clear is whether multiple actors could play a single character – for instance, if there was a young version and an old version of a character in a flashback. That occurrence doesn’t seem to happen here (there’s only one playedBy value for any character, as far as I can tell), but it could. Of course, if this did occur, the JSON data structure should be different as well – perhaps with series and playedBy nested under a single variable and constrained to be the same length, so that it’s clear which actor played in each series.\n\n\nIn this case, we don’t know what data we need (because I haven’t specified an analysis) so we need to keep all of the data.\n\n\nRight away, it is possible to conceptualize this data in a number of different ways. We can store links within a single table, using list-columns, or we could store some of the relational information in different tables, and keep our main table very simple in structure.\n \n We must generally assume that almost any field other than ID/URL can be blank, and fields like Titles, Aliases, Allegiances, Spouse, Book, pov (point of view), and TVSeries can contain multiple values.\nOf these strategies, the second strategy seems like it’s the easiest to work with for most general problems – it strikes a balance between number of tables (which will require joins to get into an analysis form, most likely) and the complexities of having to deal with list-columns.\n\n\n\nlibrary(jsonlite)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\n\n1data &lt;- read_json(\"../data/asoiaf_characters.json\")\n\n2blank_to_na &lt;- function(j) {\n3  if (is.null(j)) {\n    return(NA)\n  }\n  if (length(j) == 0) {\n    return(NA)\n  }\n4  if (length(j) &gt; 1) {\n    return(list(unlist(j)))\n  }\n5  if (j == \"\") {\n    return(NA)\n  }\n6  return(j)\n}\n\n7fix_char_data &lt;- function(i) {\n8  ifix &lt;- map(i, blank_to_na)\n9  ifixtbl &lt;- as_tibble(ifix) |&gt;\n    mutate(id = str_remove(url, \"https://anapioficeandfire.com/api/characters/\"))\n  \n10  personalInfo &lt;- ifixtbl |&gt;\n    select(id, url, name, gender, culture, born, died, titles, aliases, allegiances)\n  \n11  relationships &lt;- ifixtbl |&gt;\n    select(id, father, mother, spouse) |&gt;\n    pivot_longer(-id, names_to = \"relationship_type\", values_to = \"relative_ID\") |&gt;\n    unnest(relative_ID) |&gt;\n    mutate(relationship_type = as.character(relationship_type),\n           relative_ID = as.character(relative_ID)) |&gt;\n    filter(!is.na(relative_ID))\n  \n12  book &lt;- ifixtbl |&gt;\n    select(id, books, povBooks) |&gt;\n    pivot_longer(-id) |&gt;\n    unnest(value) |&gt;\n    mutate(pov = name == \"povBooks\") |&gt;\n    select(id, book = value, pov) |&gt;\n    mutate(book = as.character(book), pov = as.character(pov)) |&gt;\n    filter(!is.na(book))\n  \n13  tv_show &lt;- ifixtbl |&gt;\n    select(id, tvSeries, playedBy) |&gt;\n    unnest(tvSeries) |&gt;\n    unnest(playedBy) |&gt;\n    mutate(tvSeries = as.character(tvSeries), playedBy = as.character(playedBy)) |&gt;\n    filter(!is.na(tvSeries))\n  \n14  return(tibble(personal = list(personalInfo),\n                relationships = list(relationships),\n                book = list(book),\n                tv = list(tv_show)))\n}\n\n15datatbl &lt;- map_dfr(data, fix_char_data)\n\npersonalInfo &lt;- bind_rows(datatbl$personal)\nrelationships &lt;- bind_rows(datatbl$relationships)\nbook &lt;- bind_rows(datatbl$book)\ntv_show &lt;- bind_rows(datatbl$tv)\n\n\n1\n\nRead in the data\n\n2\n\nWrite a function to handle different types of ‘missingness’ and replace with NA.\n\n3\n\nIf the object is null or has length 0, return NA.\n\n4\n\nIf the object is a list, return the list, but ensure it has only one level of hierarchy. This could get us in trouble if we used this same function on a different dataset, but the character variables don’t have multiple levels of nesting, so it’s ok in this case. This addition ensures we don’t accidentally add another level of nesting, but also that we can handle an unnested list appropriately.\n\n5\n\nAnother option for a ‘blank’ value is to have an empty string. We want to also replace that with NA. 6. If none of the empty options is true, then just return the value.\n\n6\n\nNow, we write a function to create data table rows for each individual’s data, for the 4 tables we decided to create. Creating each row of the 4 tables and using nested data frames allows us to only run the fixit function once, and then slice the data into sub-tables as we want.\n\n7\n\nFix the data.\n\n8\n\nConvert the fixed data to a table so it’s easily manipulated with dplyr/tidyr\n\n9\n\nSelect the personal information into a separate table.\n\n10\n\nSelect the relationship information into a separate table. Then, pivot the relationships into relationshipType and relative_ID. Once that’s done, we can unnest, which allows for multiple values of mother, father, and spouse. The mutate() statement just ensures that all variables are typed correctly - otherwise, we might have the default NA_logical_ instead of NA_character_ and get an error when we bind all the rows together.\n\n11\n\nSelect the book information into a separate table. Then, pivot the relationships into book and pov, unnest, and create the pov variable as a logical indicating whether the book came from povBooks or books. The mutate() statement just ensures that all variables are typed correctly - otherwise, we might have the default NA_logical_ instead of NA_character_ and get an error when we bind all the rows together.\n\n12\n\nSelect the tv information into a separate table and unnest each variable. This could lead to a many-to-many relationship in the case that someone played a character in one series but another actor played the character in the next series. A more rational structure would be to have tvSeries and playedBy nested together, so that it would be possible to indicate this conclusively, but this is an issue with the original data structure that we just respond to. The mutate() statement just ensures that all variables are typed correctly - otherwise, we might have the default NA_logical_ instead of NA_character_ and get an error when we bind all the rows together.\n\n13\n\nWe return a nested tibble with each of the 4 tables we’ve created for our character.\n\n14\n\nApply the full function to each record in data.\n\n15\n\nExtract each nested tibble out and bind the rows together to get the 4 tables we wanted.\n\n\n\n\n\n\nWhen double-checking records, I like to use a combination of strategies - random spot-checking, as well as checking values I know are likely to be challenging in one way or another.\n\nset.seed(3420934)\ntests &lt;- sample(size = 3, x = 1:length(data))\ntests\n## [1]  408 2000 1161\n\n\n\nRandom individual with ID=r .QuartoInlineRender(tests[1])\ni &lt;- tests[1]\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/408\"\n## \n## $name\n## [1] \"Garth Tyrell\"\n## \n## $gender\n## [1] \"Male\"\n## \n## $culture\n## [1] \"\"\n## \n## $born\n## [1] \"\"\n## \n## $died\n## [1] \"\"\n## \n## $titles\n## $titles[[1]]\n## [1] \"Lord Seneschal\"\n## \n## \n## $aliases\n## $aliases[[1]]\n## [1] \"Garth the Gross\"\n## \n## \n## $allegiances\n## $allegiances[[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/398\"\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 408   https://ana… Gart… Male   &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr [1]&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## [1] \"Lord Seneschal\"\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## [1] \"Garth the Gross\"\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/398\"\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/1\"\n## \n## $books[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $books[[3]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## $books[[4]]\n## [1] \"https://anapioficeandfire.com/api/books/5\"\n## \n## $books[[5]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\n## \n## \n## $povBooks\n## list()\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 5 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 408   https://anapioficeandfire.com/api/books/1 FALSE\n## 2 408   https://anapioficeandfire.com/api/books/2 FALSE\n## 3 408   https://anapioficeandfire.com/api/books/3 FALSE\n## 4 408   https://anapioficeandfire.com/api/books/5 FALSE\n## 5 408   https://anapioficeandfire.com/api/books/8 FALSE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, relationship_type &lt;chr&gt;, relative_ID &lt;chr&gt;\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## list()\n## \n## $playedBy\n## list()\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, tvSeries &lt;chr&gt;, playedBy &lt;chr&gt;\n\n\n\n\nRandom individual with ID=r .QuartoInlineRender(tests[2])\ni &lt;- tests[2]\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/2000\"\n## \n## $name\n## [1] \"Thistle\"\n## \n## $gender\n## [1] \"Female\"\n## \n## $culture\n## [1] \"Free Folk\"\n## \n## $born\n## [1] \"\"\n## \n## $died\n## [1] \"In 300 AC, at Beyond the Wall\"\n## \n## $titles\n## list()\n## \n## $aliases\n## list()\n## \n## $allegiances\n## list()\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 2000  https://ana… This… Female Free F… &lt;NA&gt;  In 3… &lt;NULL&gt; &lt;NULL&gt;  &lt;NULL&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## NULL\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## NULL\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## NULL\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\n## \n## \n## $povBooks\n## list()\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 1 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 2000  https://anapioficeandfire.com/api/books/8 FALSE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, relationship_type &lt;chr&gt;, relative_ID &lt;chr&gt;\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## list()\n## \n## $playedBy\n## list()\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, tvSeries &lt;chr&gt;, playedBy &lt;chr&gt;\n\n\n\n\nRandom individual with ID=r .QuartoInlineRender(tests[3])\ni &lt;- tests[3]\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/1161\"\n## \n## $name\n## [1] \"Amabel\"\n## \n## $gender\n## [1] \"Female\"\n## \n## $culture\n## [1] \"\"\n## \n## $born\n## [1] \"\"\n## \n## $died\n## [1] \"\"\n## \n## $titles\n## list()\n## \n## $aliases\n## list()\n## \n## $allegiances\n## $allegiances[[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/427\"\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 1161  https://ana… Amab… Female &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;  &lt;NULL&gt; &lt;NULL&gt;  &lt;chr [1]&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## NULL\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## NULL\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/427\"\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $books[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## \n## $povBooks\n## list()\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 2 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 1161  https://anapioficeandfire.com/api/books/2 FALSE\n## 2 1161  https://anapioficeandfire.com/api/books/3 FALSE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, relationship_type &lt;chr&gt;, relative_ID &lt;chr&gt;\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## list()\n## \n## $playedBy\n## list()\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, tvSeries &lt;chr&gt;, playedBy &lt;chr&gt;\n\n\nThen, we move on to the nonrandom sample, picking people we know and can verify, as well as those who might reasonably have significant missing data or excessively long nested lists of attributes.\n\n\nArya Stark\ni &lt;- 148\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/148\"\n## \n## $name\n## [1] \"Arya Stark\"\n## \n## $gender\n## [1] \"Female\"\n## \n## $culture\n## [1] \"Northmen\"\n## \n## $born\n## [1] \"In 289 AC, at Winterfell\"\n## \n## $died\n## [1] \"\"\n## \n## $titles\n## $titles[[1]]\n## [1] \"Princess\"\n## \n## \n## $aliases\n## $aliases[[1]]\n## [1] \"Arya Horseface\"\n## \n## $aliases[[2]]\n## [1] \"Arya Underfoot\"\n## \n## $aliases[[3]]\n## [1] \"Arry\"\n## \n## $aliases[[4]]\n## [1] \"Lumpyface\"\n## \n## $aliases[[5]]\n## [1] \"Lumpyhead\"\n## \n## $aliases[[6]]\n## [1] \"Stickboy\"\n## \n## $aliases[[7]]\n## [1] \"Weasel\"\n## \n## $aliases[[8]]\n## [1] \"Nymeria\"\n## \n## $aliases[[9]]\n## [1] \"Squan\"\n## \n## $aliases[[10]]\n## [1] \"Saltb\"\n## \n## $aliases[[11]]\n## [1] \"Cat of the Canaly\"\n## \n## $aliases[[12]]\n## [1] \"Bets\"\n## \n## $aliases[[13]]\n## [1] \"The Blind Girh\"\n## \n## $aliases[[14]]\n## [1] \"The Ugly Little Girl\"\n## \n## $aliases[[15]]\n## [1] \"Mercedenl\"\n## \n## $aliases[[16]]\n## [1] \"Mercye\"\n## \n## \n## $allegiances\n## $allegiances[[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/362\"\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 148   https://ana… Arya… Female Northm… In 2… &lt;NA&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr [1]&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## [1] \"Princess\"\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n##  [1] \"Arya Horseface\"       \"Arya Underfoot\"       \"Arry\"                \n##  [4] \"Lumpyface\"            \"Lumpyhead\"            \"Stickboy\"            \n##  [7] \"Weasel\"               \"Nymeria\"              \"Squan\"               \n## [10] \"Saltb\"                \"Cat of the Canaly\"    \"Bets\"                \n## [13] \"The Blind Girh\"       \"The Ugly Little Girl\" \"Mercedenl\"           \n## [16] \"Mercye\"\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/362\"\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## list()\n## \n## $povBooks\n## $povBooks[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/1\"\n## \n## $povBooks[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $povBooks[[3]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## $povBooks[[4]]\n## [1] \"https://anapioficeandfire.com/api/books/5\"\n## \n## $povBooks[[5]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 5 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 148   https://anapioficeandfire.com/api/books/1 TRUE \n## 2 148   https://anapioficeandfire.com/api/books/2 TRUE \n## 3 148   https://anapioficeandfire.com/api/books/3 TRUE \n## 4 148   https://anapioficeandfire.com/api/books/5 TRUE \n## 5 148   https://anapioficeandfire.com/api/books/8 TRUE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, relationship_type &lt;chr&gt;, relative_ID &lt;chr&gt;\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## $tvSeries[[1]]\n## [1] \"Season 1\"\n## \n## $tvSeries[[2]]\n## [1] \"Season 2\"\n## \n## $tvSeries[[3]]\n## [1] \"Season 3\"\n## \n## $tvSeries[[4]]\n## [1] \"Season 4\"\n## \n## $tvSeries[[5]]\n## [1] \"Season 5\"\n## \n## $tvSeries[[6]]\n## [1] \"Season 6\"\n## \n## \n## $playedBy\n## $playedBy[[1]]\n## [1] \"Maisie Williams\"\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 6 × 3\n##   id    tvSeries playedBy       \n##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;          \n## 1 148   Season 1 Maisie Williams\n## 2 148   Season 2 Maisie Williams\n## 3 148   Season 3 Maisie Williams\n## 4 148   Season 4 Maisie Williams\n## 5 148   Season 5 Maisie Williams\n## 6 148   Season 6 Maisie Williams\n\n\n\n\nJon Snow\ni &lt;- 583\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/583\"\n## \n## $name\n## [1] \"Jon Snow\"\n## \n## $gender\n## [1] \"Male\"\n## \n## $culture\n## [1] \"Northmen\"\n## \n## $born\n## [1] \"In 283 AC\"\n## \n## $died\n## [1] \"\"\n## \n## $titles\n## $titles[[1]]\n## [1] \"Lord Commander of the Night's Watch\"\n## \n## \n## $aliases\n## $aliases[[1]]\n## [1] \"Lord Snow\"\n## \n## $aliases[[2]]\n## [1] \"Ned Stark's Bastard\"\n## \n## $aliases[[3]]\n## [1] \"The Snow of Winterfell\"\n## \n## $aliases[[4]]\n## [1] \"The Crow-Come-Over\"\n## \n## $aliases[[5]]\n## [1] \"The 998th Lord Commander of the Night's Watch\"\n## \n## $aliases[[6]]\n## [1] \"The Bastard of Winterfell\"\n## \n## $aliases[[7]]\n## [1] \"The Black Bastard of the Wall\"\n## \n## $aliases[[8]]\n## [1] \"Lord Crow\"\n## \n## \n## $allegiances\n## $allegiances[[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/362\"\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 583   https://ana… Jon … Male   Northm… In 2… &lt;NA&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr [1]&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## [1] \"Lord Commander of the Night's Watch\"\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## [1] \"Lord Snow\"                                    \n## [2] \"Ned Stark's Bastard\"                          \n## [3] \"The Snow of Winterfell\"                       \n## [4] \"The Crow-Come-Over\"                           \n## [5] \"The 998th Lord Commander of the Night's Watch\"\n## [6] \"The Bastard of Winterfell\"                    \n## [7] \"The Black Bastard of the Wall\"                \n## [8] \"Lord Crow\"\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/362\"\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/5\"\n## \n## \n## $povBooks\n## $povBooks[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/1\"\n## \n## $povBooks[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $povBooks[[3]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## $povBooks[[4]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 5 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 583   https://anapioficeandfire.com/api/books/5 FALSE\n## 2 583   https://anapioficeandfire.com/api/books/1 TRUE \n## 3 583   https://anapioficeandfire.com/api/books/2 TRUE \n## 4 583   https://anapioficeandfire.com/api/books/3 TRUE \n## 5 583   https://anapioficeandfire.com/api/books/8 TRUE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 0 × 3\n## # ℹ 3 variables: id &lt;chr&gt;, relationship_type &lt;chr&gt;, relative_ID &lt;chr&gt;\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## $tvSeries[[1]]\n## [1] \"Season 1\"\n## \n## $tvSeries[[2]]\n## [1] \"Season 2\"\n## \n## $tvSeries[[3]]\n## [1] \"Season 3\"\n## \n## $tvSeries[[4]]\n## [1] \"Season 4\"\n## \n## $tvSeries[[5]]\n## [1] \"Season 5\"\n## \n## $tvSeries[[6]]\n## [1] \"Season 6\"\n## \n## \n## $playedBy\n## $playedBy[[1]]\n## [1] \"Kit Harington\"\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 6 × 3\n##   id    tvSeries playedBy     \n##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;        \n## 1 583   Season 1 Kit Harington\n## 2 583   Season 2 Kit Harington\n## 3 583   Season 3 Kit Harington\n## 4 583   Season 4 Kit Harington\n## 5 583   Season 5 Kit Harington\n## 6 583   Season 6 Kit Harington\n\n\n\n\nCersei Lannister\ni &lt;- 238\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/238\"\n## \n## $name\n## [1] \"Cersei Lannister\"\n## \n## $gender\n## [1] \"Female\"\n## \n## $culture\n## [1] \"Westerman\"\n## \n## $born\n## [1] \"In 266 AC, at Casterly Rock\"\n## \n## $died\n## [1] \"\"\n## \n## $titles\n## $titles[[1]]\n## [1] \"Light of the West\"\n## \n## $titles[[2]]\n## [1] \"Queen Dowager\"\n## \n## $titles[[3]]\n## [1] \"Protector of the Realm\"\n## \n## $titles[[4]]\n## [1] \"Lady of Casterly Rock\"\n## \n## $titles[[5]]\n## [1] \"Queen Regent\"\n## \n## \n## $aliases\n## $aliases[[1]]\n## [1] \"Brotherfucker\"\n## \n## $aliases[[2]]\n## [1] \"The bitch queen\"\n## \n## \n## $allegiances\n## $allegiances[[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/229\"\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 238   https://ana… Cers… Female Wester… In 2… &lt;NA&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr [1]&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## [1] \"Light of the West\"      \"Queen Dowager\"          \"Protector of the Realm\"\n## [4] \"Lady of Casterly Rock\"  \"Queen Regent\"\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## [1] \"Brotherfucker\"   \"The bitch queen\"\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## [1] \"https://anapioficeandfire.com/api/houses/229\"\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/1\"\n## \n## $books[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $books[[3]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## \n## $povBooks\n## $povBooks[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/5\"\n## \n## $povBooks[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 5 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 238   https://anapioficeandfire.com/api/books/1 FALSE\n## 2 238   https://anapioficeandfire.com/api/books/2 FALSE\n## 3 238   https://anapioficeandfire.com/api/books/3 FALSE\n## 4 238   https://anapioficeandfire.com/api/books/5 TRUE \n## 5 238   https://anapioficeandfire.com/api/books/8 TRUE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"https://anapioficeandfire.com/api/characters/901\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 1 × 3\n##   id    relationship_type relative_ID                                     \n##   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                                           \n## 1 238   spouse            https://anapioficeandfire.com/api/characters/901\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## $tvSeries[[1]]\n## [1] \"Season 1\"\n## \n## $tvSeries[[2]]\n## [1] \"Season 2\"\n## \n## $tvSeries[[3]]\n## [1] \"Season 3\"\n## \n## $tvSeries[[4]]\n## [1] \"Season 4\"\n## \n## $tvSeries[[5]]\n## [1] \"Season 5\"\n## \n## $tvSeries[[6]]\n## [1] \"Season 6\"\n## \n## \n## $playedBy\n## $playedBy[[1]]\n## [1] \"Lena Headey\"\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 6 × 3\n##   id    tvSeries playedBy   \n##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      \n## 1 238   Season 1 Lena Headey\n## 2 238   Season 2 Lena Headey\n## 3 238   Season 3 Lena Headey\n## 4 238   Season 4 Lena Headey\n## 5 238   Season 5 Lena Headey\n## 6 238   Season 6 Lena Headey\n\n\n\n\nDrogo\ni &lt;- 1346\ndata[[i]][c(\"url\", \"name\", \"gender\", \"culture\", \"born\", \"died\", \"titles\", \"aliases\", \"allegiances\")] |&gt; print()\n## $url\n## [1] \"https://anapioficeandfire.com/api/characters/1346\"\n## \n## $name\n## [1] \"Drogo\"\n## \n## $gender\n## [1] \"Male\"\n## \n## $culture\n## [1] \"Dothraki\"\n## \n## $born\n## [1] \"In or around 267 AC\"\n## \n## $died\n## [1] \"In 298 AC, at Dothraki sea\"\n## \n## $titles\n## $titles[[1]]\n## [1] \"Khal\"\n## \n## \n## $aliases\n## $aliases[[1]]\n## [1] \"Great Rider\"\n## \n## $aliases[[2]]\n## [1] \"Great Khal\"\n## \n## \n## $allegiances\n## list()\nfilter(personalInfo, id==as.character(i)) |&gt; print()\n## # A tibble: 1 × 10\n##   id    url          name  gender culture born  died  titles aliases allegiances\n##   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt;  &lt;list&gt;     \n## 1 1346  https://ana… Drogo Male   Dothra… In o… In 2… &lt;chr&gt;  &lt;chr&gt;   &lt;NULL&gt;\nfilter(personalInfo, id==as.character(i))$titles |&gt; print()\n## [[1]]\n## [1] \"Khal\"\nfilter(personalInfo, id==as.character(i))$aliases |&gt; print()\n## [[1]]\n## [1] \"Great Rider\" \"Great Khal\"\nfilter(personalInfo, id==as.character(i))$allegiances |&gt; print()\n## [[1]]\n## NULL\ndata[[i]][c(\"books\", \"povBooks\")] |&gt; print()\n## $books\n## $books[[1]]\n## [1] \"https://anapioficeandfire.com/api/books/1\"\n## \n## $books[[2]]\n## [1] \"https://anapioficeandfire.com/api/books/2\"\n## \n## $books[[3]]\n## [1] \"https://anapioficeandfire.com/api/books/3\"\n## \n## $books[[4]]\n## [1] \"https://anapioficeandfire.com/api/books/8\"\n## \n## \n## $povBooks\n## list()\nfilter(book, id == as.character(i)) |&gt; print()\n## # A tibble: 4 × 3\n##   id    book                                      pov  \n##   &lt;chr&gt; &lt;chr&gt;                                     &lt;chr&gt;\n## 1 1346  https://anapioficeandfire.com/api/books/1 FALSE\n## 2 1346  https://anapioficeandfire.com/api/books/2 FALSE\n## 3 1346  https://anapioficeandfire.com/api/books/3 FALSE\n## 4 1346  https://anapioficeandfire.com/api/books/8 FALSE\ndata[[i]][c(\"mother\", \"father\", \"spouse\")] |&gt; print()\n## $mother\n## [1] \"\"\n## \n## $father\n## [1] \"\"\n## \n## $spouse\n## [1] \"https://anapioficeandfire.com/api/characters/1303\"\nfilter(relationships, id == as.character(i)) |&gt; print()\n## # A tibble: 1 × 3\n##   id    relationship_type relative_ID                                      \n##   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                                            \n## 1 1346  spouse            https://anapioficeandfire.com/api/characters/1303\ndata[[i]][c(\"tvSeries\", \"playedBy\")] |&gt; print()\n## $tvSeries\n## $tvSeries[[1]]\n## [1] \"Season 1\"\n## \n## $tvSeries[[2]]\n## [1] \"Season 2\"\n## \n## \n## $playedBy\n## $playedBy[[1]]\n## [1] \"Jason Momoa\"\nfilter(tv_show, id == as.character(i)) |&gt; print()\n## # A tibble: 2 × 3\n##   id    tvSeries playedBy   \n##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      \n## 1 1346  Season 1 Jason Momoa\n## 2 1346  Season 2 Jason Momoa\n\n\n\n\n\n\n\n[2] and [3] contain additional worked examples in R, and [4] contains an example in python for how to convert JSON/XML/API values to tabular data.\n\n\n\n\n\n\nLearn More\n\n\n\n\n\n\nR for Data Science chapter on Hierarchical Data",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#conclusions",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#conclusions",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "32.3 Conclusions",
    "text": "32.3 Conclusions\nConverting from record-based data models to relational data models is complex, in part because it depends on the structure of the data and the keys which link different tables/forms/nodes. In general, your best bet is to carefully look at the data, investigate any values you don’t understand (or values that you think may be keys to another table, but you aren’t sure), and then design a correpsonding relational table structure that makes sense for the data you have in front of you.\nWhile you’re considering how to do this, it is also important to sanity check for possible many-to-many relationships that may arise and ruin your data analysis. It’s common to make assumptions about the absence of many-to-many relationships (for instance, I did that at least twice in the demo above), but they’re usually hidden within the code and not obvious. When the data are updated, if those assumptions don’t still hold, you could end up with an analysis that doesn’t make any sense, so be careful and explicit about your assumptions with the data.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/04-xml-json-list-processing.html#references",
    "href": "part-advanced-topics/04-xml-json-list-processing.html#references",
    "title": "32  Record-based Data and List Processing Strategies",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n[1] J. D. Holster, “Chapter 7 network analysis,” in Introduction to r for data science: A LISA 2020 guidebook, 2022 [Online]. Available: https://bookdown.org/jdholster1/idsr/network-analysis.html#community-detection. [Accessed: Jul. 07, 2025]\n\n\n[2] J. Bryan, “Manipulate XML with purrr, dplyr, and tidyr.” Jun. 27, 2024 [Online]. Available: https://github.com/jennybc/manipulate-xml-with-purrr-dplyr-tidyr. [Accessed: Jul. 07, 2025]\n\n\n[3] J. Bryan, “Analyze github stuff with r.” Jul. 03, 2025 [Online]. Available: https://github.com/jennybc/analyze-github-stuff-with-r. [Accessed: Jul. 07, 2025]\n\n\n[4] Z. West, “Tabulate JSON data in python using pandas. Αlphαrithms,” Jun. 30, 2022. [Online]. Available: https://www.alpharithms.com/tabulate-json-data-python-pandas-262811/. [Accessed: Jul. 07, 2025]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Record-based Data and List Processing Strategies</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html",
    "href": "part-advanced-topics/05-APIs.html",
    "title": "33  Application Programming Interfaces",
    "section": "",
    "text": "33.1 Objectives",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#objectives",
    "href": "part-advanced-topics/05-APIs.html#objectives",
    "title": "33  Application Programming Interfaces",
    "section": "",
    "text": "Understand what APIs are and how to interface with them\nConnect to RESTful APIs with and without authentication\nRetrieve and parse data from API queries\nHandle common API challenges (pagination, rate limits, errors)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#introduction-to-apis",
    "href": "part-advanced-topics/05-APIs.html#introduction-to-apis",
    "title": "33  Application Programming Interfaces",
    "section": "\n33.2 Introduction to APIs",
    "text": "33.2 Introduction to APIs\nAn API is an application programming interface. APIs are designed to let different software and databases communicate with each other. Statisicians and data scientists often need to access APIs to acquire data from an external source. In the past, I’ve used APIs to get weather data, information about adoptable pets in my area, CRAN package statistics, and wikipedia entry edits, but other people might use them to get data on financial market performance, social media posts, and more.\n\n33.2.1 RESTful APIs\nMost APIs are RESTful – they use the REST architecture (REST stands for Representational State Transfer) [1].\n\n\n\n\n\n\nSix guiding principles of RESTful architecture\n\n\n\n\n\n\n\nUniform interface.\n\nThe interface uniquely identifies each resource involved in the interaction between the client (you) and the server (the data).\nThe server provides a uniform representation of the data that the client can use to modify the resource state.\nEach resource representation should have enough information to describe itself and additional actions that are available.\nThe client should have only the initial URI of the application, and should dynamically interact with the server through the use of hyperlinks.\n\n\nClient-Server. This enforces a separation of concerns so that the user interface (client-side) is separate from the server interface (data storage, platform, dependencies).\nStatelessness. Each request from client to server contains all information necessary to understand and complete the request. That is, the server cannot use previously stored information to complete a request. The client application must maintain the entire session state.\nCacheable. A response should (implicitly or explicitly) label itself as cacheable or non-cacheable. If cacheable, the client can reuse the response data for equivalent requests (within a certain time period).\nLayered system. The architecture is hierarchical, and each component cannot “see” beyond the immediate layer it is interacting with.\nCode on demand (optional). This allows client functionality to be extended by downloading and executing code.\n\n\n\n\nREST abstracts information into a resource, which can change state over time. At any given time, the resource state is the resource representation and consists of the data, metadata about the data, and the links that help a client transition to another state.\n\n33.2.2 URI Anatomy\nA REST API consists of interlinked resources, known as the resource model.\nThink about a set of tables in a database, where the tables are linked. A REST API is a set of syntax that allows you to access the table data using a consistent set of addresses that are called Resource URIs\nA URI (Universal Resource Identifier) is the more updated term for a URL (Universal Resource Locator). Any URI that starts with https:// or ftp:// or mailto:// is a URL – that is, all URLs are also URIs. 1\nThere are several components of an URI:\n\nthe scheme - the part before the colon, such as https, ftp, or mailto\nthe authority or endpoint - the server address, along with optional user information and port information [userinfo@]host[port].\nFor instance, RStudio’s web server is hosted by default on port 8787, so if the web host is host.com, we’d access the RStudio server by host.com:8787. By default, port 80 is used for HTTP traffic, and most web servers will map ports to different folders.\nthe path - after the server address, there may be some /folder/ paths after the address.\nSome of these paths may be more friendly ways to access different services hosted on different ports - for instance, I map :8787 to /rstudio on my web server.\nthe query or parameter string - after the path, there may be a string with ?.... Often, this will have key=value pairs separated by &.\nthe fragment #... that links to a specific part of the page\n\nURIs cannot accommodate certain characters, such as spaces – these must be encoded using ASCII values [3]. %20 is the equivalent for a space, which is why you will often see it in URLs. Not all values in the linked ASCII table need to be encoded – this usually applies to certain special characters like /, -, , (, and ).\nWhile most RESTful applications use HTTP methods and an API built around what look like web addresses, this is not required.\n\n33.2.3 HTTP Methods\nThere are several HTTP methods which are commonly used to interact with APIs [4]:\n\nGET - requests the representation of a resource. These requests should only retrieve data and should not contain a request content. Your web browser submits a GET request every time you type an address in the address bar.\nPOST - submits an entity to a resource. This may cause a change in state or side effects on the server. For instance, submitting a web form may create a POST request that uploads your data to the server and updates a database.\nPUT - this method replaces all representations of the target resource with the request content. This updates existing data (that is, be careful!)\nDELETE - this method removes all of the target resource data.\n\nHTTP requests have a request header and a request body.\n\n33.2.4 HTTP Response Codes\nHTTP requests will return a response code.\nI prefer the very general summary [5]:\n\n1xx Here’s what I’ll do.\n2xx Here’s what you want.\n3xx I don’t have what you want but I know who does.\n4xx It didn’t work but it’s your fault.\n5xx It didn’t work and it’s my fault.\n\nYou can find a full list here. You’re probably familiar with a 404 error, which is “File not found” – that is, you passed in an address that isn’t valid.\n\n\nJulia Evans [6] has some lovely Q&A flashcards on HTTP codes, and there is a cartoon-style zine for sale that is also very nice.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#accessing-apis-without-authentication",
    "href": "part-advanced-topics/05-APIs.html#accessing-apis-without-authentication",
    "title": "33  Application Programming Interfaces",
    "section": "\n33.3 Accessing APIs without Authentication",
    "text": "33.3 Accessing APIs without Authentication\n\n\n\n\n\n\nDemo: CURL and HTTP Requests\n\n\n\nLet’s try this out using curl, which is a command-line tool for URI manipulation and transfers. We’ll send a request to an API that takes a person’s name as a parameter and returns information about the likely gender of that person. Explore the API documentation.\nI’ve told curl to be verbose (to show us everything).\n\ncurl -v https://api.genderize.io?name=Terry &gt; ../data/genderize-api-response.txt 2&gt; ../data/genderize-api-curl-output.txt\n\nThe first &gt; redirects the response/output to genderize-api-response.txt; 2&gt; redirects the messages/errors/warnings to genderize-api-curl-output.txt. This allows us to examine the response and the output separately.\nThe first few lines of the output (1-42) are what’s required to make a connection using SSL (essentially, when you use https:// instead of http://, you add an extra security layer where the protocol verifies that the server is certified to be what it claims to be).\nThen, there’s a GET request:\n* Connected to api.genderize.io (165.227.126.8) port 443\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://api.genderize.io/?name=Terry\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: api.genderize.io]\n* [HTTP/2] [1] [:path: /?name=Terry]\n* [HTTP/2] [1] [user-agent: curl/8.11.0]\n* [HTTP/2] [1] [accept: */*]\n} [5 bytes data]\n&gt; GET /?name=Terry HTTP/2\n&gt; Host: api.genderize.io\n&gt; User-Agent: curl/8.11.0\n&gt; Accept: */*\n&gt; \n{ [5 bytes data]\n* Request completely sent off\n{ [5 bytes data]\nThis splits the URI up into a Host (api.genderize.io) and a query string (/?name=Terry) containing parameter(s) (name) and value(s) (Terry) with an associated protocol (HTTP/2). Curl identifies itself as having the user-agent curl/8.11.0, and provides some idea of what responses it’s looking for - in this case, */*, which is internet for “anything”. At this point, we’re not picky, and in any case, most sites have a default or use JSON exclusively.\nThe output also contains a set of header information that is easier for us to read when it is formatted as a list. Remember, HTTP responses have a header and a body – this is a formatted version of the header of the server response.\n\nhttp/2 200 (response code)\nserver: nginx/1.16.1 (web server software version)\ndate (varies)\ncontent-type: application/json; charset=utf-8 (expect a JSON file with encoding UTF-8)\ncontent-length: 65 (how many characters the content is)\nvary: accept-encoding\ncache-control: max-age=0, private, must-revalidate\nx-request-id (varies)\naccess-control-allow-credentials: true (whether you’re allowed to make requests to the server)\naccess-control-allow-origin: *\naccess-control-expose-headers: x-rate-limit-limit, x-rate-limit-remaining, x-rate-limit-reset (values determining whether we’re putting too much load on the server)\nx-rate-limit-limit: 100\nx-rate-limit-remaining: 91 (varies)\nx-rate-limit-reset: 10538\n\nThen, we look at the body of the server response\n{\"count\":89955,\"name\":\"Terry\",\"gender\":\"male\",\"probability\":0.75}\nNotice that this has 65 characters and is in JSON format. We get a frequency (count), the name we were checking up on, the likely gender, and the probability that someone with that name is of that gender (in this case, male).\n\n\n\n\n\n\n\n\nExample: CURL requests in R and Python\n\n\n\n\n\nTask\nR\nPython\n\n\n\nBoth R and Python use CURL as a backend to interact with the internet at large, but it is often easier to send requests using a request-specific library, such as httr2.\nConnect to the Evil Insult Generator API and get one insult from R in Greek (lang=el) in XML (type=xml) and one from Python in Russian (lang=ru), formatted as JSON (type=json).\nNotes:\n\nI have no idea how evil/off-color these insults might be, but the idea of an API just to insult you (in many languages) is amusing enough to demonstrate even at the risk that it is crude.\nI’ve set the task in non-English languages in the hopes of minimizing the offense (though the comment in the response gives a rough translation). If you are fluent in Russian or Greek, please feel free to substitute Spanish (es), German (de), French (fr), Greek (el), Russian (ru), Chinese (zh), Hindi (hi), Polish (pl), or another language of your choice.\nI think this API is using ISO 639 2-letter language codes, and it will return a string of asterisks if it doesn’t understand your language – it doesn’t seem to have insults in every possible language, but I’ve verified a few modern/common ones.\n\n\n\nMake sure you have the httr2 library installed (it’s a dependency of a lot of tidyverse packages, so you may already have it).\n\nlibrary(httr2)\nlibrary(xml2)\n\nreq &lt;- request(\"https://evilinsult.com/generate_insult.php/?lang=el&type=xml\") |&gt;\n  req_perform()\n\nresp_raw(req) # response header and body\n## HTTP/1.1 200 OK\n## x-powered-by: PHP/7.4.33\n## content-type: text/xml;charset=UTF-8\n## cache-control: public, max-age=172800\n## expires: Tue, 23 Sep 2025 15:08:25 GMT\n## content-length: 188\n## content-encoding: br\n## vary: Accept-Encoding,User-Agent\n## date: Sun, 21 Sep 2025 15:08:25 GMT\n## server: LiteSpeed\n## alt-svc: h3=\":443\"; ma=2592000, h3-29=\":443\"; ma=2592000, h3-Q050=\":443\"; ma=2592000, h3-Q046=\":443\"; ma=2592000, h3-Q043=\":443\"; ma=2592000, quic=\":443\"; ma=2592000; v=\"43,46\"\n## \n## &lt;?xml version=\"1.0\"?&gt;\n## &lt;insult_info&gt;\n##   &lt;number&gt;20&lt;/number&gt;\n##   &lt;language&gt;el&lt;/language&gt;\n##   &lt;insult&gt;&#x39A;&#x3B1;&#x3C1;&#x3B9;&#x3CC;&#x3BB;&#x3B1;&lt;/insult&gt;\n##   &lt;created&gt;2025-09-21 17:00:27&lt;/created&gt;\n##   &lt;shown&gt;1179&lt;/shown&gt;\n##   &lt;createdby&gt;emorfili&lt;/createdby&gt;\n##   &lt;active&gt;1&lt;/active&gt;\n##   &lt;comment&gt;Cunt&lt;/comment&gt;\n## &lt;/insult_info&gt;\nxml_resp &lt;- resp_body_string(req) # response body, encoded as a string\n\nread_xml(xml_resp)\n## {xml_document}\n## &lt;insult_info&gt;\n## [1] &lt;number&gt;20&lt;/number&gt;\n## [2] &lt;language&gt;el&lt;/language&gt;\n## [3] &lt;insult&gt;Καριόλα&lt;/insult&gt;\n## [4] &lt;created&gt;2025-09-21 17:00:27&lt;/created&gt;\n## [5] &lt;shown&gt;1179&lt;/shown&gt;\n## [6] &lt;createdby&gt;emorfili&lt;/createdby&gt;\n## [7] &lt;active&gt;1&lt;/active&gt;\n## [8] &lt;comment&gt;Cunt&lt;/comment&gt;\n\n\n\nIn Python, make sure you have the requests library installed.\n\nimport requests\n\nreq = requests.get(\"https://evilinsult.com/generate_insult.php/?lang=ru&type=json\")\nreq.headers\n## {'Connection': 'Keep-Alive', 'Keep-Alive': 'timeout=5, max=100', 'X-Powered-By': 'PHP/7.4.33', 'Content-Type': 'application/json', 'Cache-Control': 'public, max-age=172800', 'Expires': 'Tue, 23 Sep 2025 15:08:26 GMT', 'Content-Length': '215', 'Content-Encoding': 'gzip', 'Vary': 'Accept-Encoding,User-Agent', 'Date': 'Sun, 21 Sep 2025 15:08:26 GMT', 'Server': 'LiteSpeed', 'alt-svc': 'h3=\":443\"; ma=2592000, h3-29=\":443\"; ma=2592000, h3-Q050=\":443\"; ma=2592000, h3-Q046=\":443\"; ma=2592000, h3-Q043=\":443\"; ma=2592000, quic=\":443\"; ma=2592000; v=\"43,46\"'}\nreq.json() # python will format JSON as a dict, which is easier to work with\n## {'number': '905', 'language': 'ru', 'insult': 'Блядемудинный пиздопроёб!', 'created': '2025-09-21 16:46:55', 'shown': '62289', 'createdby': 'Neriman', 'active': '1', 'comment': 'just an expression without a direct meaning'}\n\n\n\n\nInterestingly, you can also pass the undocumented parameter number and get a non-random insult, but this seems to only work in the web browser - the API appears to return a random insult even when number is specified.\n\n\n\n\n\n\n\n\nExample: Weather Data\n\n\n\nThe National Weather Service provides a free API for public weather data access. The only requirement is that you provide a user-agent identifying your application and including contact information (an email address). Assemble the temperature forecast data for Moore, OK (lat = 35.339508, long = -97.486702) and plot it using a line graph. You may need to make several requests to get the data you want.\n\n\nR\nPython\n\n\n\n\nlibrary(httr2)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\nuastring &lt;- \"R/python data science demos, svanderplas2@unl.edu\" # Set this to YOUR email, please!\n\npointsreq &lt;- request(\"https://api.weather.gov/points/35.339508,-97.486702\") |&gt;\n  req_user_agent(uastring) |&gt;\n  req_perform()\n\npointsjson &lt;- resp_body_json(pointsreq)\nargs &lt;- pointsjson$properties[c(\"gridId\", \"gridX\", \"gridY\")] # This will get us the forecast info\n\nreq_url_str &lt;- sprintf(\"https://api.weather.gov/gridpoints/%s/%s,%s/forecast/hourly\", args[1], args[2], args[3])\nforecastreq &lt;- request(req_url_str) |&gt;\n  req_user_agent(uastring) |&gt;\n  req_perform()\n\nforecastjson &lt;- resp_body_json(forecastreq)\nforecastjson$properties$periods[[1]]\n## $number\n## [1] 1\n## \n## $name\n## [1] \"\"\n## \n## $startTime\n## [1] \"2025-09-21T10:00:00-05:00\"\n## \n## $endTime\n## [1] \"2025-09-21T11:00:00-05:00\"\n## \n## $isDaytime\n## [1] TRUE\n## \n## $temperature\n## [1] 72\n## \n## $temperatureUnit\n## [1] \"F\"\n## \n## $temperatureTrend\n## [1] \"\"\n## \n## $probabilityOfPrecipitation\n## $probabilityOfPrecipitation$unitCode\n## [1] \"wmoUnit:percent\"\n## \n## $probabilityOfPrecipitation$value\n## [1] 46\n## \n## \n## $dewpoint\n## $dewpoint$unitCode\n## [1] \"wmoUnit:degC\"\n## \n## $dewpoint$value\n## [1] 18.88889\n## \n## \n## $relativeHumidity\n## $relativeHumidity$unitCode\n## [1] \"wmoUnit:percent\"\n## \n## $relativeHumidity$value\n## [1] 81\n## \n## \n## $windSpeed\n## [1] \"10 mph\"\n## \n## $windDirection\n## [1] \"SSW\"\n## \n## $icon\n## [1] \"https://api.weather.gov/icons/land/day/tsra,50?size=small\"\n## \n## $shortForecast\n## [1] \"Chance Showers And Thunderstorms\"\n## \n## $detailedForecast\n## [1] \"\"\n\nmoore_temps &lt;- tibble(\n  start_time = map_vec(forecastjson$properties$periods, \"startTime\"),\n  temp = map_vec(forecastjson$properties$periods, \"temperature\")) |&gt;\n  mutate(start_time = ymd_hms(start_time))\n\nggplot(moore_temps, aes(x = start_time, y = temp)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nimport requests\nimport pandas as pd\nimport seaborn.objects as so\n\n# PLEASE edit this to have your email address before you run the code.\nheaders = {'User-Agent': 'R/python data science API demos', 'From': 'svanderplas2@unl.edu'} \n\npointsreq = requests.get(\"https://api.weather.gov/points/35.339508,-97.486702\", headers=headers)\npointsreq.headers\n## {'Server': 'nginx/1.20.1', 'Content-Type': 'application/geo+json', 'Access-Control-Allow-Origin': '*', 'Access-Control-Expose-Headers': 'X-Correlation-Id, X-Request-Id, X-Server-Id', 'X-Request-ID': '55c70274-ac03-4e2a-9545-33c017fb2836', 'X-Correlation-ID': '4c67a22', 'X-Server-ID': 'vm-bldr-nids-apiapp9.ncep.noaa.gov', 'Content-Encoding': 'gzip', 'Content-Length': '799', 'Cache-Control': 'public, max-age=85923, s-maxage=120', 'Expires': 'Mon, 22 Sep 2025 15:00:32 GMT', 'Date': 'Sun, 21 Sep 2025 15:08:29 GMT', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding, Accept,Feature-Flags,Accept-Language', 'X-Edge-Request-ID': '4d23a06e', 'Strict-Transport-Security': 'max-age=31536000 ; includeSubDomains ; preload'}\npoints_json = pointsreq.json()\ntarget_url = points_json['properties']['forecast']+'/hourly'\n\n\nforecastreq = requests.get(target_url, headers=headers)\nforecast_json = forecastreq.json()\nforecast_df = pd.DataFrame(forecast_json['properties']['periods'])\nforecast_temp = forecast_df[['startTime', 'temperature']]\n\nso.Plot(forecast_temp, \"startTime\", \"temperature\").add(so.Line()).show()\n\n\n\n\n\n\n\nThe x-axis is a bit of a mess because I haven’t spent the time to format it properly, but that’s good enough for now.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#api-authentication",
    "href": "part-advanced-topics/05-APIs.html#api-authentication",
    "title": "33  Application Programming Interfaces",
    "section": "\n33.4 API Authentication",
    "text": "33.4 API Authentication\nMany APIs require authentication to get response codes other than 4xx (unauthorized). Authentication helps API providers ensure that it’s more difficult for users to spam the service and tie up resources. In some cases, you may pay to access an API, and the API may track how many requests you make or how much server time you use in order to bill you appropriately2.\nThere are two common methods APIs use for authentication: key-based authentication, and OAuth 2.0 authentication. In addition to these methods, some APIs implement their own authentication systems that aren’t standard.\nChances are, you’ll have to read the API documentation to figure out how to authenticate, even if they are using a relatively common authentication method.\n\n33.4.1 Storing Secrets Securely\nWhether you’re using an API key or OAuth2.0 or some other authentication method, it’s important to make sure you store your secrets (passwords, logins, keys, tokens) securely [7]. There is always a balance between security and convenience (ease of access), though. I don’t guard my password for reddit in the same way that I guard my bank account information or my SSN, because the worst someone can do on reddit with my username is post nonsense or offensive stuff – they can’t bankrupt me.\nThe solutions for secret storage here are presented roughly from less to more secure (or from more to less convenient). All of these storage methods are preferable to putting the secret directly into your script.\n\n33.4.1.1 Environment files\nMost programs have the ability to read in secrets or environment variables using a separate file. The dot at the beginning of the file name indicates that the file is hidden on UNIX-based systems – just an additional tiny bit of security. In R, secrets can be stored in the .Renviron file [8], and Python uses the .env file (using this requires the python-dotenv package, available via pip). The usethis package in R can help you manage your .Renviron file, if you When you create an environment file in your project directory, you should immediately add it to your .gitignore file, so that git never sees the file and there’s no risk that you accidentally add it to your git repository. Even better, you can use the global .gitignore file and ensure that git NEVER sees the file.\nYou will have to recreate this file on each computer you’re using, though, since it isn’t backed up in git. I will often save my API keys to both the environment file and to my password manager (KeePassXC) so that I have the key stored in an encrypted location that syncs across all of my computers in addition to an environment file.\nEnvironment variables are stored in the form key=value. If your value has spaces, you may want to store them as key=\"value with spaces\" to avoid trouble, but you may not have to do so. Your keys should not have spaces.\n\n\n\n\n\n\n33.4.1.1.1 Demo: Environment File Secret Storage and Access\n\n\n\n\n\nR\nPython\n\n\n\n\nif(!\"usethis\" %in% installed.packages()){\n  install.packages(\"usethis\")\n} \n\nYou can manually create and edit the .Renviron file, or you can use the usethis package to do so. The advantage of the usethis package is that it makes it much easier to find the user-level .Renviron file. Personally, I try to use project-level secrets wherever possible, but some things are used across so many projects that it’s easier just to keep them in one place, like my GitHub access token.\n\nlibrary(usethis)\nusethis::edit_r_environ(\"project\") # Store in project folder/working dir\nusethis::edit_r_environ(\"user\") # Store at user level, accessible from any project\n\nAdd these lines to your .Renviron file:\nAPI_KEY=test-key\nAPI_SECRET=test-secret\nR will automatically load environment variables when the R session is started, but you do need to restart your session after you create or update the .Renviron file in order to access new environment variables.\nYou can then access the values using Sys.getenv(\"secret-name\")\n\nSys.getenv(\"API_KEY\")\n## [1] \"test-key\"\nSys.getenv(\"API_SECRET\")\n## [1] \"test-secret\"\n\n\n\n\n%pip install python-dotenv\n\nIn a .env file in the project directory, add these lines:\nAPI_KEY=test-key\nAPI_SECRET=test-secret\nor, run the following code in a UNIX-compatible terminal (Windows users - use git bash):\n\necho -ne \"API_KEY=test-key\\nAPI_SECRET=test-secret\" &gt;&gt; .env\n\nThen, we need to tell Python to check that file using dotenv.load_dotenv()\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\n## False\n\nSecrets in the .env file can then be accessed by name.\n\napi_key = os.getenv(\"API_KEY\")\napi_secret = os.getenv(\"API_SECRET\")\n\nprint(\"API_KEY: \", api_key)\n## API_KEY:  test-key\nprint(\"API_SECRET: \", api_secret)\n## API_SECRET:  test-secret\n\nYou can also access a dictionary of secrets using the dotenv.dotenv_values() function.\n\nfrom dotenv import dotenv_values\nsecrets = dotenv_values(\".env\")\n\nsecrets['API_KEY']\n## KeyError: 'API_KEY'\nsecrets['API_SECRET']\n## KeyError: 'API_SECRET'\n\n\n\n\n\n\n\n33.4.1.2 System Credential Storage\nAll major operating systems ( ) have a system password store available. Both R and Python can interface with this system password storage directly [9], [10], ensuring that your passwords are not stored in plaintext anywhere, including on your machine.\nYou will need the keyring package in both R and Python to use this functionality.\n\n\n\n\n\n\nDemo: System Keyring Secret Storage\n\n\n\n\n\nR\nPython\n\n\n\n\ninstall.packages(\"keyring\")\n\nWith this method, you will typically interactively set the secret value the first time.\n\nlibrary(keyring)\n# Run in an interactive session\nkey_set(service = \"default\", \"API_KEY\")\n\nRunning this command will pop up a window for you to enter the key value directly.\nThen, to get the secret back from the keyring, you just need to run key_get(\"secret-name\").\n\nkey_get(service = \"default\", \"API_KEY\")\n\n\n## [1] \"test-key\"\n\nIf you want to list out the keys which are available using a specific service (names only, no values), you can use key_list(service = \"your-service\").\n\n\n\n\n\n\nNon-default backends\n\n\n\n\n\nYou can set a different backend for storing your secrets if you would like to do so. In most cases, the operating system default is fine, but for instance on my machine, the default is to use environment variables (which is perhaps only slightly more secure than using the environment configuration files in the previous section). As it would also be perfectly reasonable to use my system keyring (KDE Wallet or Seahorse + gnome-keyring, in my case), I could configure the package to use that service instead.\n\n\n\n\n\n\n%pip install keyring\n\n\nimport keyring as kr\n\n# Run this in the python console interactively to leave no trace\nkr.set_password(\"test-service\", \"API_KEY\", \"test-key\")\n\nkr.get_password(\"test-service\", \"API_KEY\")\n## 'test-key'\n\n\n\n\n\n\n\n33.4.1.3 Additional Options\nThere are other options to store secrets, including cloud secret storage, that are used at the enterprise level but may not be that helpful for you right now. I also found a KeePassXC package for python, but couldn’t find a similar one for R.\nIf you are using Github Actions to build things, you can emulate environment variable secret storage using custom environment variables for your actions. To my knowledge, you can’t easily use system credential storage in a continuous integration environment.\n\n33.4.2 API Keys\nThe simplest method of API authentication is a key-based system. You sign up with the API and they issue you (or your application) a key – a sequence of characters that uniquely identifies your user or application.\nKeys can be used in two different ways: you either pass the key in with every query (very simple), or you use your key at the start to obtain a token, which you then include with every query or pass along in the request header.\nTo determine how the key should be used, you will have to read the API documentation. Most APIs will provide you with examples of how you would submit certain queries, so you can often figure out the authentication process from those examples.\n\n\n\n\n\n\n33.4.2.1 Demo: API Authentication with NewsAPI\n\n\n\nNewsAPI provides access to articles and headlines from around the world. Anyone can sign up for an API key that allows 1000 requests per day while you are developing the application (commercial projects and projects which are “in production” require a paid license).\nSign up for an API key so that you can follow along with this demonstration. Save your API key using your preferred method (or both of the methods) discussed above, using the service “NEWSAPI” and the username “NEWSAPI_KEY”.\nThe homepage of the API provides several example GET queries (replace &lt;your key&gt; with your API key):\n\nTop business headlines in the US right now: GET https://newsapi.org/v2/top-headlines?country=us&category=business&apiKey=&lt;your key&gt;\nTop headlines from TechCrunch right now: GET https://newsapi.org/v2/top-headlines?sources=techcrunch&apiKey=&lt;your key&gt;\nAll articles published by the Wall Street Journal in the past 6 months, sorted by recent first: https://newsapi.org/v2/everything?domains=wsj.com&apiKey=&lt;your key&gt;\n\nThese examples are often enough to give you a sense of how to construct a query, but for more detail, most sites also provide documentation.\nFor instance, if we look at the Authentication page, you can see that there are 3 options for using your API key:\n\npassed in using the apiKey querystring parameter\nvia the X-Api-Key HTTP header\nvia the Authorization HTTP header\n\nThe last two are preferable because they don’t show up in logs, as they’re hidden in the header rather than the URI.\n\n\nR\nPython\n\n\n\n\nkey &lt;- Sys.getenv(\"NEWSAPI_KEY\")\n\nlibrary(httr2)\nuastring &lt;- \"R/python data science demos, svanderplas2@unl.edu\" # Set this to YOUR email, please!\n\nquery &lt;- \"https://newsapi.org/v2/everything?domains=wsj.com&language=en\"\n\n# Option 1: Pass the API Key in using the URI\nres1 &lt;- request(paste0(query, \"&apiKey=\", key)) |&gt;\n  req_perform()\n# Redacting apiKey from various places before printing the object\nres1$url &lt;- gsub(key, \"&lt;your key&gt;\", res1$url)\nres1$request$url &lt;- gsub(key, \"&lt;your key&gt;\", res1$request$url)\nres1\n## &lt;httr2_response&gt;\n## GET https://newsapi.org/v2/everything?domains=wsj.com&language=en&apiKey=&lt;your key&gt;\n## Status: 200 OK\n## Content-Type: application/json\n## Body: In memory (20460 bytes)\n\nres2 &lt;- request(query) |&gt;\n  req_user_agent(uastring) |&gt;\n  req_headers_redacted(\"X-Api-Key\" = key) |&gt;\n  req_perform()\nres2\n## &lt;httr2_response&gt;\n## GET https://newsapi.org/v2/everything?domains=wsj.com&language=en\n## Status: 200 OK\n## Content-Type: application/json\n## Body: In memory (20460 bytes)\n\n\nres3 &lt;- request(query) |&gt;\n  req_user_agent(uastring) |&gt;\n  req_headers_redacted(\"Authorization\" = key) |&gt;\n  req_perform()\nres3\n## &lt;httr2_response&gt;\n## GET https://newsapi.org/v2/everything?domains=wsj.com&language=en\n## Status: 200 OK\n## Content-Type: application/json\n## Body: In memory (20460 bytes)\n\nThen, we can analyze the response body and re-format it to get rectangular data.\n\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nresponse &lt;- resp_body_string(res1) |&gt;\n  parse_json(simplifyVector = T)\nresponse &lt;- response$articles |&gt;\n  unnest_wider(\"source\")\nhead(response)\n## # A tibble: 6 × 9\n##   id         name  author title description url   urlToImage publishedAt content\n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;  \n## 1 the-wall-… The … Adrià… Roch… The deal s… http… https://s… 2025-09-18… \"The d…\n## 2 the-wall-… The … Garet… Tesl… The move s… http… https://s… 2025-09-15… \"Tesla…\n## 3 the-wall-… The … Telis… The … Stock- and… http… https://s… 2025-09-15… \"T. Ro…\n## 4 the-wall-… The … David… Gold… A 39% pric… http… https://s… 2025-09-15… \"Presi…\n## 5 the-wall-… The … Telis… Mort… It’s about… http… https://s… 2025-09-14… \"Getti…\n## 6 the-wall-… The … Ben D… CVC … The buyout… http… https://s… 2025-09-12… \"Betti…\n\n\n\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\n## False\nkey = os.environ.get('NEWSAPI_KEY', \"\")\n\nimport requests\n\nquery = \"https://newsapi.org/v2/everything?domains=wsj.com&language=en\"\nreq1 = requests.get(query+\"&apiKey=\"+key)\nreq2 = requests.get(query, headers={\"X-Api-Key\": key})\nreq3 = requests.get(query, headers={\"Authorization\": key})\n\nThen, we can analyze the response body and re-format it to get rectangular data.\n\nimport pandas as pd\n\nresponse = pd.json_normalize(req3.json()['articles'])\nresponse\n##                              author  ...              source.name\n## 0                   Adrià Calatayud  ...  The Wall Street Journal\n## 1                     Gareth Vipers  ...  The Wall Street Journal\n## 2                       Telis Demos  ...  The Wall Street Journal\n## 3        David Uberti and Ben Eisen  ...  The Wall Street Journal\n## 4                       Telis Demos  ...  The Wall Street Journal\n## 5     Ben Dummett and Lauren Thomas  ...  The Wall Street Journal\n## 6                   Chelsey Dulaney  ...  The Wall Street Journal\n## 7           The Wall Street Journal  ...  The Wall Street Journal\n## 8   Corrie Driebusch and Sherry Qin  ...  The Wall Street Journal\n## 9                  Christopher Otts  ...  The Wall Street Journal\n## 10                Owen Tucker-Smith  ...  The Wall Street Journal\n## 11                    Dan Gallagher  ...  The Wall Street Journal\n## 12                       Carol Ryan  ...  The Wall Street Journal\n## 13                      John Gruber  ...  The Wall Street Journal\n## 14                       Sherry Qin  ...  The Wall Street Journal\n## 15                      Connor Hart  ...  The Wall Street Journal\n## 16                     Joshua Kirby  ...  The Wall Street Journal\n## 17                   Heidi Mitchell  ...  The Wall Street Journal\n## 18   Sarah Nassauer and Connor Hart  ...  The Wall Street Journal\n## 19                       Mauro Orru  ...  The Wall Street Journal\n## 20                      Joe Wallace  ...  The Wall Street Journal\n## 21                    Kelly Cloonan  ...  The Wall Street Journal\n## 22                     Ray A. Smith  ...  The Wall Street Journal\n## \n## [23 rows x 9 columns]\n\n\n\n\n\n\n\n33.4.3 OAuth 2.0\n\n\n\n\n\n\n\nUnder Construction\n\n\n\nThis section is still under construction, as I’ve had trouble finding a ton of compelling examples of why you should use OAuth instead of API key authentication.\nYou should, however, understand the basic premise of OAuth authentication, even if we don’t use it to access data.\n\n\nKey-based authentication is reasonably secure, but it is hard to guard against keys being shared or used by others if they are exposed. Just as passwords are considered less secure than two-factor authentication, API keys are considered less secure than OAuth2.0, a method which requires multiple steps to authenticate:\n\nUser log in\nAn authorization code is generated for the application\nThe authorization code is exchanged for a limited time access token\nThe token is used to access the API\n\nIn addition, API keys do not have any fine-grained way of ensuring that certain users have access to only certain resources. OAuth2.0 allows generation of fine-grained, limited scope permissions – for instance, you might be allowed to read information but not write it, or read only information relating to certain variables.\nYou’re already likely familiar with OAuth2.0, because that’s how the “Sign in with Google/Facebook/Github” links work on various sites (here’s a list of OAuth providers). You authorize the application to see that you have a pre-existing profile from another site, and you may also allow the application to see certain information (name, profile picture, email address, …). The application is given an access token that grants it limited access to your pre-existing profile information, and that is used to allow you access to the application without creating a new profile.\n\n\nA nice explanation of how OAuth2.0 works with cute shapes [11]\nAs you might expect, preparing to use OAuth 2.0 authentication is a bit more complicated than just getting an API key. In addition, it will vary by API, so you need to carefully read the documentation. I can’t provide a demo that is going to work for all APIs out there, unfortunately.\n\n\n\n\n\n\n33.4.3.1 Demo: Using Github’s API\n\n\n\n\nNavigate to https://github.com/settings/developers and click on OAuth Apps\nCreate a New Oauth App by clicking the button in the top right corner\n\n\n\n\n\n\nFigure 33.1: Initial steps to create a github OAuth application.\n\n\n\nFill in the OAuth application form. OAuth is usually used for web applications, so some of these values don’t quite make sense for our data-gathering purposes – and that’s ok.\n\nApplication name: oauth-testing is a good option\nHomepage URL: https://github.com/&lt;username&gt; (fill in your username) is a good option\nAuthorization Callback URL: Use the same URL as you used above.\nClick Register when you’re done.\n\n\n\n\n\n\n\n\nFigure 33.2: Github application registration value example\n\n\n\nYou should be redirected to your application page, and it should look something like Figure 33.3 (I’ve censored my client ID, but yours should be readable). Click on the ‘Generate a new client secret’ button. Using your preferred secret storage method, save both your client ID and client Secret to your computer – if using a keyring, providing an ID of “GH_API” along with a username of “CLIENT_ID” and a password of “CLIENT_SECRET” should work. If using an environment file, just save GH_APP_ID=... and GH_APP_SECRET=... separately.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 33.3: Github application settings page. Generate a client secret.\n\n\n\n\n\n\nGithub application settings page after the client secret has been generated. Record the client ID and generated secret values to your secret storage mechanism of choice.\n\n\n\n\n\nScroll to the bottom of your app page and select ‘Enable Device Flow’ - this will hopefully allow you to use\nUse the documentation (or LLMs like Copilot/GPT) to figure out how to authenticate for your specific API. Note that LLMs will be most useful for common APIs.\n\n\n\nR\nPython\n\n\n\nI’m using environment variables to store my client ID and secret, so I will access them like this:\n\nclient_id &lt;- Sys.getenv(\"GH_APP_ID\")\nsecret &lt;- Sys.getenv(\"GH_APP_SECRET\")\n\n\nlibrary(httr2)\n\n# Step 1: Define the OAuth client\nclient &lt;- oauth_client(\n  id = client_id,            # Replace with your GitHub App's client ID\n  secret = secret,    # Replace with your GitHub App's client secret\n  token_url = \"https://github.com/login/oauth/access_token\"\n)\n\nThis next bit is interactive, so I can’t really easily demonstrate it for you here. Sorry!\n\n# Step 2: Set up the authorization flow\nauth &lt;- oauth_flow_auth_code(\n  client = client,\n  auth_url = \"https://github.com/login/oauth/authorize\",\n  scope = \"repo\"                    # Example scope: access to repo\n)\n\nIf this succeeds, then you’ll be able to submit the next request successfully and get Hadley Wickham’s user information.\n\n# Step 3: Create a request using the token\nreq &lt;- request(\"https://api.github.com/users/hadley\") |&gt;\n  req_auth_bearer_token(auth$access_token)\n\n# Step 4: Perform the request\nresp &lt;- req_perform(req)\n\n\n# Step 5: Print the response\nresp |&gt; \n  resp_body_json() |&gt;\n  head()\n## $login\n## [1] \"hadley\"\n## \n## $id\n## [1] 4196\n## \n## $node_id\n## [1] \"MDQ6VXNlcjQxOTY=\"\n## \n## $avatar_url\n## [1] \"https://avatars.githubusercontent.com/u/4196?v=4\"\n## \n## $gravatar_id\n## [1] \"\"\n## \n## $url\n## [1] \"https://api.github.com/users/hadley\"\n\n\n\nYou will need the requests package for this demo, which you can obtain with pip install requests.\n\nfrom requests_oauthlib import OAuth2Session\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Step 1: Define your credentials\nclient_id = os.getenv(\"GH_APP_ID\")\nclient_secret = os.getenv(\"GH_APP_SECRET\")\nredirect_uri = \"https://localhost\"  # Must match your app settings\n\n# Step 2: Define OAuth endpoints\nauthorization_base_url = \"https://github.com/login/oauth/authorize\"\ntoken_url = \"https://github.com/login/oauth/access_token\"\n\n# Step 3: Create an OAuth2 session\ngithub = OAuth2Session(client_id, redirect_uri=redirect_uri, scope=[\"repo\"])\n\n# Step 4: Redirect user to GitHub for authorization\nauthorization_url, state = github.authorization_url(authorization_base_url)\n\n\nprint(f\"Visit this URL and authorize: {authorization_url}\")\n## Visit this URL and authorize: https://github.com/login/oauth/authorize?response_type=code&client_id=Ov23li2ClKXHmZzjlJik&redirect_uri=https%3A%2F%2Flocalhost&scope=repo&state=OV6agqUeiFjyXciNqSr0bnY7ZB4FuL\n\n# Step 5: Get the authorization response URL manually (paste it into the quotes)\nredirect_response = \"https://localhost/?code=f188b7e06bf0dca942e8&state=OV6agqUeiFjyXciNqSr0bnY7ZB4FuL\"\n\n\n# Step 6: Fetch the access token\ntoken = github.fetch_token(token_url, client_secret=client_secret,\n                           authorization_response=redirect_response)\n\n# Step 7: Make an authenticated API call\nresponse = github.get(\"https://api.github.com/users/hadley\")\n\n\nprint(response.json())\n## AttributeError: 'DataFrame' object has no attribute 'json'\n\n\n\n\nNote that in both cases, the OAuth libraries (httr2, requests_oauthlib) handle passing the token to the requests - we don’t have to do that manually.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#best-practices",
    "href": "part-advanced-topics/05-APIs.html#best-practices",
    "title": "33  Application Programming Interfaces",
    "section": "\n33.5 Best Practices",
    "text": "33.5 Best Practices\nWhen working with APIs, make sure to:\n\nRead the API documentation carefully\nTest with small requests first before scaling up\n\nIf something doesn’t work in R/Python, test with curl or in a browser\n\n\nSecure your API keys using keyrings or environment variables. Do NOT push your API keys to GitHub!!!\nCache responses to limit server overload\nWrite reusable functions to make API calls where possible",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#fun-apis-to-explore",
    "href": "part-advanced-topics/05-APIs.html#fun-apis-to-explore",
    "title": "33  Application Programming Interfaces",
    "section": "\n33.6 Fun APIs To Explore",
    "text": "33.6 Fun APIs To Explore\n\nPokeAPI - An API for accessing all of the Pokemon data you could ever want, and Pokemon TCG if you prefer the card game over the critters themselves.\nHolidayAPI - An API of holidays in every country worldwide.\nCheapShark - price data for digital PC games from different stores and sales.\nUPC Database - access UPCs for millions of products around the world\nPurpleAir - real time air quality monitoring network\nOpenChargeMap - Electric vehicle charging stations, worldwide.\nNASA Open APIs - one key, hundreds of different possibilities, including imagery.\nPolygon.io API for financial data\nPublic API List – a massive list of public APIs",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#references",
    "href": "part-advanced-topics/05-APIs.html#references",
    "title": "33  Application Programming Interfaces",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nL. Gupta, “What is REST? REST API tutorial,” Apr. 01, 2025. [Online]. Available: https://restfulapi.net/. [Accessed: Jul. 07, 2025]\n\n\n[2] \nB. Zimmerman, “URI vs. URL. Bernie zimmerman,” May 14, 2020. [Online]. Available: https://web.archive.org/web/20200514092752/http://www.bernzilla.com/item.php?id=100. [Accessed: Jul. 07, 2025]\n\n\n[3] \nW3Schools, “HTML URL encoding reference,” 2025. [Online]. Available: https://www.w3schools.com/tags//ref_urlencode.asp. [Accessed: Jul. 07, 2025]\n\n\n[4] \nMozilla Documentation Network, “HTTP request methods. Mdn web docs,” Jul. 04, 2025. [Online]. Available: https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Methods. [Accessed: Jul. 07, 2025]\n\n\n[5] \n\n“All HTTP status codes are Explained,” r/ProgrammerHumor. May 2023 [Online]. Available: https://www.reddit.com/r/ProgrammerHumor/comments/138wyqn/all_http_status_codes_are_explained/. [Accessed: Aug. 19, 2025]\n\n\n[6] \nJ. Evans, “Questions about HTTP status codes. Questions: A wizard zines project,” Jun. 08, 2020. [Online]. Available: https://questions.wizardzines.com/http-status-codes. [Accessed: Jul. 11, 2025]\n\n\n[7] \n\n“INFO 2950 - securely storing API keys. INFO 2950.” [Online]. Available: https://info2950.infosci.cornell.edu/tutorials/store-api-keys.html. [Accessed: Jul. 10, 2025]\n\n\n[8] \nroelpi, “Customizing r: The .renviron file. Roel peters,” Apr. 30, 2020. [Online]. Available: https://www.roelpeters.be/what-is-the-renviron-file/. [Accessed: Jul. 10, 2025]\n\n\n[9] \n\n“Access the system credential store from r.” [Online]. Available: https://keyring.r-lib.org/index.html. [Accessed: Jul. 10, 2025]\n\n\n[10] \nG. E. G. hires external cybersecurity experts to share their unique experience {and}. knowledge in security on the G. blog M. posts by G. Expert, “How to handle secrets in python. GitGuardian blog - take control of your secrets security,” Jan. 25, 2023. [Online]. Available: https://blog.gitguardian.com/how-to-handle-secrets-in-python/. [Accessed: Jul. 10, 2025]\n\n\n[11] \nK. Sever, “OAuth2 explained with cute shapes. Medium,” Aug. 10, 2022. [Online]. Available: https://engineering.backmarket.com/oauth2-explained-with-cute-shapes-7eae51f20d38. [Accessed: Jul. 11, 2025]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/05-APIs.html#footnotes",
    "href": "part-advanced-topics/05-APIs.html#footnotes",
    "title": "33  Application Programming Interfaces",
    "section": "",
    "text": "These terms are often mixed up and this has been an issue since at least 2003 [2].↩︎\nIt’s fairly common for people who are new to working with the ChatGPT AI to accidentally push their keys to github, and then post about it, wondering why they have no credits left in their account. BE CAREFUL when using APIs and version control - you need to always make sure your keys are stored securely and not pushed to version control.↩︎",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Application Programming Interfaces</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html",
    "href": "part-advanced-topics/06-pdf-tools.html",
    "title": "34  Working with PDFs",
    "section": "",
    "text": "Objectives\nWhen I started my first job out of graduate school, one particular process used by my coworkers completely mystified me: they would print out a document, and then immediately scan it back in, with the scan emailed to themselves. This seemed like a waste of toner, paper, and time to me – why would anyone do such a thing? Eventually, I found out that the printer would automatically recognize the text and add a text layer to a PDF that previously couldn’t be highlighted. So, my coworkers found a handy workaround to manually typing out the numbers they needed from older PDF documents! As clever as this was, it was also unnecessary. A lot of paper, toner, and time could have been saved if the company had just provided Optical Character Recognition (OCR) programs and made them available to workers.\nIn this chapter, you’ll learn about PDF document structure, as well as how to use OCR programs.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#objectives",
    "href": "part-advanced-topics/06-pdf-tools.html#objectives",
    "title": "34  Working with PDFs",
    "section": "",
    "text": "Identify the type of PDF and the data it contains.\nDevelop a strategy to extract the data from the PDF programmatically, using strategies to improve the success of Optical Character Recognition (OCR) if necessary.\nAugment the PDF files with OCR to add a text layer, if necessary, before extracting information.\nExtract information from PDFs programmatically and format the information appropriately.\nImplement quality control and data cleaning measures which handle the most common OCR errors elegantly.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#introduction",
    "href": "part-advanced-topics/06-pdf-tools.html#introduction",
    "title": "34  Working with PDFs",
    "section": "34.1 Introduction",
    "text": "34.1 Introduction\nOver the objections of open data organizations, data archivists [1], and programmers [2], [3], companies and government agencies frequently use PDF (portable document format) documents to store and release data. Election results (Oregon, 2024), property appraisals (Lancaster County, Nebraska), public health reports (Centers for Disease Control Morbidity and Mortality Weekly Reports), and more, locked up in PDF format instead of stored properly in nicely formatted CSVs, spreadsheets, or databases [4]. Even though we object to the storage mechanism, learning how to deal with data stored in PDF format is a valuable skill for the aspiring data scientist. Even if you never work with PDF data in a professional capacity (and I hope you’re that lucky), these skills are very useful for public data side projects.\n\n34.1.1 PDF File Format\nThe PDF file format was created in 1993 by Adobe and was a proprietary format until 2008, when the format became an open standard under the control of the International Standards Organization [5]. As the acronym suggests, Portable Document Format is intended to be readable on any computer. This was something of a novel idea in the 1990s, when Mac users used one document creation software and Windows users another, and there was not a version of e.g. Microsoft Office available for Mac.\nThe technical details of a PDF file are complex [6], [7]. However, conceptually, there are four required components to a PDF document, as shown in Figure 34.1:\n\n\n\n\n\n\n\n\nFigure 34.1: High-level required components of a PDF document.\n\n\n\n\n\n\n\n\n\nFigure 34.2: Required components of a PDF Body\n\n\n\n\nHeader\n\nPDF version number\narbitrary sequence of binary data to prevent applications from opening the document as a text file (which would corrupt the file)\n\nBody (relationships between body components shown in Figure 34.2)\n\nPage tree - serves as the root of the document, and may be as simple as a list of pages.\nPages - each page is defined independently and contains its own metadata, links to resources, and content (defined separately).\nResources - objects required to render a page, such as fonts.\nContent - text and graphics which appear visually on the page.\nCatalog - an indication to programs as to where to start reading the document. Often this is just a link to the root page tree.\n\nCross-reference table - Records the location in the file of each object in the body of the file so that when viewing a page, only objects from that page are loaded into memory.\nTrailer - tells applications how to read the file\n\nA reference to the catalog which links to the document root\nLocation of the cross-reference table\nSize of the cross-reference table\n\n\n[8] has some good examples showing pages and the PDF document code that create the pages.\nWithin the page, streams are often used to define the page’s appearance (the other option is lattices, which can be used to divide the page up into sections). To add text, commands are issued to define the font, position the text cursor, and type the text onto the page. Text is positioned from the bottom left corner, with \\(Y\\) defining the vertical and \\(X\\) defining the horizontal location. Line breaks and other formatting features are not a part of the PDF format – these operations are performed by another program before the file is saved as PDF. As a result, text commands in PDF can be fragmented, leading to a continuous paragraph of words being written in the PDF file as separate lines, with other page elements present in between (like figure captions, page numbers, and images). In addition, PDF documents allow for changing the kerning of text (space between letters) in specific ways that may make it difficult for the characters to be separated visually. One common example of this is the sequence of characters ff or fi in a document, which can sometimes be read in interesting ways by OCR: sometimes, as unicode ﬀ or ﬁ, sometimes as Cyrillic characters, and sometimes left out entirely or misplaced.\n\n\n\n\n\n\nDemo: PDF Fun\n\n\n\nConsider a Home appraisal record from Lancaster County, NE. Opening the card in a PDF reader and selecting all the text yields a disorganized text file (and this PDF is actually created using modern methods and relatively clean!)\nA few observations, marked up in Figure 34.3:\n\nThe title on page 1 is on line 39 of the file, and it appears that the data from the first column is on lines 1-38.\nWhen there are multi-column tables, as in the “Inspection History” table in the middle of the first page, the values are listed by column, but missing values (the times of inspection) are not indicated at all!\nThe Appraised Values table has columns Land, Building, Total, and Method. Line 142 of the text file shows an entry for “Total Method”, and it is clear that the text for the two columns has been combined.\n\n\n\n\n\n\n\nFigure 34.3: Comparing text of PDF document to the rendered PDF document.\n\n\n\nNow, perhaps we could write a script that would disentangle some of this information and format it properly, though I think the missing values would still be unrecoverable without someone visually mapping the data to the corresponding lines.\nThe arrangement of the text you get from selecting all text is different in different PDF applications – I tried it with Okular and Firefox, and got totally different orders of text boxes.\nThe scope of the actual problem only becomes visible when you look at a second PDF document and the corresponding text file. Figure 34.4 shows the two PDF files and their corresponding text files, with the comparable portion of each PDF and text file highlighted.\n\n\n\n\n\n\nFigure 34.4: Lancaster county, NE real estate appraisal cards and text, with correpsonding sections highlighted across two appraisal cards. The text corresponding to the same space on the PDF has different formatting because one row of the table is blank on one of the cards.\n\n\n\n\n\nHopefully you’re beginning to understand how challenging this whole extracting data from PDFs thing can be! What we would really want to do here is detect the column boundaries somehow, and then read the data in from each table column-wise - this would be easier than postprocessing it, and we can also get coordinates for x and y to help us determine which data correspond to the same rows. Hold on to that thought – we’ll come back to it.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#types-of-pdf-files",
    "href": "part-advanced-topics/06-pdf-tools.html#types-of-pdf-files",
    "title": "34  Working with PDFs",
    "section": "34.2 Types of PDF Files",
    "text": "34.2 Types of PDF Files\n\n34.2.1 Layers\nPractically, we can think of PDFs as having a text layer, an image layer, or both (hybrid PDFs). A PDF with a text layer will allow you to select embedded text and copy it into a text file, while a PDF that just has an image layer does not. It is also possible to have a PDF that has an image layer with a corresponding text layer on top. Optical Character Recognition takes a PDF with only image layers and creates a text file (or layer, depending on the tool) by identifying the characters in the document and converting those characters to text with a corresponding \\((x,y)\\) location in the document. Different OCR programs use different conventions for this process, and the quality of the image matters a lot as well - some images are just not good enough to produce a passable transcription of the text using automatic methods.\nThus, if we want to think about classifying PDF files by type, we might come up with the following groups:\n\nA PDF file that has an image layer, but no text layer. (This is sometimes called a “raster” PDF, because an image that’s made up of pixels is a raster image.)\nA PDF file that has a text layer but no image layer is unsurprisingly called a text PDF.\nMany files have both text and images; these are hybrid PDFs.\n\nHow we ingest data from PDF files depends heavily on the type of PDF we have.\n\n\n\n\n\n\nDemo: Types of PDF Files\n\n\n\nFor this demo, I’ve converted the first page of one of the Lancaster county, NE property appraisal PDFs into:\n\nan image only PDF and\na text-based PDF (the original form).\n\n\n\n\n\n\n\nOpen these up in your favorite PDF editor and try to highlight the text in each. How does it work?\n\n\n\nWe can also examine the format of a PDF file using R and python libraries.\n\nRPython\n\n\nYou’ll need the pdftools package, which you can install with install.packages(\"pdftools\"). This may require you to install libpoppler on Linux , but versions for other operating systems should be self-contained.\nThe pdf_info function gives us information from the PDF header, and the pdf_text function tries to extract the text, if it exists.\n\nText-Based PDF\n\nlibrary(pdftools)\nlibrary(stringr)\n\npdf_info(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\")$version\npdf_text(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\") |&gt;\n1  str_split(\"\\n\") |&gt;\n2  unlist() |&gt;\n3  head()\n## [1] \"1.3\"\n## [1] \"                                                                   LANCASTER COUNTY APPRAISAL CARD\"                                                                                                        \n## [2] \"     Parcel ID: 10-24-201-025-000                                         Tax Year: 2025                                                Run Date: 7/15/2025 12:23:27 PM                  Page       1 of 2\"\n## [3] \"    OWNER NAME AND MAILING ADDRESS                                                                                 SALES INFORMATION\"                                                                      \n## [4] \"EASTDALE RENTALS LLC                           Date                Type            Sale Amount          Validity              Multi           Inst.Type                            Instrument #\"           \n## [5] \"Attn: JEFF & ANITA EASTMAN                     05/20/2022          Improved                   $0        Disqualified                          Warranty Deed                        2022025796\"             \n## [6] \"2501 S 74 ST                                   04/23/1996          Improved              $37,000        Disqualified                          Warranty Deed                        1996016959\"\n\n\n1\n\nSplit text into lines\n\n2\n\nRemove from list structure – make a vector\n\n3\n\nShow first few lines\n\n\n\n\n\n\nImage-Based PDF\n\npdf_info(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\")$version\n## [1] \"1.5\"\npdf_text(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\") \n## [1] \"\"\n\n\n\n\nYou will need the pdfplumber package [9], which you can install with pip.\nThe pdf object contains metadata and pages, and page text (if it exists) can be accessed with the pdf.pages[i].extract_text() method.\n\nText-Based PDF\n\n1import pdfplumber\n2with pdfplumber.open(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-text.pdf\") as pdf:\n3  metadata = pdf.metadata\n4  first_page =  pdf.pages[0]\n5  text = first_page.extract_text()\n6  pdf.close()\n\nmetadata\n7text.split(\"\\n\")[0:5]\n## {'CreationDate': \"D:20250715122327-05'00'\", 'Producer': 'iText# by Gerald Henson (r0.95 of lowagie.com, based on version Paulo build 103)'}\n## ['LANCASTER COUNTY APPRAISAL CARD', 'Parcel ID:10-24-201-025-000 Tax Year: 2025 Run Date: 7/15/2025 12:23:27 PM Page 1 of 2', 'OWNER NAME AND MAILING ADDRESS SALES INFORMATION', 'EASTDALE RENTALS LLC Date Type Sale Amount Validity Multi Inst.Type Instrument #', 'Attn: JEFF & ANITA EASTMAN 05/20/2022 Improved $0 Disqualified Warranty Deed 2022025796']\n\n\n1\n\nUse the pdfplumber library\n\n2\n\nOpen the file and call it pdf\n\n3\n\nGet the metadata\n\n4\n\nGet the data for the first page\n\n5\n\nExtract the text from the first page\n\n6\n\nClose the file (important to release memory)\n\n7\n\nSplit the text by \\n and show the first few lines\n\n\n\n\n\n\nImage-Based PDF\n\nwith pdfplumber.open(\"../data/Lancaster-County-NE-Real-Estate-AppraisalCard-28749-2025-147568-pg1-image.pdf\") as pdf:\n  metadata = pdf.metadata\n  first_page =  pdf.pages[0]\n  text = first_page.extract_text()\n  pdf.close()\n\nmetadata\n## {'Producer': 'cairo 1.16.0 (https://cairographics.org)', 'CreationDate': \"D:20250715133207-05'00\"}\n\ntext.split(\"\\n\")[0:5]\n## ['']\n\n\n\n\n\nNotice that the text does not exist for the image-based PDF. I created the image version of the PDF by opening the PDF in an image editor and saving the resulting file as PDF within that image editor, so it’s not surprising that the text layer is not present in that version of the file.\n\n\n\n\n34.2.2 The Trouble with Text Layers\n\nData scientists are often interested in data from tables. Unfortunately the pdf format is pretty dumb and does not have notion of a table (unlike for example HTML). Tabular data in a pdf file is nothing more than strategically positioned lines and text, which makes it difficult to extract the raw data… [10]\n\nThe quote above gives you some idea of the challenge of extracting text from a PDF, but it’s likely you’ve already come across this challenge when trying to copy text out of a PDF and into some other program for editing. Incidentally, the PDF representation of text is also a reason why some groups are shifting away from the format entirely – it is difficult to make PDF documents accessible to screen readers, because the text isn’t inherently ordered in any way1.\nText extraction from PDFs can be annoying, but extracting structured text in tables is an even harder challenge. Hopefully, the next few demonstrations will help you appreciate why it’s so challenging, as well as introducing you to the tools you may need to convert image layers to text layers.\n\n\n34.2.3 Converting Images to Text Using Optical Character Recognition\nIn order to covert image layers to text layers, we need to use Optical Character Recognition (OCR).\nMost free OCR tools are based on the tesseract library [11], which you can access using pytesseract in python [12] or the tesseract R package [13] You can also run tesseract from the command line, if you install the library for your operating system and language.\n\n\n\n\n\n\nDemo: Optical Character Recognition\n\n\n\n\nBashRPython\n\n\nFor the sake of shorter commands, let’s assume I’m working with a 1-page PDF file named file.pdf and want to create file.txt which contains the text of the image-based PDF.\n\n1pdftoppm -png ../data/file.pdf file\n2tesseract  -l eng ../data/file-1.png ../data/file-1-bash\n\n\n1\n\nConvert PDF to PNG to work with Tesseract. If the PDF file has more than one page, this will create file-1.png … file-n.png images that can be fed into tesseract using a bash for loop.\n\n2\n\nExtract the text to a text file.\n\n\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nPNG\nLink to the PNG file \n\n\n\nText\nLink to the text file\n\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN,\n## 2501 S 74 ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## MAGEE AOE NOON\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## | PROPERTY FACTORS\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parkina Type:\n## Parking Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## \n## Tax Year: 2025 Run Date: 7/15/2025 12:23:27 PM Page 1 of 2\n## \n## Date Type Sale Amount Validity Multi Inst.Type Instrument #\n## 05/20/2022 Improved $0 Disqualified Warranty Deed 2022025796\n## 04/23/1996 Improved $37,000 Disqualified Warranty Deed 1996016959\n## 09/08/1994 Improved $0 Disaualified Death Certificate 1994045456\n## \n## Number Issue Date Amount Status Type Description\n## \n## Date Time Process Reason Appraiser Contact-Code\n## \n## 07/11/2022 Interview and Measure - 01 General Review MRC Tenant - 2\n## \n## 11/20/2015 11:45AM —_—No Answer At Door, Exterior - 04 General Review afo\n## \n## 07/15/2010 Field Review - 08 Final Review CAB\n## \n## 0504/2010 No Answer At Door, Measured - 05 General Review TMJ\n## \n## 09/17/2008 Field Review - 08 Final Review CAB\n## \n## Year Level Case # Status Action Year Land Building Total\n## 2025 $45,000 $124,700 $169,700\n## 2024 $45,000 $124,700 $169,700\n## 2023 $45,000 $115,700 $160,700\n## 2022 $25,000 $69,400 $94,400\n## 2021 $25,000 $69,400 $94,400\n## \n## Land Building Total Method\n## \n## Current $45,000 $124,700 $169,700 IDXVAL\n## Prior $45,000 $124,700 $169,700 IDXVAL\n## Cost $158,280 Market $332,300 GRM $169,700\n## Income $0 MRA $160,100 Ovr\n## \n## Method Type AC/SF (Units Inft Factt Inf2 Fact2 InflC FactC Avg Unit Val Land Value\n## \n## Site RPI-Primary Interior 45,000 45,000\n## \n## Total Acres 0.15 GIS SF 6402 Mkt Land Total $45,000\n## \n## Taxable Aq Land Total $0\n\n\n\n\n\n\n\nAfter OCR, you can search for text in the text output, but it doesn’t provide all of the features that a hybrid PDF with both an image and a text layer provides – the text isn’t associated with the \\((x,y)\\) location on the page(s). Also, note that the OCR isn’t perfect: because the word “Flags:” under Exemptions in the left column is partially obscured, it is transcribed as “Flaas:”, which isn’t an english word. Using OCR can introduce errors in to the data in ways that are not necessarily predictable (though, predictable issues include confusing capital O for 0, lowercase L for 1, and so on). However, OCR is miles better than doing things manually!\n\n\nOn  Mac and  Linux, you will likely need to install some system packages to make the tesseract package installable.\nTry installing:\n * deb: libtesseract-dev libleptonica-dev (Debian, Ubuntu, etc)\n * rpm: tesseract-devel leptonica-devel (Fedora, CentOS, RHEL)\n * brew: tesseract (Mac OSX)\nThe  Windows R package should contain the dependencies required.\n\n# install.packages(\"tesseract\")\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(stringr)\n\n1pdf_convert(pdf = \"../data/file.pdf\", filenames = \"../data/file-%d-r.%s\")\n2text &lt;- ocr(\"../data/file-1-r.png\", engine = tesseract(\"eng\"))\nwriteLines(text, \"../data/file-1-r.txt\")\n## Converting page 1 to ../data/file-1-r.png... done!\n## [1] \"../data/file-1-r.png\"\n\n\n1\n\nConvert the pdf to an image. Provide placeholder %d in the string for the number, and %s in the string for the extension.\n\n2\n\nRun OCR on the image using the english language engine\n\n\n\n\n\n\n\n\n\n\nOutput Text\n\n\n\n\n\nLink to the text file created with R\n\n## Parc: 102420125000 Tox Yous 2125 Fun Date: Tn e227 PM_——_—Pooe_1 of 2\n## omen ue ano mauincavoness ale ont\n## Serer aaracsrion lowe ees So Staules tar bees ‘oro\n## PROPERTY STS ADDRESS aun pats\n## 20 S204 st hmber——sueOHe Amount Sts Toe esc\n## event Propenr?HoRNTON\n## Uineunis: 2 magento,\n## bape aot FO cen72008 Ft Bevew “08 ral lene ec\n## ronan mecenrapremnmstony —sseDvatuenisromy\n## ae a $e00 Stat Steir\n## | pnorenryractons ie Soo. oan “Bee\n## Socino: arrnatseovawues\n## [Araaupesonenon cent som st2t7— SD xvAL\n## \n## ‘cosmo Wort ABE ORM HERD\n##  asereanminconiion\n## Towlawes 015 ossr 612 etLand Tot $4500\n## \n## Taiabe Aa Land Tt 8\n\n\n\n\nWhy is this OCR version so much worse than the version using Bash? They’re using the same tesseract library under the hood!\nA look at the documentation of pdftoppm suggests that the default resolution is 150 DPI for image conversion, where the default resolution used by pdf_convert in R is 72 DPI. If we pass in 150 DPI, what happens?\n\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(stringr)\n\npdf_convert(pdf = \"../data/file.pdf\", dpi  = 150, filenames = \"../data/file-%d-r-150dpi.%s\")\n## Converting page 1 to ../data/file-1-r-150dpi.png... done!\n## [1] \"../data/file-1-r-150dpi.png\"\ntext &lt;- ocr(\"../data/file-1-r-150dpi.png\", engine = tesseract(\"eng\"))\nwriteLines(text, \"../data/file-1-r-150dpi.txt\")\n\n\n\n\n\n\n\nOutput Text at 150 DPI\n\n\n\n\n\nLink to the text file created with R at 150 DPI\n\n## LANCASTER COUNTY APPRAISAL CARD\n## Parcel ID: 10-24-201-025-000 Tax Year: 2025 Run Date: 7/15/2025 12:28:27 PM Page 1 of 2\n## EASTDALE RENTALS LLC Date Type Sale Amount Validity Multi Inst.Type Instrument #\n## Attn: JEFF & ANITA EASTMAN 05/20/2022 Improved $0 Disqualified Warranty Deed 2022025796\n## 2501 $ 74ST 04/23/1996 Improved $37,000 _Disaualified Warranty Deed 1996016959\n## LINCOLN, NE 68506 0908/1994 Improved $0 Disqualified Death Certificate 1994045456\n## Additional Owners\n## No.\n## 2250 SHELDON ST Number Issue Date Amount Status Type Description\n## LINCOLN, NE 68503\n## Prop Class: Residential Improved\n## Primary Use: Conversion-Apt\n## Livina Units: 2 INSPECTION HISTORY\n## vo. slemtial Diet Date Time Process Reason Appraiser Contact-Code\n## Zonina: R4-Residential District 07/11/2022 Interview and Measure - 01 General Review MRC Tenant -2\n## Noha: 8NCO1 - North Central - 11202015 11:45AM —_No Answer At Door, Exterior - 04 General Review afo\n## cvbu 07/15/2010 Field Review - 08 Final Review CAB\n## br: 05/04/2010 No Answer At Door, Measured - 05 General Review TMJ\n## Tax Unit Grp: 0001 09/17/2008 Field Review - 08 Final Review CAB\n## Schl Code Base: 55-0001 Lincoln\n## Exemptions: ~ RECENTAPPEALHISTORY = ASSESSEDVALUEHISTORY\n## Year Level Case # Status Action Year Land Building Total\n## Flaas: 2025 $45,000 $124,700 $169,700\n## : 2024 $45,000 $124,700 $169,700\n## | Property Factors poe peso Ser) Sjonaoo\n## 2022 $25,000 $69,400 $94,400\n## GBA: 0 2021 $25,000 $69,400 $94,400\n## NRA:\n## ponent  APPRAISEDVALUES\n## Parkina Type:\n## Parkina Quantity: Land Building Total Method\n## — = LEGAL DESCRIPTION, Current $45,000 $124,700 $169,700 IDXVAL\n## ENGLESIDE ADDITION, BLOCK 2, Lot 23 Prior $45,000 $124,700 $169,700 IDXVAL\n## Cost $158,280 Market $332,300 GRM $169,700\n## Income $0 MRA $160.100 Ovr\n## Method Type ACSFUnits Infl_ Fact! ~—sInf2._—sFact2=—SsInflC_— Fact. ~=— Av Unit Val Land Value\n## Site RPI-Primary Interior u 45,000 45,000\n## TotalAcres 0.15 GIS SF 6402 Mkt Land Total $45,000\n## Taxable Aa Land Total $0\n\n\n\n\n\n\nIf you don’t have the image created, you have to first create an image (imgBlob) for each page, and then run image_to_string on that image. Note that this method does not require that you write the image out to a separate file: the image is only stored in memory. This might be preferable to the bash method, which produces one file for each PDF page and will use up disk space unless you delete the intermediate files at some point after the text extraction is complete.\n\nfrom PIL import Image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n1pages = convert_from_path(\"../data/file.pdf\")\n\nfor pageNum,imgBlob in enumerate(pages): \n2  text = pytesseract.image_to_string(imgBlob, lang='eng')\n3  with open(f'../data/file-{pageNum+1}-py.txt', 'a') as the_file:\n    the_file.write(text)\n## 1796\n\n\n1\n\nRead in the PDF (by default, uses 200 dpi)\n\n2\n\nRun OCR on each page of the PDF\n\n3\n\nWrite a file for each page of the PDF containing the text from OCR.\n\n\n\n\nIf you already have the image created, it’s simple to get the text out with pytesseract, using Image.open(&lt;file&gt;) instead of convert_from_path().\n\npytesseract.image_to_string(Image.open('../data/file-1.png')).split(\"\\n\")[0:5] \n## ['Parcel ID: 10-24-201-025-000', '', 'EASTDALE RENTALS LLC', 'Attn: JEFF & ANITA EASTMAN,', '2501 S 74 ST']\n\n\n\n\n\n\n\nOutput Text\n\n\n\n\n\nLink to the text file created with Python\n\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n## Parcel ID: 10-24-201-025-000\n## \n## EASTDALE RENTALS LLC\n## Attn: JEFF & ANITA EASTMAN\n## 2501S 74ST\n## \n## LINCOLN, NE 68506\n## \n## Additional Owners\n## No.\n## \n## 2250 SHELDON ST\n## LINCOLN, NE 68503\n## \n## Prop Class: Residential Improved\n## \n## Primary Use: Conversion-Apt\n## \n## Living Units: 2\n## \n## Zonina: R4-Residential District\n## \n## Nbhd: 8NC01 - North Central -\n## CVDU\n## \n## Tax Unit Grp: 0001\n## \n## Schl Code Base: 55-0001 Lincoln\n## \n## Exemptions:\n## \n## Flaas:\n## \n## GBA: 0\n## \n## NRA:\n## \n## Location:\n## \n## Parking Tvpe:\n## \n## Parkina Quantitv:\n## \n## ENGLESIDE ADDITION, BLOCK 2, Lot 23\n## \n## LANCASTER COUNTY APPRAISAL CARD\n## Tax Year: 2025\n## \n## Date Type Sale Amount Validity\n## \n## 05/20/2022 Improved $0 Disqualified\n## 04/23/1996 Improved $37,000 Disaualified\n## 09/08/1994 Improved $0 Disqualified\n## \n## Number Issue Date\n## \n## Date Time Process\n## \n## 07/11/2022 Interview and Measure - 01\n## 11/20/2015 11:45 AM No Answer At Door, Exterior - 04\n## 07/15/2010 Field Review - 08\n## \n## 05/04/2010 No Answer At Door, Measured - 05\n## 09/17/2008 Field Review - 08\n## \n## Case # Status Action\n## \n## Amount _ Status Type\n## \n## Run Date: 7/15/2025 12:23:27 PM\n## \n## Description\n## \n## Reason\n## General Review\n## General Review\n## Final Review\n## General Review\n## Final Review\n## \n## Current\n## Prior\n## Cost\n## \n## Income\n## \n## Page 1 of 2\n## \n## Multi Inst.Type Instrument #\n## Warranty Deed 2022025796\n## Warranty Deed 1996016959\n## Death Certificate 1994045456\n## \n## Appraiser Contact-Code\n## MRC Tenant - 2\n## \n## afo\n## \n## CAB\n## \n## TMJ\n## \n## CAB\n## \n## Land Buildina Total\n## $45,000 $124,700 $169,700\n## $45,000 $124,700 $169,700\n## $45,000 $115,700 $160,700\n## $25,000 $69.400 $94,400\n## $25,000 $69.400 $94,400\n## \n## Land Buildina Total Method\n## $45,000 $124,700 $169,700 IDXVAL\n## $45.000 $124,700 $169,700 IDXVAL\n## \n## $158,280 Market $332,300 GRM $169,700\n## $0 MRA $160,100 Ovr\n## \n## Method Type\n## Site RPI-Primary Interior\n## \n## Total Acres 0.15 GIS SF 6402\n## \n## ACSF Units Inf1 Fact1\n## \n## Inf2 Fact2\n## \n## InflC FactC Land Value\n## \n## 45,000\n## \n## Avg Unit Val\n## 45,000\n## \n## Mkt Land Total $45,000\n## Taxable Aq Land Total $0\n\n\n\n\n\n\n\n\nAssessment of OCR Methods\nWhat is interesting is that even when controlling the DPI, the OCR programs in each language generate different text files. Bash and python are fairly similar, but R’s text file is ordered by horizontal lines, not columns. It seems likely that we could probably fix the issue if we got the right set of options, but a more straightforward option might be to crop the images into separate chunks for each table and section. That might produce cleaner and more interpretable OCR’d images. If we wanted to process many of these files and ingest the information into a database, then we would need to ensure that we could determine how to crop the images\n\n\n\nIn many cases, we don’t want to deal with only a text file - we want the context of the image, but we’d like to be able to see the text, use Ctrl/Cmd-F to find the right page, and copy the text back out of the file.\nConsider Hamlet, which I downloaded from The Internet Archive - it has the original text, but it also has a text layer that allows you to search through the document and find, for instance, the 11 instances of the word “skull”, four of which are on page 82. In general, the text layer is either not displayed, or, more commonly, situated behind the image layer, allowing the reader to access the words without showing the text over top of the image. How are these hybrid PDF documents created?\n\n\n\n\n\n\nDemo: Creating Hybrid PDFs\n\n\n\nWhen I need OCR and don’t necessarily want to bother with R, I prefer to use a program called ocrmypdf that is based on tesseract and available for most distributions. I started using ocrmypdf before I realized that it’s actually a python package that can just be called from the Linux command line. In any case, it’s possible to use this command within R or Python, but once you have the python package installed and the binary in your system path, it’s just as easy to use the program from the terminal – everything else is just overhead.\nLet’s OCR the Lancaster county, NE home appraisal image PDF and see what we come up with.\n\nocrmypdf -l eng ../data/file.pdf ../data/file-ocrmypdf.pdf\n\nThe output of ocrmypdf is a hybrid PDF that has the text and image data superimposed (the text is not visible until you highlight it).",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#working-with-pdfs-programmatically",
    "href": "part-advanced-topics/06-pdf-tools.html#working-with-pdfs-programmatically",
    "title": "34  Working with PDFs",
    "section": "34.3 Working with PDFs Programmatically",
    "text": "34.3 Working with PDFs Programmatically\n\n34.3.1 Reading Text from Raster-based PDFs with OCR\nIn many real-world situations, we may not need to read tabular data out of a PDF. For instance, Google’s Library Project scanned millions of books, allowing them to show the frequency of a word’s use over time. This wouldn’t be possible without a very good OCR library (which is one reason Google took over tesseract when IBM open-sourced it).\nThe primary challenges when the goal is reading in text from an OCR’d PDF are:\n\nIf documents are the result of lower-quality scans, noise removal, background removal, and brightness/contrast adjustments may be necessary before the OCR step.\nEnsuring word order is maintained across lines\nRemoving page numbers and header/footer information\n\n\n34.3.1.1 Extended Example: Trial Transcripts\nLet’s start out with a relatively simple case, based on a real problem I tried to tackle in 2022. We have a number of court trial transcripts, and we want to systematically search for certain phrases to determine how common they are2. We’ll ignore, for now, the problem of actually understanding what the words are saying and classifying that properly (that would require language or topic modeling), and focus on the problem of even getting the trial transcripts converted into plain text in a format which is readable.\nThe first step will be to run OCR on the first day’s trial transcript. I acquired these transcripts from a professional attorney information site for demonstration purposes, as the transcripts I was working with were confidential.\nI will show code for running OCR on all pages of the file, but it doesn’t make sense to actually run that code yet – there is always some tweaking to be done, so I usually will run 1 page, and then 5 pages, and then 10, and then the full set, optimizing the code as I go.\n\n\n\n\n\n\nStep 1: Testing OCR of full page(s)\n\n\n\n\nBashRPython\n\n\n\n#!/bin/bash\npdf=\"../data/legal-transcript-trial_-_day_1.pdf\"\npath=\"../data/legal-transcript-trial-day-1\"\nmkdir -p $path\n1pdftoppm -png -r 300 \"$pdf\" \"$path/page\"\n\n2for file in \"$path/*.png\"\ndo\n3  tesseract  -l eng $file \"${file%.*}-bash\";\ndone\n\n4tesseract  -l eng \"$path/page-001.png\" \"$path/page-001-bash\"\n\n\n1\n\nConvert each page to a separate PNG. The \\ character allows extending the command over multiple lines for readability\n\n2\n\nIterate over the PNGs (commented out because there’s no point in doing it multiple times at the moment)\n\n3\n\nConvert each PNG to a corresponding text file, removing the extension and adding -text to the end of the file name (tesseract will add the .txt extension). ${file%.*} is a bash convention to remove the file extension, and adding -text at the end ensures that we have a valid file name.\n\n4\n\nThis is the single-file version of the command in the for loop - the biggest difference being that we specify the input and output file name instead of using $file placeholders.\n\n\n\n\nWe could speed this up using gnu parallel if we wanted to do so - see this script for an example of what that would look like.\nAny further cleaning work should probably be done in some other language (we could go through the use of awk and sed here, but that’s probably beyond the scope of this book).\n\n\n\n\n\n\ntesseract file output\n\n\n\n\n\n\ncat ../data/legal-transcript-trial-day-1/page-001-bash.txt\n## e ® COPY\n## \n## CIRCUIT COURT FOR FREDERICK COUNTY\n## \n## COURT HOUSE\n## FREDERICK, MARYLAND 21701\n## \n## 24\n## \n## 29\n## \n## IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\n## EXLINE-HASSLER\n## \n## Plaintiff\n## \n## Vv. Civil Docket\n## No. 10-C-12-000410\n## PENN NATIONAL INSURANCE, ET AL.,\n## \n## Defendant\n## OFFICIAL TRANSCRIPT OF PROCEEDINGS\n## \n## (JURY TRIAL - DAY ONE)\n## \n## Frederick, Maryland\n## \n## January 22, 2013\n## \n## BEFORE:\n## THE HONORABLE JULIE S. SOLT, JUDGE\n## \n## APPEARANCES :\n## \n## For the Plaintiff:\n## LAURA C. ZOIS, ESQUIRE\n## JOHN B. BRATT, ESQUIRE\n## \n## For the Defendant:\n## WALTER E. GILLCRIST, JR., ESQUIRE\n## ANNE K. HOWARD, ESQUIRE\n## \n## For Penn National Insurance, et al.:\n## GUIDO PORCARELLI, ESQUIRE\n## \n## TRANSCRIBED BY:\n## \n## Victoria Eastridge\n## Official Transcriber\n## \n## 100 W. Patrick Street\n## Frederick, Maryland 21701\n\n\n\n\n\n\n\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(purrr)\n1# pdf_convert(\n#   pdf = \"../data/legal-transcript-trial_-_day_1.pdf\",\n#   filenames = \"../data/legal-transcript-trial-day-1/page-%03d.%s\",\n#   dpi = 300)\n2png_files &lt;- list.files(\"../data/legal-transcript-trial-day-1/\",\n                        \"png$\", full.names = T)\n3# text &lt;- map_chr(png_files, ~ocr(., engine = tesseract(\"eng\")))\n4text1 &lt;- ocr(png_files[1], engine = tesseract(\"eng\"))\n\n\n1\n\nConvert PDF pages to PNGs using R\n\n2\n\nList out all png files in the folder\n\n3\n\nOCR each PNG file and return the text in a character vector (all files)\n\n4\n\nOCR the first PNG and return the text\n\n\n\n\n\ntext1[[1]]\n## [1] \"IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\\nEXLINE-HASSLER\\nPlaintiff a\\nVv. Civil Docket\\nNo. 10-C-12-000410\\nPENN NATIONAL INSURANCE, ET AL.,\\n: Defendant\\nOFFICIAL TRANSCRIPT OF PROCEEDINGS\\n(JURY TRIAL - DAY ONE)\\nFrederick, Maryland\\nJanuary 22, 2013\\nBEFORE: . .\\nTHE HONORABLE JULIE S. SOLT, JUDGE\\nAPPEARANCES:\\n| For the Plaintiff:\\n| LAURA C. ZOIS, ESQUIRE\\nJOHN B. BRATT, ESQUIRE\\n) For the Defendant:\\nWALTER E. GILLCRIST, JR., ESQUIRE\\nANNE K. HOWARD, ESQUIRE\\n| For Penn National Insurance, et al.: |\\n/ GUIDO PORCARELLI, ESQUIRE\\n,\\n3 TRANSCRIBED BY:\\nVictoria Eastridge\\ni Official Transcriber\\n. 100 W. Patrick Street\\n&gt; Frederick, Maryland 21701\\n\"\n\n\n\n\n\n\n\nR tesseract file output\n\n\n\n\n\n\ntext1[[1]] |&gt; \n  str_split(\"\\n\", simplify = F) |&gt; unlist()\n##  [1] \"IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\"\n##  [2] \"EXLINE-HASSLER\"                                     \n##  [3] \"Plaintiff a\"                                        \n##  [4] \"Vv. Civil Docket\"                                   \n##  [5] \"No. 10-C-12-000410\"                                 \n##  [6] \"PENN NATIONAL INSURANCE, ET AL.,\"                   \n##  [7] \": Defendant\"                                        \n##  [8] \"OFFICIAL TRANSCRIPT OF PROCEEDINGS\"                 \n##  [9] \"(JURY TRIAL - DAY ONE)\"                             \n## [10] \"Frederick, Maryland\"                                \n## [11] \"January 22, 2013\"                                   \n## [12] \"BEFORE: . .\"                                        \n## [13] \"THE HONORABLE JULIE S. SOLT, JUDGE\"                 \n## [14] \"APPEARANCES:\"                                       \n## [15] \"| For the Plaintiff:\"                               \n## [16] \"| LAURA C. ZOIS, ESQUIRE\"                           \n## [17] \"JOHN B. BRATT, ESQUIRE\"                             \n## [18] \") For the Defendant:\"                               \n## [19] \"WALTER E. GILLCRIST, JR., ESQUIRE\"                  \n## [20] \"ANNE K. HOWARD, ESQUIRE\"                            \n## [21] \"| For Penn National Insurance, et al.: |\"           \n## [22] \"/ GUIDO PORCARELLI, ESQUIRE\"                        \n## [23] \",\"                                                  \n## [24] \"3 TRANSCRIBED BY:\"                                  \n## [25] \"Victoria Eastridge\"                                 \n## [26] \"i Official Transcriber\"                             \n## [27] \". 100 W. Patrick Street\"                            \n## [28] \"&gt; Frederick, Maryland 21701\"                        \n## [29] \"\"\n\n\n\n\nR seems to read in the characters closer to the left side of the page, like the gutter notes and the line numbers, where tesseract via bash did not.\n\n\n\nfrom PIL import Image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n1#pages = convert_from_path(\"../data/legal-transcript-trial_-_day_1.pdf\", dpi = 300)\n\npages = convert_from_path(\"../data/legal-transcript-trial_-_day_1.pdf\", \n2                          dpi = 300, first_page=1, last_page=1)\n\nfor pageNum,imgBlob in enumerate(pages):\n3  text = pytesseract.image_to_string(imgBlob, lang='eng')\n4  with open(f'../data/legal-transcript-trial-day-1/page-{pageNum+1:03}-py.txt', 'w') as the_file:\n    the_file.write(text)\n## 708\n\n\n1\n\nConvert all pages to images (held in memory, not saved to a file)\n\n2\n\nConvert only the first page to an image in memory\n\n3\n\nOCR the image\n\n4\n\nSave the text to a file. {pageNum+1:03}-py.txt is a way to specify the output filename - since Python is 0-indexed, we need to add 1 to the page number index to get the actual page number of the PDF (since we don’t count pages from 0, normally). The :03 component is saying that the number should be formatted as an integer with 3 digits, and the front should be padded with 0s, so 1 becomes 001, 10 becomes 010, and so on. The f at the front of the string indicates that this is an f-string – a formatted string literal.\n\n\n\n\n\n\n\n\n\n\nPython tesseract file output\n\n\n\n\n\n\n#!/bin/bash\ncat ../data/legal-transcript-trial-day-1/page-001-py.txt\n## CIRCUIT COURT FOR FREDERICK COUNTY\n## \n## COURT HOUSE\n## FREDERICK, MARYLAND 21701\n## \n## IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\n## \n## EXLINE-HASSLER\n## \n## Plaintiff\n## Vv. Civil Docket\n## \n## No. 10-C-12-000410\n## PENN NATIONAL INSURANCE, ET AL.,\n## \n## Defendant\n## \n## OFFICIAL TRANSCRIPT OF PROCEEDINGS\n## \n## (JURY TRIAL - DAY ONE)\n## \n## Frederick, Maryland\n## \n## January 22, 2013\n## \n## BEFORE:\n## \n## THE HONORABLE JULIE S. SOLT, JUDGE\n## \n## APPEARANCES :\n## \n## For the Plaintiff:\n## LAURA C. ZOIS, ESQUIRE\n## JOHN B. BRATT, ESQUIRE\n## \n## For the Defendant:\n## WALTER E. GILLCRIST, JR., ESQUIRE\n## ANNE K. HOWARD, ESQUIRE\n## \n## For Penn National Insurance, et al.:\n## GUIDO PORCARELLI, ESQUIRE\n## \n## TRANSCRIBED BY:\n## Victoria Eastridge\n## Official Transcriber\n## \n## 100 W. Patrick Street\n## Frederick, Maryland 21701\n\n\n\n\nThe python generated file looks pretty dang perfect to me, which is cool.\n\n\n\n\n\nIt’s always preferable to refine your methods before running them on the whole vector, which is why I only OCR’d the first file. Looking at the text produced by bash and R (but not python!), it’s clear that we’ll need to do some cleaning. If possible, we’d like to get line numbers out as a column, but if not, we would at least like to remove them from the text. There is no way to do this without at least looking at the PDF or PNG files; if we want to do a really good job, we’ll need to manually clean each page (ugh).\n\nStep 2: Cropping\nWe could reduce this work a bit if we could automatically recognize the vertical-ish lines separating the transcript content from the line numbers, but even doing that automatically is tricky and would likely involve line detection, rotation, and cropping for each image.\nHowever, it might be easy enough to make an educated guess, as this PDF is actually a “best case” OCR scenario. The pages don’t have a ton of dust, copy artifacts, etc., so if we can crop off the top, bottom, and left margins, we’d probably get cleaner data. In addition, the pages aren’t that skewed and are mostly in the same alignment – we might be able to get away with specifying a crop boundary for all pages.\nI could figure out how to crop images in R or python, but honestly, I think the easiest way to do this is just to use the command line. I encourage you to search for how to do &lt;task&gt; for a folder of files on the command line, because no one remembers the syntax for this stuff unless they’re doing it all the time. I consulted this page and this page and eventually pieced something together. The $(echo $f | sed 's/\\.png/-crop.png/') is probably not the most elegant way to rename the output files but it works (which the other methods I tried did not). This requires that imagemagick is installed and on the system path.\nThen, we have to figure out how much to crop. Imagemagick’s convert command uses an argument string of &lt;Width&gt;x&lt;Height&gt;+OffsetX+OffsetY. I opened a few files up in an image viewing program that had a crop function and drew some rectangles to see how different cropping arguments would work, as in Figure 34.5.\n\n\n\n\n\n\n\n\nFigure 34.5: Image viewing programs can help you determine what crop arguments should be used in the convert command.\n\n\n\n\n\n\n\n\n\nFigure 34.6: After the convert command is run, it can be helpful to quickly page through all of the images looking for potential problems, such as the rotated pages at index 239-240, where the page numbers have not been fully removed. These pages may need extra attention, or an automated rotation algorithm to be applied before cropping.\n\n\n\n\n1rm ../data/legal-transcript-trial-day-1/*-crop.png\nfor f in ../data/legal-transcript-trial-day-1/*.png; \n2  do convert -crop 2000x2700+450+300 +repage \"$f\" $(echo $f | sed 's/\\.png/-crop.png/');\ndone\n\n\n1\n\nRemoving previously cropped files prevents cropping *-crop.png files to get *-crop-crop.png files and accumulating ridiculous numbers of files.\n\n2\n\nCrop the page to a 2000 width, 2700 height page, starting (from the top left) at x=450, y = 300. The repage argument resets the coordinates of the PNG so that the canvas doesn’t have the original size. $(echo $f | sed 's/\\.png/-crop.png/') renames the output file to have -crop.png at the end… more challenging than expected.\n\n\n\n\nOnce the conversion command has run, it can be helpful to page through the cropped files looking for problems, and rerun the crop command if problems are identified, as in Figure 34.6.\nThen, we re-run the OCR process on the cropped files.\n\n\n\n\n\n\nStep 3: OCR on cropped images\n\n\n\n\nBashRPython\n\n\n\n1# for file in ../data/legal-transcript-trial-day-1/*-crop.png\n# do\n2#   tesseract  -l eng $file \"${file%.*}-bash\"\n# done\n\npath=\"../data/legal-transcript-trial-day-1\"\n3tesseract  -l eng \"$path/page-001-crop.png\" \"$path/page-001-crop-bash\"\n\n\n1\n\nIterate over the PNGs (commented out because there’s no point in doing it multiple times at the moment)\n\n2\n\nConvert each PNG to a corresponding text file, removing the extension and adding -text to the end of the file name (tesseract will add the .txt extension). ${file%.*} is a bash convention to remove the file extension, and adding -text at the end ensures that we have a valid file name.\n\n3\n\nThis is the single-file version of the command in the for loop - the biggest difference being that we specify the input and output file name instead of using $file placeholders.\n\n\n\n\n\n\n\n\n\n\ntesseract cropped file output\n\n\n\n\n\n\ncat ../data/legal-transcript-trial-day-1/page-001-crop-bash.txt\n## IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\n## EXLINE-HASSLER\n## \n## Plaintiff\n## Vv. Civil Docket\n## No. 10-C-12-000410\n## PENN NATIONAL INSURANCE, ET AL.,\n## \n## Defendant\n## \n## OFFICIAL TRANSCRIPT OF PROCEEDINGS\n## \n## (JURY TRIAL - DAY ONE)\n## \n## Frederick, Maryland\n## \n## January 22, 2013\n## \n## BEFORE:\n## \n## THE HONORABLE JULIE S. SOLT, JUDGE\n## \n## APPEARANCES :\n## \n## For the Plaintiff:\n## LAURA C. ZOIS, ESQUIRE\n## JOHN B. BRATT, ESQUIRE\n## \n## For the Defendant:\n## WALTER E. GILLCRIST, JR., ESQUIRE\n## ANNE K. HOWARD, ESQUIRE\n## \n## For Penn National Insurance, et al.:\n## GUIDO PORCARELLI, ESQUIRE\n## \n## TRANSCRIBED BY:\n## \n## Victoria Eastridge\n## Official Transcriber\n## \n## 100 W. Patrick Street\n## Frederick, Maryland 21701\n\n\n\n\nThe text looks pretty perfect to me at this point.\n\n\n\nlibrary(tesseract)\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(purrr)\n\n1png_files &lt;- list.files(\"../data/legal-transcript-trial-day-1/\",\n                        \"-crop.png$\", full.names = T)\n2textcrop &lt;- ocr(png_files[1], engine = tesseract(\"eng\"))\n\n\n1\n\nList out all png files in the folder\n\n2\n\nOCR the first PNG and return the text\n\n\n\n\n\n\n\n\n\n\nR cropped file output\n\n\n\n\n\n\ntextcrop[[1]] |&gt; \n  str_split(\"\\n\", simplify = F) |&gt; unlist()\n##  [1] \"IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\"\n##  [2] \"EXLINE-HASSLER\"                                     \n##  [3] \"Plaintiff a\"                                        \n##  [4] \"Vv. Civil Docket\"                                   \n##  [5] \"No. 10-C-12-000410\"                                 \n##  [6] \"PENN NATIONAL INSURANCE, ET AL.,\"                   \n##  [7] \": Defendant\"                                        \n##  [8] \"OFFICIAL TRANSCRIPT OF PROCEEDINGS\"                 \n##  [9] \"(JURY TRIAL - DAY ONE)\"                             \n## [10] \"Frederick, Maryland\"                                \n## [11] \"January 22, 2013\"                                   \n## [12] \"BEFORE: . .\"                                        \n## [13] \"THE HONORABLE JULIE S. SOLT, JUDGE\"                 \n## [14] \"APPEARANCES:\"                                       \n## [15] \"| For the Plaintiff:\"                               \n## [16] \"| LAURA C. ZOIS, ESQUIRE\"                           \n## [17] \"JOHN B. BRATT, ESQUIRE\"                             \n## [18] \") For the Defendant:\"                               \n## [19] \"WALTER E. GILLCRIST, JR., ESQUIRE\"                  \n## [20] \"ANNE K. HOWARD, ESQUIRE\"                            \n## [21] \"| For Penn National Insurance, et al.: |\"           \n## [22] \"/ GUIDO PORCARELLI, ESQUIRE\"                        \n## [23] \",\"                                                  \n## [24] \"3 TRANSCRIBED BY:\"                                  \n## [25] \"Victoria Eastridge\"                                 \n## [26] \"i Official Transcriber\"                             \n## [27] \". 100 W. Patrick Street\"                            \n## [28] \"&gt; Frederick, Maryland 21701\"                        \n## [29] \"\"\n\n\n\n\nR is still finding a few extra characters, but this is a vast improvement over the previous version. We could likely remove what’s left through some careful text cleaning.\n\n\n\nfrom PIL import Image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\nimage=\"../data/legal-transcript-trial-day-1/page-001-crop.png\"\n\ntext = pytesseract.image_to_string(Image.open(image))\nf = open(f'../data/legal-transcript-trial-day-1/page-001-crop-py.txt', 'w')\nf.writelines(text)\nf.close()\n\n\n\n\n\n\n\nPython tesseract file output\n\n\n\n\n\n\ncat ../data/legal-transcript-trial-day-1/page-001-crop-py.txt\n## IN THE CIRCUIT COURT FOR FREDERICK COUNTY, MARYLAND\n## EXLINE-HASSLER\n## \n## Plaintiff\n## Vv. Civil Docket\n## No. 10-C-12-000410\n## PENN NATIONAL INSURANCE, ET AL.,\n## \n## Defendant\n## OFFICIAL TRANSCRIPT OF PROCEEDINGS\n## \n## (JURY TRIAL - DAY ONE)\n## \n## Frederick, Maryland\n## \n## January 22, 2013\n## \n## BEFORE:\n## THE HONORABLE JULIE S. SOLT, JUDGE\n## \n## APPEARANCES :\n## \n## For the Plaintiff:\n## LAURA C. ZOIS, ESQUIRE\n## JOHN B. BRATT, ESQUIRE\n## \n## | For the Defendant:\n## WALTER E. GILLCRIST, JR., ESQUIRE\n## ANNE K. HOWARD, ESQUIRE\n## \n## For Penn National Insurance, et al.:\n## GUIDO PORCARELLI, ESQUIRE\n## \n## 3 TRANSCRIBED BY:\n## Victoria Eastridge\n## t Official Transcriber\n## . 100 W. Patrick Street\n## &gt; Frederick, Maryland 21701\n\n\n\n\nIn this case, the output from the cropped version is actually not as clean as the output from the cleaned version. This can occur because of settings that detect the main part of the page - if there isn’t enough margin, these settings sometimes don’t work as well. It’s important to calibrate the pipeline to the tools you have available.\n\n\n\n\n\n\n\nStep 4: Cleaning The Text Output\nOnce the pages have been cropped and OCR’d, then we need to clean up the text output. This might involve some of the following steps:\n\nConcatenating the text files into a single file\nJoining adjacent lines that are part of the same thought and from the same speaker\nAssigning line numbers based on the order of the transcript\nSeparating out the speaker information, so that there is a column for speaker and a column for text\nRunning the transcript through spell check to handle any minor OCR errors, like using a zero instead of a capital O\nCleaning up punctuation marks that are often confused, like ( and { to ensure that all brackets match\nIdentifying portions of the transcript that are from depositions (which have Q and A at the beginning of each line, instead of speakers)\n\nWhich steps are undertaken depends heavily on the goal of the analysis. We might not care about punctuation marks if we’re going to apply text processing algorithms that require that we strip all punctuation out of the text, but we may need to remove all of the speaker information so that only spoken text remains.\nAll of these steps are string processing tasks, which will not be repeated in this chapter.\n\n\n\n\n34.3.2 Reading Tabular Data from Text or Hybrid PDFs\nAs mentioned above, tabular data is a particular challenge to read from PDF files, as the PDF specification doesn’t actually have any way to represent structured text. There are two common open-source libraries recommended for extracting tabular data from PDFs - tabula, which is a Java library [14], and camelot, a Python library [15]. There are interfaces to the tabula library in both R and Python (tabulapdf and tabula-py, respectively), but there is no R interface to camelot, as far as I am aware. In this chapter, I will focus primarily on tabula, since it works across both R and Python, but if you ever run into issues using it, consider camelot as well – it has some cool features [16].\nInstallation of tabulapdf in R depends on rJava, which can be a bit tricky, particularly on Windows. The tabulapdf github page has more detailed instructions for how to install Java for Windows using Chocolatey.\n\n\n\n\n\n\nExample: Bureau of Labor Statistics Consumer Price Index\n\n\n\nThe Bureau of Labor Statistics provides monthly Consumer Price Index news releases. An archive of these releases is available at https://www.bls.gov/bls/news-release/cpi.htm. Let’s acquire 2 years worth of monthly CPI reports, and focus on trying to extract the first table, “Consumer Price Index for All Urban Consumers (CPI-U): US city average, by expenditure category”.\n\n\n\n\n\n\nAcquiring 2 years of BLS CPI news releases\n\n\n\n\n\nAs this material is covered in Chapter 31, I’m just going to provide the code to do this in R – you can see equivalent python commands in Chapter 31. After having done this in R, I realized I probably could have accomplished the same task with a single wget command in bash, the lesson being that it is important to pick your tools wisely.\n\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(stringr)\n\nsave_dir &lt;- \"../data/bls-pdfs/\"\ndir.create(save_dir, showWarnings = F)\n\nurl &lt;- \"https://www.bls.gov/bls/news-release/cpi.htm\"\n\nsession &lt;- read_html_live(url)\n\n# PDFs are the 2nd link in each entry\nlinks &lt;- session$html_elements(\"li a:nth-child(2)\")  \n\n# Get the last 2 years of entries\nlink_tbl &lt;- tibble(link = html_attr(links, \"href\"), \n                   date = str_extract(link, \"\\\\d{8}\")) |&gt;\n  na.omit() |&gt;\n  mutate(datestr = date, date = mdy(date)) |&gt;\n  filter(today() - years(2) &lt;= date)\n\n\nua &lt;- \"Mozilla/5.0 (Windows NT x.y; Win64; x64; rv:10.0) Gecko/20100101 Firefox/10.0\"\noptions(HTTPUserAgent = ua)\n\nfilelist &lt;- paste0(\"https://www.bls.gov\", link_tbl$link)\nfilesave &lt;- paste0(save_dir, basename(link_tbl$link))\n\n# The site is finicky about user agents, so we need to \n# specifically pass that in to the download.file method.\nwalk2(filelist, filesave, ~download.file(.x, destfile = .y, method = \"wget\", extra = paste0(\"-U \\\"\", ua, \"\\\"\")))\n\n\n\n\n\nYou can either run the code above (assuming you have wget on your machine), or you can download a zip file of the PDFs.\n\nRTabula-Py\n\n\n\n# install.packages(\"tabulapdf\")\nlibrary(tabulapdf)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(stringr)\n\nfiles &lt;- list.files(path = \"../data/bls-pdfs\", pattern = \".pdf$\", full.names=T)\n\nfind_page_number &lt;- function(file) {\n  txt &lt;- pdf_text(file)\n  txt_by_page &lt;- map_chr(txt, ~paste(., collapse=\" \"))\n  which(str_detect(txt_by_page, \"Table 1\"))\n}\n\n# page_numbers &lt;- map_int(files, find_page_number)\npage_numbers &lt;- c(9, 10, 8, 9, 8, 8, 9, 8, 9, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 9, 10, 9)\n\ntables &lt;- extract_tables(files[1], page = page_numbers[1],  output = \"tibble\")[[1]]\n\nhead(tables)\n## # A tibble: 6 × 5\n##   ...1                 ...2    ...3  `Unadjusted percent` Seasonally adjusted …¹\n##   &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;                 \n## 1 &lt;NA&gt;                 Relati… Unad… change               change                \n## 2 &lt;NA&gt;                 impor-  &lt;NA&gt;  &lt;NA&gt;                 &lt;NA&gt;                  \n## 3 Expenditure category tance   &lt;NA&gt;  Dec. Nov.            Sep. Oct. Nov.        \n## 4 &lt;NA&gt;                 Nov.    Dec.… 2022- 2023-          2023- 2023- 2023-     \n## 5 &lt;NA&gt;                 2023    2022… Dec. Dec.            Oct. Nov. Dec.        \n## 6 &lt;NA&gt;                 &lt;NA&gt;    &lt;NA&gt;  2023 2023            2023 2023 2023        \n## # ℹ abbreviated name: ¹​`Seasonally adjusted percent`\n\nIt appears that tabulapdf isn’t separating the columns the way we’d prefer. Let’s see if we can fix that….\nThere’s a function, locate_areas(), that works interactively - it opens a viewer tab and you select the table using the mouse, as in Figure 34.8.\n\nlocate_areas provides a sequence of coordinates that are relatively consistent across multiple full-page tables, so we might try to use those coordinates to improve our table parsing.\n\nlocate_areas(files[1], pages = rep(page_numbers[1], 5))\n\nHere’s what I got running this 5 times for the first PDF in the list – this gives me boundaries for each column (without including the header).\nListening on http://127.0.0.1:6481\n[[1]]\n     top     left   bottom    right \n128.1042 209.2557 583.8484 244.2939 \n\n[[2]]\n     top     left   bottom    right \n128.1042 289.0649 584.8223 327.9962 \n\n[[3]]\n     top     left   bottom    right \n128.1042 369.8473 586.7699 410.7252 \n\n[[4]]\n     top     left   bottom    right \n128.1042 452.5763 586.7699 489.5611 \n\n[[5]]\n     top     left   bottom    right \n126.1566 535.3054 586.7699 575.2099 \n\nWe can actually run this for each column, keeping track of the left and right values, to get an even more precise way to read our data in. Here are my rough column alignments, using cpi_01112024.pdf as a test.\n\nTable Start - 35\nCol2 - 209\nCol3 - 244\nCol4 - 289\nCol5 - 328\nCol6 - 370\nCol7 - 411\nCol8 - 453\nCol9 - 490\nCol10 - 535\nTable End - 575\n\n\ntables &lt;- extract_tables(\n  files[1], page = page_numbers[1], \n  guess = F,\n  col_names = F, \n  area = list(c(128, 35, 586, 575)), \n  columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535))\n)[[1]]\n\nhead(tables)\n## # A tibble: 6 × 10\n##   X1                          X2    X3    X4    X5    X6    X7    X8    X9   X10\n##   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 All items.. . . . . . … 100     297.  307.  307.   3.4  -0.1   0     0.1   0.3\n## 2 Food.. . . . . . . . .…  13.4   317.  325.  325.   2.7   0.1   0.3   0.2   0.2\n## 3 Food at home.. . . . .…   8.55  299.  303.  303.   1.3  -0.1   0.3   0.1   0.1\n## 4 Cereals and bakery pro…   1.16  345.  356.  354.   2.6  -0.7   0.2   0.5  -0.3\n## 5 Meats, poultry, fish, …   1.78  320.  320.  320.  -0.1   0.1   0.7  -0.2   0.5\n## 6 Dairy and related prod…   0.78  271.  268.  268.  -1.3   0.1   0.3   0.1   0.3\n\nOk, that looks good - let’s apply it to the rest of the reports now.\n\ntables &lt;- map2(files, page_numbers, ~extract_tables(\n    .x, page = .y, \n    guess = F,\n    col_names = F, \n    area = list(c(128, 35, 586, 575)), \n    columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535))\n    )[[1]]\n  )\n\nThen, we can read in the dates that are present in the header row, assuming that the major dividers stay the same between reports. I used locate_areas() to get the coordinates of each header that we care about.\n\n\n\n\n\n\nlocate_areas() output\n\n\n\n\n\n&gt; locate_areas(files[1], pages = page_numbers[1])\n\nListening on http://127.0.0.1:7339\n[[1]]\n      top      left    bottom     right \n 89.15168  36.98473 127.13037 572.29009 \n\n\n\n\n1headers &lt;- map2(\n  files, page_numbers,\n  ~extract_tables(\n    .x, page = .y,\n    guess = F,\n    col_names = F,\n    area = list(c(89, 35, 128, 575)),\n    columns = list(c(209, 244, 289, 328, 370, 411, 453, 490, 535))\n  )[[1]]\n)\n\n2fix_headers &lt;- function(tbl) {\n3modifiers &lt;- c(\"\", \"Rel_imp.\", rep(\"Unadj_idx.\", 3), rep(\"Unadj_pct_chg.\", 2), rep(\"Seas_adj_pct_chg.\", 3))\n  \n4  vars &lt;- c(tbl[1,1],\n    paste(unlist(tbl[2:3, 2]), collapse=\"\"),\n    paste(unlist(tbl[2:3, 3]), collapse=\"\"),\n    paste(unlist(tbl[2:3, 4]), collapse=\"\"),\n    paste(unlist(tbl[2:3, 5]), collapse=\"\"),\n    paste(unlist(tbl[1:4, 6]), collapse=\"\"),\n    paste(unlist(tbl[1:4, 7]), collapse=\"\"),\n    paste(unlist(tbl[1:4, 8]), collapse=\"\"),\n    paste(unlist(tbl[1:4, 9]), collapse=\"\"),\n    paste(unlist(tbl[1:4, 10]), collapse=\"\")\n    ) |&gt; \n5    str_remove_all(\"\\\\.\") |&gt;\n    str_replace_all(\"[ -]\", \"_\")\n  \n6  paste0(modifiers, vars)\n}\n\n7headers_fixed = map(headers, fix_headers)\n\nlibrary(magrittr)\n8tables &lt;- map2(tables, headers_fixed, ~set_names(.x, .y))\n\ntables[[1]]\n## # A tibble: 41 × 10\n##    Expenditure_category      Rel_imp.Nov2023 Unadj_idx.Dec2022 Unadj_idx.Nov2023\n##    &lt;chr&gt;                               &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n##  1 All items.. . . . . . . …          100                 297.              307.\n##  2 Food.. . . . . . . . . .…           13.4               317.              325.\n##  3 Food at home.. . . . . .…            8.55              299.              303.\n##  4 Cereals and bakery produ…            1.16              345.              356.\n##  5 Meats, poultry, fish, an…            1.78              320.              320.\n##  6 Dairy and related produc…            0.78              271.              268.\n##  7 Fruits and vegetables. .…            1.47              349.              351.\n##  8 Nonalcoholic beverages a…           NA                  NA                NA \n##  9 materials. . . . . . . .…            1.03              210.              216.\n## 10 Other food at home.. . .…            2.33              263.              270.\n## # ℹ 31 more rows\n## # ℹ 6 more variables: Unadj_idx.Dec2023 &lt;dbl&gt;,\n## #   Unadj_pct_chg.Dec2022_Dec2023 &lt;dbl&gt;, Unadj_pct_chg.Nov2023_Dec2023 &lt;dbl&gt;,\n## #   Seas_adj_pct_chg.Sep2023_Oct2023 &lt;dbl&gt;,\n## #   Seas_adj_pct_chg.Oct2023_Nov2023 &lt;dbl&gt;,\n## #   Seas_adj_pct_chg.Nov2023_Dec2023 &lt;dbl&gt;\n\n\n1\n\nPull headers out using roughly the same command as we used to get the tables, but with a different top and bottom area.\n\n2\n\nWrite a function to clean the headers up a bit\n\n3\n\nmodifiers are the top row of the variable names that aren’t captured by our headers object. They’re consistent from report to report. We’ll separate the modifier from the dates using ., for easier cleaning.\n\n4\n\nExtract only the components of the headers object that are needed – this depends on whether we’re talking about a single month-to-month comparison, or a time span. In the first column, we only need the “expenditure category” object.\n\n5\n\nRemove all . characters from the names so they don’t mess up our delimiter, and replace spaces and dashes with _.\n\n6\n\nPaste the two vectors together to get the names.\n\n7\n\nApply the function to each header\n\n8\n\nSet the names of the variables in each table to the corresponding header.\n\n\n\n\nThen, we just need to clean things up a bit more.\n\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\n1report_date &lt;- str_replace(basename(files), \"cpi_(.*)\\\\.pdf\", \"\\\\1\") |&gt; mdy()\n\n2cpi_data &lt;- map2(tables, report_date, ~mutate(.x, report_date = .y))\n\ncpi_data &lt;- map(\n  cpi_data, \n3  ~pivot_longer(., -c(Expenditure_category, report_date),\n                names_to=\"var\",\n                values_to = \"val\") |&gt;\n4    separate(var, c(\"varname\", \"vardate\"), sep = \"\\\\.\") |&gt;\n5    pivot_wider(id_cols = c(\"Expenditure_category\", \"report_date\", \"vardate\"), names_from = \"varname\", values_from = \"val\")\n)\n\ncpi_data &lt;- cpi_data |&gt;\n6  bind_rows() |&gt;\n  mutate(Expenditure_category = str_remove_all(Expenditure_category, \"[\\\\. ]{1,}$\") |&gt;\n7           str_trim())\n\ndim(cpi_data)\ncpi_data\n## [1] 6601    7\n## # A tibble: 6,601 × 7\n##    Expenditure_category report_date vardate      Rel_imp Unadj_idx Unadj_pct_chg\n##    &lt;chr&gt;                &lt;date&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n##  1 All items            2024-01-11  Nov2023        100        307.          NA  \n##  2 All items            2024-01-11  Dec2022         NA        297.          NA  \n##  3 All items            2024-01-11  Dec2023         NA        307.          NA  \n##  4 All items            2024-01-11  Dec2022_Dec…    NA         NA            3.4\n##  5 All items            2024-01-11  Nov2023_Dec…    NA         NA           -0.1\n##  6 All items            2024-01-11  Sep2023_Oct…    NA         NA           NA  \n##  7 All items            2024-01-11  Oct2023_Nov…    NA         NA           NA  \n##  8 Food                 2024-01-11  Nov2023         13.4      325.          NA  \n##  9 Food                 2024-01-11  Dec2022         NA        317.          NA  \n## 10 Food                 2024-01-11  Dec2023         NA        325.          NA  \n## # ℹ 6,591 more rows\n## # ℹ 1 more variable: Seas_adj_pct_chg &lt;dbl&gt;\n\n\n1\n\nDetermine the date of the report from the filename\n\n2\n\nAdd a column with the corresponding report date to each table\n\n3\n\nConvert each table to long form with expenditure category and report date as ID columns.\n\n4\n\nSplit the variable names from the period over which the variable is calculated. In theory, we should be able to determine the lag for each of these and not care about the date, but I don’t trust that the report has been that consistent over 2 years… paranoia.\n\n5\n\nPivot wider, so that there’s a column for each variable name.\n\n6\n\nBind all the tables together into a single table\n\n7\n\nClean up the expenditure category names so that the dots are gone.\n\n\n\n\nWe could probably get this data cleaner – the lagged columns should be specified better, but this will do for now. Let’s at least do something interesting with this data that wouldn’t have been possible without reading data in from the tables.\n\nlibrary(ggplot2)\nlibrary(dplyr)\ncpi_data |&gt;\n  filter(Expenditure_category %in% c(\"Energy\", \"Food\", \"Shelter\", \"Medical care services\", \"commodities\", \"Transportation services\")) |&gt;\n  mutate(Category = str_replace_all(Expenditure_category, c(\"commodities\"= \"Non-food Goods\", \"Medical care services\" = \"Medical\", \"Transportation services\" =\"Transportation\")) |&gt;\n           factor(levels = c(\"Shelter\", \"Non-food Goods\", \"Energy\", \"Food\", \"Medical\", \"Transportation\"))) |&gt;\n  select(Category, report_date, Rel_imp) |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x = report_date, y = Rel_imp, color = Category)) + geom_line() + \n  xlab(\"Date\") + ylab(\"Relative Importance in CPI-U Calculation\") + \n  theme_bw()\n\n\n\n\nChart of the relative importance of shelter, goods, energy, food, medical, and transportation costs in the CPI-U calculation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: The chunks of code here are set to not evaluate, because it turns out calling Java from Python from R works fine, but doing it from inside a quarto document might be a step too far. I’ve tested the code on my own machine and it works great, but if you have problems, leave a comment below. If you’d like to have a simple script to run, you can find the one I assembled for debugging here.\n\n\n\nIf we learned anything from doing this in R, it’s that it probably won’t work the first time. So this code saves a bit of evaluation time by using some of the info we got from R, like the page numbers (I’ve adjusted these to match python indexing).\n\nimport pdfplumber\nimport tabula\nfrom glob import glob\nimport numpy as np\n\nfiles = glob(\"../data/bls-pdfs/*.pdf\")\n\ndef find_page_number(file):\n  pdf = pdfplumber.open(file)\n  has_tb1 = [\"Table 1\" in i.extract_text() for i in pdf.pages]\n  pdf.close()\n  return int(np.where(has_tb1)[0][0])\n\n# page_numbers = [find_page_number(i) for i in files]\n# page_numbers = [i[0] for i in page_numbers]\npage_numbers = [8, 8, 7, 8, 8, 8, 7, 8, 9, 8, 8, 7, 8, 8, 8, 9, 8, 8, 8, 7, 7, 9, 8]\n\n\ntables = tabula.io.read_pdf(files[0], pages=page_numbers[0]+1)\n## NameError: name 'tabula' is not defined\ntables \n## NameError: name 'tables' is not defined\n\nOk, so this time, we get 5 columns, which isn’t quite right - it seems as if the major headers are determining the column structure.\nLet’s see if we can define the table area and help things out.\n\ntbl = tabula.io.read_pdf(files[0], pages=page_numbers[0]+1, area=[128, 35, 586, 575], pandas_options={'header': None})[0] \ntbl\n\n\nfrom itertools import chain\nimport pandas as pd \n\nheader = tabula.io.read_pdf(files[0], pages=page_numbers[0]+1, area=[89, 35, 128, 575], pandas_options={'header': None})[0]\n\n1def fix_headers(tbl):\n2  modifiers=[[\"\", \"Rel_imp.\"], [\"Unadj_idx.\"]*3, [\"Unadj_pct_chg.\"]*2, [\"Seas_adj_pct_chg.\"]*3]\n3  modifiers=list(chain.from_iterable(modifiers))\n  # https://stackoverflow.com/questions/11860476/how-to-unnest-a-nested-list\n  \n4  spans=pd.Series([tbl.loc[0,0],\n  ''.join(tbl.loc[1:2,1]),\n  ''.join(tbl.loc[1:2,2]),\n  ''.join(tbl.loc[1:2,3]),\n  ''.join(tbl.loc[1:2,4]),\n  ''.join(tbl.loc[0:3,5]),\n  ''.join(tbl.loc[0:3,6]),\n  ''.join(tbl.loc[0:3,7]),\n  ''.join(tbl.loc[0:3,8]),\n  ''.join(tbl.loc[0:3,9])])\n5  spans=spans.str.replace(\"\\.\", \"\", regex = True)\n  spans=spans.str.replace(\"[ -]\", \"_\", regex = True)\n  \n  return modifiers + spans\n\n6header = fix_headers(header)\nheader\n\n7def read_table_1(file, page_number):\n8  tbl    = read_pdf(file, pages=page_number+1, area=[128, 35, 586, 575], pandas_options={'header': None})[0]\n9  header = read_pdf(file, pages=page_number+1, area=[ 89, 35, 128, 575], pandas_options={'header': None})[0]\n10  header=fix_headers(header)\n11  tbl = tbl.rename(header, axis=1)\n  return tbl\n\ntables = [read_table_1(file, page_numbers[i]) for i,file in enumerate(files)]\ntables[2]\n\n\n1\n\nDefine a function to fix header text\n\n2\n\nFirst, create a nested list of modifiers that will repeat as many times as there are nested columns\n\n3\n\nUnnest the list of modifiers\n\n4\n\nPut the pieces of each header together properly\n\n5\n\nClean up the header pieces a bit\n\n6\n\nApply the function to one header to see if it works\n\n7\n\nWrite a function to read table 1 from each report\n\n8\n\nFirst, read the contents of the table\n\n9\n\nThen, read in the header from the table\n\n10\n\nFix the header using the fix_header function\n\n11\n\nRename the columns of the table contents with the header values.\n\n\n\n\nThen we just need to clean things up a bit more.\n\nimport pandas as pd\nimport os\n \n1report_date = pd.Series([os.path.basename(i) for i in files])\nreport_date = report_date.str.replace(\"cpi_|\\.pdf$\", \"\", regex=True)\nreport_date = pd.to_datetime(report_date, format=\"%m%d%Y\")\n\ncpi_data = pd.DataFrame()\nfor i,tbl in enumerate(tables):\n2  tbl['report_date'] = report_date[i]\n3  tbl = tbl.melt(id_vars=['Expenditure_category', 'report_date'], value_name='val', var_name='var')\n  cols = pd.DataFrame(tbl['var'].str.split(\"\\.\").to_list(), columns=['varname', 'vardate'])\n4  tbl = pd.concat([tbl, cols], axis = 1)\n5  tbl = tbl.set_index(['report_date', 'Expenditure_category', 'vardate'])\n  tbl = tbl.drop(['var'], axis=1)\n  tbl_wide = tbl.pivot(columns='varname', values = 'val')\n  tbl_wide = tbl_wide.reset_index()\n6  cpi_data = pd.concat([cpi_data, tbl_wide], axis=0)\n\n7cpi_data['Expenditure_category'] = cpi_data['Expenditure_category'].str.replace(\"[ \\.]{1,}$\", \"\", regex=True)\n\ncpi_data.shape\ncpi_data.head\n\n\n1\n\nDetermine the date of the report from the filename\n\n2\n\nAdd a column with the corresponding report date to each table\n\n3\n\nConvert each table to long form with expenditure category and report date as ID columns.\n\n4\n\nSplit the variable names from the period over which the variable is calculated. In theory, we should be able to determine the lag for each of these and not care about the date, but I don’t trust that the report has been that consistent over 2 years… paranoia.\n\n5\n\nPivot wider, so that there’s a column for each variable name.\n\n6\n\nBind all the tables together into a single table\n\n7\n\nClean up the expenditure category names so that the dots are gone.\n\n\n\n\nWe could probably get this data cleaner – the lagged columns should be specified better, but this will do for now. Let’s at least do something interesting with this data that wouldn’t have been possible without reading data in from the tables.\n\ntmp = cpi_data.query(\"~vardate.str.contains(r'_')\")\ntmp = tmp.assign(vardate = pd.to_datetime(tmp['vardate'], format=\"%b%Y\")) \n\ntmp2 = tmp.query('Expenditure_category.isin([\"Energy\", \"Food\", \"Shelter\", \"Medical care services\", \"commodities\", \"Transportation services\"])')\n\ntmp2 = tmp2.assign(year = lambda x: x['vardate'].dt.year,\n                   days = lambda x: (pd.to_datetime(x['year']+1, format='%Y') - \n                          pd.to_datetime(x['year'], format='%Y')).dt.days,\n                   var_dec_date = lambda x: x.year + (x['vardate']-pd.to_datetime(x.year, format='%Y'))/ (x.days * pd.to_timedelta(1, unit=\"D\")))\n\ncat_repl = {'Medical care services':'Medical', 'commodities':'Goods', 'Transportation services':'Transit'}\ntmp2=tmp2.rename(columns = {'Expenditure_category':'Category', 'var_dec_date': 'date'})\nfor old,new in cat_repl.items():\n  tmp2.loc[:,'Category'] = tmp2.Category.str.replace(old, new, regex=False)\n\ntmp_plot = tmp2[['date', 'Unadj_idx', 'Category']]\ntmp_plot = tmp_plot.drop_duplicates()\ntmp_plot = tmp_plot.assign(Unadj_idx = lambda x: pd.to_numeric(x.Unadj_idx))\n\nimport seaborn.objects as so\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplot = sns.lineplot(data = tmp_plot, x = 'date', y = 'Unadj_idx', hue = 'Category')\nplot.set(xlabel=\"Date\", ylabel=\"Unadjusted Index\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 34.7: A screenshot of Table 1 of the CPI monthly report from December 2023\n\n\n\n\n\n\n\n\n\n\nFigure 34.8: A screenshot of one page of one of the PDFs, showing how locate_areas works.\n\n\n\n\n\n\n\n\n\n\n34.4 Other PDF Options to Explore\n\n\n\n\n\n\nparsemypdf [17], a collection of AI-based parsing libraries\ncamelot, a python library for table parsing",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#references",
    "href": "part-advanced-topics/06-pdf-tools.html#references",
    "title": "34  Working with PDFs",
    "section": "References",
    "text": "References\n\n\n\n\n[1] M. Klindt, “PDF/a considered harmful for digital preservation,” in iPRES 2017 conference proceedings, 2017 [Online]. Available: https://phaidra.univie.ac.at/o:931063. [Accessed: Jul. 15, 2025]\n\n\n[2] B. Edwards, “Why extracting data from PDFs is still a nightmare for data experts. Ars technica,” Mar. 11, 2025. [Online]. Available: https://arstechnica.com/ai/2025/03/why-extracting-data-from-pdfs-is-still-a-nightmare-for-data-experts/. [Accessed: Jul. 15, 2025]\n\n\n[3] Q. Zhang et al., “Document parsing unveiled: Techniques, challenges, and prospects for structured information extraction.” arXiv, Oct. 28, 2024 [Online]. Available: http://arxiv.org/abs/2410.21169. [Accessed: Jul. 15, 2025]\n\n\n[4] J. B. Merrill, “Purifying the sea of PDF data, automatically. Medium,” Jul. 24, 2017. [Online]. Available: https://open.nytimes.com/purifying-the-sea-of-pdf-data-automatically-99e6043a09b3. [Accessed: Jul. 25, 2025]\n\n\n[5] Wikimedia contributors, “History of PDF,” Wikipedia. Oct. 30, 2024 [Online]. Available: https://en.wikipedia.org/w/index.php?title=History_of_PDF&oldid=1254285186. [Accessed: Jul. 15, 2025]\n\n\n[6] GNUpdf project, “Introduction to PDF,” Oct. 10, 2014. [Online]. Available: https://web.archive.org/web/20141010035745/http://gnupdf.org/Introduction_to_PDF. [Accessed: Jul. 15, 2025]\n\n\n[7] R. Hodson, PDF succinctly. Morrisville, NC: Syncfusion, Inc, 2012 [Online]. Available: https://web.archive.org/web/20140706124739/http://www.syncfusion.com/Content/downloads/ebook/PDF_Succinctly.pdf. [Accessed: Jul. 15, 2025]\n\n\n[8] J. C. King, “Adobe: Introduction to the insides of PDF,” Apr. 26, 2005 [Online]. Available: https://web.archive.org/web/20141212020737/http://www.adobe.com/content/dam/Adobe/en/technology/pdfs/PDF_Day_A_Look_Inside.pdf. [Accessed: Jul. 15, 2025]\n\n\n[9] J. Singer-Vine and The pdfplumber contributors, “Pdfplumber.” Jun. 2025 [Online]. Available: https://github.com/jsvine/pdfplumber. [Accessed: Jul. 15, 2025]\n\n\n[10] J. Ooms, “Pdftools: Text extraction, rendering and converting of PDF documents.” rOpenSci, Mar. 03, 2025 [Online]. Available: https://packages.ropensci.org/pdftools. [Accessed: Jul. 16, 2025]\n\n\n[11] S. Weil, R. Smith, and Z. Podobny, “Tesseract.” Google, Jul. 18, 2025 [Online]. Available: https://github.com/tesseract-ocr/tesseract. [Accessed: Jul. 18, 2025]\n\n\n[12] M. A. Lee and S. Hoffstaetter, “Pytesseract: Python-tesseract is a python wrapper for google’s tesseract-OCR.” Aug. 15, 2024 [Online]. Available: https://github.com/madmaze/pytesseract. [Accessed: Jul. 18, 2025]\n\n\n[13] J. Ooms, “Tesseract: Open source OCR engine.” 2025 [Online]. Available: https://CRAN.R-project.org/package=tesseract\n\n\n[14] M. Aristarán, M. Tigas, J. B. Merrill, J. Das, D. Frackman, and T. Swicegood, “Tabulapdf/tabula.” Tabula, Jul. 18, 2025 [Online]. Available: https://github.com/tabulapdf/tabula. [Accessed: Jul. 18, 2025]\n\n\n[15] vinayak-mehta and bosn, “Camelot: PDF table extraction for humans.” camelot-dev, Jul. 18, 2025 [Online]. Available: https://github.com/camelot-dev/camelot. [Accessed: Jul. 18, 2025]\n\n\n[16] Y. Dennis, “Tabula-py vs. Camelot: A duel of PDF table extraction titans. Medium,” Mar. 04, 2024. [Online]. Available: https://python.plainenglish.io/tabula-py-vs-camelot-a-duel-of-pdf-table-extraction-titans-61a534c5134d. [Accessed: Jul. 15, 2025]\n\n\n[17] R. Srivastava, “Genieincodebottle/parsemypdf.” Jul. 13, 2025 [Online]. Available: https://github.com/genieincodebottle/parsemypdf. [Accessed: Jul. 16, 2025]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-pdf-tools.html#footnotes",
    "href": "part-advanced-topics/06-pdf-tools.html#footnotes",
    "title": "34  Working with PDFs",
    "section": "",
    "text": "In the US, the Department of Justice’s interpretation of Title II of the Americans with Disabilities Act requires state and government services (including education) to meet digital accessibility standards. Many universities directed professors to shift materials to Word or HTML instead of PDF, as making PDFs accessible requires proprietary Adobe software.↩︎\nI actually acquired 3 days worth of transcripts from a single trial, but each day has 200+ pages, so working with only one day seems reasonable.↩︎",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Working with PDFs</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html",
    "title": "35  Animated and Interactive Graphics",
    "section": "",
    "text": "35.1  Objectives\nInteractive and animated graphics are one of the major advantages of using the quarto or Rmarkdown (or Jupyter) ecosystems - because you can easily create web pages in markdown (without the pain of HTML), you aren’t limited by paper any more. This chapter will focus on two different technologies that allow you to create different types of interactive charts, graphs, and interfaces: Shiny, and Plotly. There are many other options out there - d3.js, Observable.js, highCharts, crosstalk (an interface specifically designed for Shiny and markdown) [1].\nIt is helpful to think about interactivity in a couple of different ways:\n(This is not a full list of all of the types of interactivity, just a few of the more common options)\nWhen deciding on an interactive tool, it is important to consider the types of interactivity you need to accomplish the task, because not all tools support all types of interactivity.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html#objectives",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html#objectives",
    "title": "35  Animated and Interactive Graphics",
    "section": "",
    "text": "Identify appropriate types of interactivity to accomplish a visualization task\nSelect an appropriate interactive toolkit that supports the required type(s) of interactivity\nCreate animated or interactive charts that facilitate viewer understanding",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html#plotly",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html#plotly",
    "title": "35  Animated and Interactive Graphics",
    "section": "\n35.2 Plotly",
    "text": "35.2 Plotly\nPlotly PlotlyOpenSource2022? is a graphing library that uses javascript to add interactivity to graphics. There are several different ways to create plotly graphs in R or python. Here, we’ll discuss 3 approaches: - Working with plotly in R directly - Working with plotly in python directly - Using ggplotly, which converts a ggplot to a plotly plot automatically\nResources:\n\nR Plotly cheat sheet\nPython Plotly cheat sheet\n\nWe’ll demonstrate plotly’s capabilities using the volcanoes data from Tidy Tuesday.\n\n\n\n\n\n\nData set up\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"plotly\" %in% installed.packages()) \n  install.packages(\"plotly\")\n\nlibrary(plotly)\n\n\nlibrary(readr) # reading in data\nlibrary(dplyr) # cleaning data\nlibrary(tidyr) # merging data\nlibrary(lubridate) # dates and times\nlibrary(stringr) # string manipulation\nlibrary(ggplot2) # plotting\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano &lt;- read_csv(paste0(url_stub, \"volcano.csv\"))\neruptions &lt;- read_csv(paste0(url_stub, \"eruptions.csv\"))\nevents &lt;- read_csv(paste0(url_stub, \"events.csv\"))\nsulfur &lt;- read_csv(paste0(url_stub, \"sulfur.csv\"))\ntrees &lt;- read_csv(paste0(url_stub, \"tree_rings.csv\"))\n\n\n\n\n# Uncomment and run this line if you don't have plotly installed\n# %pip install plotly\n\nimport plotly.express as px\nimport plotly.io as pio # this allows plotly to play nice with markdown\n\nimport pandas as pd\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano = pd.read_csv(url_stub + \"volcano.csv\")\neruptions = pd.read_csv(url_stub + \"eruptions.csv\")\nevents = pd.read_csv(url_stub + \"events.csv\")\nsulfur = pd.read_csv(url_stub + \"sulfur.csv\")\ntrees = pd.read_csv(url_stub + \"tree_rings.csv\")\n\n\n\n\n\n\n\nLet’s try out plotly while doing a bit of exploratory data analysis on this dataset.\n\n\n\n\n\n\nCleaning up volcano\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nvolcano &lt;- volcano %&gt;%\n  filter(tectonic_settings != \"Unknown\") %&gt;%\n  separate(tectonic_settings, into = c(\"zone\", \"crust\"), \n           sep = \"/\", remove = F) %&gt;%\n  # Remove anything past the first punctuation character \n  # catch (xx) and ?\n  mutate(volcano_type = str_remove(primary_volcano_type, \n                                   \"[[:punct:]].*$\"))\n\n\n\n\nvolcano2 = volcano.query(\"tectonic_settings != 'Unknown'\")\nvolcano2[['zone', 'crust']] = volcano2.tectonic_settings.\\\n                              str.split(\" / \", expand = True)\n# Remove anything after ( as well as ? if it exists\nvolcano2 = volcano2.assign(volcano_type =\n            volcano2['primary_volcano_type'].\\\n            str.replace(r\"(\\(.*)?\\??$\", \"\", regex = True))\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship Between Elevation and Zone\n\n\n\n\n\nLet’s start by seeing whether the elevation of a volcano changes based on the type of zone it’s on - we might expect that Rift zone volcanos (where plates are pulling away from each other) might not be as high.\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = zone, fill = zone, y = elevation)) +\n  geom_boxplot() +\n  coord_flip()\nggplotly(p)\n\n\n\n\n\n\n\nThe plot_ly function is pipe friendly.\nVariable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\nlibrary(plotly)\nplot_ly(volcano, color= ~zone, x = ~elevation, type = \"box\")\n\n\n\n\n\n\n\n\nfig = px.box(volcano2, x = \"elevation\", color = \"zone\")\n\nfile = 'plotly-python/boxplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n## NameError: name 'pl' is not defined\n\n\n## Error: object 'py' not found\n\n\n\n\nIt doesn’t really look like there’s much difference.\n\n\n\n\n\n\n\n\n\nExamining Volcano Type\n\n\n\n\n\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug()\nggplotly(p)\n\n\n\n\n\n\n\nSince I’m trying to do this without the tidyverse, I’ll try out the new base R pipe, |&gt;, and the corresponding new anonymous function notation, \\().1\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type))\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly as pl\n\n# This creates a list of vectors, one for each type of volcano\nvolcano = volcano.groupby(\"volcano_type\")[\"elevation\"].count()\n## KeyError: 'volcano_type'\ntype_list = volcano.groupby(\"volcano_type\").elevation.apply(list)\n## KeyError: 'volcano_type'\ntype_labels = volcano.volcano_type.unique()\n## AttributeError: 'DataFrame' object has no attribute 'volcano_type'\nfig = ff.create_distplot(type_list, group_labels = type_labels, show_hist=False)\n## NameError: name 'type_list' is not defined\n\nfile = 'plotly-python/distplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nHere, the interactivity actually helps a bit: we don’t need to use the legend to see what each curve corresponds to. We can see that submarine volcanoes are typically much lower in elevation (ok, duh), but also that subglacial volcanoes are found in a very limited range. If we double-click on a legend entry, we can get rid of all other curves and examine each curve one by one.\nI added the rug layer after the initial bout because I was curious how much data each of these curves were based on. If we want only curves with n &gt; 10 observations, we can do that:\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug(aes(text = paste0(volcano_name, \", \", country)))\nggplotly(p)\n\n\n\n\n\nIf we want to specify additional information that should show up in the tooltip, we can do that as well by adding the text aesthetic even though geom_rug doesn’t take a text aesthetic. You may notice that ggplot2 complains about the unknown aesthetic I’ve added to geom_rug: That allows us to mouse over each data point in the rug plot and see what volcano it belongs to. So we can tell from the rug plot that the tallest volcano is Ojas de Salvado, in Chile/Argentina (I believe that translates to Eyes of Salvation?).\n\n\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n    tmp$n = nrow(df)\n    tmp\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type)) |&gt;\n  subset(n &gt; 15)\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, \n        type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\nvolcano = volcano.assign(count = volcano.groupby(\"volcano_type\").\\\n                                  volcano_type.transform(\"count\"))\n## KeyError: 'volcano_type'\n\ncommon_volcano = volcano.query(\"count &gt; 15\").sort_values([\"volcano_type\"])\n## pandas.errors.UndefinedVariableError: name 'count' is not defined\ncommon_volcano[\"label\"] = common_volcano.volcano_name + \", \" + common_volcano.country\n## NameError: name 'common_volcano' is not defined\n# This creates a list of vectors, one for each type of volcano\ntype_list = common_volcano.groupby(\"volcano_type\").elevation.apply(list)\n## NameError: name 'common_volcano' is not defined\n# rug_text = common_volcano.groupby(\"volcano_type\").label.apply(list)\ntype_labels = common_volcano.volcano_type.unique()\n## NameError: name 'common_volcano' is not defined\nfig = ff.create_distplot(type_list, group_labels = type_labels, rug_text = rug_text, show_hist=False)\n## NameError: name 'type_list' is not defined\nfig.show()\n\n                        \n\n\n\n\n\n\n\n\n\nAt any rate, there isn’t nearly as much variation as I was expecting in the elevation of different types of volcanoes.\nggplotly makes it very easy to generate plots that have a ggplot2 equivalent; you can customize these plots further using plotly functions that we’ll see in the next section. But first, try the interface out on your own.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nggplotly\nR + plotly\nPython\n\n\n\nConduct an exploratory data analysis of the eruptions dataset. What do you find?\n\n\n\nhead(eruptions)\n## # A tibble: 6 × 15\n##   volcano_number volcano_name eruption_number eruption_category area_of_activity\n##            &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n## 1         266030 Soputan                22354 Confirmed Erupti… &lt;NA&gt;            \n## 2         343100 San Miguel             22355 Confirmed Erupti… &lt;NA&gt;            \n## 3         233020 Fournaise, …           22343 Confirmed Erupti… &lt;NA&gt;            \n## 4         345020 Rincon de l…           22346 Confirmed Erupti… &lt;NA&gt;            \n## 5         353010 Fernandina             22347 Confirmed Erupti… &lt;NA&gt;            \n## 6         273070 Taal                   22344 Confirmed Erupti… &lt;NA&gt;            \n## # ℹ 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;,\n## #   start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;,\n## #   end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\nsummary(eruptions %&gt;% mutate(eruption_category = factor(eruption_category)))\n##  volcano_number   volcano_name       eruption_number\n##  Min.   :210010   Length:11178       Min.   :10001  \n##  1st Qu.:263310   Class :character   1st Qu.:12817  \n##  Median :290050   Mode  :character   Median :15650  \n##  Mean   :300284                      Mean   :15667  \n##  3rd Qu.:343030                      3rd Qu.:18464  \n##  Max.   :600000                      Max.   :22355  \n##                                                     \n##             eruption_category area_of_activity        vei       \n##  Confirmed Eruption  :9900    Length:11178       Min.   :0.000  \n##  Discredited Eruption: 166    Class :character   1st Qu.:1.000  \n##  Uncertain Eruption  :1112    Mode  :character   Median :2.000  \n##                                                  Mean   :1.948  \n##                                                  3rd Qu.:2.000  \n##                                                  Max.   :7.000  \n##                                                  NA's   :2906   \n##    start_year        start_month       start_day      evidence_method_dating\n##  Min.   :-11345.0   Min.   : 0.000   Min.   : 0.000   Length:11178          \n##  1st Qu.:   680.0   1st Qu.: 0.000   1st Qu.: 0.000   Class :character      \n##  Median :  1847.0   Median : 1.000   Median : 0.000   Mode  :character      \n##  Mean   :   622.8   Mean   : 3.451   Mean   : 7.015                         \n##  3rd Qu.:  1950.0   3rd Qu.: 7.000   3rd Qu.:15.000                         \n##  Max.   :  2020.0   Max.   :12.000   Max.   :31.000                         \n##  NA's   :1          NA's   :193      NA's   :196                            \n##     end_year      end_month         end_day         latitude      \n##  Min.   :-475   Min.   : 0.000   Min.   : 0.00   Min.   :-77.530  \n##  1st Qu.:1895   1st Qu.: 3.000   1st Qu.: 4.00   1st Qu.: -6.102  \n##  Median :1957   Median : 6.000   Median :15.00   Median : 17.600  \n##  Mean   :1917   Mean   : 6.221   Mean   :13.32   Mean   : 16.866  \n##  3rd Qu.:1992   3rd Qu.: 9.000   3rd Qu.:21.00   3rd Qu.: 40.821  \n##  Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   : 85.608  \n##  NA's   :6846   NA's   :6849     NA's   :6852                     \n##    longitude      \n##  Min.   :-179.97  \n##  1st Qu.: -77.66  \n##  Median :  55.71  \n##  Mean   :  31.57  \n##  3rd Qu.: 139.39  \n##  Max.   : 179.58  \n## \n\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet’s start out seeing what month most eruptions occur in…\n\n# Note, I'm using the original month, so 0 = unknown\np &lt;- ggplot(erupt, aes(x = factor(start_month))) + geom_bar()\nggplotly(p)\n\n\n\n\n# I could rename some of the factors to make this pretty, but... nah\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index,\np &lt;- ggplot(erupt, aes(x = vei)) + geom_bar()\nggplotly(p)\n\n\n\n\n\nWe can also look at the frequency of eruptions over time. We’ll expect some historical bias - we don’t have exact dates for some of these eruptions, and if no one was around to write the eruption down (or the records were destroyed) there’s not going to be a date listed here.\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  filter(start_year &gt; 0) %&gt;%\n\nggplot(aes(x = start_date, xend = start_date,\n                  y = 0, yend = duration,\n                  color = evidence_method_dating)) +\n  geom_segment() +\n  geom_point(size = .5, aes(text = volcano_name)) +\n  xlab(\"Eruption Start\") +\n  ylab(\"Eruption Duration (days)\") +\n  facet_wrap(~vei, scales = \"free_y\")\nggplotly(p)\n\n\n\n\n\nAs expected, it’s pretty rare to see many eruptions before ~1800 AD, which is about when we have reliable historical records2 for most of the world (exceptions include e.g. Vestuvius, which we have extensive written information about).\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\nggplot(aes(x = factor(vei), y = duration)) +\n  geom_violin() +\n  xlab(\"VEI\") +\n  ylab(\"Eruption Duration (days)\") +\n  scale_y_sqrt()\nggplotly(p)\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully).\n\n\n\nhead(eruptions)\n## # A tibble: 6 × 15\n##   volcano_number volcano_name eruption_number eruption_category area_of_activity\n##            &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n## 1         266030 Soputan                22354 Confirmed Erupti… &lt;NA&gt;            \n## 2         343100 San Miguel             22355 Confirmed Erupti… &lt;NA&gt;            \n## 3         233020 Fournaise, …           22343 Confirmed Erupti… &lt;NA&gt;            \n## 4         345020 Rincon de l…           22346 Confirmed Erupti… &lt;NA&gt;            \n## 5         353010 Fernandina             22347 Confirmed Erupti… &lt;NA&gt;            \n## 6         273070 Taal                   22344 Confirmed Erupti… &lt;NA&gt;            \n## # ℹ 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;,\n## #   start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;,\n## #   end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet’s start out seeing what month most eruptions occur in…\n\n# Note, I'm using the original month, so 0 = unknown\nerupt %&gt;%\n  count(start_month) %&gt;%\n  plot_ly(\n    data = .,\n    x = ~start_month,\n    y = ~n,\n    type = \"bar\"\n)\n\n\n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nerupt %&gt;%\n  count(vei) %&gt;%\n  plot_ly(x = ~vei, y = ~n, type = \"bar\")\n\n\n\n\n\n\nerupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\n  plot_ly(x = ~ factor(vei),\n          y = ~ duration, \n          split = ~factor(vei),\n          type = \"violin\") %&gt;%\n  layout(yaxis = list(type=\"log\"))\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully).\n\n\nIn Python, negative dates are even more of a pain to work with if you’re using standard libraries, so we’ll install the astropy class with pip install astropy. BCE dates are still a pain in the … but they at least work.\n\neruptions.head()\n##    volcano_number            volcano_name  ...  latitude longitude\n## 0          266030                 Soputan  ...     1.112   124.737\n## 1          343100              San Miguel  ...    13.434   -88.269\n## 2          233020  Fournaise, Piton de la  ...   -21.244    55.708\n## 3          345020      Rincon de la Vieja  ...    10.830   -85.324\n## 4          353010              Fernandina  ...    -0.370   -91.550\n## \n## [5 rows x 15 columns]\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfrom astropy.time import Time,TimeDelta\nimport numpy as np\nimport math\n\ndef fix_date(yyyy, mm, dd):\n  # The zero, one columns allow using pd.max(axis = 1) where we'd use pmax in R\n  neg = yyyy &lt;= 0\n  nyear = -yyyy\n  \n  year = max([yyyy, 1])\n  subtract_year = max([nyear, 0]) + neg\n  \n  day = dd\n  month = mm\n  \n  if math.isnan(day): \n    day = 1\n  if math.isnan(month): \n    month = 1\n      \n  if day == 0: \n    day = 1\n  if month == 0: \n    month = 1\n  \n  dateformat = \"%04d-%02d-%02d\" % (year, month, day)\n  \n  date = Time(dateformat, format = \"iso\", scale = 'ut1')\n  datefix = date - TimeDelta(subtract_year*365, format= 'jd')\n  return datefix\n\nerupt = eruptions.query(\"eruption_category == 'Confirmed Eruption'\")\nerupt.fillna(0, inplace = True)\nerupt['start_date'] = erupt.apply(lambda x: fix_date(x.start_year, x.start_month, x.start_day), axis = 1)\nerupt['end_date'] = erupt.apply(lambda x: fix_date(x.end_year, x.end_month, x.end_day), axis = 1)\nerupt['duration'] = erupt.end_date - erupt.start_date\n# Convert back to numeric\nerupt['duration'] = erupt.duration.apply(lambda x: x.to_value(\"jd\", \"decimal\")) # Julian day\n\nLet’s start out seeing what month most eruptions occur in…\n\nimport plotly.express as px\ntmp = erupt.groupby(\"start_month\").count()\ntmp = tmp.reset_index()\n# Note, I'm using the original month, so 0 = unknown\nfig = px.bar(tmp, x = 'start_month', y = 'volcano_number')\n\nfile = 'plotly-python/eruptplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nfig = px.bar(\n  erupt.groupby(\"vei\").count().reset_index(),\n  x = \"vei\", y = \"volcano_number\")\n\nfile = 'plotly-python/vei-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\nerupt[\"duration_yr\"] = erupt.duration/365.25\n## TypeError: unsupported operand type(s) for /: 'decimal.Decimal' and 'float'\nfig = px.box(\n  erupt,\n  x = \"vei\",\n  y = \"duration_yr\",\n  points = \"all\"\n)\n## ValueError: Value of 'y' is not the name of a column in 'data_frame'. Expected one of ['volcano_number', 'volcano_name', 'eruption_number', 'eruption_category', 'area_of_activity', 'vei', 'start_year', 'start_month', 'start_day', 'evidence_method_dating', 'end_year', 'end_month', 'end_day', 'latitude', 'longitude', 'start_date', 'end_date', 'duration'] but received: duration_yr\n\nfile = 'plotly-python/vei-duration-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully)\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Interactivity\n\n\n\n\n\nPlotly integration with ggplot2 is nice, but obviously not a universal summary of what it can do. Let’s look at another example of plotly in R/python without ggplot2 integration.\nWe start with a scatterplot of volcanoes along the earth’s surface:\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, lat = volcano$latitude)\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, lon = \"longitude\", lat = \"latitude\")\n\nfile = 'plotly-python/scatter-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nAnd then we can start customizing.\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, \n        lat = volcano$latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano$volcano_name, \"\\n\",\n                      \"Last Erupted: \", volcano$last_eruption_year),\n        # Change the markers because why not?\n        marker = list(color = \"#d00000\", opacity = 0.25)\n        )\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, \n  lon = \"longitude\", lat = \"latitude\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\nfig.update_traces(marker=dict(size=12, opacity = 0.25, color = 'red'),\n                  selector=dict(mode='markers'))\n\n                        \n\n\n\nfile = 'plotly-python/scatter-hover-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nPlotly will handle some variable mappings for you, depending on which “trace” (type of plot) you’re using.\n\n\nR\nPython\n\n\n\nThe plot_ly function is also pipe friendly. Variable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\n# Load RColorBrewer for palettes\nlibrary(RColorBrewer)\n\nvolcano %&gt;%\n  group_by(volcano_type) %&gt;% \n  mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\nplot_ly(type = \"scattergeo\", lon = ~longitude, lat = ~latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano_name, \"\\n\",\n                      \"Last Erupted: \", last_eruption_year),\n        color = ~ volcano_type,\n        # Specify a palette\n        colors = brewer.pal(length(unique(.$volcano_type)), \"Paired\"),\n        # Change the markers because why not?\n        marker = list(opacity = 0.5)\n        )\n\n\n\n\n\n\n\n\nvolc_sub = volcano.groupby(\"volcano_type\").agg({'volcano_number': ['size']})\n## KeyError: 'volcano_type'\nvolc_sub.columns = [\"n\"]\n## NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.reset_index()\n## NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.query(\"n &gt;= 15\")\n## NameError: name 'volc_sub' is not defined\nvolc_sub = pd.merge(volc_sub['volcano_type'], volcano, on = 'volcano_type', how = 'inner')\n## NameError: name 'volc_sub' is not defined\n\nfig = px.scatter_geo(volc_sub, \n  lon = \"longitude\", lat = \"latitude\", \n  color = \"volcano_type\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\n## NameError: name 'volc_sub' is not defined\nfig.update_traces(marker=dict(size=12, opacity = 0.25),\n                  selector=dict(mode='markers'))\n\n                        \n\n\n\nfile = 'plotly-python/scatter-hover-update-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html#leaflet-maps",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html#leaflet-maps",
    "title": "35  Animated and Interactive Graphics",
    "section": "\n35.3 Leaflet maps",
    "text": "35.3 Leaflet maps\n\n\nI’m sorry, but I haven’t managed to redo this part of the chapter in python as well as R. Hopefully I’ll get to it soon. You can find a tutorial for Python + Leaflet here [2].\nLeaflet is another javascript library that allows for interactive data visualization. We’re only going to briefly talk about it here, but there is extensive documentation that includes details of how to work with different types of geographical data, chloropleth maps, plugins, and more.\n\n\n\n\n\n\nBigfoot Sightings\n\n\n\n\n\nTo explore the leaflet package, we’ll start out playing with a dataset of Bigfoot sightings assembled from the Bigfoot Field Researchers Organization’s Google earth tool\n\nif (!\"leaflet\" %in% installed.packages()) install.packages(\"leaflet\")\n\nlibrary(leaflet)\nlibrary(readr)\n\nbigfoot_data &lt;- read_csv(\"https://query.data.world/s/egnaxxvegdkzzrhfhdh4izb6etmlms\")\n\nWe can start out by plotting a map with the location of each sighting. I’ve colored the points in a seasonal color scheme, and added the description of each incident as a mouseover label.\n\nbigfoot_data %&gt;%\n  filter(classification == \"Class A\") %&gt;%\n  mutate(seasoncolor = str_replace_all(season, c(\"Fall\" = \"orange\",\n                                                 \"Winter\" = \"skyblue\",\n                                                 \"Spring\" = \"green\",\n                                                 \"Summer\" = \"yellow\")),\n         # This code just wraps the description to the width of the R terminal\n         # and inserts HTML for a line break into the text at appropriate points\n         desc_wrap = purrr::map(observed, ~strwrap(.) %&gt;%\n                                  paste(collapse = \"&lt;br/&gt;\") %&gt;%\n                                  htmltools::HTML())) %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~longitude, ~latitude, color = ~seasoncolor, label = ~desc_wrap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSquirrels of New York City\n\n\n\n\n\nOf course, because this is an interactive map library, we aren’t limited to any one scale. We can also plot data at the city level:\n\n# library(nycsquirrels18)\n# data(squirrels)\n\nsquirrels &lt;- readr::read_csv(\"data/nycsquirrels.csv\")\n## Error: 'data/nycsquirrels.csv' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics').\nhead(squirrels)\n## Error: object 'squirrels' not found\n\nsquirrels %&gt;%\n  mutate(color = tolower(primary_fur_color)) %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~long, ~lat, color = ~color)\n## Error: object 'squirrels' not found\n\n\n\n\n\n\n\n\n\n\nEcological Regions\n\n\n\n\n\nWe can also plot regions, instead of just points. I downloaded a dataset released by the state of California, Crotch’s Bumble Bee Range - CDFW dataset, which shows the range of the Crotch’s Bumble Bee (Bombus crotchii).\nI’ve set this chunk to not evaluate because it causes the book to be painfully large.\n\nlibrary(sf)\nbees &lt;- st_read(\"../data/Crotch_s_Bumble_Bee_Range_-_CDFW_[ds3095].geojson\")\nbees &lt;- sf::st_transform(bees, 4326)\n\nbees %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = F, fillOpacity = 0.25,\n              fillColor = \"yellow\")\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nStarter R code\nR solution\n\n\n\nDownload the Shapefiles for the 116th Congress Congressional Districts. Unzip the file and read it in using the code below (you’ll have to change the file path). Use the MIT Election Data and Science Lab’s US House election results, and merge this data with the shapefiles to plot the results of the 2018 midterms in a way that you think is useful (you can use any of the available data).\nSome notes:\n\nFIPS codes are used to identify the state and district, with 00 indicating at-large districts (one district for the state) and 98 indicating non-voting districts.\nIf you would like to add in the number of citizens of voting age, you can get that information here but you will have to do some cleaning in order to join the table with the others.\nMinnesota’s Democratic-farmer-labor party caucuses with the Democrats but maintains its name for historical reasons. You can safely recode this if you want to.\n\n\n\n\nlibrary(sf)\n# Read in the districts\nziptemp &lt;- tempfile(fileext=\".zip\")\nshapeurl &lt;- \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_cd116_5m.zip\"\ndownload.file(shapeurl, destfile = ziptemp, mode = \"wb\")\nunzip(ziptemp, exdir = \"data/116_congress\")\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\")\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Read in the results\nelection_results &lt;- read_csv(\"data/1976-2020-house.csv\") %&gt;%\n  filter(year == 2018) %&gt;%\n  mutate(state_fips = sprintf(\"%02d\", as.integer(state_fips)),\n         district = sprintf(\"%02d\", as.integer(district)))\n## Error: 'data/1976-2020-house.csv' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics').\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n\n\n\nlibrary(sf)\nlibrary(htmltools) # to mark labels as html code\n\n# Read in the results\nelection_results &lt;- election_results %&gt;%\n  group_by(state, state_fips, state_po, district, stage) %&gt;%\n  arrange(candidatevotes) %&gt;%\n  mutate(pct = candidatevotes/totalvotes) %&gt;%\n  mutate(party = str_to_lower(party)) %&gt;%\n  # Keep the winner only\n  filter(pct == max(pct)) %&gt;%\n  # Fix Minnesota\n  mutate(party = ifelse(party == \"democratic-farmer-labor\", \"democrat\", party))\n## Error: object 'election_results' not found\n\n# Read in the districts\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\") %&gt;%\n  mutate(geometry = st_transform(geometry, crs = st_crs(\"+proj=longlat +datum=WGS84\")))\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n# Merge\ncongress_districts &lt;- congress_districts %&gt;%\n  left_join(election_results, by = c(\"STATEFP\" = \"state_fips\", \"CD116FP\" = \"district\")) %&gt;%\n  mutate(party = factor(party, levels = c(\"republican\", \"democrat\")),\n         short_party = ifelse(party == \"republican\", \"R\", \"D\"),\n         label = paste0(state_po, \"-\", district, candidate, \" (\", short_party, \")\"))\n## Error: object 'election_results' not found\n\n# Define a palette\nregion_pal &lt;- colorFactor(c(\"#e9141d\", \"#0015bc\"), congress_districts$party)\n\ncongress_districts %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = TRUE, fillOpacity = ~pct/2,\n              # still want to see what's underneath, even in safe districts\n              fillColor = ~region_pal(party), color = ~region_pal(party),\n              label = ~label)\n## Error in eval(f[[2]], metaData(data), environment(f)): object 'label' not found",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html#shiny",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html#shiny",
    "title": "35  Animated and Interactive Graphics",
    "section": "\n35.4 Shiny",
    "text": "35.4 Shiny\nTake a few minutes and poke around the RStudio Shiny user showcase. It helps to have some motivation, and to get a sense of what is possible before you start learning something.\n\n\nOne of the more amusing ones I found was an exploration of lego demographics.\nShiny is a framework for building interactive web applications in R (and now in Python too!). Unlike plotly and other graphics engines, Shiny depends on an R instance on a server to do computations. This means Shiny is much more powerful and has more capabilities, but also that it’s harder to share and deploy - you have to have access to a web server with R installed on it. If you happen to have a server like that, though, Shiny is pretty awesome. Posit runs a service called shinyapps.io that will provide some limited free hosting, as well as paid plans for apps that have more web traffic, but you can also create Shiny apps for local use - I often do this for model debugging when I’m using neural networks, because they’re so complicated.\nPosit has a set of well produced video tutorials to introduce Shiny. I’d recommend you at least listen to the introduction if you’re a visual/audio learner (the whole tutorial is about 2 hours long). There is also a written tutorial if you prefer to learn in written form (7 lessons, each is about 20 minutes long).\nI generally think it’s better to send you to the source when there are well-produced resources, rather than trying to rehash something to put my own spin on it.\nOne other interesting feature to keep in mind when using Shiny - you can integrate Shiny reactivity into Rmarkdown by adding runtime: shiny to the markdown header.\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nShiny articles\nReactivity in Shiny\nLeaflet introduction for R\n\n\n35.4.1 Other interactive tools\n\nhtmlwidgets - a generic wrapper for any Javascript library (htmlwidgets is used under the hood in both Leaflet and Plotly R integration)\ndash - Another dashboard program supported by plotly. dash is the python equivalent of shiny, but also has R integration (though I’m not sure how well it’s supported).\n\n35.4.2 Debugging\n\nDebugging with Dean - Shiny debugging - YouTube video with debugging in realtime.\nShinyJS - Using Shiny and JavaScript together\nUsing Shiny in Production - Joe Cheng\n\n\n\n\n\n\n\n\n[1] \nT. Mock, “Client-side interactivity - do more with Crosstalk,” The MockUp. May 2020 [Online]. Available: https://themockup.blog/posts/2020-05-29-client-side-interactivity-do-more-with-crosstalk/. [Accessed: Aug. 05, 2025]\n\n\n[2] \nK. Pham, “Web mapping with python and leaflet,” Programming Historian, Aug. 2017 [Online]. Available: https://programminghistorian.org/en/lessons/mapping-with-python-leaflet. [Accessed: Aug. 01, 2023]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/07-intro-interactive-graphics.html#footnotes",
    "href": "part-advanced-topics/07-intro-interactive-graphics.html#footnotes",
    "title": "35  Animated and Interactive Graphics",
    "section": "",
    "text": "This is me experimentally trying to replace the tidyverse, and honestly, I’m not a fan.↩︎\nThere are obviously exceptions - we can figure out the exact date and approximate time that there was an earthquake along the Cascadia subduction zone based on a combination of oral histories of the indigenous people and records of a massive tsunami in Japan Excellent read, if you’re interested, and the Nature paper.↩︎",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Animated and Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/10-databases.html",
    "href": "part-advanced-topics/10-databases.html",
    "title": "38  Databases",
    "section": "",
    "text": "38.1 Microsoft Access\nComing soon!\nThere are many different database formats. Some of the most common databases are SQL* related formats and (for older datasets) Microsoft Access files.\nThis excellent GitHub repo contains code to connect to multiple types of databases in R, python, PHP, Java, SAS, and VBA\nTo get access to MS Access databases, you will need to become familiar with how to install ODBC drivers. These drivers tell your operating system how to connect to each type of database (so you need a different driver to get to MS Access databases than to get to SQL databases).\nMy hope is that you never actually need to get at data in an MS Access database - the format seems to be largely dying out.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/10-databases.html#microsoft-access",
    "href": "part-advanced-topics/10-databases.html#microsoft-access",
    "title": "38  Databases",
    "section": "",
    "text": "Install ODBC Drivers on your machine\n\n\n\n\n\n and  This set of instructions appears to contain all of the right steps for Windows and Mac and has been updated recently (Feb 2022) [1].\n I installed mdbtools on Ubuntu and have the following entry in my /etc/odbcinst.ini file:\n[MDBTools]\nDescription=MDBTools Driver\nDriver=libmdbodbc.so\nSetup=libmdbodbc.so\nFileUsage=1\nUsageCount=1\nAdding this entry to the file may be part of the mdbtools installation - I certainly have no memory of doing it myself, but this may help if you’re troubleshooting, so I’ve included it.\n\n\n\n\n\n\n\n\n\n38.1.1 Database IO: Scottish Witchcraft\n\n\n\nFor this demo, we’ll be using the Scottish Witchcraft Database[2], which you can download from their website, or acquire from the course data folder if you don’t want to register with the authors. A description of the dataset is also available.\n\nR\n\n\nIn R, we can read in MS Access files using the Hmisc package, as long as the mdbtools library is available on your computer1.\n\nif (!\"Hmisc\" %in% installed.packages()) install.packages(\"Hmisc\")\nlibrary(Hmisc)\ndb_loc &lt;- \"../data/Witchcraftsurvey_download.mdb\"\n\nmdb.get(db_loc, tables = TRUE) # get table list\n## Error in system(paste(\"mdb-tables -1\", file), intern = TRUE): error in running command\nmdb.get(db_loc, tables = \"WDB_Trial\")[1:6,1:10] # get table of trials, print first 6 rows and 10 cols\n## Error in system(paste(\"mdb-schema -T\", shQuote(tab), file), intern = TRUE): error in running command\n\nMany databases have multiple tables with keys that connect information in each table. We’ll spend more time on databases later in the semester - for now, it’s enough to be able to get data out of one. #### Python {-}\nThere are several tutorials out there to access MS Access databases using packages like pyodbc e.g. [3]. I couldn’t quite get these working on Linux, but it is possible you may have better luck on another OS. With that said, the solution using pandas_access seems to be much simpler and require less OS configuration, so it’s what I’ll show here.\nFirst, we have to install pandas_access using pip install pandas_access.\n\nimport pandas_access as mdb\ndb_filename = '../data/Witchcraftsurvey_download.mdb'\n\n# List tables\nfor tbl in mdb.list_tables(db_filename):\n  print(tbl)\n## FileNotFoundError: [Errno 2] No such file or directory: 'mdb-tables'\n\n# Read a small table.\ntrials = mdb.read_table(db_filename, \"WDB_Trial_Person\")\n## FileNotFoundError: [Errno 2] No such file or directory: 'mdb-schema'\n\nThis isn’t perfectly stable - I tried to read WDB_Trial and got errors about NA values in an integer field - but it does at least work.\n\n\n\n\n\n\n\n38.1.2 SQLite\nSQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. They function as databases but are more portable than SQL databases that require a server instance to run and connecting over a network (or running a server on your machine locally). As a result, they provide an opportunity to demonstrate most of the skills required for working with databases without all of the configuration overhead.\n\n\n\n\n\n\nDemo\n\n\n\n\n\nR\nPython\n\n\n\nLet’s try working with a sqlite file that has only one table in R:\n\nif (!\"RSQLite\" %in% installed.packages()) install.packages(\"RSQLite\")\nif (!\"DBI\" %in% installed.packages()) install.packages(\"DBI\")\nlibrary(RSQLite)\nlibrary(DBI)\n\n# Download the baby names file:\ndownload.file(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", destfile = \"../data/ssa-babynames-2015.sqlite\")\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \"../data/ssa-babynames-2015.sqlite\")\ndbListTables(con) # List all the tables\n## [1] \"babynames\"\nbabyname &lt;- dbReadTable(con, \"babynames\")\nhead(babyname, 10) # show the first 10 obs\n##    state year    name sex count rank_within_sex per_100k_within_sex\n## 1     AK 2015  Olivia   F    56               1              2367.9\n## 2     AK 2015    Liam   M    53               1              1590.6\n## 3     AK 2015    Emma   F    49               2              2071.9\n## 4     AK 2015    Noah   M    46               2              1380.6\n## 5     AK 2015  Aurora   F    46               3              1945.0\n## 6     AK 2015   James   M    45               3              1350.5\n## 7     AK 2015  Amelia   F    39               4              1649.0\n## 8     AK 2015     Ava   F    39               4              1649.0\n## 9     AK 2015 William   M    44               4              1320.5\n## 10    AK 2015  Oliver   M    41               5              1230.5\n\nYou can of course write formal queries using the DBI package, but for many databases, it’s easier to do the querying in R. We’ll cover both options later - the R version will be in the next module.\n\n\nThis example was created using [4] as a primary reference.\nIf you haven’t already downloaded the database file, you can do that automatically in python using this code:\n\nimport urllib.request\nurllib.request.urlretrieve(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", \"../data/babynames-2015.sqlite\")\n\nYou don’t have to install the sqlite3 module in python using pip because it’s been included in base python since Python 2.5.\n\nimport pandas as pd\nimport sqlite3\n\ncon = sqlite3.connect('../data/babynames-2015.sqlite')\n\nbabyname = pd.read_sql_query(\"SELECT * from babynames\", con)\n## pandas.errors.DatabaseError: Execution failed on sql 'SELECT * from babynames': no such table: babynames\nbabyname\n## NameError: name 'babyname' is not defined\n\ncon.close() # You must close any connection you open!",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/10-databases.html#references",
    "href": "part-advanced-topics/10-databases.html#references",
    "title": "38  Databases",
    "section": "\n38.2 References",
    "text": "38.2 References\n\n\n\n\n[1] \nTeam Exploratory, “How to import data from microsoft access database with ODBC. Exploratory.io,” Feb. 26, 2022. [Online]. Available: https://exploratory.io/note/exploratory/How-to-import-Data-from-Microsoft-Access-Database-with-ODBC-zIJ2bjs2. [Accessed: Jun. 13, 2022]\n\n\n[2] \nJulian Goodare, Lauren Martin, Joyce Miller, and Louise Yeoman, “The survey of scottish witchcraft,” Jan. 2003. [Online]. Available: www.shc.ed.ac.uk/witches/. [Accessed: Jun. 13, 2022]\n\n\n[3] \nData to Fish, “How to connect python to MS access database using pyodbc. Data to fish,” Aug. 21, 2021. [Online]. Available: https://datatofish.com/how-to-connect-python-to-ms-access-database-using-pyodbc/. [Accessed: Jun. 13, 2022]\n\n\n[4] \nData Carpentry, “Accessing SQLite databases using python and pandas – data analysis and visualization in python for ecologists. Data analysis and visualization in python for ecologists,” Jun. 04, 2019. [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html. [Accessed: Jun. 13, 2022]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/10-databases.html#footnotes",
    "href": "part-advanced-topics/10-databases.html#footnotes",
    "title": "38  Databases",
    "section": "",
    "text": "A currently maintained version of the library is here and should work for UNIX platforms. It may be possible to install the library on Windows using the UNIX subsystem, per this thread↩︎",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "graveyard.html",
    "href": "graveyard.html",
    "title": "41  Other Topics",
    "section": "",
    "text": "41.1 Using the Computer\nThis chapter is mostly here to provide a place for me to stick useful references that may help people who are wanting to expand their skills beyond tools covered in this textbook.",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#using-the-computer",
    "href": "graveyard.html#using-the-computer",
    "title": "41  Other Topics",
    "section": "",
    "text": "41.1.1 Shell Commands\nWhen talking to computers, sometimes it is convenient to cut through the graphical interfaces, menus, and so on, and just tell the computer what to do directly, using the system shell (aka terminal, command line prompt, console).\nMost system shells are fully functioning programming languages in their own right. This section isn’t going to attempt to teach you those skills - we’ll focus instead on the basics - how to change directories, list files, and run programs.\n\n41.1.1.1 Launching the system terminal\nIn RStudio, you can access a system terminal in the lower left corner by clicking on the tab labeled Terminal. If the tab does not exist, then go to Tools -&gt; Terminal -&gt; New Terminal in the main application toolbar.\nSometimes, it is preferable to launch a terminal separate from RStudio. Here’s how to do that:\n\n\n Windows\n Mac\n Linux\n\n\n\nOption 1: Default Windows terminal (cmd.exe)\n\nGo to the search bar/start menu\nType in cmd.exe\nA black window should appear.\n\nOption 2: Git bash (if you have git installed)\n\nGo to the search bar/start menu\nType in bash\nClick on the Git Bash application\n\nIf you choose option 2, use the commands for Bash/Linux below. Bash tends to be a bit less clunky than the standard windows terminal.\n\n\nOption 1: Dock\n\nClick the launchpad icon\nType Terminal in the search field\nClick Terminal\n\nOption 2: Finder\n\nOpen the Applications/Utilities folder\nDouble-click on Terminal\n\n\n\nOn most systems, pressing Ctrl-Alt-T or Super-T (Windows-T) will launch a terminal.\nOtherwise, launch your system menu (usually with the Super/Windows key) and type Terminal. You may have multiple options here; I prefer Konsole but I’m usually using KDE as my desktop environment. Other decent options include Gnome-terminal and xterm, and these are usually associated with Gnome and XFCE desktop environments, respectively.\n\n\n\n\n41.1.1.2 Basic Terminal Commands\nI have listed commands here for the most common languages used in each operating system. If you are using Git Bash on Windows, follow the commands for Linux/Bash. If you are using Windows PowerShell, google the commands.\nIn most cases, Mac/Zsh is similar to Linux/Bash, but there are a few differences1.\n\n\n\n\n\n\n\n\n\nTask\n\n Windows/CMD\n\n Mac/Zsh\n\n Linux/Bash\n\n\n\nList your current working directory\ncd\npwd\npwd\n\n\nChange directory\ncd &lt;path to new dir&gt;\ncd &lt;path to new dir&gt;\ncd &lt;path to new dir&gt;\n\n\nList files and folders in current directory\ndir\nls\nls\n\n\nCopy file\nxcopy &lt;source&gt; &lt;destination&gt; &lt;arguments&gt;\ncp &lt;arguments&gt; &lt;source&gt; &lt;destination&gt;\ncp &lt;arguments&gt; &lt;source&gt; &lt;destination&gt;\n\n\nCreate directory\nmkdir &lt;foldername&gt;\nmkdir &lt;foldername&gt;\nmkdir &lt;foldername&gt;\n\n\nDisplay file contents\ntype &lt;filename&gt;\ncat &lt;filename&gt;\ncat &lt;filename&gt;",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#general-programming",
    "href": "graveyard.html#general-programming",
    "title": "41  Other Topics",
    "section": "\n41.2 General Programming",
    "text": "41.2 General Programming\n\n41.2.1 MIT’s Missing Semester\nThis set of 11 1-hour lectures [1] covers topics that will help you develop general programming/computing skills. The topics covered are (mostly) adjacent to things covered in this book (with the exception of version control), but it seems like an excellent way to bone up on skills like how to work with the command line, how to accomplish basic tasks at the command line, and various other things that students tend to struggle with but that we don’t usually have time to go over in class in great detail.\n\n41.2.2 Controlling Loops with Break, Next, Continue\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n41.2.2.1 Break Statement\n\n\nA break statement is used to exit a loop prematurely\n\n\n41.2.2.2 Next/Continue Statement\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\n\nExample: Next/continue and Break statements\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\n\nR\nPython\n\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n## [1] 1\n## [1] 2\n## [1] \"Divisible by 3\"\n## [1] 4\n## [1] \"Divisible by 5\"\n## [1] \"Divisible by 3\"\n## [1] 7\n## [1] 8\n## [1] \"Divisible by 3\"\n## [1] \"Divisible by 5\"\n## [1] 11\n## [1] \"Divisible by 3\"\n## [1] 13\n## [1] 14\n## [1] \"Exiting now\"\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n## 1\n## 2\n## Divisible by 3\n## 4\n## Divisible by 5\n## Divisible by 3\n## 7\n## 8\n## Divisible by 3\n## Divisible by 5\n## 11\n## Divisible by 3\n## 13\n## 14\n## Exiting now\n\n\n\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use.\n\n41.2.3 Recursion\nUnder construction.\nIn the meantime, check out [2] (R) and [3] (Python) for decent coverage of the basic idea of recursive functions.\n\n41.2.4 Text Encoding\nI’ve left this section in because it’s a useful set of tricks, even though it does primarily deal with SAS.\nDon’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding!\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters).\nWhile writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS.\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv &gt; ../data/pokemon_gen_1-8.csv\";\nonly run this once to download the file... */\nfilename pokeloc '../data/pokemon_gen_1-8.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\nIf I fix the file in R (because I know how to fix it there… another option is to fix it manually),\n\nlibrary(readr)\nlibrary(dplyr)\ntmp &lt;- read_csv(\"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\")[,-1]\nwrite_csv(tmp, \"../data/pokemon_gen_1-8.csv\")\n\ntmp &lt;- select(tmp, -japanese_name) %&gt;%\n  # iconv converts strings from UTF8 to ASCII by transliteration - \n  # changing the characters to their closest A-Z equivalents.\n  # mutate_all applies the function to every column\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\n\nwrite_csv(tmp, \"../data/pokemon_gen_1-8_ascii.csv\", na='.')\n\nThen, reading in the new file allows us to actually see the output.\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"../data/pokemon_gen_1-8_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \nThis trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate.",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#advanced-topics-and-resources",
    "href": "graveyard.html#advanced-topics-and-resources",
    "title": "41  Other Topics",
    "section": "\n41.3 Advanced Topics and Resources",
    "text": "41.3 Advanced Topics and Resources\n\nBuilding reproducible analytical pipelines with R by Bruno Rodrigues [4] - software engineering techniques for R programming",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-other-topics-refs",
    "href": "graveyard.html#sec-other-topics-refs",
    "title": "41  Other Topics",
    "section": "\n41.4 References",
    "text": "41.4 References\n\n\n\n\n[1] \nAnish Athalye, Jon Gjengset, and Jose Javier, “The missing semester of your CS education. Missing semester.” [Online]. Available: https://missing.csail.mit.edu/. [Accessed: Apr. 20, 2023]\n\n\n[2] \nDataMentor, “R recursion. DataMentor,” Nov. 24, 2017. [Online]. Available: https://www.datamentor.io/r-programming/recursion/. [Accessed: Jan. 10, 2023]\n\n\n[3] \nParewa Labs Pvt. Ltd., “Python recursion. Learn python interactively,” 2020. [Online]. Available: https://www.programiz.com/python-programming/recursion. [Accessed: Jan. 10, 2023]\n\n\n[4] \nB. Rodrigues, Building reproducible analytical pipelines with R, 1st ed. Leanpub, 2023 [Online]. Available: https://raps-with-r.dev/. [Accessed: May 10, 2023]",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#footnotes",
    "href": "graveyard.html#footnotes",
    "title": "41  Other Topics",
    "section": "",
    "text": "Mac used to use bash but switched to Zsh in 2019 for licensing reasons.↩︎",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Other Topics</span>"
    ]
  }
]