[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing using R and Python",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#content-overload",
    "href": "index.html#content-overload",
    "title": "Statistical Computing using R and Python",
    "section": "Content Overload!",
    "text": "Content Overload!\nThis book is designed to demonstrate introductory statistical programming concepts and techniques. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It‚Äôs far better to learn how to code by ‚Ä¶ coding.\nI hope that you will work through this book week by week over the semester. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this book includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I‚Äôve attempted to make this book comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this book in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-format-guide",
    "href": "index.html#book-format-guide",
    "title": "Statistical Computing using R and Python",
    "section": "Book Format Guide",
    "text": "Book Format Guide\nI‚Äôve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\nButtons/Links\nThe book contains a number of features which should help you navigate, use, improve, and respond to the textbook.\n\n\nTextbook features, menus, and interactive options\n\nSpecial Sections\nSome instructions depend on your operating system. Where it‚Äôs shorter, I will use tabs to provide you with OS specific instructions. Here are the icons I will use:\n\n\n Windows\n Mac\n Linux\n\n\n\nWindows-specific instructions\n\n\nMac specific instructions\n\n\nLinux specific instructions. I will usually try to make this generic, but if it‚Äôs gui based, my instructions will usually be for KDE.\n\n\n\n\n\n\n\n\n\nWarnings\n\n\n\nThese sections contain things you may want to look out for: common errors, mistakes, and unfortunate situations that may arise when programming.\n\n\nDemonstrations\nThese sections demonstrate how the code being discussed is used (in a simple way).\n\n\n\n\n\n\nExamples\n\n\n\nThese sections contain illustrations of the concepts discussed in the chapter. Don‚Äôt skip them, even though they may be long!\n\n\n\n\n\n\n\n\nTry it out\n\n\n\nThese sections contain activities you should do to reinforce the things you‚Äôve just read. You will be much more successful if you read the material, review the example, and then try to write your own code. Most of the time, these sections will have a specific format:\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe problem will be in the first tab for you to start with\n\n\nA solution will be provided in R, potentially with an explanation.\n\n\nA solution will be provided in Python as well.\n\n\n\nIn some cases, the problem will be more open-ended and may not adhere to this format, but most try it out sections in this book will have solutions provided. I highly recommend that you attempt to solve the problem yourself before you look at the solutions - this is the best way to learn. Passively reading code does not result in information retention.\n\n\n\n\n\n\n\n\nEssential Reading\n\n\n\nThese sections may direct you to additional reading material that is essential for understanding the topic. For instance, I will sometimes link to other online textbooks rather than try to rehash the content myself when someone else has done it better.\n\n\nLearn More\nThese sections will direct you to additional resources that may be helpful to consult as you learn about a topic. You do not have to use these sections unless you are 1) bored, or 2) hopelessly lost. They‚Äôre provided to help but are not expected reading (Unlike the essential reading sections in red).\n\n\n\n\n\n\nNotes\n\n\n\nThese generic sections contain information I may want to call attention to, but that isn‚Äôt necessarily urgent or a common error trap.\n\n\nAdvanced\nThese sections are intended to apply to more advanced courses. If you are taking an introductory course, feel free to skip that content for now.\nExpandable Sections\nThese are expandable sections, with additional information when you click on the line\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\n\n\n\nAnother type of expandable note\n\n\n\n\n\nAnswers or punchlines may be hidden in this type of expandable section as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#analytics",
    "href": "index.html#analytics",
    "title": "Statistical Computing using R and Python",
    "section": "Analytics",
    "text": "Analytics\nI have enabled Google Analytics on this site for the purposes of measuring this work‚Äôs impact and use both in my own classes and elsewhere. I‚Äôm not using the individual tracking/ad-targeting settings (to the best of my knowledge) - my only purpose in using Google Analytics is to assess how often this site is used, and where its‚Äô users are located at a rough (state/regional) level.\nIf you are using this site and aren‚Äôt affiliated with the University of Nebraska Lincoln, or have found it useful, please let me know by making a comment in Giscus (below) or sending me an email! These affirmations help me make a case that spending time on this resource is actually a good investment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Computing using R and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe cover of this book is an amalgam of different images by the lovely @allison_horst, which are released under the cc-by 4.0 license. I have modified them to remove most of the R package references and arrange them to represent the topics covered in this book.\nLaptop icon used in the tab/logo created by Good Ware - Flaticon\nThroughout this book, I have borrowed liberally from other online tutorials, published books, and blog posts. I have tried to ensure that I link to the source material throughout the book and provide appropriate credit to anyone whose examples I have used, modified, or repurposed. Special thanks to the tutorials provided by Posit/RStudio and the tidyverse project.\nI don‚Äôt have official editors, but thank you to those who make use of the giscus comment box to let me know about issues and typos. So far, you‚Äôve helped me fix at least 3 issues so far!\n\n\n\n\n\n\nTools\n\n\n\n\n\nThis book was built with the following parameters/settings/library versions:\n\nimport os\nimport sys\n\nitemlist = [\"PWD\", \"SHELL\", \"USER\", \"PYTHONIOENCODING\", \"VIRTUAL_ENV\", \"RETICULATE_PYTHON\", \"R_HOME\", \"R_PLATFORM\", \"LD_LIBRARY_PATH\", \"R_LIBS_USER\", \"R_LIBS_SITE\",\"RENV_PROJECT\", \"RSTUDIO_PANDOC\", \"RMARKDOWN_MATHJAX_PATH\", \"R_SESSION_INITIALIZED\", \"PYTHONPATH\"]\n\nitemlist = list(set(itemlist) & set(os.environ))\nitemlist.sort()\n\nfor item in itemlist:\n    print(f'{item}{\" : \"}{os.environ[item]}')\n## LD_LIBRARY_PATH : /opt/R/4.4.1/lib/R/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-17-openjdk-amd64/lib/server:/opt/R/4.4.1/lib/R/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-17-openjdk-amd64/lib/server\n## PWD : /home/susan/Projects/Class/stat-computing-r-python\n## PYTHONIOENCODING : utf-8\n## PYTHONPATH : /usr/lib/python311.zip:/usr/lib/python3.11:/usr/lib/python3.11/lib-dynload:/home/susan/.virtualenvs/book/lib/python3.11/site-packages:/home/susan/.cache/R/renv/cache/v5/linux-debian-bookworm/R-4.4/x86_64-pc-linux-gnu/reticulate/1.39.0/e1a5d04397edc1580c5e0ed1dbdccf76/reticulate/python\n## RENV_PROJECT : /home/susan/Projects/Class/stat-computing-r-python\n## RETICULATE_PYTHON : ~/.virtualenvs/book/bin/python\n## RMARKDOWN_MATHJAX_PATH : /usr/lib/rstudio/resources/app/resources/mathjax-27\n## RSTUDIO_PANDOC : /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64\n## R_HOME : /opt/R/4.4.1/lib/R\n## R_LIBS_SITE : /opt/R/4.4.1/lib/R/site-library\n## R_LIBS_USER : /home/susan/Projects/Class/stat-computing-r-python/renv/library/linux-debian-bookworm/R-4.4/x86_64-pc-linux-gnu\n## R_PLATFORM : x86_64-pc-linux-gnu\n## R_SESSION_INITIALIZED : PID=361327:NAME=\"reticulate\"\n## SHELL : /bin/bash\n## USER : susan\n## VIRTUAL_ENV : /home/susan/.virtualenvs/book\n\nprint(sys.path)\n## ['', '/usr/bin', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/home/susan/.virtualenvs/book/lib/python3.11/site-packages', '/home/susan/.cache/R/renv/cache/v5/linux-debian-bookworm/R-4.4/x86_64-pc-linux-gnu/reticulate/1.39.0/e1a5d04397edc1580c5e0ed1dbdccf76/reticulate/python', '/home/susan/.virtualenvs/book/lib/python311.zip', '/home/susan/.virtualenvs/book/lib/python3.11', '/home/susan/.virtualenvs/book/lib/python3.11/lib-dynload']\n\n\nlibrary(devtools)\ndevtools::session_info()\n## ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n##  setting  value\n##  version  R version 4.4.1 (2024-06-14)\n##  os       Debian GNU/Linux 12 (bookworm)\n##  system   x86_64, linux-gnu\n##  ui       X11\n##  language (EN)\n##  collate  en_US.UTF-8\n##  ctype    en_US.UTF-8\n##  tz       America/Chicago\n##  date     2024-12-17\n##  pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n## \n## ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n##  ! package     * version  date (UTC) lib source\n##  P cachem        1.1.0    2024-05-16 [?] CRAN (R 4.4.1)\n##  P cli           3.6.3    2024-06-21 [?] CRAN (R 4.4.1)\n##  P devtools    * 2.4.5    2022-10-11 [?] CRAN (R 4.4.1)\n##  P digest        0.6.37   2024-08-19 [?] CRAN (R 4.4.1)\n##  P ellipsis      0.3.2    2021-04-29 [?] CRAN (R 4.4.1)\n##  P evaluate      1.0.1    2024-10-10 [?] CRAN (R 4.4.1)\n##  P fastmap       1.2.0    2024-05-15 [?] CRAN (R 4.4.1)\n##  P fontawesome * 0.5.2    2023-08-19 [?] CRAN (R 4.4.1)\n##  P fs            1.6.5    2024-10-30 [?] CRAN (R 4.4.1)\n##  P glue          1.8.0    2024-09-30 [?] CRAN (R 4.4.1)\n##  P htmltools     0.5.8.1  2024-04-04 [?] CRAN (R 4.4.1)\n##  P htmlwidgets   1.6.4    2023-12-06 [?] CRAN (R 4.4.1)\n##  P httpuv        1.6.15   2024-03-26 [?] CRAN (R 4.4.1)\n##  P jsonlite      1.8.9    2024-09-20 [?] CRAN (R 4.4.1)\n##  P knitr         1.48     2024-07-07 [?] CRAN (R 4.4.1)\n##  P later         1.3.2    2023-12-06 [?] CRAN (R 4.4.1)\n##  P lattice       0.22-6   2024-03-20 [?] CRAN (R 4.4.1)\n##  P lifecycle     1.0.4    2023-11-07 [?] CRAN (R 4.4.1)\n##  P magrittr      2.0.3    2022-03-30 [?] CRAN (R 4.4.1)\n##  P Matrix        1.7-1    2024-10-18 [?] CRAN (R 4.4.1)\n##  P memoise       2.0.1    2021-11-26 [?] CRAN (R 4.4.1)\n##  P mime          0.12     2021-09-28 [?] CRAN (R 4.4.1)\n##  P miniUI        0.1.1.1  2018-05-18 [?] CRAN (R 4.4.1)\n##  P pkgbuild      1.4.5    2024-10-28 [?] CRAN (R 4.4.1)\n##  P pkgload       1.4.0    2024-06-28 [?] CRAN (R 4.4.1)\n##  P png           0.1-8    2022-11-29 [?] CRAN (R 4.4.1)\n##  P profvis       0.4.0    2024-09-20 [?] CRAN (R 4.4.1)\n##  P promises      1.3.0    2024-04-05 [?] CRAN (R 4.4.1)\n##  P purrr         1.0.2    2023-08-10 [?] CRAN (R 4.4.1)\n##  P R6            2.5.1    2021-08-19 [?] CRAN (R 4.4.1)\n##  P Rcpp          1.0.13-1 2024-11-02 [?] CRAN (R 4.4.1)\n##  P remotes       2.5.0    2024-03-17 [?] CRAN (R 4.4.1)\n##  P renv          1.0.11   2024-10-12 [?] CRAN (R 4.4.1)\n##  P reticulate    1.39.0   2024-09-05 [?] CRAN (R 4.4.1)\n##  P rlang         1.1.4    2024-06-04 [?] CRAN (R 4.4.1)\n##  P rmarkdown     2.28     2024-08-17 [?] CRAN (R 4.4.1)\n##  P rstudioapi    0.17.1   2024-10-22 [?] CRAN (R 4.4.1)\n##  P sessioninfo   1.2.2    2021-12-06 [?] CRAN (R 4.4.1)\n##  P shiny         1.9.1    2024-08-01 [?] CRAN (R 4.4.1)\n##  P urlchecker    1.0.1    2021-11-30 [?] CRAN (R 4.4.1)\n##  P usethis     * 3.0.0    2024-07-29 [?] CRAN (R 4.4.1)\n##  P vctrs         0.6.5    2023-12-01 [?] CRAN (R 4.4.1)\n##  P xfun          0.49     2024-10-31 [?] CRAN (R 4.4.1)\n##  P xtable        1.8-4    2019-04-21 [?] CRAN (R 4.4.1)\n##  P yaml          2.3.10   2024-07-26 [?] CRAN (R 4.4.1)\n## \n##  [1] /home/susan/Projects/Class/stat-computing-r-python/renv/library/linux-debian-bookworm/R-4.4/x86_64-pc-linux-gnu\n##  [2] /home/susan/.cache/R/renv/sandbox/linux-debian-bookworm/R-4.4/x86_64-pc-linux-gnu/22bdb599\n## \n##  P ‚îÄ‚îÄ Loaded and on-disk path mismatch.\n## \n## ‚îÄ Python configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n##  python:         /home/susan/.virtualenvs/book/bin/python\n##  libpython:      /usr/lib/python3.11/config-3.11-x86_64-linux-gnu/libpython3.11.so\n##  pythonhome:     /home/susan/.virtualenvs/book:/home/susan/.virtualenvs/book\n##  version:        3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]\n##  numpy:          /home/susan/.virtualenvs/book/lib/python3.11/site-packages/numpy\n##  numpy_version:  1.26.4\n##  \n##  NOTE: Python version was forced by RETICULATE_PYTHON\n## \n## ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html",
    "href": "part-tools/01-computer-basics.html",
    "title": "1¬† Computer Basics",
    "section": "",
    "text": "1.1  Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#objectives",
    "href": "part-tools/01-computer-basics.html#objectives",
    "title": "1¬† Computer Basics",
    "section": "",
    "text": "Know the meaning of computer hardware and operating system terms such as hard drive, memory, CPU, OS/operating system, file system, directory, and system paths\nUnderstand the basics of how the above concepts relate to each other and contribute to how a computer works\nUnderstand the file system mental model for computers",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#hardware",
    "href": "part-tools/01-computer-basics.html#hardware",
    "title": "1¬† Computer Basics",
    "section": "1.2 Hardware",
    "text": "1.2 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we‚Äôve asked our processor to do too much). Most of the other details aren‚Äôt necessary (for now).\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#operating-systems",
    "href": "part-tools/01-computer-basics.html#operating-systems",
    "title": "1¬† Computer Basics",
    "section": "1.3 Operating Systems",
    "text": "1.3 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#file-systems",
    "href": "part-tools/01-computer-basics.html#file-systems",
    "title": "1¬† Computer Basics",
    "section": "1.4 File Systems",
    "text": "1.4 File Systems\n\nFor this class, it will probably be important to distinguish between local file storage (C:/ drive , /user/your-name/ , or /home/your-name/ ) and network/virtual file systems, such as OneDrive and iCloud. Over time, it has become harder to ensure that you are working on a local machine, but working ‚Äúin the cloud‚Äù can cause odd errors when programming and in particular when working with version control systems1.\nYou want to save your files in this class to your physical hard drive. This will save you a lot of troubleshooting time.\n\nEvidently, there has been a bit of generational shift as computers have evolved: the ‚Äúfile system‚Äù metaphor itself is outdated because no one uses physical files anymore. This article [1] is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\n\n\n\n\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system ‚Äì a way to organize data stored on a hard drive. Since data is always stored as 0‚Äôs and 1‚Äôs, it‚Äôs important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\nThat‚Äôs not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your ‚Äúbase folder‚Äù is known as your working directory - the place your program thinks of as home.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#system-paths",
    "href": "part-tools/01-computer-basics.html#system-paths",
    "title": "1¬† Computer Basics",
    "section": "1.5 System Paths",
    "text": "1.5 System Paths\nWhen you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don‚Äôt need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu, and you find your programs there.\nUnfortunately, that isn‚Äôt sufficient when you‚Äôre programming, because you may need to know where a program is in order to reference that program ‚Äì for instance, if you need to pop open a browser window as part of your program, you‚Äôre (most likely) going to have to tell your computer where that browser executable file lives.\nTo simplify this process, operating systems have what‚Äôs known as a ‚Äúsystem path‚Äù or ‚Äúuser path‚Äù - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available.\n\n\n\n\n\n\nHow To Modify System Paths\n\n\n\n\n\nHow to set system paths (general)\nOperating-system specific instructions cobbled together from a variety of different sources:\n\n On Windows\n On Mac\n On Linux\n\n\n\n\nIf you run across an error that says something along the lines of\n\ncould not locate xxx.exe\nThe system cannot find the path specified\nCommand Not Found\n\nyou might start thinking about whether your system path is set correctly for what you‚Äôre trying to do.\nIf you want to locate where an executable is found (in this example, we‚Äôll use git), you can run where git on windows, or which git on OSX/Linux.\nSome programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools &gt; Global Options &gt; Git/SVN, you can set the path to git.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "href": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "title": "1¬† Computer Basics",
    "section": "1.6 References",
    "text": "1.6 References\n\n\n\n\n[1] D. Robitzski, ‚ÄúGen z kids apparently don‚Äôt understand how file systems work. Futurism,‚Äù Sep. 24, 2021. [Online]. Available: https://futurism.com/the-byte/gen-z-kids-file-systems. [Accessed: Jan. 09, 2023]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/01-computer-basics.html#footnotes",
    "href": "part-tools/01-computer-basics.html#footnotes",
    "title": "1¬† Computer Basics",
    "section": "",
    "text": "To disable onedrive sync for certain windows folders, use this guide‚Ü©Ô∏é",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Computer Basics</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html",
    "href": "part-tools/02-setting-up-computer.html",
    "title": "2¬† Setting Up Your Computer",
    "section": "",
    "text": "2.1  Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#objectives",
    "href": "part-tools/02-setting-up-computer.html#objectives",
    "title": "2¬† Setting Up Your Computer",
    "section": "",
    "text": "Set up RStudio, R, Quarto, and python\nBe able to run demo code in R and python",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#what-are-you-asking-me-to-install",
    "href": "part-tools/02-setting-up-computer.html#what-are-you-asking-me-to-install",
    "title": "2¬† Setting Up Your Computer",
    "section": "\n2.2 What are you asking me to install?",
    "text": "2.2 What are you asking me to install?\nIt‚Äôs generally a good idea to be skeptical when someone is telling you to install things. ü§® Here‚Äôs a very broad overview of what each of these programs or services does and why I‚Äôm asking you to install or sign up for them.\n\nWhat each program does, in general terms\n\n\n\n\n\n\nProgram\nLogo\nPurpose\n\n\n\nR\n\nA statistical programming language built around working with data\n\n\nPython\n\nA general-purpose programming language that is popular for machine learning tasks.\n\n\nRStudio IDE\n\nAn integrated desktop environment created to make it easy to work with R, Python, and other data-science programming tools.\n\n\nQuarto\n\nA document creation system based on pandoc. Quarto allows you to include code, results, and pictures generated from data within a document so that they automatically update when the document is recompiled.\n\n\nGit\n\nA version control system used to track changes to files.\n\n\nGitHub\n\nAn online collaboration platform based on git that makes it easy to back up, share, and collaborate on programming projects.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#why-do-i-need-to-install-this-stuff",
    "href": "part-tools/02-setting-up-computer.html#why-do-i-need-to-install-this-stuff",
    "title": "2¬† Setting Up Your Computer",
    "section": "\n2.3 Why do I need to install this stuff?",
    "text": "2.3 Why do I need to install this stuff?\nPresumably, if you‚Äôre reading this book, you either want to know how to do some statistical programming task, or you‚Äôre taking a class on statistical programming that is using this textbook. Hopefully, if you‚Äôre reading this for a class, you also want to learn how to work with data in some fashion. While some parts of this book are fairly language-agnostic (R and python are both good languages for working with data), this setup is opinionated - the book focuses on a set of programs which are useful for doing statistical programming, including writing reports, keeping track of code, visualizing data, cleaning data, and getting set up for modeling data and producing results.\nIf you‚Äôre just trying to learn R, perhaps you don‚Äôt need to install python or quarto. If you‚Äôre not working with other people, maybe you don‚Äôt need to install git. You are welcome to make those executive decisions for yourself, but if you‚Äôre not sure, you might just want to install the whole toolbox - you‚Äôll hopefully learn how to use all of the tools along the way, and it‚Äôll be less confusing later if you already have access to all of the tools and don‚Äôt need to go back and get something else when you need it.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#installation-process",
    "href": "part-tools/02-setting-up-computer.html#installation-process",
    "title": "2¬† Setting Up Your Computer",
    "section": "\n2.4 Installation Process",
    "text": "2.4 Installation Process\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class message board or contact me for help.\n\n\nDownload and run the R installer for your operating system from CRAN:\n\n\n Windows: https://cran.rstudio.com/bin/windows/base/\n\n\n Mac: https://cran.rstudio.com/bin/macosx/\n\n\n Linux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\nIf you are on  Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages.\nIf you are on  Mac, you should also install XCode, which is a set of developer tools. You can get it from the App store, but it‚Äôs better to set yourself up with Homebrew and then use homebrew to install XCode. If you prefer a different package manager, that‚Äôs fine - Homebrew is widely used, but there are other options. Ultimately, you just need to have XCode so that you can compile R packages.\n\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install mas # Search the apple store\nmas search xcode # find xcode\nmas install 497799835 # install the program by ID\n\n\nDownload and install the latest version of python 3\n\n Windows: check the box that asks if you want to add Python to the system path. This will save you a lot of time and frustration. If you didn‚Äôt do this, you can follow these instructions to fix the issue (you‚Äôll need to restart your machine).\nIf you‚Äôre interested in python, you should install Jupyter using the instructions here (I would just do pip3 install jupyterlab)\nWe will not use jupyter much in this book - I prefer quarto - but the python community has decided to distribute code primarily in jupyter notebooks, so having it on your machine may be useful so that you can run other people‚Äôs code.\nAdditional instructions for installing Python 3 from Python for Everybody if you have trouble.\n\n\nDownload and install the latest version of RStudio for your operating system. RStudio is a integrated development environment (IDE) for R, created by Posit. It contains a set of tools designed to make writing R, python, javascript, and other data-related code easier.\nDownload and install the latest version of Quarto for your operating system. Quarto is a command-line tool released by Posit that allows you to create documents using R or python, combining code, results, and written text.\n\nThe following steps may be necessary depending on which class you‚Äôre in. If you want to be safe, go ahead and complete these steps as well.\n\nInstall git using the instructions here. Consult the troubleshooting guide if you have issues. If that fails, then seek help in office hours.\n\nInstall LaTeX and rmarkdown:\n\nLaunch R, and type the following commands into the console:\n\n\ninstall.packages(c(\"tinytex\", \"knitr\", \"rmarkdown\", \"quarto\"))\nlibrary(tinytex)\ninstall_tinytex()\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nOpen RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\ninstall.packages(\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n)\n\nCan you find the text editor?\n\nCreate a new quarto document (File -&gt; New File -&gt; Quarto Document).\nPaste in the contents of this document.\nCompile the document (Ctrl/Cmd + Shift + K) and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, R, and Python set up correctly on your machine.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Setting Up Your Computer</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html",
    "href": "part-tools/03-Rstudio-interface.html",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "",
    "text": "3.1  Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#objectives",
    "href": "part-tools/03-Rstudio-interface.html#objectives",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "",
    "text": "Locate different panes of RStudio\nUse cues such as buttons and icons to identify what type of file is open and what language is being interpreted",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#overview",
    "href": "part-tools/03-Rstudio-interface.html#overview",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "3.2 Overview",
    "text": "3.2 Overview\n An RStudio window is by default divided into 4 panes, each of which may contain several tabs. You can reconfigure the locations of these tabs based on your preferences by selecting the toolbar button with 4 squares (just left of the Addins dropdown menu).\nIn the default configuration, - The top left is the editor pane, where you will write code and other content. - The bottom left is the console pane, which contains your R/python interactive consoles as well as a system terminal and location for checking the status of background jobs. - The top right contains the environment and history tabs (among others) - The top left contains the files and help tabs (among others)\nYou do not need to know what all of these tabs do right now. For the moment, it‚Äôs enough to get a sense of the basics - where to write code (top left), where to look for results (bottom left), where to get help (bottom right), and where to monitor what R/python are doing (top right).",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-editorfile-pane-top-left",
    "href": "part-tools/03-Rstudio-interface.html#the-editorfile-pane-top-left",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "3.3 The Editor/File Pane (Top Left)",
    "text": "3.3 The Editor/File Pane (Top Left)\nThe buttons and layout within this pane change based on the type of file you have open.\n\nR scriptPython scriptQuarto markdownText file\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a python file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a quarto markdown file is open, there is a render button at the top which allows you to compile the file to see its ‚Äúpretty‚Äù, non-markup form. In the same toolbar, there are buttons to add a code chunk as well as to run a selcted line of code or chunk of code. You can toggle between source (shown) and visual mode to see a more word-like rendering of the quarto markdown file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the text file indicates the file type. When a text file (or other unknown file extension) is open, there are very few buttons in the editor window. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-console-pane-bottom-left",
    "href": "part-tools/03-Rstudio-interface.html#the-console-pane-bottom-left",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "3.4 The Console Pane (Bottom Left)",
    "text": "3.4 The Console Pane (Bottom Left)\nLet‚Äôs compare what the console pane looks like when we run a line of R code compared to a line of python code. The differences will help you figure out whether you need to exit out of Python to run R code and may help you debug some errors.\n\nPythonR\n\n\n\n\n\nWhen running python code from a script file, the console will show you that you are running in python by the logo at the top of the console pane. You will initially see lines indicating that you‚Äôre running R, and then you‚Äôll see the lines highlighted in red which show R running the code in python ‚Äì this is what converts the console from R to python. The command you ran will appear after &gt;&gt;&gt;, and the results will appear immediately below. A &gt;&gt;&gt; waits for a new command - to get back to R, you will need to type exit (as instructed by the red text). In the environment pane, you can see another indicator that you‚Äôre viewing the python environment, with an object named ‚Äòr‚Äô that will allow you to move data back and forth between the two languages if you want to do so.\n\n\n\n\n\n\n\nWhen running R code from a script file, the console will show you that you are running in R by the logo at the top of the console pane. You will initially see lines indicating that you‚Äôre running R (they‚Äôre missing here because this isn‚Äôt the first command I ran in this session). The command you ran will appear after &gt;, and the results will appear immediately below, with boxed numbers in front of each sequential line. A &gt; waits for a new command . In the environment pane, you may see a new value pop up named .Last.value - this is part of user settings and you can stop it from appearing if you want to.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-top-right-pane",
    "href": "part-tools/03-Rstudio-interface.html#the-top-right-pane",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "3.5 The Top Right Pane",
    "text": "3.5 The Top Right Pane\nThis pane contains a set of tabs that change based on your project and what you have enabled. If you‚Äôre using git with an Rstudio project, then this tab will show your git repository. If you‚Äôre working with an Rstudio project that has multiple files, such as a book or a website, then the pane will also have a Build tab that will build all of your project files.\nFor now, though, let‚Äôs assume you‚Äôre not in an Rstudio project and you just want to know what the heck an Environment pane (or any of the other tabs in here by default) is. We‚Äôre going to focus on two of the tabs that are the most relevant to you right now: Environment, and History.\n\n3.5.1 Environment tab\nThe Environment tab shows you any objects which are defined in memory in whatever language you‚Äôre currently using (as long as it‚Äôs R or python). You‚Äôll see headers like ‚ÄúData‚Äù, ‚ÄúValues‚Äù, and ‚ÄúFunctions‚Äù within this table, and two columns - the name of the thing, and the value of the thing (if it‚Äôs a complicated object, you‚Äôll see what type of object it is and possibly how long it is).\n\n\n\nThe environment tab shows you all of the objects in memory that the language you‚Äôre working in knows about.\n\n\nIf you‚Äôre working in both R and python, you can toggle which language‚Äôs environment you‚Äôre looking at using the language drop down button on the far left side.\n\n\n3.5.2 History tab\nAnother useful tab in this pane is the History tab, which shows you a running list of every command you‚Äôve ever run. While I strongly encourage you to write your code in a text file in the editor pane, sometimes you deleted a line of code accidentally and want to get it back‚Ä¶ and the history tab has you covered (unless you‚Äôve cleared the history out).\n\n\n\nThe history tab shows you a list of all commands you‚Äôve run and allows you to send them to the console or to source (the text editor).",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/03-Rstudio-interface.html#the-bottom-right-pane",
    "href": "part-tools/03-Rstudio-interface.html#the-bottom-right-pane",
    "title": "3¬† RStudio‚Äôs Interface",
    "section": "3.6 The Bottom Right Pane",
    "text": "3.6 The Bottom Right Pane\nThis pane also contains a mishmash of tabs that have various uses. Here, we‚Äôll focus on 3: Files, Packages, and Help. But first, to quickly summarize the remaining tabs, the Plots tab shows any plots you‚Äôve generated (which we haven‚Äôt done yet), and the Viewer/Presentation tabs show you compiled documents (markdown), interactive graphics, and presentations.\n\n3.6.1 Files tab\n\n\n\nThe files tab shows you the files in your current working directory (by default), though you can navigate through it and find other files as necessary. If you want to return to your working directory, there‚Äôs a button for that in the ‚ÄúMore‚Äù menu. One of the most important pieces of information in this pane is your path - you can construct the file path by using ~/ for home, and then for each folder, adding a slash between. The path to the folder we‚Äôre looking at here is thus ~/Projects/Class/stat-computing-r-python/.\n\n\n\n\n3.6.2 Packages tab\nThe packages tab isn‚Äôt quite relevant yet, but it will be soon. R and python both work off of packages - extensions to the default language that make it easier to accomplish certain tasks, like reading data from Excel files or drawing pretty charts. This tab shows all of the R packages you have installed on your machine, and which ones are currently loaded.\n\n\n\nYou can get important information from the packages tab, like what packages are loaded, easy access to documentation for each package, and what version of the package is installed.\n\n\nUnfortunately, the packages tab doesn‚Äôt cover python packages yet.\n\n\n3.6.3 Help tab\nThe help tab is a wonderful way to get help with how to use an R or python function.\n\n\n\nThe help tab makes it easy to get access to function documentation within Rstudio, so you don‚Äôt have to switch windows.\n\n\nBy default, you can search for an R function name in the search window, and documentation for matching functions will appear in the main part of the pane. To get help with python functions, you need to (in the python console) use ?&lt;function name, so I would type in at the &gt;&gt;&gt; prompt ?print to get the equivalent python help file.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>RStudio's Interface</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html",
    "href": "part-tools/04-scripts-notebooks.html",
    "title": "4¬† Scripts and Notebooks",
    "section": "",
    "text": "4.1  Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#objectives",
    "href": "part-tools/04-scripts-notebooks.html#objectives",
    "title": "4¬† Scripts and Notebooks",
    "section": "",
    "text": "Understand the different ways you can interact with a programming language\nIdentify which interface (terminal, interactive, script, notebook) and language are being used based on the appearance of the interface\nSelect the appropriate way of interacting with a computer for a given task given considerations such as target audience, human intervention, and need to repeat the analysis.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "href": "part-tools/04-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "title": "4¬† Scripts and Notebooks",
    "section": "\n4.2 A Short History of Talking to Computers",
    "text": "4.2 A Short History of Talking to Computers\nThe fundamental goal of this chapter is to learn how to talk to R and Python. Before we get there, though, it‚Äôs helpful to learn a bit about how we talk to computers more generally. This will not only give you context for R and Python - it will also help you identify things that need to be done before you can complete a task.\n\n4.2.1 In the beginning‚Ä¶\nIn the very beginning, people told computers what to do using punch cards [1]. Actually, punch cards somewhat predate computers - they were used to tabulate census results long before modern machines that we would consider actual computers.\nPunch cards required that you have every step of your program and data planned out in advance - you‚Äôd submit your punch cards to the computer, and then come back 24-72 hours later to find out you‚Äôd gotten two cards out of order. Dropping a tray of punch cards was ‚Ä¶ problematic.\n![Punch cards for a computer program. The red diagonal line on the top is a way to ensure the program is properly sorted. Image from Wikimedia, By ArnoldReinhold - Own work, CC BY-SA 3.0, ]](../images/tools/punch-cards-wikimedia.jpg)\nThankfully, we‚Äôre mostly free of the days where being a bit clumsy could erase a semester of hard work. As things grew more evolved, engineers developed visual displays (monitors). This enabled a new mode of interacting with computers: directly typing commands in, and receiving a response as soon as the task was completed. The primary way this interactivity happened, at least at first, was using interfaces called terminals or command prompts.\n\n4.2.2 Terminals\nBroadly speaking, a terminal is a text interface where you give instructions to a computer that tell the computer what to do. A command prompt is another, similar, term (often used interchangeably) which refers to the character used to indicate that the computer is waiting for a command. On different systems, this character might be &gt;&gt;&gt;, &gt;, $, %, or something you can customize yourself. Most systems allow you to tinker with the configuration of the terminal - what is shown on your machine may not be similar to what is shown in the images below, but the goal of looking at multiple prompts is to help you recognize the common components across operating systems enough to make sense of your machine.\n\n\nWhen I first learned how to use a computer, circa 1992, terminal interfaces (DOS) were the primary way you used a computer. My parents set up a custom menu that would allow me to launch computer games by typing different numbers. Windows 3.1 was an absolute revelation ‚Äì we had to buy a mouse, and you could actually move things around a screen! And the screen had more than 4 colors üåà!\nMost modern computer users don‚Äôt engage with terminal interfaces very often (or at all). As you learn to program, you will become more comfortable with the terminal for completing basic tasks like moving files around, running programs, and obtaining diagnostic information. Usually, you pick up this information over time and when you‚Äôre frustrated trying to do a task some other way.\nFor now, it‚Äôs enough to know what a terminal is and to recognize it when you see it.\nLet‚Äôs look at a terminal window for each operating system for a minute and examine the important parts.\n\n\n Windows\n Mac\n Linux\n\n\n\nThe default terminal on Windows is cmd.exe.\n\n\nWindows terminal. The location on the computer is shown first, and the prompt character is &gt;.\n\nMany people dislike cmd.exe and prefer to install PowerShell, which is a more fully featured terminal program. \n\n\n\n\nIn a Mac terminal, you can see the username, computer name, location, and prompt character ‚Äò%‚Äô. (base) in this image is printed because this user has a conda environment loaded ‚Äì this is something that may happen depending on how you install python. Macs use Zsh, which is a command line program (Z-shell) that interprets user input.\n\n\n\n\n\nThis is an example of a KDE-based terminal on Linux. The Username, computer name, location, and prompt character ‚Äò$‚Äô are all present. This linux terminal uses BASH, which is a slightly different command line program than the Z-shell used on Macs. In Linux, it is common to choose the terminal program you most prefer, and there are many options.\n\n\n\n\nSystem terminals have their own languages, and they‚Äôre not consistent across operating systems. By default, terminals come with a set of commands described as  Batch (Windows),  Zsh (Mac), or  Bash (default in most Linux systems).\nThe most important things to know how to do in a system terminal are:\n\nLaunch a program like python or R\nChange your location/working directory (dir &lt;path\\to\\folder&gt; on Windows, cd &lt;path/to/folder&gt; on Linux/Mac)\n\nThere are lots of other things you can do, but those are the two big ones.\nLet‚Äôs try launching R and python from a system terminal, and then see that we can get the same windows within RStudio. Keeping all of the windows you need for programming in one place is one of the most important features of an integrated development environment (IDE) like RStudio.\n\n\n\n\n\n\nYour Turn - Interactive Command Prompts in R and Python\n\n\n\n\n\nR - System Terminal\nPython - System Terminal\nRStudio - R\nRStudio - Python\n\n\n\nOpen your system terminal and type R. Hit enter. You can issue commands directly to R by typing something in at the &gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nLaunching R from the system terminal\n\n\n\nOpen your system terminal and type python (on some systems, like mine, you may have to type python3 instead). Hit enter. You can issue commands directly to python by typing something in at the &gt;&gt;&gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nLaunching python from the system terminal\n\n\n\nOpen RStudio and navigate to the Console tab. You can issue commands directly to R by typing something in at the &gt; prompt.\nTry typing in 2+2 and hit enter.\n\n\nThe Console tab is the left-most tab in RStudio - you can see that R‚Äôs welcome message shows up first, along with version information. The last component visible is the &gt;, which indicates that R is waiting for you to tell it to do something. Hitting enter will submit the command to calculate 2+2. Hopefully, you get 4.\n\n\n\nOpen RStudio and navigate to the Terminal tab. This is a ‚Äúsystem terminal‚Äù - that is, where you tell the computer what to do.\nWe tell the computer we want to work in python by typing in python3 or python (depending on how your computer is set up). This will launch an interactive python session (ipython).\nYou should get a prompt that looks like this: &gt;&gt;&gt;\n Type in 2+2 and hit enter.\n\n\n\n\n\nUsing the console interactively can be useful for quick things, like performing a simple arithmetic calculation, but imagine doing a complicated analysis that requires typing in 10, 15, or 20 commands! You‚Äôd have to re-type the same commands any time you wanted to bring up the results, which very quickly gets tedious.\n\n4.2.3 Scripts\nAt one point, I wanted to keep a record of the temperature around my house so that I could examine how my heating bill changed with the temperature and determine if an upgrade to a heat pump was cost effective. I decided to record the outside temperature every 6 hours, writing that information to a file along with the date and time. This only required a few commands, but I wasn‚Äôt willing to commit to being at the computer every 6 hours for the rest of my life, and I wanted the data to be complete.\nEnter scripts.\nIf you need to repeat the same analysis, or even just remember what commands you used, typing each command in each time is not ideal. A script is a text file which records a series of commands so that they can be run together.\nInteractive mode is useful for quick, one-off analyses, but if you need to repeat an analysis (or remember what you did), interactive mode is just awful. Once you close the program, the commands (and results) are gone. This is particularly inconvenient when you need to run the same task multiple times.\nTo somewhat address this issue, most computing languages allow you to provide a sequence of commands in a text file, or a script. In many languages, scripts are intended to run on their own, from start to finish. We often call this executing a script, and this is typically done from a terminal prompt.\n\n\n\n\n\n\nYour Turn - Scripts and Terminals\n\n\n\nLet‚Äôs take a minute and see how someone might call or run a script from the terminal.\n\nDownload scripts.zip and unzip the file.\nOpen a system terminal in the directory where you unzipped the files.\nFollow the directions below exactly to ensure that you have the terminal open in the correct location.\n\n\n\n Windows\n Mac\n Linux\n\n\n\nOpen the folder. Type cmd into the location bar at the top of the window and hit enter. The command prompt will open in the desired location.\n\n\nOpen a finder window and navigate to the folder you want to use. If you don‚Äôt have a path bar at the bottom of the finder window, choose View &gt; Show Path Bar. Control-click the folder in the path bar and choose Open in Terminal.\n\n\nOpen the folder in your file browser. Select the path to the folder in the path bar and copy it to the clipboard. Launch a terminal and type cd, and then paste the copied path. Hit enter. (There may be more efficient ways to do this, but these instructions work for most window managers).\n\n\n\n\nNow, let‚Äôs try out running a script from the terminal in R and Python!\n\n\n\nR\nPython\n\n\n\nThis assumes that the R binary has been added to your system path. If these instructions don‚Äôt work, please ask for help or visit office hours.\nIn the terminal, type Rscript words.R dickens-oliver-twist.txt\nYou should get some output that looks like this:\nuser@computer:~/scripts$ Rscript words.R dickens-oliver-twist.txt \ntext\n the  and        to   of    a  his   in   he  was \n8854 4902 4558 3767 3763 3569 2272 2224 1931 1684\n\n\nThis assumes that the python binary has been added to your system path. If these instructions don‚Äôt work, please ask for help or visit office hours.\nIn the terminal, type python3 words.py and hit Enter. You will be prompted for the file name. Enter dickens-oliver-twist.txt and hit Enter again.\nYou should get some output that looks like this:\nuser@computer:~/scripts$ python3 words.py \nEnter file:dickens-oliver-twist.txt\nthe 8854\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information about how to use system terminals, see Section 31.1.\n\n\nScripts, and compiled programs generated from scripts, are responsible for much of what you interact with on a computer or cell phone day-to-day. When the goal is to process a file or complete a task in exactly the same way each time, a script is the right choice for the job.\n\n4.2.3.1 Using Scripts Interactively\nScripts are also used interactively in some languages, like R and python, when doing data analysis. Because data analysis depends on the data, and data isn‚Äôt ever exactly the same or 100% what you expect it to be, people programming with data often use scripts and run the code in the script interactively. About 70% of my day-to-day computing is done using R or python scripts that are run interactively.\nWhen a script is ready for ‚Äúproduction‚Äù - that is, ready to be used without interactive human supervision, data scientists may use it that way.\nBut many data scientists and statisticians never move beyond using scripts interactively ‚Äì and that is ok!\nWhen you look for help online, though, it‚Äôs important to be able to distinguish between help that assumes you‚Äôre calling or executing a script (running it from the command line) and help that assumes you‚Äôre working with a script and using it interactively.\n\n4.2.3.2 Sourcing Scripts\nTo make this even more confusing, it‚Äôs possible to run an entire script, or a chunk of a script, within RStudio. When we talk about running an entire script file within RStudio, we will often say we‚Äôre sourcing the file. This is because in R, to include a file of commands within another script, you run the command source(\"path/to/file.R\").\nThe difference between sourcing, running, and executing a script is fairly nuanced and the vocabulary is often used interchangeably, which doesn‚Äôt help you as you‚Äôre learning!\n\n\n\n\n\n\nYour Turn - Sourcing a Script\n\n\n\nIf you haven‚Äôt already, download scripts.zip and unzip the file.\nOpen RStudio and use RStudio to complete the following tasks.\n\n\nR\nPython\n\n\n\n\nUse RStudio to open the words-noinput.R file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"&lt;paste path here&gt;\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.R file, hit the ‚Äúsource‚Äù button in the top right. Do you get the same output that you got from running the file as a script from the terminal? Why do you think that is?\nClick on the last line of the file and hit Run (or Ctrl/Cmd + Enter). Do you get the output now?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What could you learn from doing this?\n\n\n\n\nUse RStudio or your preferred python editor to open the words-noinput.py file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"&lt;paste path here&gt;\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.py file, hit the ‚Äúsource‚Äù button in the top right. Do you get the same output that you got from running the file as a script from the terminal? What changes?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What do you learn from doing this?\n\n\n\n\n\n\nUsing scripts interactively allows us to see what is happening in the code step-by-step, and to examine the results during the program‚Äôs evaluation. This can be beneficial when applying a script to a new dataset, because it allows us to change things on the fly while still keeping the same basic order of operations.\nYou can run single lines of code within a script file by clicking on the line and using the ‚ÄúRun‚Äù button in RStudio, or by typing Ctrl/Cmd + Enter, which will run the selected line(s) of code in the Console.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#writing-code-for-people",
    "href": "part-tools/04-scripts-notebooks.html#writing-code-for-people",
    "title": "4¬† Scripts and Notebooks",
    "section": "\n4.3 Writing Code for People",
    "text": "4.3 Writing Code for People\nOne problem with scripts and interactive modes of using programming languages is that we‚Äôre spending most of our time writing code for computers to read ‚Äì which doesn‚Äôt necessarily imply that our code is easy for humans to read. As you become more proficient at programming, you will realize quickly that writing readable code is more challenging than writing working code. It is hard to remember what a specific block of code did when you come back to it months after you wrote it!\nThere are two solutions to this problem, and I encourage you to make liberal use of both of them (together).\n\n4.3.1 Code Comments\nA comment is a part of computer code which is intended only for people to read. It is not evaluated or run by the computing language.\nTo ‚Äúcomment out‚Äù a single line of code in R or python, put a # (pound sign/hashtag) just before the part of the code you do not want to be evaluated. This works in both R and Python. Other languages also have so-called inline comments, but may use a different character to indicate that something is a comment. // Comment text and &lt;!-- Comment text --&gt; are in-line comments for JavaScript and HTML, respectively.\n\n4.3.1.1 Adding Comments to Code\n\n\nR\nPython\n\n\n\n\n2 + 2 + 3\n\n[1] 7\n\n2 + 2 # + 3\n\n[1] 4\n\n# This line is entirely commented out\n\n\n\n\n2 + 2 + 3\n\n7\n\n2 + 2 # + 3\n\n4\n\n# This line is entirely commented out\n\n\n\n\nMany computing languages, such as Java, C/C++, and JavaScript have mechanisms to comment out an entire paragraph.\nNeither R nor Python has so-called ‚Äúblock comments‚Äù - instead, you can use keyboard shortcuts in RStudio to comment out an entire chunk of code (or text) using Ctrl/Cmd-Shift-C.\n\n4.3.2 Literate Programming - Notebooks and more!\nWhile code comments add human-readable text to code, scripts with comments are still primarily formatted for the computer‚Äôs convenience. However, most of the time spent on any given document is spent by people, not by computers. Some groups write parallel documents - code, user manuals, internal wikis, tutorials, etc. which explain the purpose of code and how to use it, but this can get clumsy over time, and requires updating multiple documents (sometimes in multiple places), which can lead to the documentation getting out-of-sync from the code.\nTo solve this problem, Donald Knuth invented the concept of literate programming: interspersing text and code in the same document using structured text to indicate which lines are code and which lines are intended for human consumption.\nThis textbook is written using a literate format - quarto markdown - which allows me to include code chunks in R, python, and other languages, alongside the text, pictures, and other formatting necessary to create a textbook.\nOne major side effect of literate programming is that it is easy to include the results of a block of code inside the primary document. This means that you can generate plots, tables, and other information using code, and include them into a document, without having to copy and paste or insert figures within the text. While this advantage may not seem worth it to you at this point, once you have a ton of plots in a document and the data changes just slightly, the advantage becomes crystal clear.\nWriting documents using literate programming saved me so much time in as an industry data scientist that I really only had about 8 hours of work in a week - I‚Äôd automated the rest away by building re-usable reports that updated when new data arrived.\n\n4.3.2.1 Quarto\nOne type of literate programming document is a quarto markdown document.\nWe will use quarto markdown documents for most of the components of this class because they allow you to answer assignment questions, write reports with figures and tables generated from data, and provide code all in the same file.\nWhile literate documents aren‚Äôt ideal for jobs where a computer is doing things unobserved (such as pulling data from a web page every hour), they are extremely useful in situations where it is desireable to have both code and an explanation of what the code is doing and what the results of that code are in the same document.\n\n\n\n\n\n\nYour Turn - Words in quarto\n\n\n\nIn the scripts.zip file you downloaded earlier, there is a scripts.qmd file as well as a scripts.html file.\nOpen scripts.html in your browser, and open scripts.qmd in RStudio. scripts.html was generated from scripts.qmd.\n\nRead through scripts.html and find the corresponding sections of scripts.qmd.\n\n\n\nQuarto text (left) and rendered HTML (right)\n\n\nIn RStudio, click the ‚ÄúRender‚Äù button. The HTML file should appear at the bottom left in the ‚ÄúViewer‚Äù tab.\n\n\n\nThe render button is located at the top of the text editor/script pane (usually, the top left panel).\n\nCongratulations, you‚Äôve just compiled your first Quarto document!\n\n\n\n\n\n\n\n\nYour turn - Quarto Markdown\n\n\n\nIn RStudio, create a new quarto markdown document: File &gt; New File &gt; Quarto Document. Give your document a title and an author, and select HTML as the output.\nCopy the following text into your document and hit the ‚ÄúRender‚Äù button at the top of the file.\nThis defines an R code chunk. The results will be included in the compiled HTML file.\n\n```{r}\n2 + 2 \n```\n\nThis defines a python code chunk. The results will be included in the compiled HTML file.\n\n```{python}\n2 + 2\n```\n\n# This is a header\n\n## This is a subheader\n\nI can add paragraphs of text, as well as other structured text such as lists:\n\n1. First thing\n2. Second thing\n  - nested list\n  - nested list item 2\n3. Third thing\n\nI can even include images and [links](https://www.oldest.org/entertainment/memes/)\n\n![Goodwin's law is almost as old as the internet itself.](https://www.oldest.org/wp-content/uploads/2017/10/Godwins-Law.jpg)\n\n\nMarkdown is a format designed to be readable and to allow document creators to focus on content rather than style.\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it‚Äôs been marked up with tags or formatting instructions. ‚Äì John Gruber\n\nYou can read more about pandoc markdown (and quarto markdown, which is a specific type of pandoc markdown) here [2].\nMarkdown documents are compiled into their final form (usually, HTML, PDF, Docx) in multiple stages:\n\nAll code chunks are run and the results are saved and inserted into the markdown document.\nRmd/qmd -&gt; md\nThe markdown document is converted into its final format using pandoc, a program that is designed to ensure you can generate almost any document format. This may involve conversion to an intermediate file (e.g.¬†.tex files for PDF documents).\n\nAn error in your code will likely cause a failure at stage 1 of the process. An error in the formatting of your document, or missing pictures, and miscellaneous other problems may cause errors in stage 2.\n\n\n\n\n\n\nHistory\n\n\n\nQuarto markdown is the newest version of a long history of literate document writing in R. A previous version, Rmarkdown, had to be compiled using R; quarto can be compiled using R or python or the terminal directly.\nPrior to Rmarkdown, the R community used knitr and Sweave to integrate R code with LaTeX documents (another type of markup document that has a steep learning curve and is harder to read).\n\n\n\n4.3.2.2 Jupyter\nWhere quarto comes primarily out of the R community and those who are agnostic whether R or Python is preferable for data science related computing, Jupyter is an essentially equivalent notebook that comes from the python side of the world.\nQuarto supports using the jupyter engine for chunk compilation, but jupyter notebooks have some (rather technical) features that make them less desirable for an introductory computing class [3]. As a result, this book makes an opinionated decision to prefer quarto over jupyter.\n\n4.3.2.3 Learn More about Notebooks\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don‚Äôt Like Notebooks‚Äù by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel‚Äôs talk).\n\nYihui Xie is the person responsible for knitr and Rmarkdown and was involved in the development of quarto.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#video-recap",
    "href": "part-tools/04-scripts-notebooks.html#video-recap",
    "title": "4¬† Scripts and Notebooks",
    "section": "\n4.4 Video Recap",
    "text": "4.4 Video Recap\nI recorded a very rough, un-edited walkthrough of the different Words examples to hopefully demonstrate some of how this all works. Try to do the same type of exploration on your machine - work through each of the Your Turn examples, and see if you can get the same results I got.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/04-scripts-notebooks.html#part-tools-03-refs",
    "href": "part-tools/04-scripts-notebooks.html#part-tools-03-refs",
    "title": "4¬† Scripts and Notebooks",
    "section": "\n4.5 References",
    "text": "4.5 References\n\n\n\n\n[1] \n\n‚ÄúPunched card input/output.‚Äù Jan. 08, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Punched_card_input/output&oldid=1132250858\n\n\n\n[2] \nPosit PBC, ‚ÄúQuarto - markdown basics,‚Äù 2023. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html. [Accessed: Jan. 09, 2023]\n\n\n[3] \nY. Xie, ‚ÄúThe first notebook war,‚Äù Sep. 10, 2018. [Online]. Available: https://yihui.org/en/2018/09/notebook-war/. [Accessed: Jan. 09, 2023]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Scripts and Notebooks</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html",
    "href": "part-tools/05-git-and-github.html",
    "title": "5¬† Version Control with Git",
    "section": "",
    "text": "5.1  Objectives",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#objectives",
    "href": "part-tools/05-git-and-github.html#objectives",
    "title": "5¬† Version Control with Git",
    "section": "",
    "text": "Install git\nCreate a github account\nUnderstand why version control is useful and what problems it can solve\nUnderstand the distinction between git and github, and what each is used for\nUse version control to track changes to a document (git add, commit, push, pull)",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#installation",
    "href": "part-tools/05-git-and-github.html#installation",
    "title": "5¬† Version Control with Git",
    "section": "\n5.2 Installation",
    "text": "5.2 Installation\n\nInstall git using the instructions here.\n\nConsult the troubleshooting guide if you have issues.\nIf 1-2 fail, seek help in office hours.\n\n\n\n\n\n\n\n Mac Warning\n\n\n\nWith each version upgrade, you may find that git breaks. To fix it, you will have to reinstall Mac command line tools. Once you do this, git will start working again. See [2] for more information.\n\n\n\n5.2.1 Optional: Install a git client\nInstructions\nI don‚Äôt personally use a git client other than RStudio, but you may prefer to have a client that allows you to use a point-and-click interface. It‚Äôs up to you.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#what-is-version-control",
    "href": "part-tools/05-git-and-github.html#what-is-version-control",
    "title": "5¬† Version Control with Git",
    "section": "\n5.3 What is Version Control ?",
    "text": "5.3 What is Version Control ?\n\n\n\n\n\n\nNote\n\n\n\nMost of this section is either heavily inspired by Happy Git and Github for the UseR [1] or directly links to that book.\n\n\n\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is the answer to this file naming problem. Image Source ‚ÄúPiled Higher and Deeper‚Äù by Jorge Cham www.phdcomics.com\n\nGit manages a collection of files in a structured way - rather like ‚Äútrack changes‚Äù in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is ‚Ä¶ essential. In this class, we‚Äôll be using both git and github, and your homework will be managed with GitHub Classroom.\n\n5.3.1 Git Basics\n\n\nIf that doesn‚Äôt fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‚ÄòIt‚Äôs really pretty simple, just think of branches as‚Ä¶‚Äô and eventually you‚Äôll learn the commands that will fix everything. Image by Randall Munroe (XKCD) CC-A-NC-2.5.\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called ‚Äúcommits‚Äù). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don‚Äôt have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\n\n\n\n\n\n\nEssential Reading: Git\n\n\n\nThe git material in this chapter is just going to link directly to the book ‚ÄúHappy Git with R‚Äù by Jenny Bryan. It‚Äôs amazing, amusing, and generally well written. I‚Äôm not going to try to do better.\nGo read Chapter 1, if you haven‚Äôt already.\n\n\nNow that you have a general idea of how git works and why we might use it, let‚Äôs talk a bit about GitHub.\n\n5.3.2 GitHub: Git on the Web\n\n\n\n\n\n\nSet up a GitHub Account Now\n\n\n\nInstructions for setting up a GitHub account.\nBe sure you remember your signup email, username, and password - you will need them later.\n\n\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people‚Äôs git repositories. You can use git without GitHub, but you can‚Äôt use GitHub without git.\n\n\n\n\n\n\nGit and Github: Slightly crude (but memorable) analogy\n\n\n\n\n\nGit is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it‚Äôs similarly silly to think of GitHub as the only place you can use git repositories.\n\n\n\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don‚Äôt have in 3 different places is data you don‚Äôt care about.1",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "href": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "title": "5¬† Version Control with Git",
    "section": "\n5.4 Using Version Control (with RStudio)",
    "text": "5.4 Using Version Control (with RStudio)\nThe first skill you need to actually practice in this class is using version control. By using version control from the very beginning, you will learn better habits for programming, but you‚Äôll also get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\n\n\n\n\n\n\n\n\n\n\n\nSo, what does your typical git/GitHub workflow look like? I‚Äôll go through this in (roughly) chronological order. This is based off of a relatively high-level understanding of git - I do not have any idea how it works under the hood, but I‚Äôm pretty comfortable with the clone/push/pull/commit/add workflows, and I‚Äôve used a few of the more complicated features (branches, pull requests) on occasion.\n\n5.4.1 Introduce yourself to git and set up SSH authentication\nYou need to tell git what your name and email address are, because every ‚Äúcommit‚Äù you make will be signed. This needs to be done once on each computer you‚Äôre using.\nFollow the instructions here, or run the lines below:\n\n\n\n\n\n\nNote\n\n\n\nThe lines of code below use interactive prompts. Click the copy button in the upper right corner of the box below, and then paste the whole thing into the R console. You will see a line that says ‚ÄúYour full name:‚Äù - type your name into the console. Similarly, the next line will ask you for an email address.)\n\n\n\n\nuser_name &lt;- readline(prompt = \"Your full name: \")\nuser_email &lt;- readline(prompt = \"The address associated w your github account: \")\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = user_name, user.email = user_email, scope = \"user\")\n\n# Tell git to ignore all files that are OS-dependent and don't have useful data.\ngit_vaccinate() \n\n# Create a ssh key if one doesn't already exist\nif (!file.exists(git2r::ssh_path(\"id_rsa.pub\"))) {\n  # Create an ssh key (with no password - less secure, but simpler)\n  system(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ''\") \n  # Find the ssh-agent that will keep track of the password\n  system(\"eval $(ssh-agent -s)\")\n  # Add the key\n  system(\"ssh-add ~/.ssh/id_rsa\")\n} \n\nThen, in RStudio, go to Tools &gt; Global Options &gt; Git/SVN. View your public key, and copy it to the clipboard.\nThen, proceed to github. Make sure you‚Äôre signed into GitHub. Click on your profile pic in upper right corner and go Settings, then SSH and GPG keys. Click ‚ÄúNew SSH key‚Äù. Paste your public key in the ‚ÄúKey‚Äù box. Give it an informative title. For example, you might use 2022-laptop to record the year and computer. Click ‚ÄúAdd SSH key‚Äù.\n\n5.4.2 Create a Repository\nRepositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. If you combine a git repository with an RStudio project, you get a very powerful combination that will make your life much easier, allowing you to focus on writing code instead of figuring out where all of your files are for each different project you start.\nTo create a repository, you can start with your local computer first, or you can start with the online repository first.\n\n\n\n\n\n\nImportant\n\n\n\nBoth methods are relatively simple, but the options you choose depend on which method you‚Äôre using, so be careful not to get them confused.\n\n\n\n5.4.2.1 Local repository first\nLet‚Äôs suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You‚Äôve created a starter document, say, a text file named README with ‚Äúhello world‚Äù written in it.\nIf you want, you can use the following R code to set this up:\n\ndir &lt;- \"./hello-world-1\"\nif (!dir.exists(dir)) {\n  dir.create(dir)\n}\nfile &lt;- file.path(dir, \"README\")\nif (!file.exists(file)) {\n  writeLines(\"hello world\", con = file)\n}\n\nTo create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in\ngit init\nAlternately, if you prefer a GUI (graphical user interface) approach, that will work too:\n\nOpen Rstudio\nProject (upper right corner) -&gt; New Project -&gt; Existing Directory. Navigate to the directory.\n(In your new project) Tools -&gt; Project options -&gt; Git/SVN -&gt; select git from the dropdown, initialize new repository. RStudio will need to restart.\nNavigate to your new Git tab on the top right.\n\n\n\n\n\nThe next step is to add our file to the repository.\nUsing the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\".\nUsing the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (‚ÄúAdd readme file‚Äù), and click the commit button.\n\n\n\n\nThe final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you‚Äôre logged in. Create a new repository using the ‚ÄúNew‚Äù button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time).\nYou‚Äôll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) ‚Äì you can access this within RStudio, as shown below. Paste those lines in, and you‚Äôll be good to go.\n\n\n\n\n\n5.4.2.2 GitHub repository first\nIn the GitHub-first method, you‚Äôll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally).\nGUI method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the ‚ÄúCode‚Äù button on the repo homepage\nOpen RStudio -&gt; Project -&gt; New Project -&gt; From version control. Paste your repository URL into the box. Hit enter.\nMake a change to the README file\nClick commit, then push your changes\nCheck that the remote repository (Github) updated\n\n\n\n\n\nCommand line method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the ‚ÄúCode‚Äù button on the repo homepage\nNavigate to the location you want your repository to live on your machine.\nClone the repository by using the git shell or terminal: git clone &lt;your repo url here&gt;. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git\n\nMake a change to your README file and save the change\nCommit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking).\nPush your changes to the remote (GitHub) repository and check that the repo has updated: git push\n\n\n\n\n\n\n\n5.4.3 Adding files\ngit add tells git that you want it to track a particular file.\n\n\ngit add diagram: add tells git to add the file to the index of files git monitors.\n\nYou don‚Äôt need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren‚Äôt logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git ‚Äúknows about‚Äù, and what it thinks belongs in each commit.\nIf you use the RStudio GUI for your git interface, you generally won‚Äôt have to do much with git add; it‚Äôs (sort-of, kind-of) equivalent to clicking the check box.\n\n5.4.3.1 What files should I add to git?\nGit is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g.¬†images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB2. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free.\nIn general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule ‚Äì this book is hosted on GitHub, which means I‚Äôve pushed the compiled book to the GitHub repository).\nYou should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you‚Äôre collaborating). Tracking changes to these files also doesn‚Äôt really do much good.\nI highly recommend that you make a point to only add and commit files which you consciously want to track.\n\n5.4.4 Staging your changes\nIn RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you‚Äôve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file.\nMore formally, staging is saying ‚ÄúI‚Äôd like these changes to be added to the current version, I think‚Äù. Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out.\nIndividually staging lines of a file is most useful in situations where you‚Äôve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio‚Äôs ‚Äústage selection‚Äù interface. Both will work, though git can‚Äôt always separate changes quite as finely as you might want (and as a result, RStudio‚Äôs interface sometimes seems unresponsive, even though the underlying issue is with what git can do).\n\n5.4.5 Committing your changes\nA git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache.\n\n\nHere, we commit the red line as a change to our file.\n\nIn general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you‚Äôve accomplished a small task. This will do two things:\n\nYou‚Äôll have small, bite-sized changes that are briefly described to serve as a record of what you‚Äôve done (and what still needs doing)\nWhen you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred.\n\n5.4.6 Pushing and Pulling\nWhen you‚Äôre working alone, you generally won‚Äôt need to worry about having to update your local copy of the repository (unless you‚Äôre using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document.\n\nIf you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU‚ÄôRE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document.\nFor this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren‚Äôt a huge deal, but they slow the workflow down, and are best avoided where possible.\n\nPulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are ‚Äúin the cloud‚Äù (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up).\nPushing describes the process of updating the copy of the repository on another machine (e.g.¬†on GitHub) so that it has the most recent changes you‚Äôve made to your machine.\n\n\n\n\n\n\ngit push copies the version of the project on your computer to GitHub\n\n\n\n\n\ngit pull copies the version of the project on GitHub to your computer\n\n\n\n\n\nFigure¬†5.1: Git push and git pull are used to sync your computer with the remote repository (usually hosted on GitHub)\n\n\nIn general, your workflow will be\n\nClone the project or create a new repository\nMake some changes\nStage the changes with git add\nCommit the changes with git commit\nPull any changes from the remote repository\nResolve any merge conflicts\nPush the changes (and merged files) with git push\n\nIf you‚Äôre working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#references",
    "href": "part-tools/05-git-and-github.html#references",
    "title": "5¬† Version Control with Git",
    "section": "\n5.5 References",
    "text": "5.5 References\n\n\n\n\n[1] \nJ. Bryan, J. Hester, and {The Stat 545 TAs}, Happy git and GitHub for the useR. 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 09, 2022]\n\n\n[2] \ndustbuster, ‚ÄúAnswer to \"git is not working after macOS update (xcrun: Error: Invalid active developer path (/library/developer/CommandLineTools)\". Stack overflow,‚Äù Sep. 26, 2018. [Online]. Available: https://stackoverflow.com/a/52522566/2859168. [Accessed: Jan. 13, 2023]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/05-git-and-github.html#footnotes",
    "href": "part-tools/05-git-and-github.html#footnotes",
    "title": "5¬† Version Control with Git",
    "section": "",
    "text": "Yes, I‚Äôm aware that this sounds paranoid. It‚Äôs been a very rare occasion that I‚Äôve needed to restore something from another backup. You don‚Äôt want to take chances. I knew a guy who had to retype his entire masters thesis from the printed out version the night before it was due because he had stored it on a network drive that was decommissioned. You don‚Äôt want to be that person.‚Ü©Ô∏é\nYes, I‚Äôm seriously pushing it with this book; several of the datasets are ~30 MB‚Ü©Ô∏é",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html",
    "href": "part-tools/06-documents.html",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "",
    "text": "6.1  Objectives\nThis chapter will be shorter in length than many of the rest, but you should not devote less time to it. Instead, you should spend the time playing with the different options presented here and deciding which one of each is your favorite. Rather than detailing all of the customization options in each package, I think you‚Äôll have an easier time looking at examples, trying to customize them yourself to get the effect you want, and figuring out how to do that by reading the documentation, stackoverflow posts, and other help files ‚Äì those are the skills you‚Äôll need when you try to put this knowledge into action.\nAt the end of this chapter there are a few extras ‚Äì for instance, how to use GitHub to host your documents, how to create a blog with blogdown, and more. You should feel free to investigate, but as long as you are able to create presentation slides, posters, and a CV, you‚Äôre good to go.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#objectives",
    "href": "part-tools/06-documents.html#objectives",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "",
    "text": "Create professional documents (slides, posters, CVs) using LaTeX and/or markdown\n\n\n\n\n\n\n\nReproducibility with Rmarkdown (by Allison Horst)",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "href": "part-tools/06-documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.2 Literate Programming, knitr, rmarkdown, and quarto\n",
    "text": "6.2 Literate Programming, knitr, rmarkdown, and quarto\n\nLiterate programming is a programming method where you explain the code in natural language (e.g.¬†English) in roughly the same space that you write the code (in a programming language). This solves two problems: code isn‚Äôt always clear as to what its goals are, and natural language descriptions of algorithms aren‚Äôt always clear enough to contain the details of how something is actually implemented.\nThe knitr, Rmarkdown, and quarto packages are all implementations of literate programming. The packages tend to overlap a bit, because knitr and Rmarkdown were written by the same author, Yihui Xie, and quarto is the next generation of Rmarkdown that incorporates more options for using other data-science related programming languages.\n\n\nknitr is primarily focused on the creation of Rnw (r no weave) files, which are essentially LaTeX files with R code inside. Rnw files are compiled into pdfs.\n\nrmarkdown uses Rmd or Rmarkdown files, which can then be compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nquarto uses qmd files, which are compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nAll of these programs work essentially the same way: code chunks are run in the specified language, figures are saved, tables are created, and the results are added to the intermediate file (.tex or .md). Then, another program (LaTeX or Pandoc) compiles the intermediate file into the final result. Understanding this process is key to being able to debug any errors you may encounter, because you need to identify which program is having the error - the code chunk? adding the results to the intermediate file? compiling from the intermediate file to the end result?\n\n\nknitr\nrmarkdown\nquarto\n\n\n\n\n\nKnitr uses R to produce a tex (.tex) file, which is then compiled to PDF using LaTeX.\n\n\n\n\n\nrmarkdown uses R to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\n\nquarto uses R or python to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\nOne major advantage of literate programming packages from a practical perspective is that it largely removes the need to keep track of graphs and charts when you‚Äôre writing a paper, making a presentation, etc. The charts and tables based on your method automatically update when the document is recompiled.\nYou‚Äôve probably been using quarto to submit your homework throughout the semester. In this chapter, we‚Äôre going to explore some other applications of literate programming: creating slides, posters, and more.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#review-quarto-formatting",
    "href": "part-tools/06-documents.html#review-quarto-formatting",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.3 Review: Quarto Formatting",
    "text": "6.3 Review: Quarto Formatting\nThis section‚Äôs material is stolen copied directly from the Quarto documentation [1].\nText Formatting\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n*italics* and **bold**\n\nitalics and bold\n\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\nHeadings\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n# Header 1\nHeader 1\n\n\n## Header 2\nHeader 2\n\n\n### Header 3\nHeader 3\n\n\n#### Header 4\nHeader 4\n\n\n##### Header 5\nHeader 5\n\n\n###### Header 6\nHeader 6\n\n\n\nLinks & Images\n\n\nMarkdown Syntax\nOutput\n\n\n\n&lt;https://quarto.org&gt;\nhttps://quarto.org\n\n\n[Quarto](https://quarto.org)\nQuarto\n\n\n![Caption](elephant.png)\n\n\nCaption\n\n\n\n[![Caption](elephant.png)](https://quarto.org)\n\n\n\n[![Caption](elephant.png)](https://quarto.org \"An elephant\")\n\n\n\n[![](elephant.png){fig-alt=\"Alt text\"}](https://quarto.org)\n\n\n\nLists\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n\n\nunordered list\n\nsub-item 1\n\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n\n\n*   item 2\n\n    Continued (indent 4 spaces)\n\n\nitem 2\nContinued (indent 4 spaces)\n\n\n\n\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\nordered list\n\nitem 2\n\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\n(@)  A list whose numbering\n\ncontinues after\n\n(@)  an interruption\n\n\nA list whose numbering\n\ncontinues after\n\nan interruption\n\n\n\n\nterm\n: definition\n\nterm\n\ndefinition\n\n\n\n\nTables\n\n\nMarkdown Syntax\nOutput\n\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nLearn more in the article on Tables.\nEquations\nUse $ delimiters for inline math and $$ delimiters for display math. For example:\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\ninline math: $E = mc^{2}$\ninline math: \\(E=mc^{2}\\)\n\n\n\ndisplay math:\n\n$$E = mc^{2}$$\ndisplay math:\\[E = mc^{2}\\]\n\n\n\n\nIf you want to define custom TeX macros, include them within $$ delimiters enclosed in a .hidden block. For example:\n::: {.hidden}\n$$\n \\def\\RR{{\\bf R}}\n \\def\\bold#1{{\\bf #1}}\n$$\n:::\nFor HTML math processed using MathJax (the default) you can use the \\def, \\newcommand, \\renewcommand, \\newenvironment, \\renewenvironment, and \\let commands to create your own macros and environments.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#a-very-brief-introduction-to-latex",
    "href": "part-tools/06-documents.html#a-very-brief-introduction-to-latex",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.4 A Very Brief Introduction to LaTeX",
    "text": "6.4 A Very Brief Introduction to LaTeX\nLaTeX is a document preparation utility that attempts to take the focus off of layout (so you don‚Äôt have to spend 30 minutes trying to get the page break in the right place in e.g.¬†Word) and bibliographic details.\n\n\nI‚Äôm not convinced LaTeX succeeds at freeing you from layout concerns, but it‚Äôs certainly true that it is much more powerful than Word for layout purposes.\nThe philosophy of LaTeX is that presentation shouldn‚Äôt get in the way of content: you should be able to change the presentation formatting systematically, without having to mess with the content. This (theoretically) allows you to switch templates easily, make document-wide changes in a single command, and more.\n\n\n\n\n\n\nTry it out\n\n\n\nIn Rstudio, copy the text in the document below, paste it into a text file in the editor window, and name it test.tex. You should see a Compile PDF button show up at the top of the document. Click that button to compile the document.\n\\documentclass{article} % this tells LaTeX what type of document to make\n% Note, comments are prefaced by a % sign. If you need to type the actual symbol\n% you will have to escape it with \\%.\n\n\\begin{document}\nHello \\LaTeX!\n\\end{document}\n\n\nMost commonly, you‚Äôll use the article document class for papers, and beamer for presentations and posters. Other useful classes include moderncv (for CVs) and book.\n\n\nThere is a LaTeX class maintained by the UNL math department for thesis formatting. You can easily add R code chunks to a LaTeX file by changing the extension of any .tex file to .Rnw.\nThe Statistics graduate students maintain a bookdown (rmarkdown) version of the UNL thesis class on github here. At some point, hopefully someone will port this to quarto.\nThere are several types of latex commands:\n\n\nDeclarations: statements like \\documentclass, \\usepackage or \\small, which are stated once and take effect until further notice.\n\nEnvironments: statements with matching \\begin{xxx} and \\end{xxx} clauses that define a block of the document which is treated differently. Common environments include figures and tables.\n\nSpecial characters: another type of command that don‚Äôt define formatting or structure, but may print special characters, e.g.¬†\\% to print a literal % character.\n\nBoth declarations and environments may come with both optional and required arguments. Required arguments are placed in {...} brackets, while optional arguments are placed in [...] brackets. You can, for instance, start your document with \\documentclass[12pt]{article} to specify the base font size.\nOne of the most useful features in LaTeX is math mode, which you can enter by enclosing text in $ ... $ (for inline statements), $$ ... $$ or \\[ ... \\] (for statements on their own line), or using other environments like \\begin{array} ... \\end{array} that come in math-specific packages. Once in math mode, you can use math symbol commands to get characters like \\(\\theta, \\pi, \\sum, \\int, \\infty\\), and more.\n\n\n\n\n\n\nTry it out\n\n\n\nWith any document creation software, the easiest way to learn how to do it is to find a sample document, tinker with it, see if you can make things the way you want them to be, and then google the errors when you inevitably screw something up.\n\n\nProblem\nSolution\n\n\n\nTake the sample document up above and see if you can do the following tasks: (I‚Äôve linked to documentation that may be useful)\n\nAdd an image\nAdd the quadratic formula and the PDF of a normal distribution to the document\nIn extremely large text, print LaTeX using the \\LaTeX command\nIn extremely small, italic text, print your name\n\n\n\n\\documentclass{article} % this tells LaTeX what type of document to make\n\n% Add the graphicx package so that we can include images\n\\usepackage{graphicx}\n\n\\begin{document}\nHello \\LaTeX!\n\n% Include a figure\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=.5\\textwidth]{../../images/gen-prog/ms-frizzle.png}\n\\caption{Ms. Frizzle, amazing teacher and driver of the Magic School Bus.}\n\\end{figure}\n\n% Add the quadratic formula and the normal PDF to the document\n$y = ax^2 + bx + c$ can be solved to get $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\nThe PDF of a normal distribution is $$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n\n% In extremely large text, print \\LaTeX\n\n\\Huge\\LaTeX\n\n% In extremely small italic text, print your name\n\n\\tiny\\emph{Your name}\n\n\\end{document}\nYou can see the compiled pdf here.\n\n\n\n\n\n\n6.4.1 Knitr\nA LaTeX document has the file extension .tex, but it‚Äôs very easy to convert a LaTeX document into a .Rnw (R-no-weave) document: change the file extension. Then, you can add R code chunks, and the .Rnw document will be compiled to a .tex document in R, and then the .tex document will be compiled to .pdf using LaTeX.\nR code chunks are embedded in LaTeX documents using:\n% start of chunk\n&lt;&lt;chunk-name, ...options...&gt;&gt;=\n\n@\n% end of chunk\nYou can embed numerical results inline using \\Sexpr{...} where your R code goes in the ....\nYou could in theory use python within knitr via the reticulate package [2], but it will be easier by far to use quarto. Pick the tool that does the job well.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#slides",
    "href": "part-tools/06-documents.html#slides",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.5 Slides",
    "text": "6.5 Slides\n\n6.5.1 Beamer (LaTeX) and knitr\nBeamer is a powerful LaTeX class which allows you to create slides. The only change necessary to turn a beamer slide deck into a knitr slide deck is to add fragile as an option to any slide with verbatim content.\nYou can also create Beamer slides with Rmarkdown. Example presentation. Standard trade-offs (formatting details vs.¬†document complexity) apply.\n\n\nCheck out the UNL-themed Beamer quarto template\n\n\n\n\n\n\nTry it out\n\n\n\nDownload and compile beamer-demo.Rnw.\nCan you change the theme of the presentation?\nAdd another slide, and on that slide, show an appropriate style ggplot2 graph of the distribution of board game ratings, reading in the board game ratings using the following code:\n\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-12/board_games.csv\")\n\n\n\n\n\nKarl Broman has a set of slides that show how to use beamer + knitr to make reproducible slides with notes.\nYou can also create Beamer slides using Rmarkdown or quarto, if you want, but you may have more control over the fine details if you go straight to the Rnw file without going through markdown first. It‚Äôs a trade-off ‚Äì the file will probably be simpler in markdown, but you won‚Äôt have nearly as much control.\n\n6.5.2 HTML slides\nRStudio has a host of other options for html slide presentations. There are some definite advantages to HTML presentations: they‚Äôre easy to share (via URL), you can add gifs, emojis, and interactive graphics, and you can set up github to host the presentations as well.\n\n\nI have a repository for all of the presentations I‚Äôve given, and I use github pages to render the html presentations. Very easy, convenient, and I never have to carry a flash drive around at a conference or mess with the conference computers.\nThe downside to HTML slides is that there are approximately 100000 different javascript libraries that create HTML slides, and all of them have different capabilities. Many of these libraries have extensions that will let you create markdown slides, but they each have slightly different markdown syntax and capabilities.\n\n\nRmarkdown slide options available by default in RStudio\n\nYou can get the full details of any fully supported slide class in Rmarkdown by looking at the Rmarkdown book [3], which is freely available online. These guidelines will give you specifics about how to customize slides, add incremental information, change transitions, print your slides to PDF, and include speaker notes.\nQuarto has simplified the slide options available to you - for HTML slides, you have one option, which is to use reveal.js. While this may sound limiting, it‚Äôs really not - RStudio/Posit (the company behind quarto) has done a ton of work to make quarto a lovely experience, and that extends to the slides. I have almost entirely switched to using quarto for everything because it‚Äôs so much easier to arrange figures, add alt-text, and style presentations. See the quarto presentation documentation here [4]. If you have collaborators who are stuck on MS Office, quarto allows you to compile to a PowerPoint presentation.\nRather than repeat the documentation for each slide package in this document, I think it is probably easier just to link you to the documentation and a sample presentation for each option.\nQuarto:\n\n\nreveal.js Example presentation\n\n\nRmarkdown:\n\n\nreveal.js Example presentation Example with UNL CSS Theme\n\n\nioslides Example presentation\n\n\nslidy Example presentation\n\n\nxaringan Example presentation, Example presentation 2 using UNL CSS theme\n\n\nIf you‚Äôre familiar with CSS (or happier tinkering to get the look of something exactly right) then xaringan and reveal.js are excellent full-featured options.\n\n\nI relied heavily on 2D slide layouts available in reveal.js during my PhD prelim and defense.\nA nice feature of reveal.js presentations is support for 2D slide layouts, so you can have multiple sections in your presentation, and move vertically through each section, or horizontally between sections. That is useful for presentations where you may not plan on covering everything, but where you want to have all of the information available if necessary.\n\n\nUNL themed HTML presentations:\n\n\nxaringan (zip of all required files)\n\nquarto reveal.js (zip of all required files)\n\n\n\n\n\n\n\nTry it out\n\n\n\nTake a few minutes and try each of them out to see what feels right to you. Each one has a slightly different ‚Äúflavor‚Äù of markdown, so read through the example to get a sense for what is different.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#posters",
    "href": "part-tools/06-documents.html#posters",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.6 Posters",
    "text": "6.6 Posters\nPosters are another common vehicle for presenting academic project results. Because posters are typically printed on paper or fabric, the standard file format is still PDF. As some venues move to digital posters, it is becoming more realistic to use HTML poster layouts that contain interactive elements.\n\n6.6.1 LaTeX\nOverleaf has a fantastic gallery of posters made in LaTeX.\nThere are several LaTeX options for making scientific posters: baposter, beamerposter, tikzposter are among the most common. We‚Äôll focus on beamerposter here, but you are free to explore the other poster classes at will. As with beamer, you can easily integrate knitr code chunks into a document, so that you are generating your images reproducibly.\nBasic code for a poster in beamer (along with the necessary style files) that I‚Äôve minimally customized to meet UNL branding requirements can be found here.\n\n\n\n\n\n\nTry it out\n\n\n\nDownload the beamer template and do the following:\n\nChange the 3-column span box to a 2-column span box.\nMake the ‚ÄúBlock Colors‚Äù box purple\nMove the References block up to fill the 4th column.\n\n\n\n\n6.6.2 Markdown\nWhile most posters are still put together in PDF form, there is growing support for HTML posters, and many conferences have digital poster options for display. This may allow you to use interactive graphics and other features in a poster that would not translate well to PDF. Here is a list of Rmarkdown poster options; some even have PDF export capabilities so that you can have the interactive version plus a static version.\n\n6.6.2.1 Posterdown\nTo start, install posterdown with install.packages(\"posterdown\").\n\n\nUse the RStudio menu to create a posterdown presentation file ‚Äì with a prefilled template\n\n\n\nUNL-themed posterdown template\nYou can also find additional customization options here. As with other markdown items, you can customize things even more using CSS. The nice thing about HTML posters, though, is that you can directly link to them if they‚Äôre hosted on a site.\nYou can also print a poster to PDF by running the following command: pagedown::chrome_print(\"myfile.Rmd\").\n\n6.6.2.2 Pagedown\nThe pagedown package also has a couple of poster templates, including poster-relaxed and poster-jacobs.\nThere are also templates for letters, business cards, and more in pagedown, if you‚Äôre feeling ambitious.\n\n\n\n\n\n\nTry it out\n\n\n\nDownload the pagedown template and do the following:\n\nChange the 3-column layout to 4 columns. Adjust the breaks ({.mybreak}) accordingly to make the poster look good.\nMake the 2nd-level headers #249ab5 (cerulean)\nMove the References block to the 4th column.\nPrint your poster to a PDF",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#resumecv",
    "href": "part-tools/06-documents.html#resumecv",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.7 Resume/CV",
    "text": "6.7 Resume/CV\nYou can also create resumes and CVs in markdown and LaTeX. There is no real substitute for playing around with these classes, but I really like moderncv in LaTeX.\n\n\nYou can see my highly customized CV here, with timelines and numbered publications. It has to be compiled multiple times to get everything right.\nPagedown also comes with a html resume template (Use the menu -&gt; Rmarkdown -&gt; From Template -&gt; HTML Resume) that can be printed to html and pdf simultaneously. There is also the vitae package, which has even more templates, integration with other packages/sites, and more.\n\n\nAt this point, the biggest reason I haven‚Äôt switched to HTML is that I really like my timeline CV and I don‚Äôt have enough time to fiddle with it more.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#hosting-content-with-github-pages",
    "href": "part-tools/06-documents.html#hosting-content-with-github-pages",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.8 Hosting Content with Github Pages",
    "text": "6.8 Hosting Content with Github Pages\nGithub will host HTML content for you using Github pages (case in point: this textbook). This means you can version control your content (for instance, presentations or your CV) and have GitHub do the hosting (so you don‚Äôt have to find a webserver, buy a domain name, etc).\n\n\n\n\n\n\nSetting up Github Pages\n\n\n\n\nCreate a new repository named username.github.io on your personal github site (not the unl-stat850 classroom group)\nClone your repository\nModify your README.md file and push your changes\nGo to https://username.github.io and see your README.md file rendered as HTML.\n\n\n\n\n\n\n\nGithub will render any README.md file as actual HTML; it will also allow you to host plain HTML pages. By default, the README file is rendered first, but in subsequent directories, a file named index.html will be rendered as the ‚Äúhome page‚Äù for the subdirectory, if you have such a file. Otherwise you‚Äôll have to know the file name.\nI tend to separate things out into separate repositories, but you can host HTML content on other repositories too, by enabling github pages in the repository settings. On my personal page, I have repositories for my CV, Presentations, etc. Each repository that has pages enabled can be accessed via https://srvanderplas.github.io/\\&lt;repository name\\&gt;/\\&lt;repository file path\\&gt;. So, to see my stat-computing-r-python repository, you‚Äôd go to https://srvanderplas.github.io/stat-computing-r-python/ (Oh, wait, you‚Äôre likely already there!).\n\n\nI‚Äôve been putting my presentations on Github since 2014, so it has a pretty good record of every set of slides I‚Äôve created. I highly recommend this strategy - storing everything online makes it easy to share your work with others, reference later, and more importantly, easy for you to find in 3 years. One thing I learned, though, was that it‚Äôs helpful to create a presentation repository by year ‚Äì I eventually hit the maximum repository limit (which was irritating), and it‚Äôs also nice to be able to find things quickly and remember when you gave that particular presentation.\nThis mechanism provides a very convenient way to showcase your work, share information with collaborators, and more - instead of sending files, you can send a URL and no one has to download anything overtly.\n\n\n\n\n\n\nSetting up Github Pages in an existing repository\n\n\n\n\n\n\n\n\n\nIf you want to track your quarto/rmarkdown code and then render the output to a separate folder, you can use the docs/ folder. Github has this as an option as well ‚Äì where we selected ‚Äúmain‚Äù branch above, we would select ‚Äúdocs/‚Äù instead (it‚Äôs grayed out b/c there isn‚Äôt a docs folder in the repo). That is how this book is hosted - the book compiles to the docs/ folder, and that way the book is rendered in final form and you don‚Äôt have to see all of the other crud that is in the repository.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#additional-resources-to-explore",
    "href": "part-tools/06-documents.html#additional-resources-to-explore",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.9 Additional Resources to Explore",
    "text": "6.9 Additional Resources to Explore\nThere are many other XXXdown packages made for Rmarkdown. Quarto is more multi-functional and contains blog, book, and website capabilities in a single package. However, most of the things which worked in Rmarkdown also work in Quarto, and Quarto has clearly been built off of the success of the ___down packages for Rmarkdown.\n\nblogdown\nbookdown (what I used to make this book in the SAS + R era)\npkgdown (to easily build documentation websites for R packages)\nROpenSci tutorial: How to set up hosting on github\nliftr - use Docker to make persistently reproducible documents\n\nIn addition, @mcanouil maintains a list of Quarto talks, topics, tools, and examples that is worth a look.",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-tools/06-documents.html#references",
    "href": "part-tools/06-documents.html#references",
    "title": "6¬† Reproducibility and Professional Communication",
    "section": "\n6.10 References",
    "text": "6.10 References\n\n\n\n\n[1] \nPosit, ‚ÄúMarkdown Basics,‚Äù Quarto. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html#text-formatting. [Accessed: Oct. 17, 2022]\n\n\n[2] \nK. Ushey, J. Allaire, and Y. Tang, Reticulate: Interface to ‚Äôpython‚Äô. 2022 [Online]. Available: https://CRAN.R-project.org/package=reticulate\n\n\n\n[3] \nY. Xie, J. J. Allaire, and G. Grolemund, ‚ÄúChapter 4 Presentations,‚Äù in R Markdown: The Definitive Guide, 1st ed., CRC Press, 2018 [Online]. Available: https://bookdown.org/yihui/rmarkdown/presentations.html. [Accessed: Sep. 28, 2022]\n\n\n[4] \nPosit, ‚ÄúQuarto - Presentations,‚Äù Quarto. [Online]. Available: https://quarto.org/docs/presentations/. [Accessed: Sep. 28, 2022]",
    "crumbs": [
      "Part I: Tools",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducibility and Professional Communication</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html",
    "href": "part-gen-prog/00-intro.html",
    "title": "7¬† Introduction to Programming",
    "section": "",
    "text": "7.1 What is Programming?\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\nIn this book, we‚Äôll be using both R and Python, and we‚Äôll be using these languages to solve problems that are related to working with data. At first, we‚Äôll start with smaller, simpler problems that don‚Äôt involve data, but by the end, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical ‚Äúsentences‚Äù. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they‚Äôll last you a lifetime.\nJust as you wouldn‚Äôt expect to learn French or Mandarin fluently after taking a single class, you cannot expect to be fluent in R or python once you‚Äôve worked through this book. Fluency takes years of work and practice, and lots of mistakes along the way. You cannot learn a language (programming or otherwise) if you‚Äôre worried about making mistakes. Take a minute and put those concerns away somewhere, take a deep breath, and remember the Magic School Bus Motto:",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#what-is-programming",
    "href": "part-gen-prog/00-intro.html#what-is-programming",
    "title": "7¬† Introduction to Programming",
    "section": "",
    "text": "Programming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\n\n\nbreak a problem down into smaller, easily solvable problems\nsolve the small problems\ncommunicate the solution to a computer using a programming language\n\n\n\n\n\n\nFor those who don‚Äôt know, the Magic School Bus is a PBS series that aired in the 1990s and was brought back by Netflix in 2017. It taught kids about different principles of science and the natural world.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "href": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "title": "7¬† Introduction to Programming",
    "section": "\n7.2 Programming Vocabulary: Hello World",
    "text": "7.2 Programming Vocabulary: Hello World\nI particularly like the way that Python for Everybody [1] explains vocabulary:\n\nUnlike human languages, the Python vocabulary is actually pretty small. We call this ‚Äúvocabulary‚Äù the ‚Äúreserved words‚Äù. These are words that have very special meaning to Python. When Python sees these words in a Python program, they have one and only one meaning to Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of Python‚Äôs reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like ‚Äúsit‚Äù, ‚Äústay‚Äù, and ‚Äúfetch‚Äù. When you talk to a dog and don‚Äôt use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, ‚ÄúI wish more people would walk to improve their overall health‚Äù, what most dogs likely hear is, ‚Äúblah blah blah walk blah blah blah blah.‚Äù That is because ‚Äúwalk‚Äù is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while\n\nThat is it, and unlike a dog, Python is already completely trained. When you say ‚Äòtry‚Äô, Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the Python equivalent of ‚Äúspeak‚Äù (in human-to-dog language). The nice thing about telling Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nprint('Hello world!')\n## Hello world!\n\n\nAnd we have even written our first syntactically correct Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nR has a slightly smaller set of reserved words:\nif          else     repeat      while\nfor         in       next        break\nTRUE        FALSE    NULL        Inf\nNA_integer_ NA_real_ NA_complex_ NA_character_\nNaN         NA       function    ...\nIn R, the ‚ÄúHello World‚Äù program looks exactly the same as it does in python.\n\nprint('Hello world!')\n## [1] \"Hello world!\"\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history [2], because it is also similar to Lisp, but both languages are still very similar to C and run C or C++ code in the background.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#getting-help",
    "href": "part-gen-prog/00-intro.html#getting-help",
    "title": "7¬† Introduction to Programming",
    "section": "\n7.3 Getting help",
    "text": "7.3 Getting help\nIn both R and python, you can access help with a ? - the order is just slightly different.\nSuppose we want to get help on a for loop in either language.\nIn R, we can run this line of code to get help on for loops.\n\n?`for`\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we‚Äôre talking about the function itself. Most other function help can be accessed using ?function_name. The backtick trick also works for functions that don‚Äôt start with letters, like +.\nIn python, we use for? to access the same information.\n\nfor? # help printed in the terminal\n?for # help printed in the help pane\n\n(You will have to run this in interactive mode for it to work in either language)\nw3schools has an excellent python help page that may be useful as well. Searching for help using google also works well, particularly if you know what sites are likely to be helpful, like w3schools and stackoverflow. A similar set of pages exists for R help on basic functions\n\n\n\n\n\n\nLearn More\n\n\n\nA nice explanation of the difference between an interpreter and a compiler. Both Python and R are interpreted languages that are compiled from lower-level languages like C.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "href": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "title": "7¬† Introduction to Programming",
    "section": "\n7.4 References",
    "text": "7.4 References\n\n\n\n\n[1] \nD. C. R. Severance, Python for Everybody: Exploring Data in Python 3. Ann Arbor, MI: CreateSpace Independent Publishing Platform, 2016 [Online]. Available: https://www.py4e.com/html3/\n\n\n\n[2] \nR. Ihaka, ‚ÄúR : Past and future history,‚Äù 1998 [Online]. Available: https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Introduction to Programming</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html",
    "href": "part-gen-prog/01-basic-var-types.html",
    "title": "8¬† Variables and Basic Data Types",
    "section": "",
    "text": "8.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#objectives",
    "href": "part-gen-prog/01-basic-var-types.html#objectives",
    "title": "8¬† Variables and Basic Data Types",
    "section": "",
    "text": "Know the basic data types and what their restrictions are\nKnow how to test to see if a variable is a given data type\nUnderstand the basics of implicit and explicit type conversion\nWrite code that assigns values to variables",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "href": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.2 Basic Definitions",
    "text": "8.2 Basic Definitions\nFor a general overview, [1] is an excellent introduction to data types:\n\n\n\n\nLet‚Äôs start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a ‚Äústring‚Äù of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn most programming languages (including R and python), there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric- decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (‚Äúdouble precision‚Äù).1\n\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\n\n\n\n\n\n\nCapitalization matters!\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important ‚Äì especially when we start reading in data.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#variables",
    "href": "part-gen-prog/01-basic-var-types.html#variables",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.3 Variables",
    "text": "8.3 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\n\n8.3.1 Assignment\nWe assign variables values using the syntax object_name &lt;- value (R) or object_name = value (python). You can read this as ‚Äúobject name gets value‚Äù in your head.\n\n\nDataCamp Introduction to R Chapter 1: Intro to basics\nDataCamp Introduction to Python for Data Science Chapter 1: Python Basics\n\n\nIn R, &lt;- is used for assigning a value to a variable. So x &lt;- \"R is awesome\" is read ‚Äúx gets ‚ÄòR is awesome‚Äô‚Äù or ‚Äúx is assigned the value ‚ÄòR is awesome‚Äô‚Äù. Technically, you can also use = to assign things to variables in R, but most style guides consider this to be poor programming practice, so seriously consider defaulting to &lt;-.\nIn Python, = is used for assigning a value to a variable. This tends to be much easier to say out loud, but lacks any indication of directionality.\n\n8.3.1.1 Demo: Assignment\n\n\nR\nPython\n\n\n\n\nmessage &lt;- \"So long and thanks for all the fish\"\nyear &lt;- 2025\nthe_answer &lt;- 42L\nearth_demolished &lt;- FALSE\n\n\n\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in R, we assign variables values using the &lt;- operator, where in Python, we assign variables values using the = operator. Technically, = will work for assignment in both languages, but &lt;- is more common than = in R by convention.\n\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n8.3.2 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n‚Äì Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R. In Python, object names must start with a letter and can consist of letters, numbers, and _ (that is, . is not a valid character in a Python variable name). While it is technically fine to use uppercase variable names in Python, it‚Äôs recommended that you use lowercase names for variables (you‚Äôll see why later).\nWhat happens if we try to create a variable name that isn‚Äôt valid?\nIn both languages, starting a variable name with a number will get you an error message that lets you know that something isn‚Äôt right - ‚Äúunexpected symbol‚Äù in R and ‚Äúinvalid syntax‚Äù in python.\n\n8.3.2.1 Invalid Names\n\n\nR\nPython\n\n\n\n\n1st_thing &lt;- \"check your variable names!\"\n## Error: &lt;text&gt;:1:2: unexpected symbol\n## 1: 1st_thing\n##      ^\n\n\n\n\n1st_thing &lt;- \"check your variable names!\"\n\nNote: Run the above chunk in your python window - the book won‚Äôt compile if I set it to evaluate üò•. It generates an error of SyntaxError: invalid syntax (&lt;string&gt;, line 1)\n\nsecond.thing &lt;- \"this isn't valid\"\n## name 'second' is not defined\n\nIn python, trying to have a . in a variable name gets a more interesting error: ‚Äú is not defined‚Äù. This is because in python, some objects have components and methods that can be accessed with .. We‚Äôll get into this more later, but there is a good reason for python‚Äôs restriction about not using . in variable names.\n\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n8.3.2.2 Learn More\nWhy is naming things hard? - Blog post by Neil Kakkar\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\n\nsome.people.use.periods (in R, obviously this doesn‚Äôt work in python)\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated üëø\n\nAs long as you pick ONE naming convention and don‚Äôt mix-and-match, you‚Äôll be fine. It will be easier to remember what you named your variables (or at least guess) and you‚Äôll have fewer moments where you have to go scrolling through your script file looking for a variable you named.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#types",
    "href": "part-gen-prog/01-basic-var-types.html#types",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.4 Types",
    "text": "8.4 Types\n\n8.4.1 Testing Types\nYou can use different functions to test whether a variable has a specific type.\n\n\nR\nPython\n\n\n\n\nis.logical(FALSE)\nis.integer(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nis.integer(2)\nis.numeric(2)\nis.character(\"Hello, programmer!\")\nis.function(print)\n## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] TRUE\n## [1] TRUE\n## [1] TRUE\n\nIn R, you use is.xxx functions, where xxx is the name of the type in question.\n\n\n\nisinstance(False, bool)\nisinstance(2, int)\nisinstance(2, (int, float)) # Test for one of multiple types\nisinstance(3.1415, float)\nisinstance(\"This is python code\", str)\n## True\n## True\n## True\n## True\n## True\n\nIn python, test for types using the isinstance function with an argument containing one or more data types in a tuple ((int, float) is an example of a tuple - a static set of multiple values).\nIf we want to test for whether something is callable (can be used like a function), we have to get slightly more complicated:\n\ncallable(print)\n## True\n\nThis is glossing over some much more technical information about differences between functions and classes (that we haven‚Äôt covered) [2].\n\n\n\n\n\n\n\n\n\nExample: Assignment and Testing Types\n\n\n\n\n\nCharacter\nLogical\nInteger\nDouble\nNumeric\n\n\n\n\nx &lt;- \"R is awesome\"\ntypeof(x)\n## [1] \"character\"\nis.character(x)\n## [1] TRUE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\n\nx = \"python is awesome\"\ntype(x)\n## &lt;class 'str'&gt;\nisinstance(x, str)\n## True\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- FALSE\ntypeof(x)\n## [1] \"logical\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] TRUE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\nIn R, is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs.¬†shorthand for true and false. 2\n\nx = False\ntype(x)\n## &lt;class 'bool'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## True\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\nNote that in python, boolean variables are also integers. If your goal is to test whether something is a T/F value, you may want to e.g.¬†test whether its value is one of 0 or 1, rather than testing whether it is a boolean variable directly, since integers can also function directly as bools in Python.\n\n\n\nx &lt;- 2\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\n\nWait, 2 is an integer, right?\n2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically.\n\nx &lt;- 2L # The L immediately after the 2 indicates that it is an integer.\ntypeof(x)\n## [1] \"integer\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] TRUE\nis.double(x)\n## [1] FALSE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2\ntype(x)\n## &lt;class 'int'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- 2.45\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2.45\ntype(x)\n## &lt;class 'float'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## True\n\n\n\nA fifth common ‚Äútype‚Äù3, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing.\nThe numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is ‚Ä¶ trickier. Testing for numeric variables guarantees that we‚Äôll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work.\nThe general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "href": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.5 Type Conversions",
    "text": "8.5 Type Conversions\nProgramming languages will generally work hard to seamlessly convert variables to different types. This is called implicit type casting - the computer implicitly changes the variable type to avoid a conflict.\n\n8.5.1 Implicit Type Conversion\n\n\nR\nPython\n\n\n\n\nTRUE + 2\n## [1] 3\n\n2L + 3.1415\n## [1] 5.1415\n\n\"abcd\" + 3\n## Error in \"abcd\" + 3: non-numeric argument to binary operator\n\n\n\n\nTrue + 2\n## 3\n\nint(2) + 3.1415\n## 5.141500000000001\n\n\"abcd\" + 3\n## can only concatenate str (not \"int\") to str\n\n\n\n\nThis conversion doesn‚Äôt always work - there‚Äôs no clear way to make ‚Äúabcd‚Äù into a number we could use in addition. So instead, R or python will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops.\nWhen you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be:\n\n8.5.2 Explicit Type Conversion\n\n\nR\nPython\n\n\n\n\nas.double(TRUE) + 2\n## [1] 3\n\nas.double(2L) + 3.1415\n## [1] 5.1415\n\nas.numeric(\"abcd\") + 3\n## [1] NA\n\n\n\n\nint(True) + 2\n## 3\n\nfloat(2) + 3.1415\n## 5.141500000000001\n\nfloat(\"abcd\") + 3\n## could not convert string to float: 'abcd'\n\nimport pandas as pd # Load pandas library\npd.to_numeric(\"abcd\", errors = 'coerce') + 3\n## nan\n\n\n\n\nWhen we make our intent explicit (convert ‚Äúabcd‚Äù to a numeric variable) we get an NA - a missing value - in R. In Python, we get a more descriptive error by default, but we can use the pandas library (which adds some statistical functionality) to get a similar result to the result we get in R.\nThere‚Äôs still no easy way to figure out where ‚Äúabcd‚Äù is on a number line, but our math will still have a result - NA + 3 is NA.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "href": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.6 What Type is it?",
    "text": "8.6 What Type is it?\nIf you don‚Äôt know what type a value is, both R and python have functions to help you with that.\n\n8.6.1 Determining Variable Types\n\n\nR\nPython\n\n\n\nIf you are unsure what the type of a variable is, use the typeof() function to find out.\n\nw &lt;- \"a string\"\nx &lt;- 3L\ny &lt;- 3.1415\nz &lt;- FALSE\n\ntypeof(w)\n## [1] \"character\"\ntypeof(x)\n## [1] \"integer\"\ntypeof(y)\n## [1] \"double\"\ntypeof(z)\n## [1] \"logical\"\n\n\n\nIf you are unsure what the type of a variable is, use the type() function to find out.\n\nw = \"a string\"\nx = 3\ny = 3.1415\nz = False\n\ntype(w)\n## &lt;class 'str'&gt;\ntype(x)\n## &lt;class 'int'&gt;\ntype(y)\n## &lt;class 'float'&gt;\ntype(z)\n## &lt;class 'bool'&gt;\n\n\n\n\n\n\n\n\n\n\nTry It Out: Variables and Types\n\n\n\n\n\nR\nPython\nR Solution\nPython Solution\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring &lt;- \ninteger &lt;- \ndecimal &lt;- \nlogical &lt;- \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring = \ninteger = \ndecimal = \nlogical = \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nstring &lt;- \"hi, I'm a string\"\ninteger &lt;- 4L\ndecimal &lt;- 5.412\nlogical &lt;- TRUE\n\nlogical + decimal\n## [1] 6.412\ninteger + decimal\n## [1] 9.412\nas.numeric(string) + integer\n## [1] NA\n\n\"abcd\" + \"efgh\"\n## Error in \"abcd\" + \"efgh\": non-numeric argument to binary operator\nTRUE + TRUE\n## [1] 2\n\nIn R, adding a string to a string creates an error (‚Äúnon-numeric argument to binary operator‚Äù). Adding a logical to a logical, e.g.¬†TRUE + TRUE, results in 2, which is a numeric value.\nTo concatenate strings in R (like the default behavior in python), we would use the paste0 function: paste0(\"abcd\", \"efgh\"), which returns abcdefgh.\n\n\n\nimport pandas as pd\n\nstring = \"hi, I'm a string\"\ninteger = 4\ndecimal = 5.412\nlogical = True\n\nlogical + decimal\n## 6.412\ninteger + decimal\n## 9.411999999999999\npd.to_numeric(string, errors='coerce') + integer\n## nan\n\n\"abcd\" + \"efgh\"\n## 'abcdefgh'\nTrue + True\n## 2\n\nIn Python, when a string is added to another string, the two strings are concatenated. This differs from the result in R, which is a ‚Äúnon-numeric argument to binary operator‚Äù error.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "href": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "title": "8¬† Variables and Basic Data Types",
    "section": "\n8.7 References",
    "text": "8.7 References\n\n\n\n\n[1] \n\nWhy TRUE + TRUE = 2: Data Types. (Feb. 03, 2020) [Online]. Available: https://www.youtube.com/watch?v=6otW6OXjR8c. [Accessed: May 18, 2022]\n\n\n[2] \nRyan, ‚ÄúAnswer to \"how do i detect whether a variable is a function?\". Stack overflow,‚Äù Mar. 09, 2009. [Online]. Available: https://stackoverflow.com/a/624948/2859168. [Accessed: Jan. 10, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#footnotes",
    "href": "part-gen-prog/01-basic-var-types.html#footnotes",
    "title": "8¬† Variables and Basic Data Types",
    "section": "",
    "text": "This means that doubles take up more memory but can store more decimal places. You don‚Äôt need to worry about this much in R, and only a little in Python, but in older and more precise languages such as C/C++/Java, the difference between floats and doubles can be important.‚Ü©Ô∏é\nThere is also an R package dedicated to pure evil that will set F and T randomly on startup. Use this information wisely.‚Ü©Ô∏é\nnumeric is not really a type, it‚Äôs a mode. Run ?mode for more information.‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Variables and Basic Data Types</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html",
    "href": "part-gen-prog/02-prog-functions.html",
    "title": "9¬† Using Functions and Libraries",
    "section": "",
    "text": "9.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#objectives",
    "href": "part-gen-prog/02-prog-functions.html#objectives",
    "title": "9¬† Using Functions and Libraries",
    "section": "",
    "text": "Understand how functions are used in R and python\nUnderstand how to install packages in R and python\nUnderstand how to load packages in R and python\nUse pipes to restructure code so that it is more readable",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "href": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.2 Mathematical Operators",
    "text": "9.2 Mathematical Operators\nLet‚Äôs first start with a special class of functions that you‚Äôre probably familiar with from your math classes - mathematical operators.\nHere are a few of the most important ones:\n\n\nTable¬†9.1: Mathematical operators in R and Python\n\n\n\nOperation\nR symbol\nPython symbol\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\n\n\n\nThese operands are all for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated (and different between R and python).\n\n\n\n\n\n\nExample: Integer and Modular Division\n\n\n\nInteger division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nLet‚Äôs demonstrate with the problem 14/3, which evaluates to 4.6666667 when division is used, but has integer part 4 and remainder 2.\n\n\nR\nPython\n\n\n\n14 %/% 3 in R would be 4, and 14 %% 3 in R would be 2.\n\n14 %/% 3\n## [1] 4\n14 %% 3\n## [1] 2\n\n\n\n\n14 // 3\n## 4\n14 % 3\n## 2",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "href": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.3 Order of Operations",
    "text": "9.3 Order of Operations\nBoth R and Python operate under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n\nR\nPython\n\n\n\n\n(1+1)^(5-2) \n## [1] 8\n1 + 2^3 * 4 \n## [1] 33\n3*1^3 \n## [1] 3\n\n\n\n\n(1+1)**(5-2)\n## 8\n1 + 2**3*4\n## 33\n3*1**3\n## 3",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "href": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.4 Simple String Operations",
    "text": "9.4 Simple String Operations\nPython has some additional operators that work on strings. In R, you will have to use functions to perform these operations, as R does not have string operators.\n\n\n\nPython\nR\n\n\n\nIn Python, + will concatenate (stick together) two strings. Multiplying a string by an integer will repeat the string the specified number of times.\n\n\"first \" + \"second\"\n## 'first second'\n\"hello \" * 3\n## 'hello hello hello '\n\n\n\nIn R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n## [1] \"first second\"\npaste(\"first\", \"second\", collapse = \" \")\n## [1] \"first second\"\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works w/ 2 objects passed in\n## [1] \"first\"  \"second\"\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n## [1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n## [1] \"a-first b-second c-third d-fourth\"\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n## [1] \"a\" \"b\" \"c\"\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n## [1] \"abc\"\n\nYou don‚Äôt need to understand the details of this code at this point in the class, but it is useful to know how to combine strings in both languages.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#sec-logical-ops",
    "href": "part-gen-prog/02-prog-functions.html#sec-logical-ops",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.5 Logical Operators",
    "text": "9.5 Logical Operators\nLogical variables can be combined through the use of logical operators in much the same way that numerical variables are combined through mathematical operators.\nThere are specific logical operators which are used to aggregate and combine multiple logical variables: the primary logical operators are and, or, and not 1.\nIn pseudocode, which is human-readable logic structured like computer code but without the syntax, we usually write these out in all caps.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n(X XOR Y) requires that one (and only one) of X or Y is true. Sometimes called exclusive or.\n\nWhen constructing a logical expression that combines Boolean variables, it can be helpful to build a truth table that lists all possible inputs on the left and the output of the operator on the right. A truth table demonstrating the logical operators and, or, not and xor is provided in Table¬†9.2.\n\n\nTable¬†9.2: Truth table for each of the common logical operators.\n\n\n\na\nb\na and b\na or b\n\nnot a\n\nnot b\na xor b\n\n\n\nT\nT\nT\nT\nF\nF\nF\n\n\nT\nF\nF\nT\nF\nT\nT\n\n\nF\nT\nF\nT\nT\nF\nT\n\n\nF\nF\nF\nF\nT\nT\nF\n\n\n\n\n\n\n\n\nTable¬†9.3: Logical operators in R and Python. These operators are intended for single values; evaluation of vectors may require different operators.\n\n\n\nOperation\nR\nPython\n\n\n\nand\n&\n\n& or and\n\n\n\nor\n|\n\n| or or\n\n\n\nnot\n!\nnot\n\n\nxor\nxor()\n^\n\n\n\n\n\n\nWhen writing code, we use the logical operators in R and Python shown in Table¬†9.3.\n\n\n\n\n\n\nGenerating Truth Tables\n\n\n\nWe can thus generate each entry in the truth table using the relevant logical operators in R and python.\n\n\nAND\nOR\nNOT\nXOR\n\n\n\nIn R, and comparisons use & as the operator.\n\nTRUE & TRUE\n## [1] TRUE\nTRUE & FALSE\n## [1] FALSE\nFALSE & TRUE\n## [1] FALSE\nFALSE & FALSE\n## [1] FALSE\n\nIn Python, and expressions use & as the operator.\n\nTrue & True\n## True\nTrue & False\n## False\nFalse & True\n## False\nFalse & False\n## False\n\nAlternately, in Python, you can also spell out the whole word and use and explicitly.\n\nTrue and True\n## True\nTrue and False\n## False\nFalse and True\n## False\nFalse and False\n## False\n\n\n\nIn R, or is denoted with | (the vertical bar, shift + the button above the enter key on most keyboards).\n\nTRUE | TRUE\n## [1] TRUE\nTRUE | FALSE\n## [1] TRUE\nFALSE | TRUE\n## [1] TRUE\nFALSE | FALSE\n## [1] FALSE\n\nIn Python, or expressions use | as the operator.\n\nTrue | True\n## True\nTrue | False\n## True\nFalse | True\n## True\nFalse | False\n## False\n\nAlternately, in Python, you can also spell out the whole word and use or explicitly.\n\nTrue or True\n## True\nTrue or False\n## True\nFalse or True\n## True\nFalse or False\n## False\n\n\n\nIn R, negation occurs using the ! operator.\n\n!TRUE\n## [1] FALSE\n!FALSE\n## [1] TRUE\n\nIn Python, negation occurs using the not operator.\n\nnot True\n## False\nnot False\n## True\n\n\n\nIn R, exclusive or uses the xor() function.\n\nxor(TRUE, TRUE)\n## [1] FALSE\nxor(TRUE, FALSE)\n## [1] TRUE\nxor(FALSE, TRUE)\n## [1] TRUE\nxor(FALSE, FALSE)\n## [1] FALSE\n\nIn Python, exclusive or uses the ^ operator.\n\nTrue ^ True\n## False\nTrue ^ False\n## True\nFalse ^ True\n## True\nFalse ^ False\n## False\n\n\n\n\n\n\n\n9.5.1 Order of Operations\nJust as with mathematical operators, there is an order of operations to logical operators, where NOT takes precedence over AND, which takes precedence over OR.\n\n\nR\nPython\n\n\n\n\na1 &lt;- TRUE\nb1 &lt;- FALSE\nc1 &lt;- FALSE\n\na1 | b1 & c1 # AND takes precedence\n## [1] TRUE\na1 | (b1 & c1) # same as above, with parens\n## [1] TRUE\n(a1 | b1) & c1 # force OR to be first using parentheses\n## [1] FALSE\n\n\n\n\na1 = True\nb1 = False\nc1 = False\n\na1 or b1 and c1 # AND takes precedence\n## True\na1 or (b1 and c1) # same as above, with parens\n## True\n(a1 or b1) and c1 # force OR to be first using parentheses\n## False\n\n\n\n\n\n9.5.2 De Morgan‚Äôs Laws\nDe Morgan‚Äôs Laws are a set of rules for how to combine logical statements, similar to distributive laws in numerical operations. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan‚Äôs First Law\nDeMorgan‚Äôs Second Law\n\n\n\n\n\nVenn Diagram of Set A and Set B\n\nSuppose that we set the convention that .\n\n\n\n\nA venn diagram illustration of De Morgan‚Äôs laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\nR\nPython\n\n\n\n\n!(TRUE | TRUE)\n## [1] FALSE\n!(TRUE | FALSE)\n## [1] FALSE\n!(FALSE | TRUE)\n## [1] FALSE\n!(FALSE | FALSE)\n## [1] TRUE\n\n!TRUE & !TRUE\n## [1] FALSE\n!TRUE & !FALSE\n## [1] FALSE\n!FALSE & !TRUE\n## [1] FALSE\n!FALSE & !FALSE\n## [1] TRUE\n\n\n\n\nnot(True or True)\n## False\nnot(True or False)\n## False\nnot(False or True)\n## False\nnot(False or False)\n## True\n\nnot(True) and not(True)\n## False\nnot(True) and not(False)\n## False\nnot(False) and not(True)\n## False\nnot(False) and not(False)\n## True\n\n\n\n\n\n\n\n\nA venn diagram illustration of De Morgan‚Äôs laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\n\n\n\nR\nPython\n\n\n\n\n!(TRUE & TRUE)\n## [1] FALSE\n!(TRUE & FALSE)\n## [1] TRUE\n!(FALSE & TRUE)\n## [1] TRUE\n!(FALSE & FALSE)\n## [1] TRUE\n\n!TRUE | !TRUE\n## [1] FALSE\n!TRUE | !FALSE\n## [1] TRUE\n!FALSE | !TRUE\n## [1] TRUE\n!FALSE | !FALSE\n## [1] TRUE\n\n\n\n\nnot(True and True)\n## False\nnot(True and False)\n## True\nnot(False and True)\n## True\nnot(False and False)\n## True\n\nnot(True) or not(True)\n## False\nnot(True) or not(False)\n## True\nnot(False) or not(True)\n## True\nnot(False) or not(False)\n## True",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#using-functions",
    "href": "part-gen-prog/02-prog-functions.html#using-functions",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.6 Using Functions",
    "text": "9.6 Using Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, mathematical operators (like those above) are a special type of functions ‚Äì but we aren‚Äôt going to get into that now.\nWe‚Äôre also not going to talk about how to create our own functions just yet. Instead, I‚Äôm going to show you how to use functions.\n\n\n\n\n\n\nCheat Sheets!\n\n\n\nIt may be helpful at this point to print out the R reference card2 and the Python reference card3 . These cheat sheets contain useful functions for a variety of tasks in each language .\n\n\nMethods are a special type of function that operate on a specific variable type. In Python, methods are applied using the syntax variable.method_name(). So, you can get the length of a string variable my_string using my_string.length().\nR has methods too, but they are invoked differently. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\n\n\n\n\nYour Turn\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nTry out some of the functions mentioned on the R and Python cheatsheets.\nCan you figure out how to define a list or vector of numbers? If so, can you use a function to calculate the maximum value?\nCan you find the R functions that will allow you to repeat a string variable multiple times or concatenate two strings? Can you do this task in Python?\n\n\n\n# Define a vector of numbers\nx &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the maximum\nmax(x)\n## [1] 5\n\n# function to repeat a variable multiple times\nrep(\"test\", 3)\n## [1] \"test\" \"test\" \"test\"\n# Concatenate strings, using \"ing... \" as the separator\npaste(rep(\"test\", 3), collapse = \"ing... \")\n## [1] \"testing... testing... test\"\n\n\n\n\n# Define a list of numbers\nx = [1, 2, 3, 4, 5]\n\n# Calculate the maximum\nmax(x)\n## 5\n\n# Repeat a string multiple times\nx = (\"test\", )*3 # String multiplication \n                 # have to use a tuple () to get separate items\n# Then use 'yyy'.join(x) to paste items of x together with yyy as separators\n'ing... '.join(x)\n## 'testing... testing... test'",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#overpowered-calculators",
    "href": "part-gen-prog/02-prog-functions.html#overpowered-calculators",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.7 Overpowered Calculators",
    "text": "9.7 Overpowered Calculators\nNow that you‚Äôre familiar with how to use functions, if not how to define them, you are capable of using R or python as a very fancy calculator. Obviously, both languages can do many more interesting things, which we‚Äôll get to, but let‚Äôs see if we can make R and Python do some very basic stuff that hopefully isn‚Äôt too foreign to you.\n\n\n\n\n\n\nExample: Triangle Side Length\n\n\n\n\n\nA right triangle with sides a, b, and hypotenuse c labeled.\n\nConsider this triangle. I‚Äôve measured the sides in an image editor and determined that \\(a = 212\\) pixels, \\(b = 345\\) pixels, and \\(c = 406\\) pixels. I suspect, however, that my measurements aren‚Äôt quite right - for one thing, I tried to measure in the center of the line, but it wasn‚Äôt easy on the diagonal.\nLet‚Äôs assume that my measurements for \\(a\\) and \\(b\\) are accurate and calculate how far off my estimate was for side \\(c\\).\n\n\nR\nPython\n\n\n\n\n# Define variables for the 3 sides of the triangle\na &lt;- 212\nb &lt;- 345\nc_meas &lt;- 406\nc_actual &lt;- sqrt(a^2 + b^2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error &lt;- (c_meas - c_actual)/c_actual * 100\npct_error\n## [1] 0.2640307\n\n\n\n\n# To get the sqrt function, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 212\nb = 345\nc_meas = 406\nc_actual = math.sqrt(a**2 + b**2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error = (c_meas - c_actual)/c_actual * 100\npct_error\n## 0.264030681414134\n\n\n\n\nInteresting, I wasn‚Äôt as inaccurate as I thought!\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nOf course, if you remember trigonometry, we don‚Äôt have to work with right triangles. Let‚Äôs see if we can use trigonometric functions to do the same task with an oblique triangle.\n\n\nProblem\nR solution\nPython solution\n\n\n\nJust in case you‚Äôve forgotten your Trig, the Law of Cosines says that \\[c^2 = a^2 + b^2 - 2 a b \\cos(C),\\] where \\(C\\) is the angle between sides \\(a\\) and \\(b\\).\n\n\nAn oblique triangle with sides labeled a, b, and c, and angles labeled as A, B, C with capital letter opposite the lowercase side.\n\nI measure side \\(a = 291\\) pixels, side \\(b = 414\\) pixels, and the angle between \\(a\\) and \\(b\\) to be \\(67.6^\\circ\\). What will I likely get for the length of side \\(c\\) in pixels?\nRemember to check whether R and python compute trig functions using radians or degrees! As a reminder, \\(\\pi\\) radians = \\(180^\\circ\\).\n\n\n\n# Define variables for the 3 sides of the triangle\na &lt;- 291\nb &lt;- 414\nc_angle &lt;- 67.6\nc_actual &lt;- sqrt(a^2 + b^2 - 2*a*b*cos(c_angle/180*pi))\nc_actual\n## [1] 405.2886\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n# To get the sqrt and cos functions, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 291\nb = 414\nc_angle = 67.6\nc_actual = math.sqrt(a**2 + b**2 - 2*a*b*math.cos(c_angle/180*math.pi))\nc_actual\n## 405.28860699402117\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n\n\nCongratulations, if you used a TI-83 in high school to do this sort of stuff, you‚Äôre now just about as proficient with R and python as you were with that!",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#libraries",
    "href": "part-gen-prog/02-prog-functions.html#libraries",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.8 Libraries",
    "text": "9.8 Libraries\nBoth R and python have a very robust system for extending the language with user-written packages. These packages will give you access to features that aren‚Äôt present in the base language, including new statistical methods, all sorts of plotting and visualization libraries, ways of interacting with data that are way more convenient than the default base language methods, and more.\nThere are tons of considerations to think about when using a new library, like how well it‚Äôs maintained, how many dependencies it has, and whether the developers of the package prioritize backwards-compatibility. For the moment, we‚Äôre going to ignore most of those considerations in favor of learning how to install packages and how to use functions from packages in our code.\n\n9.8.1 Environment management\nBefore we talk about how to install packages, though, we need to step back and think a little bit about the pros and cons of different ways of managing packages, if only because the most common R and python setups use very different approaches.\nImagine that you‚Äôre an accomplished programmer, and you are juggling multiple different projects. Each project uses some of the same packages, but some different packages as well. You open up a project that you haven‚Äôt run in a year, and you find out that one of the packages you‚Äôve updated more recently breaks a bunch of code you wrote a year ago, because the functions in the package have been renamed.\nWhat could prevent this from happening?\nOne way to solve this problem is to store the packages used in each project inside the project directory, in what we might call a project environment. This will keep each project isolated from the others, so that if you update a package in one project, it doesn‚Äôt affect any other project.\nHowever, this approach results in a lot of duplication: for one thing, you have copies of each package hanging around in every folder on your computer. That‚Äôs not storage efficient, but it does keep your code from breaking as frequently.\nTypically, Python programmers prefer the first approach (project-specific virtual environments), and R programmers default to the second approach (installing packages at the user or system level).\nThis is one of the things that can make starting to learn python so difficult - it can be hard to make sure you‚Äôre using the right environment. It doesn‚Äôt help that there are several different environment management systems in python - virtualenv, pipenv, and conda are the main options.\n\n9.8.1.1 Python environments\nconda and virtualenv (venv) are both virtual environment management systems. conda is sometimes preferred for scientific computing because it handles the complex dependencies that arise from large packages like numpy and scipi and pandas a bit better than pip does alone.\nThis guide assumes you have conda set up already. By default, Chapter 2 just installs python at the system level. If you want to use anaconda or miniconda you should go read the documentation for those installers and follow those steps first. Alternately, you can install and load the reticulate R package and then run install_miniconda() - this will install miniconda somewhere that RStudio can find it, but it may make using miniconda outside of RStudio difficult.\n\n\n\n\n\n\nConsistency is critical\n\n\n\n\n\nI highly recommend that you pick one of these options and use that consistently, rather than trying the advantages and disadvantages of each option in different projects. Here is a webcomic to serve as a cautionary tale if you do not heed this warning.\n\n\nPython Environment, by Randall Munroe of [xkcd](https://xkcd.com/1987/). CC-By-NC-2.5The Python environmental protection agency wants to seal it in a cement chamber, with pictorial messages to future civilizations warning them about the danger of using sudo to install random Python packages.\n\n\n\n\n\n\nSystem\nvenv (System console)\nvenv (RStudio)\nconda (System console)\nconda (RStudio)\n\n\n\nYou can absolutely install all python packages at the user/system level using pip. This has the previously mentioned disadvantages, but has the major advantage of being very simple.\nTo install a python package &lt;package name&gt; using pip, run this command\n\npip3 install &lt;package name&gt;\n\nSome computers may prefer that you use pip instead of pip3 - figure out which one your computer requires and use that.\n\n\nIn your system terminal, navigate to your project directory. Items within &lt; &gt; are intended to be replaced with values specific to your situation.\n\ncd &lt;project-directory&gt;\npip3 install virtualenv # install virtual environments\n\n# Create a virtual environment\nvirtualenv &lt;env-name&gt;\n\n# Activate your virtual environment\nsource &lt;env-name&gt;/bin/activate\n\n# Install packages\npip install &lt;pkg1&gt; &lt;pkg2&gt; &lt;pkg3&gt;\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nvirtualenv_create(envname = \"&lt;env-name&gt;\",\n                  packages = c(\"&lt;pkg1&gt;\", \"&lt;pkg2&gt;\", \"&lt;pkg3&gt;\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\n# Activate your virtual environment\nuse_virtualenv(\"&lt;env-name&gt;\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"&lt;pkg1&gt;|&lt;pkg2&gt;|&lt;pkg3&gt;\",\n     x = as.character(py_list_packages(envname = \"&lt;env-name&gt;\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\nThese steps constructed from [1].\n\ncd &lt;project-directory&gt;\n# Create conda environment and install specific python version and packages \nconda create --prefix ./&lt;env-name&gt; python=&lt;python-version&gt; &lt;pkg1&gt; &lt;pkg2&gt; &lt;pkg3&gt; \n\n# Activate your virtual environment\nconda activate ./&lt;env-name&gt;\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"./&lt;env-name&gt;/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nconda_create(envname = \"&lt;env-name&gt;\",\n             packages = c(\"&lt;pkg1&gt;\", \"&lt;pkg2&gt;\", \"&lt;pkg3&gt;\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"&lt;env-name&gt;/bin/python\") \n\n# Activate your virtual environment\nuse_condaenv(\"&lt;env-name&gt;\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"&lt;pkg1&gt;|&lt;pkg2&gt;|&lt;pkg3&gt;\",\n     x = as.character(py_list_packages(envname = \"&lt;env-name&gt;\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\n\n\n9.8.1.2 R environments\nSome R programmers have adopted the python philosophy of project-specific package management, using an R package called renv [2].\nrenv documentation can be found here if you wish to try it out. I find that it is most useful for projects where package updates may break things - e.g.¬†projects which run on shared systems or which are intended to work for a long period of time without maintenance.\nIf you want to use renv, you can do that by following these steps:\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n# Activate renv for a project\nrenv::activate()\n\n# this will install from github or CRAN\nrenv::install(c(\"pkg1\", \"pkg2\", \"githubuser/pkg3\")) \n\nI use renv for this textbook, because if a package update breaks things, I need to systematically check all the code chunks in the textbook to make sure they all work. I don‚Äôt want to do that every time someone fixes a minor bug, so I don‚Äôt update the packages the textbook uses more than once a semester (normally).\n\n9.8.2 Package repositories\nBoth R and Python have package systems, though generally, R is a bit more straightforward to deal with than python (in my opinion). Python has more package and environment management systems and I don‚Äôt fully understand them all, where all R packages seem to go through the same basic installation process and are just hosted in different places.\n\n\n\nFormally Published\nInformally Published/Beta\n\n\n\nR\n\nCRAN, Bioconductor\n\ngithub and other version control. See the remotes package documentation for all of the options.\n\n\nPython\nPyPi\ngithub and other version control systems\n\n\n\n9.8.3 Package Installation\n\n9.8.3.1 Installing packages in Python\nMany of the instructions here are modified from [3].\nYou can manage your python packages at the system level (e.g.¬†when you update one package, everything gets updated for every project ‚Äì this can break things!), with venv, or with conda.\nWhichever method (system, venv, conda) you use to manage your Python environment, when you go to install a new package, you have a few different options for how to do so.\n\n\nSystem console\nRStudio\nPython Chunk Magic\n\n\n\nIn python, you will typically want to install packages using a system terminal.\n\nMake sure your virtual environment/conda environment is activated (if applicable)\nInstallation commands:\n\nIf you are using system or venv, pip3 install &lt;package name&gt; should install your package.\nIf you are using conda, conda install &lt;package name&gt; is preferable, and if that doesn‚Äôt work, then try using pip3 install &lt;package name&gt;.\n\n\n\n\n# If you're using virtualenv\npip install &lt;pkg1&gt;\n\n# If you're using conda, try this first\nconda install &lt;pkg1&gt;\n# If that fails, try pip\n\n\n\nNote: This does NOT work for system package installation.\n\nMake sure R is using the correct python installation\nIn the R terminal, run reticulate::py_install(\"package name\")\n\n\n\n\nThis is less elegant, but nearly foolproof as long as RStudio knows where to find python.\n\nAt the top of the chunk, write %pip install &lt;package name&gt;\n\nRun this code (Cmd/Ctrl + Enter)\n\nComment the code out, so that you aren‚Äôt reinstalling the package every time you run the chunk.\n\n\n%pip install &lt;pkg1&gt;\n\nA slightly less elegant but more robust way to do this is to use the sys package. Loading the sys package ensures that you‚Äôre using the version of python that your file will be compiled with to install the package.\n\nimport sys\n# For pip installation\n!{sys.executable} -m pip install &lt;pkg1&gt;\n\n# For conda installation\n!{sys.executable} -m conda install &lt;pkg1&gt;\n\nOnce you‚Äôve installed the package on your machine, you can comment these lines out so that they don‚Äôt run every time - this makes it a bit easier when you try to run old code on a new machine, as you can just uncomment those lines.\n\n\n\n\n9.8.3.2 Installing packages in R\nPackage management in R is a bit simpler than package management in python.\nIn almost every case, you can install packages from CRAN with install.packages(\"package name\"). If your package is not on CRAN, and is instead on e.g.¬†GitHub, you may have to use the remotes package to install it with remotes::install_github(\"user/repo\")\n\n# CRAN packages\ninstall.packages(\"&lt;pkg1&gt;\")\n\n# Github packages\nremotes::install_github(\"username/reponame\")\n\n\n9.8.4 Loading Packages\nOnce you have the package installed, you need to load the package into memory so that you can use the functions and data contained within. Again, R and python differ slightly in how programmers conventionally handle this process.\n\nR: Load all of the package‚Äôs functions, overwriting already loaded functions if necessary\nPython: Load all of the package‚Äôs functions, contained within an object that is either the package name or a shortened alias.\n\nNow, both R and python can load packages in either way, so this isn‚Äôt an either/or thing - it‚Äôs about knowing what the conventions of the language are, and then deciding whether or not it is appropriate to follow those conventions in your project.\n\n9.8.4.1 Import the whole package and all functions\nTo demonstrate this approach, let‚Äôs create a simple plot with a plotting library (ggplot2 in R, plotnine in Python).\n\n\nR\nPython\n\n\n\nAll of the other packages in this plot are present by default in any new R environment.\n\nlibrary(ggplot2)\n\n# This code lists all the functions available to be called\npkgs &lt;- search()\npkgs &lt;- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns &lt;- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns &lt;- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg &lt;- gsub(\"package:\", \"\", pkg_fns$pkg)\n\n\nggplot(pkg_fns, aes(x = pkg, y = after_stat(count), fill = pkg)) + \n  geom_bar() + theme(legend.position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\n# I have no clue how to get all callable objects in python \n# classes and methods for those classes make this a lot harder... ugh\n\npkg_fns = r.pkg_fns # This is just the same data from R\n\n(\n  ggplot(pkg_fns, aes(x = \"pkg\", fill = \"pkg\")) + \n  geom_bar(aes(y = after_stat(\"count\"))) + \n  theme(legend_position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n9.8.4.2 Use functions from the package without loading everything\n\n\nR\nPython\n\n\n\n\n# This code lists all the functions available to be called\npkgs &lt;- search()\npkgs &lt;- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns &lt;- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns &lt;- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg &lt;- gsub(\"package:\", \"\", pkg_fns$pkg)\n\nggplot2::ggplot(pkg_fns, ggplot2::aes(x = pkg, fill = pkg)) + \n  ggplot2::geom_bar(y = ggplot2::after_stat(count)) + \n  ggplot2::theme(legend.position = \"none\") + \n  ggplot2::xlab(\"Package\") + ggplot2::ylab(\"# Functions\")\n## Error: object 'count' not found\n\n\n\n\nimport plotnine as p9\npkg_fns = r.pkg_fns\n\n(\n  p9.ggplot(pkg_fns, p9.aes(x = \"pkg\", fill = \"pkg\")) + \n  p9.geom_bar(y = p9.after_stat(\"count\")) + \n  p9.theme(legend_position = \"none\") + \n  p9.xlab(\"Package\") + p9.ylab(\"# Functions\")\n)\n## TypeError: ufunc 'isinf' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n\n\nIn python, you can use import package as nickname, or you can just use import package and reference the package name directly. There are some packages which have typical aliases, and it‚Äôs best to use those so that you can look things up and not get too confused.\n\nCommon Python package aliases\n\n\n\n\n\n\nPackage\nCommon Alias\nExplanation\n\n\n\npandas\npd\nshorter\n\n\nnumpy\nnp\nshorter\n\n\nseaborn\nsns\nThis is a reference to Samuel Norman Seaborn, played by Rob Lowe, in the TV show The West Wing\n\n\nplotnine\np9\n\n\n\nBeautifulSoup (bs4)\nbs\nBeautifulSoup is a reference to Alice in Wonderland. The package name in PyPi is actually bs4.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#pipes",
    "href": "part-gen-prog/02-prog-functions.html#pipes",
    "title": "9¬† Using Functions and Libraries",
    "section": "\n9.9 Pipes",
    "text": "9.9 Pipes\nPipes are useful items for moving things from one place to another. In programming, and in particular, in data programming, pipes are operators that let us move data around. In R, we have two primary pipes that are similar (you may see both used if you google for code online). Any R version after 4.1 has a built-in pipe, |&gt;; the tidyverse libraries use a pipe from the magrittr package, %&gt;%.\nFor right now, it‚Äôs ok to think of the two pipes as essentially the same (but you can read about the differences [4]).\nFundamentally, a pipe allows you to take a function b() and apply it to x, like b(x), but write it as x |&gt; b() or x %&gt;% b(). This is particularly useful in cases where there are multiple sequential analysis steps, because where in regular notation you have to read the functions from the inside out to understand the sequential steps, with pipes, you have a clear step-by-step list of the order of operations.\nIn Python, there is a pipe function in the Pandas library that works using .pipe(function) notation [5]. From what I‚Äôve seen reading code online, however, pipes are less commonly used in Python code than they are in R code. That‚Äôs ok - languages have different conventions, and it is usually best to adopt the convention of the language you‚Äôre working in so that your code can be read, run, and maintained by others more easily.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nGenerate 100 draws from a standard normal distribution and calculate the mean.\nIn R, simulate from a normal distribution with rnorm. In python, use np.random.normal - you‚Äôll have to import numpy as np first.\nUse 3 approaches: 1. Store the data in a variable, then calculate the mean of the variable 2. Calculate the mean of the data by nesting the two functions (e.g.¬†mean(generate_normal(100)) in pseudocode) 3. Calculate the mean of the data using the pipe (e.g.¬†generate_normal(100) |&gt; mean())\nConsider: What are the advantages and disadvantages of each approach? Would your answer change if there were more steps/functions required to get to the right answer?\n\n\n\ndata &lt;- rnorm(100)\nmean(data)\n## [1] 0.05510718\n\nmean(rnorm(100))\n## [1] 0.1315696\n\nlibrary(magrittr) # load the pipe %&gt;%\n\nrnorm(100) %&gt;%\n  mean()\n## [1] 0.07846986\n\nrnorm(100) |&gt; mean()\n## [1] 0.1644766\n\n\n\nIn python, task 3 isn‚Äôt really possible, because of the way Python function chaining works, but task 2 is basically the equivalent.\n\nimport numpy as np\nimport pandas as pd\n\nnums = pd.Series(np.random.normal(size = 100))\nnums.mean()\n## -0.15886105056147792\n\nnp.random.normal(size=100).mean()\n## 0.016334630288730757\n\nThe conclusion here is that it‚Äôs far easier to not use the pipe in python because the .function notation that python uses mimics the step-by-step approach of pipes in R even without using the actual pipe function. When you use data frames instead of Series, you might start using the pipe, but only in some circumstances - with user-defined functions, instead of methods. Methods are functions that are attached to a data type (technically, a class) and only work if they are defined for that class - for instance, .mean() is defined for both Pandas series and numpy arrays.\n\n\n\n\n\n\n\n\n\n\n[1] \nD. Blackwood, ‚ÄúHow to use python in r with reticulate and conda. Save the data,‚Äù Nov. 04, 2021. [Online]. Available: https://medium.com/save-the-data/how-to-use-python-in-r-with-reticulate-and-conda-36685534f06a. [Accessed: Jan. 23, 2023]\n\n\n[2] \nK. Ushey and H. Wickham, Renv: Project environments. 2023 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[3] \nG. Makarov, ‚ÄúUse python in rstudio. RPubs,‚Äù May 02, 2022. [Online]. Available: https://rpubs.com/georgy_makarov/897844. [Accessed: Jan. 23, 2023]\n\n\n[4] \nS. Machlis, ‚ÄúUse the new r pipe built into r 4.1. InfoWorld,‚Äù Jun. 10, 2021. [Online]. Available: https://www.infoworld.com/article/3621369/use-the-new-r-pipe-built-into-r-41.html. [Accessed: Jan. 13, 2023]\n\n\n[5] \nshadowtalker, ‚ÄúAnswer to \"functional pipes in python like %&gt;% from r‚Äôs magrittr\". Stack overflow,‚Äù Jun. 24, 2015. [Online]. Available: https://stackoverflow.com/a/31037901/2859168. [Accessed: Jan. 13, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#footnotes",
    "href": "part-gen-prog/02-prog-functions.html#footnotes",
    "title": "9¬† Using Functions and Libraries",
    "section": "",
    "text": "A fourth commonly used logical operator is exclusive or (xor). xor is True if only one of the two conditions is True, but False if both are True. xor is not a basic boolean operator, as it can be written as a combination of other operators: A xor B = (A or B) and not(A and B).‚Ü©Ô∏é\nFrom https://cran.r-project.org/doc/contrib/Short-refcard.pdf‚Ü©Ô∏é\nFrom http://sixthresearcher.com/wp-content/uploads/2016/12/Python3_reference_cheat_sheet.pdf‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using Functions and Libraries</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html",
    "href": "part-gen-prog/03-data-struct.html",
    "title": "10¬† Data Structures",
    "section": "",
    "text": "10.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#objectives",
    "href": "part-gen-prog/03-data-struct.html#objectives",
    "title": "10¬† Data Structures",
    "section": "",
    "text": "Understand the differences between lists, vectors, data frames, matrices, and arrays in R and python\nBe able to use location-based indexing in R or python to pull out subsets of a complex data object",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#python-package-installation",
    "href": "part-gen-prog/03-data-struct.html#python-package-installation",
    "title": "10¬† Data Structures",
    "section": "\n10.2 Python Package Installation",
    "text": "10.2 Python Package Installation\nYou will need the numpy and pandas packages for this section. Pick one of the following ways to install python packages:\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install numpy pandas lxml\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don‚Äôt try to install packages this way).\n\nreticulate::py_install(c(\"numpy\", \"pandas\", \"lxml\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called ‚ÄúIPython magic‚Äù commands, so if it doesn‚Äôt work for you, try the System Terminal method instead.\n\n%pip3 install numpy pandas lxml",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "href": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "title": "10¬† Data Structures",
    "section": "\n10.3 Data Structures Overview",
    "text": "10.3 Data Structures Overview\nIn Chapter 8, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complex arrangements of information, but they are still (usually) created using the same data types we have previously discussed.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThose of you who have taken programming classes that were more computer science focused will realize that I am leaving out a lot of information about lower-level structures like pointers. I‚Äôm making a deliberate choice to gloss over most of those details in this chapter, because it‚Äôs already hard enough to learn 2 languages worth of data structures at a time. In addition, R doesn‚Äôt have pointers No Pointers in R, [1], so leaving out this material in python streamlines teaching both two languages, at the cost of overly simplifying some python concepts. If you want to read more about the Python concepts I‚Äôm leaving out, check out [2].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#lists",
    "href": "part-gen-prog/03-data-struct.html#lists",
    "title": "10¬† Data Structures",
    "section": "\n10.4 Lists",
    "text": "10.4 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nR\nPython\n\n\n\n\nx &lt;- list(\"a\", 3, FALSE)\nx\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n## \n## [[3]]\n## [1] FALSE\n\n\n\n\nx = [\"a\", 3, False]\nx\n## ['a', 3, False]\n\n\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n10.4.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)1.\n\n\nR concept\nR code\nPython concept\nPython code\n\n\n\nIn R, we count from 1.\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\n\nx &lt;- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n## [[1]]\n## [1] \"a\"\nx[1:2] # This returns multiple elements in the list\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n\nx[[1]] # This returns the item\n## [1] \"a\"\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n## Error in x[[1:2]]: subscript out of bounds\n\nIn R, list indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\n\n\nIn Python, we count from 0.\n\n\nA python-indexed lego list, counting from 0 to 4\n\n\n\n\nx = [\"a\", 3, False]\n\nx[0]\n## 'a'\nx[1]\n## 3\nx[0:2]\n## ['a', 3]\n\nIn Python, we can use single brackets to get an object or a list back out, but we have to know how slices work. Essentially, in Python, 0:2 indicates that we want objects 0 and 1, but want to stop at 2 (not including 2). If you use a slice, Python will return a list; if you use a single index, python just returns the value in that location in the list.\n\n\n\nWe‚Äôll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n10.4.2 Concatenation\nAnother important thing to know about lists is how to combine them. If I have rosters for two classes and I want to make a list of all of my students, I need to somehow merge the two lists together.\n\n\nR\nPython\n\n\n\n\nclass1 &lt;- c(\"Benjamin Sisko\", \"Odo\", \"Julian Bashir\", \"Jadzia Dax\", \"Miles O'Brien\", \"Quark\", \"Kira Nerys\", \"Elim Garak\")\nclass2 &lt;- c(\"Jean-Luc Picard\", \"William Riker\", \"Geordi La Forge\", \"Worf\", \"Miles O'Brien\", \"Beverly Crusher\", \"Deanna Troi\", \"Data\")\n\nstudents &lt;- c(class1, class2)\nstudents\n##  [1] \"Benjamin Sisko\"  \"Odo\"             \"Julian Bashir\"   \"Jadzia Dax\"     \n##  [5] \"Miles O'Brien\"   \"Quark\"           \"Kira Nerys\"      \"Elim Garak\"     \n##  [9] \"Jean-Luc Picard\" \"William Riker\"   \"Geordi La Forge\" \"Worf\"           \n## [13] \"Miles O'Brien\"   \"Beverly Crusher\" \"Deanna Troi\"     \"Data\"\n\nunique(students) # get only unique names\n##  [1] \"Benjamin Sisko\"  \"Odo\"             \"Julian Bashir\"   \"Jadzia Dax\"     \n##  [5] \"Miles O'Brien\"   \"Quark\"           \"Kira Nerys\"      \"Elim Garak\"     \n##  [9] \"Jean-Luc Picard\" \"William Riker\"   \"Geordi La Forge\" \"Worf\"           \n## [13] \"Beverly Crusher\" \"Deanna Troi\"     \"Data\"\n\n\n\n\nclass1 = [\"Benjamin Sisko\", \"Odo\", \"Julian Bashir\", \"Jadzia Dax\", \"Miles O'Brien\", \"Quark\", \"Kira Nerys\", \"Elim Garak\"]\nclass2 = [\"Jean-Luc Picard\", \"William Riker\", \"Geordi La Forge\", \"Worf\", \"Miles O'Brien\", \"Beverly Crusher\", \"Deanna Troi\", \"Data\"]\n\nstudents = class1 + class2\nstudents\n## ['Benjamin Sisko', 'Odo', 'Julian Bashir', 'Jadzia Dax', \"Miles O'Brien\", 'Quark', 'Kira Nerys', 'Elim Garak', 'Jean-Luc Picard', 'William Riker', 'Geordi La Forge', 'Worf', \"Miles O'Brien\", 'Beverly Crusher', 'Deanna Troi', 'Data']\nlist(dict.fromkeys(students)) # get only unique names\n## ['Benjamin Sisko', 'Odo', 'Julian Bashir', 'Jadzia Dax', \"Miles O'Brien\", 'Quark', 'Kira Nerys', 'Elim Garak', 'Jean-Luc Picard', 'William Riker', 'Geordi La Forge', 'Worf', 'Beverly Crusher', 'Deanna Troi', 'Data']",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#vectors",
    "href": "part-gen-prog/03-data-struct.html#vectors",
    "title": "10¬† Data Structures",
    "section": "\n10.5 Vectors",
    "text": "10.5 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want:\n\n\nvectors of different data types\n\n\n10.5.1 Indexing by Location\nEach element in a vector has an index - an integer telling you what the item‚Äôs position within the vector is. I‚Äôm going to demonstrate indices with the string vector\n\n\n\n\n\n\nR\nPython\n\n\n\n1-indexed language\n0-indexed language\n\n\nCount elements as 1, 2, 3, 4, ‚Ä¶, N\nCount elements as 0, 1, 2, 3, , ‚Ä¶, N-1\n\n\n\n\n\n\n\n\n\nR\nPython Vectors\nPython Series (Pandas)\n\n\n\nIn R, we create vectors with the c() function, which stands for ‚Äúconcatenate‚Äù - basically, we stick a bunch of objects into a row.\n\ndigits_pi &lt;- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n## [1] 3\ndigits_pi[2]\n## [1] 1\ndigits_pi[3]\n## [1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n## numeric(0)\ndigits_pi[11]\n## [1] 5\n\n# Print out the vector\ndigits_pi\n##  [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\n\nIn python, we create vectors using the array function in the numpy module. To add a python module, we use the syntax import &lt;name&gt; as &lt;nickname&gt;. Many modules have conventional (and very short) nicknames - for numpy, we will use np as the nickname. Any functions we reference in the numpy module will then be called using np.fun_name() so that python knows where to find them.2\n\nimport numpy as np\ndigits_list = [3,1,4,1,5,9,2,6,5,3,5]\ndigits_pi = np.array(digits_list)\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n## 4\n\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\ndigits_pi[0]\n## 3\ndigits_pi[11] \n## IndexError: index 11 is out of bounds for axis 0 with size 11\n\n# multiplication works on the whole vector at once\ndigits_pi * 2\n## array([ 6,  2,  8,  2, 10, 18,  4, 12, 10,  6, 10])\n\n# Print out the vector\nprint(digits_pi)\n## [3 1 4 1 5 9 2 6 5 3 5]\n\n\n\nPython has multiple things that look like vectors, including the pandas library‚Äôs Series structure. A Series is a one-dimensional array-like object containing a sequence of values and an associated array of labels (called its index).\n\nimport pandas as pd\ndigits_pi = pd.Series([3,1,4,1,5,9,2,6,5,3,5])\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n## 4\n\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\ndigits_pi[0]\n## 3\ndigits_pi[11] \n## KeyError: 11\n\n# logical indexing works here too\ndigits_pi[digits_pi &gt; 3]\n## 2     4\n## 4     5\n## 5     9\n## 7     6\n## 8     5\n## 10    5\n## dtype: int64\n# simple multiplication works in a vectorized manner\n# that is, the whole vector is multiplied at once\ndigits_pi * 2\n## 0      6\n## 1      2\n## 2      8\n## 3      2\n## 4     10\n## 5     18\n## 6      4\n## 7     12\n## 8     10\n## 9      6\n## 10    10\n## dtype: int64\n\n# Print out the series\nprint(digits_pi)\n## 0     3\n## 1     1\n## 2     4\n## 3     1\n## 4     5\n## 5     9\n## 6     2\n## 7     6\n## 8     5\n## 9     3\n## 10    5\n## dtype: int64\n\nThe Series object has a list of labels in the first printed column, and a list of values in the second. If we want, we can specify the labels manually to use as e.g.¬†plot labels later:\n\nimport pandas as pd\nweekdays = pd.Series(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], index = ['S', 'M', 'T', 'W', 'R', 'F', 'Sat'])\n\n\n# access individual objs\nweekdays[0]\n## 'Sunday'\nweekdays[1]\n## 'Monday'\nweekdays['S']\n## 'Sunday'\nweekdays['Sat']\n## 'Saturday'\n\n# access the index\nweekdays.index\n## Index(['S', 'M', 'T', 'W', 'R', 'F', 'Sat'], dtype='object')\nweekdays.index[6] = 'Z' # you can't assign things to the index to change it\n## TypeError: Index does not support mutable operations\n\nweekdays\n## S         Sunday\n## M         Monday\n## T        Tuesday\n## W      Wednesday\n## R       Thursday\n## F         Friday\n## Sat     Saturday\n## dtype: object\n\n\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\n\n\nR\nPython\n\n\n\n\nfavorite_cats &lt;- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"\n\nfavorite_cats[2] &lt;- \"Nyan Cat\"\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"\n\n\n\n\nfavorite_cats = [\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"]\n\nfavorite_cats\n## ['Grumpy', 'Garfield', 'Jorts', 'Jean']\n\nfavorite_cats[1] = \"Nyan Cat\"\n\nfavorite_cats\n## ['Grumpy', 'Nyan Cat', 'Jorts', 'Jean']\n\n\n\n\nIf you‚Äôre curious about any of these cats, see the footnotes3.\n\n\n10.5.2 Indexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\nIf we let the black lego represent ‚ÄúTrue‚Äù and the grey lego represent ‚ÄúFalse‚Äù, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\nBlack == True, Grey == False\nGrey == True, Black == False\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we‚Äôre indexing. This constraint will return when we talk about data frames, but for now, just keep in mind that logical indexing doesn‚Äôt make sense when this constraint isn‚Äôt true.\n\n\nR\nPython\n\n\n\n\n# Define a character vector\nweekdays &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend &lt;- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days &lt;- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days &lt;- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n## [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days &lt;- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n## [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"\n\n\n\n\nimport numpy as np;\n\nanimals = np.array([\"Cat\", \"Dog\", \"Snake\", \"Lizard\", \"Tarantula\", \"Hamster\", \"Gerbil\", \"Otter\"])\n\n# Define a logical vector\ngood_pets = np.array([True, True, False, False, False, True, True, False])\nbad_pets = np.invert(good_pets) # Invert the logical vector \n                                # so True -&gt; False and False -&gt; True\n\nanimals[good_pets]\n## array(['Cat', 'Dog', 'Hamster', 'Gerbil'], dtype='&lt;U9')\nanimals[bad_pets]\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='&lt;U9')\n\nanimals[~good_pets] # equivalent to using bad_pets\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='&lt;U9')\n\n\n\n\n\n10.5.3 Math with Vectors\nIn order to talk about mathematical operations on (numerical) vectors, we first need to consider different ways we could combine vectors. If the vectors are the same length, we could perform mathematical operations on the elements (and if they‚Äôre not the same length we could come up with some convention to coerce them to be the same length).\n\\[\\begin{align}\n\\left[\\begin{array}{c}a_1\\\\a_2\\\\a_3\\end{array}\\right] + \\left[\\begin{array}{c}b_1\\\\b_2\\\\b_3\\end{array}\\right] = \\left[\\begin{array}{c}a_1+b_1\\\\a_2 + b_2\\\\a_3 + b_3\\end{array}\\right]\n\\end{align}\\]\n: Element-wise sum of two vectors\nWe could also think about performing mathematical operations on all of the entries in a vector - e.g.¬†taking the sum of all vector entries, or the mean, or the standard deviation. This reduces a vector of numbers to a single (scalar) number via a defined function. We can use standard built-in functions, or we could define our own (see Chapter 13).\nAnother option is to think of vectors as a way to specify items, and define mathematical ways to combine different sets of items. These vector operations are more akin to set operations, where the product of two vectors is the product set of all combinations of an item from vector 1 and an item from vector 2. Technically, these operations are typically also defined for lists, but they may make more sense in practice when the vector same-type constraint is imposed.\nWe can finally combine vectors using linear algebra; if you are interested in these definitions, see Chapter 11.\n\n10.5.3.1 Element-wise Operations\nWhen using numeric vectors, the element-wise operations are the same for vectors and scalars (Table¬†10.1 is the same exact table as Table¬†9.1 in Chapter 9).\n\n\nTable¬†10.1: Element-wise mathematical operators in R and Python\n\n\n\nOperation\nR symbol\nPython symbol\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\na &lt;- c(1:5)\nb &lt;- c(6:10)\n\na + b\n## [1]  7  9 11 13 15\nb - a\n## [1] 5 5 5 5 5\na * b\n## [1]  6 14 24 36 50\nb / a\n## [1] 6.000000 3.500000 2.666667 2.250000 2.000000\nb %/% a\n## [1] 6 3 2 2 2\nb %% a\n## [1] 0 1 2 1 0\na ^ b\n## [1]       1     128    6561  262144 9765625\n\n\n\n\nimport numpy as np\n\na = np.array([1, 2, 3, 4, 5])\nb = np.array([6, 7, 8, 9, 10])\n\na + b\n## array([ 7,  9, 11, 13, 15])\nb - a\n## array([5, 5, 5, 5, 5])\na * b\n## array([ 6, 14, 24, 36, 50])\nb / a\n## array([6.        , 3.5       , 2.66666667, 2.25      , 2.        ])\nb // a\n## array([6, 3, 2, 2, 2])\nb % a\n## array([0, 1, 2, 1, 0])\na ** b\n## array([      1,     128,    6561,  262144, 9765625])\n\n\n\n\n\n10.5.3.2 Vector-to-Scalar Operations\nLet‚Äôs cover a few built-in or commonly-used vector summary operations here, focusing on those which are most useful for statistics.\n\n\nFunction\nR\nPython\n\n\n\nlength\nlength(x)\nlen(x)\n\n\nmean\nmean(x)\nx.average()\n\n\nvariance\nvar(x)\nx.var()\n\n\nstandard deviation\nsd(x)\nx.std()\n\n\nmaximum\nmax(x)\nx.max()\n\n\nlocation of maximum\nwhich.max(x)\nx.argmax()\n\n\nminimum\nmin(x)\nx.min()\n\n\nlocation of minimum\nwhich.min(x)\nx.argmin()\n\n\n\n\n\nR\nPython\n\n\n\n\nset.seed(30420983)\nx &lt;- sample(1:100, size = 10)\nx\n##  [1] 43 91 25 40 60 39 18 53 58 99\nlength(x)\n## [1] 10\nmean(x)\n## [1] 52.6\nvar(x)\n## [1] 678.4889\nsd(x)\n## [1] 26.04782\nmax(x)\n## [1] 99\nmin(x)\n## [1] 18\nwhich.max(x)\n## [1] 10\nwhich.min(x)\n## [1] 7\n\n# 5 number summary + mean\nsummary(x)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   18.00   39.25   48.00   52.60   59.50   99.00\n\n\n\n\nimport numpy as np\nimport random # need for setting the seed\n\nrandom.seed(30420983)\nx = random.sample(range(1, 101), 10)\nx = np.array(x)\nx\n## array([28, 61, 65, 35, 90, 26, 19, 12, 38,  9])\nlen(x)\n## 10\nx.mean()\n## 38.3\nx.var()\n## 609.21\nx.std()\n## 24.68217980649197\nx.max()\n## 90\nx.min()\n## 9\nx.argmax()\n## 4\nx.argmin()\n## 9\n\nIf we want to use Pandas, we can get a summary of our vector using .describe(), but this requires converting our numpy array to a Pandas series.\n\nimport pandas as pd\nx = pd.Series(x)\nx\n## 0    28\n## 1    61\n## 2    65\n## 3    35\n## 4    90\n## 5    26\n## 6    19\n## 7    12\n## 8    38\n## 9     9\n## dtype: int64\nx.describe()\n## count    10.000000\n## mean     38.300000\n## std      26.017302\n## min       9.000000\n## 25%      20.750000\n## 50%      31.500000\n## 75%      55.250000\n## max      90.000000\n## dtype: float64\n\n\n\n\n\n10.5.3.3 Set Operations\nThere are 3 basic set operations: union, intersection, and set difference, as well as the Cartesian product. The Cartesian product creates a set of points in vector space (in math terms) or tuples (in python terms), which is usually represented as something other than a vector (in R, a data frame).\nGenerally speaking, all of these operations would be valid on lists as well as vectors in R, though in python, these functions are part of numpy and require 1-dimensional arrays. There are certainly ways to implement set operations in base python, but they typically do not have convenient named functions - you‚Äôd need to write your own or import another library.\n\n\n\n\n\n\n\n\n\nOperation\nDefinition\nSymbol (Math)\nR function\nPython function\n\n\n\nUnion\nAll elements in A or B\n\\(A \\cup B\\)\nunion(A, B)\nnp.union1d(A, B)\n\n\nIntersection\nElements in both A and B\n\\(A \\cap B\\)\nintersect(A, B)\nnp.intersect1d(A, B)\n\n\nSet Difference\nElements in A but not B\n\\(A \\setminus B\\)\nsetdiff(A, B)\nnp.setdiff1d(A, B)\n\n\nCartesian Product\nCombination of each element in A with each element in B\n\\(expand.grid(A, B)\\)\n‚Ä¶ it‚Äôs complicated, see code.\n\n\n\n\n\n\nR\nPython\n\n\n\n\nA = c(1:10)\nB = c(1:5) * 2 # evens\n\nunion(A, B)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nintersect(A, B)\n## [1]  2  4  6  8 10\nsetdiff(A, B)\n## [1] 1 3 5 7 9\nsetdiff(B, A)\n## numeric(0)\nexpand.grid(A, B) # this is a data frame\n##    Var1 Var2\n## 1     1    2\n## 2     2    2\n## 3     3    2\n## 4     4    2\n## 5     5    2\n## 6     6    2\n## 7     7    2\n## 8     8    2\n## 9     9    2\n## 10   10    2\n## 11    1    4\n## 12    2    4\n## 13    3    4\n## 14    4    4\n## 15    5    4\n## 16    6    4\n## 17    7    4\n## 18    8    4\n## 19    9    4\n## 20   10    4\n## 21    1    6\n## 22    2    6\n## 23    3    6\n## 24    4    6\n## 25    5    6\n## 26    6    6\n## 27    7    6\n## 28    8    6\n## 29    9    6\n## 30   10    6\n## 31    1    8\n## 32    2    8\n## 33    3    8\n## 34    4    8\n## 35    5    8\n## 36    6    8\n## 37    7    8\n## 38    8    8\n## 39    9    8\n## 40   10    8\n## 41    1   10\n## 42    2   10\n## 43    3   10\n## 44    4   10\n## 45    5   10\n## 46    6   10\n## 47    7   10\n## 48    8   10\n## 49    9   10\n## 50   10   10\n\n\n\n\nimport numpy as np\n\nA = np.array(range(1, 11))\nB = np.array(range(1, 6)) * 2\n\nnp.union1d(A, B)\n## array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\nnp.intersect1d(A, B)\n## array([ 2,  4,  6,  8, 10])\nnp.setdiff1d(A, B)\n## array([1, 3, 5, 7, 9])\nnp.setdiff1d(B, A)\n## array([], dtype=int64)\n\nTo combine numpy arrays with a Cartesian product in base Python, we have to use something called list comprehension. I‚Äôll leave this example here, but you don‚Äôt need to fully understand how it works yet, as it is using for loops in a python-centric way.\n\n[[a0, b0] for a0 in A for b0 in B] # sets of coordinates\n## [[1, 2], [1, 4], [1, 6], [1, 8], [1, 10], [2, 2], [2, 4], [2, 6], [2, 8], [2, 10], [3, 2], [3, 4], [3, 6], [3, 8], [3, 10], [4, 2], [4, 4], [4, 6], [4, 8], [4, 10], [5, 2], [5, 4], [5, 6], [5, 8], [5, 10], [6, 2], [6, 4], [6, 6], [6, 8], [6, 10], [7, 2], [7, 4], [7, 6], [7, 8], [7, 10], [8, 2], [8, 4], [8, 6], [8, 8], [8, 10], [9, 2], [9, 4], [9, 6], [9, 8], [9, 10], [10, 2], [10, 4], [10, 6], [10, 8], [10, 10]]\n[[(a0, b0) for a0 in A] for b0 in B] # vectors of coordinates\n## [[(1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2)], [(1, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4), (7, 4), (8, 4), (9, 4), (10, 4)], [(1, 6), (2, 6), (3, 6), (4, 6), (5, 6), (6, 6), (7, 6), (8, 6), (9, 6), (10, 6)], [(1, 8), (2, 8), (3, 8), (4, 8), (5, 8), (6, 8), (7, 8), (8, 8), (9, 8), (10, 8)], [(1, 10), (2, 10), (3, 10), (4, 10), (5, 10), (6, 10), (7, 10), (8, 10), (9, 10), (10, 10)]]\n\nWe can also use a library that implements the Cartesian product explicitly, and then convert the result to a data type we would prefer to work with, like an array of coordinates, or a data frame.\n\n# Using a library\nimport itertools\nnp.array(list(itertools.product(A, B))) # array of 2d coords\n## array([[ 1,  2],\n##        [ 1,  4],\n##        [ 1,  6],\n##        [ 1,  8],\n##        [ 1, 10],\n##        [ 2,  2],\n##        [ 2,  4],\n##        [ 2,  6],\n##        [ 2,  8],\n##        [ 2, 10],\n##        [ 3,  2],\n##        [ 3,  4],\n##        [ 3,  6],\n##        [ 3,  8],\n##        [ 3, 10],\n##        [ 4,  2],\n##        [ 4,  4],\n##        [ 4,  6],\n##        [ 4,  8],\n##        [ 4, 10],\n##        [ 5,  2],\n##        [ 5,  4],\n##        [ 5,  6],\n##        [ 5,  8],\n##        [ 5, 10],\n##        [ 6,  2],\n##        [ 6,  4],\n##        [ 6,  6],\n##        [ 6,  8],\n##        [ 6, 10],\n##        [ 7,  2],\n##        [ 7,  4],\n##        [ 7,  6],\n##        [ 7,  8],\n##        [ 7, 10],\n##        [ 8,  2],\n##        [ 8,  4],\n##        [ 8,  6],\n##        [ 8,  8],\n##        [ 8, 10],\n##        [ 9,  2],\n##        [ 9,  4],\n##        [ 9,  6],\n##        [ 9,  8],\n##        [ 9, 10],\n##        [10,  2],\n##        [10,  4],\n##        [10,  6],\n##        [10,  8],\n##        [10, 10]])\npd.DataFrame(list(itertools.product(A, B))) # data frame\n##      0   1\n## 0    1   2\n## 1    1   4\n## 2    1   6\n## 3    1   8\n## 4    1  10\n## 5    2   2\n## 6    2   4\n## 7    2   6\n## 8    2   8\n## 9    2  10\n## 10   3   2\n## 11   3   4\n## 12   3   6\n## 13   3   8\n## 14   3  10\n## 15   4   2\n## 16   4   4\n## 17   4   6\n## 18   4   8\n## 19   4  10\n## 20   5   2\n## 21   5   4\n## 22   5   6\n## 23   5   8\n## 24   5  10\n## 25   6   2\n## 26   6   4\n## 27   6   6\n## 28   6   8\n## 29   6  10\n## 30   7   2\n## 31   7   4\n## 32   7   6\n## 33   7   8\n## 34   7  10\n## 35   8   2\n## 36   8   4\n## 37   8   6\n## 38   8   8\n## 39   8  10\n## 40   9   2\n## 41   9   4\n## 42   9   6\n## 43   9   8\n## 44   9  10\n## 45  10   2\n## 46  10   4\n## 47  10   6\n## 48  10   8\n## 49  10  10\n\n\n\n\n\n10.5.4 Logical Operations on Vectors\nXXX TODO XXX\n\n10.5.5 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently typed things?\n\n\nR\nPython\n\n\n\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n## [1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n## [1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n## [1] 2 0\n\n\n\n\nimport numpy as np\n\nnp.array([2, False, 3.1415, \"animal\"]) # all converted to strings\n## array(['2', 'False', '3.1415', 'animal'], dtype='&lt;U32')\n\nnp.array([2, False, 3.1415]) # converted to floats\n## array([2.    , 0.    , 3.1415])\n\nnp.array([2, False]) # converted to integers\n## array([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - R and python decide what type to use for you, going with the type that doesn‚Äôt lose data but takes up as little space as possible.\nTry it Out!\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nCreate a vector of the integers from one to 30. Use logical indexing to pick out only the numbers that are multiples of 3.\n\n\n\nx &lt;- 1:30\nx [ x %% 3 == 0]\n##  [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31)) # because python is 0 indexed\nx[ x % 3 == 0]\n## array([ 3,  6,  9, 12, 15, 18, 21, 24, 27, 30])\n\n\n\n\n\n\nChallenge\nGeneral Solution\nR Solution\nPython Solution\n\n\n\nExtra challenge: Pick out numbers which are multiples of 2 or 3, but not multiples of 6!\n\n\nThis operation is xor, a.k.a. exclusive or. That is, X or Y, but not X AND Y.\nWe can write xor as (X OR Y) & !(X AND Y) ‚Äì or we can use a predefined function: xor() in R, ^ in python.\n\n\n\nx &lt;- 1:30\n\nx2 &lt;- x %% 2 == 0 # multiples of 2\nx3 &lt;- x %% 3 == 0 # multiples of 3\nx2xor3 &lt;- xor(x2, x3)\nx2xor3_2 &lt;- (x2 | x3) & !(x2 & x3)\nx[x2xor3]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\nx[x2xor3_2]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31))\n\nx2 = x % 2 == 0 # multiples of 2\nx3 = x % 3 == 0 # multiples of 3\nx2xor3 = x2 ^ x3\n\nx[x2xor3]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#matrices",
    "href": "part-gen-prog/03-data-struct.html#matrices",
    "title": "10¬† Data Structures",
    "section": "\n10.6 Matrices",
    "text": "10.6 Matrices\nA matrix is the next step after a vector - it‚Äôs a set of values arranged in a two-dimensional, rectangular format.\n\n\nMatrix (Lego)\nR\nPython\n\n\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    0    0    0\n## [2,]    0    1    0    0\n## [3,]    0    0    1    0\n## [4,]    0    0    0    1\n\n\n\nIn python, matrices are just a special case of a class called ndarray - n-dimensional arrays.\n\nimport numpy as np\n# Minimal ndarray in python by typing in the values in a structured format\nnp.array([[0,  1,  2],\n          [3,  4,  5],\n          [6,  7,  8],\n          [9, 10, 11]])\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\n# This syntax creates a list of the rows we want in our matrix\n\n# Matrix in python using a data vector and size parameters\nnp.reshape(range(0,12), (3,4))\n## array([[ 0,  1,  2,  3],\n##        [ 4,  5,  6,  7],\n##        [ 8,  9, 10, 11]])\nnp.reshape(range(0,12), (4,3))\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4), order = 'F')\n## array([[ 0,  3,  6,  9],\n##        [ 1,  4,  7, 10],\n##        [ 2,  5,  8, 11]])\n\nIn python, we create 2-dimensional arrays (aka matrices) either by creating a list of rows to join together or by reshaping a 1-dimensional array. The trick with reshaping the 1-dimensional array is the order argument: ‚ÄòF‚Äô stands for ‚ÄúFortran-like‚Äù and ‚ÄòC‚Äô stands for ‚ÄúC-like‚Äù‚Ä¶ so to go by column, you use ‚ÄòF‚Äô and to go by row, you use ‚ÄòC‚Äô. Totally intuitive, right?\n\n\n\nMost of the problems we‚Äôre going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we‚Äôll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\nFor more on matrix operations and matrix calculations, see Chapter 11.\n\n10.6.1 Indexing in Matrices\nBoth R and python use [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix in R, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\nR\nPython\n\n\n\n\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] &lt;- 500\n\nmy_mat\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]  500   10   11   12\n\n\n\nRemember that zero-indexing!\n\nimport numpy as np\n\nmy_mat = np.reshape(range(1, 13), (3,4))\n\nmy_mat[2,0] = 500\n\nmy_mat\n## array([[  1,   2,   3,   4],\n##        [  5,   6,   7,   8],\n##        [500,  10,  11,  12]])\n\n\n\n\n\n10.6.2 Matrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\n\ntranspose - flip the matrix across the left top -&gt; right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\n\nmatrix multiplication (dot product) - If you haven‚Äôt had this in Linear Algebra, here‚Äôs a preview. See [3] for a better explanation \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\n\nR\nPython\n\n\n\n\nx &lt;- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny &lt;- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n3 * x\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n\n# Transpose\nt(x)\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nt(y)\n##      [,1] [,2]\n## [1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n##      [,1]\n## [1,]   17\n## [2,]   39\n\n\n\n\nimport numpy as np\nx = np.array([[1,2],[3,4]])\ny = np.array([[5],[6]])\n\n# scalar multiplication\nx*3\n## array([[ 3,  6],\n##        [ 9, 12]])\n3*x\n## array([[ 3,  6],\n##        [ 9, 12]])\n\n# transpose\nx.T # shorthand\n## array([[1, 3],\n##        [2, 4]])\nx.transpose() # Long form\n## array([[1, 3],\n##        [2, 4]])\n\n# Matrix multiplication (dot product)\nnp.dot(x, y)\n## array([[17],\n##        [39]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#arrays",
    "href": "part-gen-prog/03-data-struct.html#arrays",
    "title": "10¬† Data Structures",
    "section": "\n10.7 Arrays",
    "text": "10.7 Arrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don‚Äôt think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\n\nR\nPython\n\n\n\n\narray(1:8, dim = c(2,2,2))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    5    7\n## [2,]    6    8\n\nNote that displaying this requires 2 slices, since it‚Äôs hard to display 3D information in a 2D terminal arrangement.\n\n\n\nimport numpy as np\n\nnp.array([[[1,2],[3,4]],[[5,6], [7,8]]])\n## array([[[1, 2],\n##         [3, 4]],\n## \n##        [[5, 6],\n##         [7, 8]]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-frames",
    "href": "part-gen-prog/03-data-struct.html#data-frames",
    "title": "10¬† Data Structures",
    "section": "\n10.8 Data Frames",
    "text": "10.8 Data Frames\nIn the previous sections, we talked about homogeneous structures: arrangements of data, like vectors and matrices, where every entry in the larger structure has the same type. In the rest of this chapter, we‚Äôll be talking about the root of most data science analysis projects: the data frame.\nLike an excel spreadsheet, data frames are arrangements of data in columns and rows.\nThis format has two main restrictions:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\n\n\nA lego data frame of 4 columns and 12 rows. Each column is a separate color hue (data type), with slight variations in the hue of each individual bricks.\n\nThe picture above shows a data frame of 4 columns, each with a different data type (brick size/hue). The data frame has 12 rows. This picture may look similar to one that we used to show logical indexing in the last chapter, and that is not a coincidence. You can get everything from a data frame that you would get from a collection of 4 separate vectors‚Ä¶ but there are advantages to keeping things in a data frame instead.\n\n\n\n\n\n\nMotivating Data Frames: Working with Multiple Vectors\n\n\n\nConsider for a moment https://worldpopulationreview.com/states, which lists the population of each state. You can find this dataset in CSV form here.\nIn the previous sections, we learned how to make different vectors in R, numpy, and pandas. Let‚Äôs see what happens when we work with the data above as a set of vectors/Series compared to what happens when we work with data frames.\n\n\nPython\nR\n\n\n\n(I‚Äôm going to cheat and read this in using pandas functions we haven‚Äôt learned yet to demonstrate why this stuff matters.)\n\nimport pandas as pd\n\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## ImportError: Missing optional dependency 'lxml'.  Use pip or conda to install lxml.\nlist(data.columns) # get names\n## NameError: name 'data' is not defined\n\n# Create a few population series\npopulation2024 = pd.Series(data['2024 Population'].values, index = data['State'].values)\n## NameError: name 'data' is not defined\npopulation2023 = pd.Series(data['2023 Population'].values, index = data['State'].values)\n## NameError: name 'data' is not defined\npopulation2020 = pd.Series(data['2020 Population'].values, index = data['State'].values)\n## NameError: name 'data' is not defined\n\nSuppose that we want to sort each population vector by the population in that year.\n\nimport pandas as pd\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## ImportError: Missing optional dependency 'lxml'.  Use pip or conda to install lxml.\n\npopulation2024 = pd.Series(data['2024 Population'].values, index = data['State'].values).sort_values()\n## NameError: name 'data' is not defined\npopulation2023 = pd.Series(data['2023 Population'].values, index = data['State'].values).sort_values()\n## NameError: name 'data' is not defined\npopulationCensus = pd.Series(data['2020 Population'].values, index = data['State'].values).sort_values()\n## NameError: name 'data' is not defined\n\npopulation2024.head()\n## NameError: name 'population2024' is not defined\npopulation2023.head()\n## NameError: name 'population2023' is not defined\npopulationCensus.head()\n## NameError: name 'populationCensus' is not defined\n\nThe only problem is that by doing this, we‚Äôve now lost the ordering that matched across all 3 vectors. Pandas Series are great for this, because they use labels that allow us to reconstitute which value corresponds to which label, but in R or even in numpy arrays, vectors don‚Äôt inherently come with labels. In these situations, sorting by one value can actually destroy the connection between two vectors!\n\n\n\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/population2024.csv\")\n\n# Use vectors instead of the data frame\nstate &lt;- df$State\npop2024 &lt;- df$X2024.Population\npop2023 &lt;- df$X2023.Population\npop2020 &lt;- df$X2020.Population\n\n# Create a vector to index population in 2022 in order\norder2024 &lt;- order(pop2024)\n\n# To keep variables together, we have to do things like this:\nhead(state[order2024])\n## [1] \"Wyoming\"      \"Vermont\"      \"Alaska\"       \"North Dakota\" \"South Dakota\"\n## [6] \"Delaware\"\nhead(pop2024[order2024])\n## [1]  586485  647818  733536  788940  928767 1044321\n\n# It makes more sense just to reorder the whole data frame:\nhead(df[order2024,])\n##     X Rank        State X2024.Population Growth.Rate X2023.Population\n## 50 49   50      Wyoming           586485       0.42%           584057\n## 49 48   49      Vermont           647818       0.06%           647464\n## 48 47   48       Alaska           733536       0.02%           733406\n## 47 46   47 North Dakota           788940       0.64%           783926\n## 46 45   46 South Dakota           928767       1.03%           919318\n## 45 44   45     Delaware          1044321       1.21%          1031890\n##    X2020.Population Growth.Since.2020 X..of.US Density...mi..\n## 50           577664             1.53%    0.17%              6\n## 49           642936             0.76%    0.19%             70\n## 48           732964             0.08%    0.22%              1\n## 47           779563              1.2%    0.23%             11\n## 46           887852             4.61%    0.28%             12\n## 45           991862             5.29%    0.31%            536\n\n\n\n\n\n\nThe primary advantage to data frames is that rows of data are kept together. Since we often think of a row of data as a single observation in a sample, this is an extremely important feature that makes data frames a huge improvement on a collection of vectors of the same length: it‚Äôs much harder for observations in a single row to get shuffled around and mismatched!\n\n10.8.1 Data Frame Basics\nIn R, data frames are built in as type data.frame, though there are packages that provide other implementations of data frames that have additional features, such as the tibble package used in many other common packages. We will cover functions from both base R and the tibble package in this chapter.\nIn Python, we will use the pandas library, which is conventionally abbreviated pd. So before you use any data frames in python, you will need to add the following line to your code: import pandas as pd.\n\n10.8.1.1 Examining Data Frames\n\n\nR\nPython\n\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head(n) command shows the first \\(n\\) rows of a data frame (enough to see what‚Äôs there, not enough to overflow your screen).\n\ndata(mtcars) # Load the data -- included in base R\nhead(mtcars) # Look at the first 6 rows\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nstr(mtcars) # Examine the structure of the object\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nYou can change column values or add new columns easily using assignment. The summary() function can be used on specific columns to perform summary operations (a 5-number summary useful for making e.g.¬†boxplots is provided by default).\n\nmtcars$gpm &lt;- 1/mtcars$mpg # gpm is sometimes used to assess efficiency\n\nsummary(mtcars$gpm)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615\nsummary(mtcars$mpg)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\n\nOften, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary().\n\nsummary(mtcars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb            gpm         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02950  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04386  \n##  Median :0.0000   Median :4.000   Median :2.000   Median :0.05208  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.05423  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.06483  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.09615\ndim(mtcars)\n## [1] 32 12\nnrow(mtcars)\n## [1] 32\nncol(mtcars)\n## [1] 12\n\nMissing variables in an R data frame are indicated with NA.\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The df.head(n) command shows the first \\(n\\) rows of a data frame (enough to see what‚Äôs there, not enough to overflow your screen).\n\nmtcars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmtcars.head(5)\n##             rownames   mpg  cyl   disp   hp  ...   qsec  vs  am  gear  carb\n## 0          Mazda RX4  21.0    6  160.0  110  ...  16.46   0   1     4     4\n## 1      Mazda RX4 Wag  21.0    6  160.0  110  ...  17.02   0   1     4     4\n## 2         Datsun 710  22.8    4  108.0   93  ...  18.61   1   1     4     1\n## 3     Hornet 4 Drive  21.4    6  258.0  110  ...  19.44   1   0     3     1\n## 4  Hornet Sportabout  18.7    8  360.0  175  ...  17.02   0   0     3     2\n## \n## [5 rows x 12 columns]\n\nmtcars.info()\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 32 entries, 0 to 31\n## Data columns (total 12 columns):\n##  #   Column    Non-Null Count  Dtype  \n## ---  ------    --------------  -----  \n##  0   rownames  32 non-null     object \n##  1   mpg       32 non-null     float64\n##  2   cyl       32 non-null     int64  \n##  3   disp      32 non-null     float64\n##  4   hp        32 non-null     int64  \n##  5   drat      32 non-null     float64\n##  6   wt        32 non-null     float64\n##  7   qsec      32 non-null     float64\n##  8   vs        32 non-null     int64  \n##  9   am        32 non-null     int64  \n##  10  gear      32 non-null     int64  \n##  11  carb      32 non-null     int64  \n## dtypes: float64(5), int64(6), object(1)\n## memory usage: 3.1+ KB\n\nYou can change column values or add new columns easily using assignment. It‚Äôs also easy to access specific columns to perform summary operations. You can access a column named xyz using df.xyz or using df[\"xyz\"]. To create a new column, you must use df[\"xyz\"].\n\nmtcars[\"gpm\"] = 1/mtcars.mpg # gpm is sometimes used to assess efficiency\n\nmtcars.gpm.describe()\n## count    32.000000\n## mean      0.054227\n## std       0.016424\n## min       0.029499\n## 25%       0.043860\n## 50%       0.052083\n## 75%       0.064834\n## max       0.096154\n## Name: gpm, dtype: float64\nmtcars.mpg.describe()\n## count    32.000000\n## mean     20.090625\n## std       6.026948\n## min      10.400000\n## 25%      15.425000\n## 50%      19.200000\n## 75%      22.800000\n## max      33.900000\n## Name: mpg, dtype: float64\n\nOften, it is useful to know the dimensions of a data frame. The dimensions of a data frame (rows x columns) can be accessed using df.shape. There is also an easy way to get a summary of each column in the data frame, using df.describe().\n\nmtcars.describe()\n##              mpg        cyl        disp  ...       gear     carb        gpm\n## count  32.000000  32.000000   32.000000  ...  32.000000  32.0000  32.000000\n## mean   20.090625   6.187500  230.721875  ...   3.687500   2.8125   0.054227\n## std     6.026948   1.785922  123.938694  ...   0.737804   1.6152   0.016424\n## min    10.400000   4.000000   71.100000  ...   3.000000   1.0000   0.029499\n## 25%    15.425000   4.000000  120.825000  ...   3.000000   2.0000   0.043860\n## 50%    19.200000   6.000000  196.300000  ...   4.000000   2.0000   0.052083\n## 75%    22.800000   8.000000  326.000000  ...   4.000000   4.0000   0.064834\n## max    33.900000   8.000000  472.000000  ...   5.000000   8.0000   0.096154\n## \n## [8 rows x 12 columns]\nmtcars.shape\n## (32, 13)\n\nMissing variables in a pandas data frame are indicated with nan or NULL.\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nSetup\nProblem\nR Solution\nPython Solution\n\n\n\nThe dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below.\n\ndata(state)\nstate_facts &lt;- data.frame(state.x77)\nstate_facts &lt;- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) \n# State names were stored as row labels\n# Store them in a variable instead, and add it to the data frame\n\nrow.names(state_facts) &lt;- NULL # get rid of row names\n\nhead(state_facts)\n##        state Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## 1    Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## 2     Alaska        365   6315        1.5    69.31   11.3    66.7   152 566432\n## 3    Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n## 4   Arkansas       2110   3378        1.9    70.66   10.1    39.9    65  51945\n## 5 California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## 6   Colorado       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n# Write data out so that we can read it in using Python\nwrite.csv(state_facts, file = \"../data/state_facts.csv\", row.names = F)\n\nWe can write out the built in R data and read it in using pd.read_csv, which creates a DataFrame in pandas.\n\nimport pandas as pd\n\nstate_facts = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/state_facts.csv\")\n\n\n\n\nHow many rows and columns does it have? Can you find different ways to get that information?\nThe Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands.\nCalculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density?\n\n\n\n\n# 3 ways to get rows and columns\nstr(state_facts)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Population: num  3615 365 2212 2110 21198 ...\n##  $ Income    : num  3624 6315 4530 3378 5114 ...\n##  $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n##  $ Life.Exp  : num  69 69.3 70.5 70.7 71.7 ...\n##  $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n##  $ HS.Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n##  $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n##  $ Area      : num  50708 566432 113417 51945 156361 ...\ndim(state_facts)\n## [1] 50  9\nnrow(state_facts)\n## [1] 50\nncol(state_facts)\n## [1] 9\n\n# Illiteracy\nstate_facts$TotalNumIlliterate &lt;- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) \n\n# Population Density\nstate_facts$PopDensity &lt;- state_facts$Population * 1e3/state_facts$Area \n# in people per square mile\n\n# minimum population\nstate_facts$state[which.min(state_facts$PopDensity)]\n## [1] \"Alaska\"\n\n\n\n\n# Ways to get rows and columns\nstate_facts.shape\n## (50, 9)\nstate_facts.index.size # rows\n## 50\nstate_facts.columns.size # columns\n## 9\nstate_facts.info() # columns + rows + missing counts + data types\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 50 entries, 0 to 49\n## Data columns (total 9 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   state       50 non-null     object \n##  1   Population  50 non-null     int64  \n##  2   Income      50 non-null     int64  \n##  3   Illiteracy  50 non-null     float64\n##  4   Life.Exp    50 non-null     float64\n##  5   Murder      50 non-null     float64\n##  6   HS.Grad     50 non-null     float64\n##  7   Frost       50 non-null     int64  \n##  8   Area        50 non-null     int64  \n## dtypes: float64(4), int64(4), object(1)\n## memory usage: 3.6+ KB\n\n# Illiteracy\nstate_facts[\"TotalNumIlliterate\"] = state_facts[\"Population\"] * 1e3 * state_facts[\"Illiteracy\"]/100\n\n# Population Density\nstate_facts[\"PopDensity\"] = state_facts[\"Population\"] * 1e3/state_facts[\"Area\"] \n# in people per square mile\n\n# minimum population\nmin_dens = state_facts[\"PopDensity\"].min()\n# Get location of minimum population\nloc_min_dens = state_facts.PopDensity.isin([min_dens])\n# Pull out matching state\nstate_facts.state[loc_min_dens]\n## 1    Alaska\n## Name: state, dtype: object\n\n\n\n\n\n\n\n10.8.2 Creating Data Frames\nIt is possible to create data frames from scratch by building them out of simpler components, such as lists of vectors or dicts of Series. This tends to be useful for small data sets, but it is more common to read data in from e.g.¬†CSV files, which I‚Äôve used several times already but haven‚Äôt yet shown you how to do (see Chapter 17 for the full how-to).\n\n10.8.2.1 Data Frames from Scratch\n\n\nR\nPython\n\n\n\n\nmath_and_lsd &lt;- data.frame(\n  lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41),\n  test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97))\nmath_and_lsd\n##   lsd_conc test_score\n## 1     1.17      78.93\n## 2     2.97      58.20\n## 3     3.26      67.47\n## 4     4.69      37.47\n## 5     5.83      45.65\n## 6     6.00      32.92\n## 7     6.41      29.97\n\n# add a column - character vector\nmath_and_lsd$subjective &lt;- c(\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\")\n\nmath_and_lsd\n##   lsd_conc test_score                             subjective\n## 1     1.17      78.93                    finally coming back\n## 2     2.97      58.20                         getting better\n## 3     3.26      67.47                    it's totally better\n## 4     4.69      37.47                    really tripping out\n## 5     5.83      45.65                            is it over?\n## 6     6.00      32.92                              whoa, man\n## 7     6.41      29.97 I can taste color, but I can't do math\n\n\n\n\nmath_and_lsd = pd.DataFrame({\n  \"lsd_conc\": [1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41],\n  \"test_score\": [78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97]})\nmath_and_lsd\n##    lsd_conc  test_score\n## 0      1.17       78.93\n## 1      2.97       58.20\n## 2      3.26       67.47\n## 3      4.69       37.47\n## 4      5.83       45.65\n## 5      6.00       32.92\n## 6      6.41       29.97\n\n# add a column - character vector\nmath_and_lsd[\"subjective\"] = [\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\"]\n\nmath_and_lsd\n##    lsd_conc  test_score                              subjective\n## 0      1.17       78.93                     finally coming back\n## 1      2.97       58.20                          getting better\n## 2      3.26       67.47                     it's totally better\n## 3      4.69       37.47                     really tripping out\n## 4      5.83       45.65                             is it over?\n## 5      6.00       32.92                               whoa, man\n## 6      6.41       29.97  I can taste color, but I can't do math\n\n\n\n\nWhile it‚Äôs not so hard to create data frames from scratch for small data sets, it‚Äôs very tedious if you have a lot of data (or if you can‚Äôt type accurately). An easier way to create a data frame (rather than typing the whole thing in) is to read in data from somewhere else - a file, a table on a webpage, etc. We‚Äôre not going to go into the finer points of this (you‚Äôll get into that in Chapter 17), but it is useful to know how to read neatly formatted data.\nOne source of (relatively neat) data is the TidyTuesday github repository4\n\n10.8.2.2 Reading in Data\n\n\nBase R\nreadR package\nPandas\n\n\n\nIn Base R, we can read the data in using the read.csv function\n\nairmen &lt;- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n##                    name last_name    first_name      graduation_date\n## 1   Adams, John H., Jr.     Adams  John H., Jr. 1945-04-15T00:00:00Z\n## 2           Adams, Paul     Adams          Paul 1943-04-29T00:00:00Z\n## 3 Adkins, Rutherford H.    Adkins Rutherford H. 1944-10-16T00:00:00Z\n## 4    Adkins, Winston A.    Adkins    Winston A. 1944-02-08T00:00:00Z\n## 5 Alexander, Halbert L. Alexander    Halbert L. 1944-11-20T00:00:00Z\n## 6  Alexander, Harvey R. Alexander     Harvey R. 1944-04-15T00:00:00Z\n##   rank_at_graduation     class graduated_from    pilot_type\n## 1             2nd Lt   SE-45-B           TAAF Single engine\n## 2             2nd Lt   SE-43-D           TAAF Single engine\n## 3             2nd Lt SE-44-I-1           TAAF Single engine\n## 4             2nd Lt   TE-44-B           TAAF   Twin engine\n## 5             2nd Lt   SE-44-I           TAAF Single engine\n## 6             2nd Lt   TE-44-D           TAAF   Twin engine\n##   military_hometown_of_record state aerial_victory_credits\n## 1                 Kansas City    KS                   &lt;NA&gt;\n## 2                  Greenville    SC                   &lt;NA&gt;\n## 3                  Alexandria    VA                   &lt;NA&gt;\n## 4                     Chicago    IL                   &lt;NA&gt;\n## 5                  Georgetown    IL                   &lt;NA&gt;\n## 6                  Georgetown    IL                   &lt;NA&gt;\n##   number_of_aerial_victory_credits reported_lost reported_lost_date\n## 1                                0          &lt;NA&gt;               &lt;NA&gt;\n## 2                                0          &lt;NA&gt;               &lt;NA&gt;\n## 3                                0          &lt;NA&gt;               &lt;NA&gt;\n## 4                                0          &lt;NA&gt;               &lt;NA&gt;\n## 5                                0          &lt;NA&gt;               &lt;NA&gt;\n## 6                                0          &lt;NA&gt;               &lt;NA&gt;\n##   reported_lost_location                                   web_profile\n## 1                   &lt;NA&gt;     https://cafriseabove.org/john-h-adams-jr/\n## 2                   &lt;NA&gt;          https://cafriseabove.org/paul-adams/\n## 3                   &lt;NA&gt; https://cafriseabove.org/rutherford-h-adkins/\n## 4                   &lt;NA&gt;                                          &lt;NA&gt;\n## 5                   &lt;NA&gt; https://cafriseabove.org/halbert-l-alexander/\n## 6                   &lt;NA&gt;  https://cafriseabove.org/harvey-r-alexander/\n\n\n\nIf we want instead to create a tibble, we can use the readr package‚Äôs read_csv function, which is a bit more robust and has a few additional features.\n\nlibrary(readr)\nairmen &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n## # A tibble: 6 √ó 16\n##   name         last_name first_name graduation_date     rank_at_graduation class\n##   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;              &lt;chr&gt;\n## 1 Adams, John‚Ä¶ Adams     John H., ‚Ä¶ 1945-04-15 00:00:00 2nd Lt             SE-4‚Ä¶\n## 2 Adams, Paul  Adams     Paul       1943-04-29 00:00:00 2nd Lt             SE-4‚Ä¶\n## 3 Adkins, Rut‚Ä¶ Adkins    Rutherfor‚Ä¶ 1944-10-16 00:00:00 2nd Lt             SE-4‚Ä¶\n## 4 Adkins, Win‚Ä¶ Adkins    Winston A. 1944-02-08 00:00:00 2nd Lt             TE-4‚Ä¶\n## 5 Alexander, ‚Ä¶ Alexander Halbert L. 1944-11-20 00:00:00 2nd Lt             SE-4‚Ä¶\n## 6 Alexander, ‚Ä¶ Alexander Harvey R.  1944-04-15 00:00:00 2nd Lt             TE-4‚Ä¶\n## # ‚Ñπ 10 more variables: graduated_from &lt;chr&gt;, pilot_type &lt;chr&gt;,\n## #   military_hometown_of_record &lt;chr&gt;, state &lt;chr&gt;,\n## #   aerial_victory_credits &lt;chr&gt;, number_of_aerial_victory_credits &lt;dbl&gt;,\n## #   reported_lost &lt;chr&gt;, reported_lost_date &lt;dttm&gt;,\n## #   reported_lost_location &lt;chr&gt;, web_profile &lt;chr&gt;\n\n\n\nIn pandas, we can read the csv using pd.read_csv\n\nimport pandas as pd\n\nairmen = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv\")\nairmen.head()\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## 4  Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n10.8.3 Working with Data Frames\nOften, we want to know what a data frame contains. R and pandas both have easy summary methods for data frames.\n\n10.8.3.1 Data Frame Summaries\nNotice that the type of summary depends on the data type.\n\n\nR\nPython\n\n\n\n\nsummary(airmen)\n##      name            last_name          first_name       \n##  Length:1006        Length:1006        Length:1006       \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character  \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  graduation_date                   rank_at_graduation    class          \n##  Min.   :1942-03-06 00:00:00.000   Length:1006        Length:1006       \n##  1st Qu.:1943-10-22 00:00:00.000   Class :character   Class :character  \n##  Median :1944-05-23 00:00:00.000   Mode  :character   Mode  :character  \n##  Mean   :1944-07-02 13:18:52.462                                        \n##  3rd Qu.:1945-04-15 00:00:00.000                                        \n##  Max.   :1948-10-12 00:00:00.000                                        \n##  NA's   :11                                                             \n##  graduated_from      pilot_type        military_hometown_of_record\n##  Length:1006        Length:1006        Length:1006                \n##  Class :character   Class :character   Class :character           \n##  Mode  :character   Mode  :character   Mode  :character           \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     state           aerial_victory_credits number_of_aerial_victory_credits\n##  Length:1006        Length:1006            Min.   :0.0000                  \n##  Class :character   Class :character       1st Qu.:0.0000                  \n##  Mode  :character   Mode  :character       Median :0.0000                  \n##                                            Mean   :0.1118                  \n##                                            3rd Qu.:0.0000                  \n##                                            Max.   :4.0000                  \n##                                                                            \n##  reported_lost      reported_lost_date   reported_lost_location\n##  Length:1006        Min.   :1943-07-02   Length:1006           \n##  Class :character   1st Qu.:1943-07-02   Class :character      \n##  Mode  :character   Median :1943-07-02   Mode  :character      \n##                     Mean   :1943-07-02                         \n##                     3rd Qu.:1943-07-02                         \n##                     Max.   :1943-07-02                         \n##                     NA's   :1004                               \n##  web_profile       \n##  Length:1006       \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n##                    \n## \n\nlibrary(skimr) # Fancier summaries\nskim(airmen)\n\n\nData summary\n\n\nName\nairmen\n\n\nNumber of rows\n1006\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n9\n28\n0\n1003\n0\n\n\nlast_name\n0\n1.00\n3\n12\n0\n617\n0\n\n\nfirst_name\n0\n1.00\n3\n17\n0\n804\n0\n\n\nrank_at_graduation\n5\n1.00\n3\n14\n0\n7\n0\n\n\nclass\n20\n0.98\n3\n9\n0\n72\n0\n\n\ngraduated_from\n0\n1.00\n4\n23\n0\n4\n0\n\n\npilot_type\n0\n1.00\n11\n13\n0\n5\n0\n\n\nmilitary_hometown_of_record\n9\n0.99\n3\n19\n0\n366\n0\n\n\nstate\n11\n0.99\n2\n5\n0\n48\n0\n\n\naerial_victory_credits\n934\n0.07\n31\n137\n0\n50\n0\n\n\nreported_lost\n1004\n0.00\n1\n1\n0\n1\n0\n\n\nreported_lost_location\n1004\n0.00\n23\n23\n0\n1\n0\n\n\nweb_profile\n813\n0.19\n34\n95\n0\n190\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nnumber_of_aerial_victory_credits\n0\n1\n0.11\n0.46\n0\n0\n0\n0\n4\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\ngraduation_date\n11\n0.99\n1942-03-06\n1948-10-12\n1944-05-23\n52\n\n\nreported_lost_date\n1004\n0.00\n1943-07-02\n1943-07-02\n1943-07-02\n1\n\n\n\n\n\n\n\n\n# All variables - strings are summarized with NaNs\nairmen.describe(include = 'all')\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## mean                   NaN  ...                                                NaN\n## std                    NaN  ...                                                NaN\n## min                    NaN  ...                                                NaN\n## 25%                    NaN  ...                                                NaN\n## 50%                    NaN  ...                                                NaN\n## 75%                    NaN  ...                                                NaN\n## max                    NaN  ...                                                NaN\n## \n## [11 rows x 16 columns]\n\n# Only summarize numeric variables\nairmen.describe(include = [np.number])\n##        number_of_aerial_victory_credits\n## count                       1006.000000\n## mean                           0.111829\n## std                            0.457844\n## min                            0.000000\n## 25%                            0.000000\n## 50%                            0.000000\n## 75%                            0.000000\n## max                            4.000000\n\n# Only summarize string variables (objects)\nairmen.describe(include = ['O'])\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## \n## [4 rows x 15 columns]\n\n# Get counts of how many NAs in each column\nairmen.info(show_counts=True)\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 1006 entries, 0 to 1005\n## Data columns (total 16 columns):\n##  #   Column                            Non-Null Count  Dtype  \n## ---  ------                            --------------  -----  \n##  0   name                              1006 non-null   object \n##  1   last_name                         1006 non-null   object \n##  2   first_name                        1006 non-null   object \n##  3   graduation_date                   995 non-null    object \n##  4   rank_at_graduation                999 non-null    object \n##  5   class                             986 non-null    object \n##  6   graduated_from                    1006 non-null   object \n##  7   pilot_type                        1006 non-null   object \n##  8   military_hometown_of_record       997 non-null    object \n##  9   state                             995 non-null    object \n##  10  aerial_victory_credits            72 non-null     object \n##  11  number_of_aerial_victory_credits  1006 non-null   float64\n##  12  reported_lost                     2 non-null      object \n##  13  reported_lost_date                2 non-null      object \n##  14  reported_lost_location            2 non-null      object \n##  15  web_profile                       193 non-null    object \n## dtypes: float64(1), object(15)\n## memory usage: 125.9+ KB\n\nIn pandas, you will typically want to separate out .describe() calls for numeric and non-numeric columns. Another handy function in pandas is .info(), which you can use to show the number of non-NA values. This is particularly useful in sparse datasets where there may be a LOT of missing values and you may want to find out which columns have useful information for more than just a few rows.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "href": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "title": "10¬† Data Structures",
    "section": "\n10.9 References",
    "text": "10.9 References\n\n\n\n\n[1] \nN. Matloff, The art of r programming: A tour of statistical software design. No Starch Press, 2011 [Online]. Available: https://books.google.com/books?id=o2aLBAAAQBAJ\n\n\n\n[2] \nM. Fripp, ‚ÄúAnswer to \"python pandas dataframe, is it pass-by-value or pass-by-reference\". Stack overflow,‚Äù Aug. 12, 2016. [Online]. Available: https://stackoverflow.com/a/38925257/2859168. [Accessed: Jan. 10, 2023]\n\n\n[3] \nMathIsFun.com, ‚ÄúHow to multiply matrices. Math is fun,‚Äù 2021. [Online]. Available: https://www.mathsisfun.com/algebra/matrix-multiplying.html. [Accessed: Jan. 10, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#footnotes",
    "href": "part-gen-prog/03-data-struct.html#footnotes",
    "title": "10¬† Data Structures",
    "section": "",
    "text": "Throughout this section (and other sections), lego pictures are rendered using https://www.mecabricks.com/en/workshop. It‚Äôs a pretty nice tool for building stuff online!‚Ü©Ô∏é\nA similar system exists in R libraries, but R doesn‚Äôt handle multiple libraries having the same function names well, which leads to all sorts of confusion. At least python is explicit about it.‚Ü©Ô∏é\nGrumpy cat, Garfield, Nyan cat. Jorts and Jean: The initial post and the update (both are worth a read because the story is hilarious). The cats also have a Twitter account where they promote workers rights.‚Ü©Ô∏é\nTidy Tuesday is a collaborative project where the R community gets together and explores a dataset, cleaning it, visualizing it, and generally working to collectively hone R skills together. You can find some very nice YouTube livestreams, as well as lots of examples using the #tidytuesday twitter tag.‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html",
    "href": "part-gen-prog/035-matrix-calcs.html",
    "title": "11¬† Matrix Calculations",
    "section": "",
    "text": "11.1  Objectives\nWhile R and Python are extremely powerful statistical programming languages, the core of most programming languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn‚Äôt quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in whatever language you are planning to use to work with data.\nIn this section, we will essentially be using our programming language as overgrown calculators.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#objectives",
    "href": "part-gen-prog/035-matrix-calcs.html#objectives",
    "title": "11¬† Matrix Calculations",
    "section": "",
    "text": "Understand how to do matrix algebra in relevant programming languages",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "href": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "title": "11¬† Matrix Calculations",
    "section": "\n11.2 Matrix Operations",
    "text": "11.2 Matrix Operations\n\n\nTable¬†11.1: Table of common mathematical and matrix operations in R and Python [1].\n\n\n\nOperation\nR\nPython\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nElementwise Multiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nModulo (Remainder)\n%%\n%\n\n\nInteger Division\n%/%\n//\n\n\nElementwise Exponentiation\n^\n**\n\n\nMatrix/Vector Multiplication\n%*%\nnp.dot()\n\n\nMatrix Exponentiation\n^\nnp.exp()\n\n\nMatrix Transpose\nt(A)\nnp.transpose(A)\n\n\nMatrix Determinant\ndet(A)\nnp.linalg.det(A)\n\n\nMatrix Diagonal\ndiag(A)\nnp.linalg.diag(A)\n\n\nMatrix Inverse\nsolve(A)\nnp.linalg.inv(A)\n\n\n\n\n\n\n\n11.2.1 Basic Mathematical Operators\n\n\nR\nPython\n\n\n\n\nx &lt;- 1:10\ny &lt;- seq(3, 30, by = 3)\n\nx + y\n##  [1]  4  8 12 16 20 24 28 32 36 40\nx - y\n##  [1]  -2  -4  -6  -8 -10 -12 -14 -16 -18 -20\nx * y\n##  [1]   3  12  27  48  75 108 147 192 243 300\nx / y\n##  [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n##  [8] 0.3333333 0.3333333 0.3333333\nx^2\n##  [1]   1   4   9  16  25  36  49  64  81 100\nt(x) %*% y\n##      [,1]\n## [1,] 1155\n\n\n\n\nimport numpy as np\n\nx = np.array(range(1, 11))\ny = np.array(range(3, 33, 3)) # python indexes are not inclusive\n\nx + y\n## array([ 4,  8, 12, 16, 20, 24, 28, 32, 36, 40])\nx - y\n## array([ -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20])\nx * y\n## array([  3,  12,  27,  48,  75, 108, 147, 192, 243, 300])\nx / y\n## array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n##        0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])\nx ** 2\n## array([  1,   4,   9,  16,  25,  36,  49,  64,  81, 100])\nnp.dot(x.T, y)\n## 1155\n\n\n\n\n\n11.2.2 Matrix Operations\nOther matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy:\n\n\nR\nPython\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\nmat\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    6    4    5\n## [3,]    7    8    9\nt(mat) # transpose\n##      [,1] [,2] [,3]\n## [1,]    1    6    7\n## [2,]    2    4    8\n## [3,]    3    5    9\ndet(mat) # get the determinant\n## [1] 18\ndiag(mat) # get the diagonal\n## [1] 1 4 9\ndiag(diag(mat)) # get a square matrix with off-diag 0s\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    4    0\n## [3,]    0    0    9\ndiag(1:3) # diag() also will create a diagonal matrix if given a vector\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    2    0\n## [3,]    0    0    3\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nmat\n## array([[1, 2, 3],\n##        [6, 4, 5],\n##        [7, 8, 9]])\nmat.T\n## array([[1, 6, 7],\n##        [2, 4, 8],\n##        [3, 5, 9]])\nnp.linalg.det(mat) # numerical precision...\n## 18.000000000000004\nnp.diag(mat)\n## array([1, 4, 9])\nnp.diag(np.diag(mat))\n## array([[1, 0, 0],\n##        [0, 4, 0],\n##        [0, 0, 9]])\nnp.diag(range(1, 4))\n## array([[1, 0, 0],\n##        [0, 2, 0],\n##        [0, 0, 3]])\n\n\n\n\n\n11.2.3 Matrix Inverse\nThe other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we‚Äôd like to see‚Ä¶ Instead, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1‚Äôs on diagonal, 0‚Äôs off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. In Python, we use the np.linalg.inv() function to invert a matrix, which may be a bit more linguistically familiar.\n\n\nR\nPython\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\n\nminv &lt;- solve(mat) # get the inverse\n\nminv\n##            [,1]       [,2]       [,3]\n## [1,] -0.2222222  0.3333333 -0.1111111\n## [2,] -1.0555556 -0.6666667  0.7222222\n## [3,]  1.1111111  0.3333333 -0.4444444\nmat %*% minv \n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nminv = np.linalg.inv(mat)\nminv\n## array([[-0.22222222,  0.33333333, -0.11111111],\n##        [-1.05555556, -0.66666667,  0.72222222],\n##        [ 1.11111111,  0.33333333, -0.44444444]])\nnp.dot(mat, minv)\n## array([[ 1.00000000e+00,  0.00000000e+00,  1.11022302e-16],\n##        [-8.88178420e-16,  1.00000000e+00, -5.55111512e-16],\n##        [ 0.00000000e+00,  2.22044605e-16,  1.00000000e+00]])\nnp.round(np.dot(mat, minv), 2)\n## array([[ 1.,  0.,  0.],\n##        [-0.,  1., -0.],\n##        [ 0.,  0.,  1.]])",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#references",
    "href": "part-gen-prog/035-matrix-calcs.html#references",
    "title": "11¬† Matrix Calculations",
    "section": "\n11.3 References",
    "text": "11.3 References\n\n\n\n\n[1] \nQuartz25, Jesdisciple, H. R√∂st, D. Ross, L. D‚ÄôOliveiro, and BLibrestez55, Python Programming. Wikibooks, 2016 [Online]. Available: https://en.wikibooks.org/wiki/Python_Programming. [Accessed: May 28, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Matrix Calculations</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html",
    "href": "part-gen-prog/04-control-struct.html",
    "title": "12¬† Control Structures",
    "section": "",
    "text": "12.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#objectives",
    "href": "part-gen-prog/04-control-struct.html#objectives",
    "title": "12¬† Control Structures",
    "section": "",
    "text": "Understand how to use conditional statements\nUnderstand how conditional statements are evaluated by a program\nUse program flow diagrams to break a problem into parts and evaluate how a program will execute\nUnderstand how to use loops\nSelect the appropriate type of loop for a problem",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#mindset",
    "href": "part-gen-prog/04-control-struct.html#mindset",
    "title": "12¬† Control Structures",
    "section": "\n12.2 Mindset",
    "text": "12.2 Mindset\nBefore we start on the types of control structures, let‚Äôs get in the right mindset. We‚Äôre all used to ‚Äúif-then‚Äù logic, and use it in everyday conversation, but computers require another level of specificity when you‚Äôre trying to provide instructions.\nCheck out this video of the classic ‚Äúmake a peanut butter sandwich instructions challenge‚Äù:\n\n\n\nHere‚Äôs another example:\n\n\n‚ÄòIf you‚Äôre done being pedantic, we should get dinner.‚Äô ‚ÄòYou did it again!‚Äô ‚ÄòNo, I didn‚Äôt.‚Äô Image from Randal Munroe, xkcd.com, available under a CC-By 2.5 license.\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you‚Äôre going to have a bad time.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#conditional-statements",
    "href": "part-gen-prog/04-control-struct.html#conditional-statements",
    "title": "12¬† Control Structures",
    "section": "\n12.3 Conditional Statements",
    "text": "12.3 Conditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\nLet‚Äôs try this out:\n\n\nR\nPython\n\n\n\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) { \n  y &lt;- 8\n} else {\n  y &lt;- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x &gt; 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) y &lt;- 8 else y &lt;- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x &gt; 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n## x = 3 ; y = 8\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x &gt; 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it ‚Äúbelongs‚Äù.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python‚Ä¶ but it‚Äôs a consequence of trying to make the code more readable.\n\n\n\n12.3.1 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n12.3.2 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\n\n\n\n\n\nExample - Conditional Evaluation\n\n\n\nSuppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can‚Äôt be in the 25-40 bracket and the 41-55 bracket.\n\n\nProgram Flow Map\nR\nPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as &lt;18, 18-25, 26-40, 41-55, 56-65, and &gt;65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\nThe important thing to realize when examining this program flow map is that if age &lt;= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age &lt;= 18), (age &lt;= 25), and (age &lt;= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‚Äò26-40‚Äô is evaluated when age = 40.\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - &lt;40 is evaluated first, and so &lt;= 25 and &lt;= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage &lt;- 40 # change this as you will to see how the code works\n\nif (age &lt; 18) {\n  bracket &lt;- \"&lt;18\"\n} else if (age &lt;= 25) {\n  bracket &lt;- \"18-25\"\n} else if (age &lt;= 40) {\n  bracket &lt;- \"26-40\"\n} else if (age &lt;= 55) {\n  bracket &lt;- \"41-55\" \n} else if (age &lt;= 65) {\n  bracket &lt;- \"56-65\"\n} else {\n  bracket &lt;- \"&gt;65\"\n}\n\nbracket\n## [1] \"26-40\"\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don‚Äôt indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age &lt; 18:\n  bracket = \"&lt;18\"\nelif age &lt;= 25:\n  bracket = \"18-25\"\nelif age &lt;= 40:\n  bracket = \"26-40\"\nelif age &lt;= 55:\n  bracket = \"41-55\"\nelif age &lt;= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \"&gt;65\"\n  \nbracket\n## '26-40'\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out - Chained If/Else Statements\n\n\n\n\n\nProblem\nFlow Map\nR Solution\nPython Solution\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we‚Äôre ignoring the personal exemption and the standard deduction, so we‚Äôre already simplifying the tax code.\nWrite a set of if statements that assess someone‚Äôs income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n\n\nThe control flow diagram for the tax brackets\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you‚Äôre debugging). It can be very helpful to map out your program flow as you‚Äôre untangling a problem.\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- 0\n\nif (x &lt;= 10275) {\n  y &lt;- x*.1 # tax paid\n  x &lt;- 0 # All money has been taxed\n} else {\n  y &lt;- y + 10275 * .1\n  x &lt;- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x &lt;= (41775 - 10275)) {\n  y &lt;- y + x * .12\n  x &lt;- 0\n} else {\n  y &lt;- y + (41775 - 10275) * .12\n  x &lt;- x - (41775 - 10275) \n}\n\nif (x &lt;= (89075 - 41775)) {\n  y &lt;- y + x * .22\n  x &lt;- 0\n} else {\n  y &lt;- y + (89075 - 41775) * .22\n  x &lt;- x - (89075 - 41775)\n}\n\nif (x &lt;= (170050 - 89075)) {\n  y &lt;- y + x * .24\n  x &lt;- 0\n} else {\n  y &lt;- y + (170050 - 89075) * .24\n  x &lt;- x - (170050 - 89075)\n}\n\nif (x &lt;= (215950 - 170050)) {\n  y &lt;- y + x * .32\n  x &lt;- 0\n} else {\n  y &lt;- y + (215950 - 170050) * .32\n  x &lt;- x - (215950 - 170050)\n}\n\nif (x &lt;= (539900 - 215950)) {\n  y &lt;- y + x * .35\n  x &lt;- 0\n} else {\n  y &lt;- y + (539900 - 215950) * .35\n  x &lt;- x - (539900 - 215950)\n}\n\nif (x &gt; 0) {\n  y &lt;- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed &lt;= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed &lt;= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed &lt;= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed &lt;= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed &lt;= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed &lt;= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed &gt; 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n## Total Tauntaxed Rate on $ 200000  in income =  22.12 %\n\nWe will find a better way to represent this calculation once we discuss loops - we can store each bracket‚Äôs start and end point in a vector and loop through them. Any time you find yourself copy-pasting code and changing values, you should consider using a loop (or eventually a function) instead.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#loops",
    "href": "part-gen-prog/04-control-struct.html#loops",
    "title": "12¬† Control Structures",
    "section": "\n12.4 Loops",
    "text": "12.4 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don‚Äôt know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n12.4.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true. The simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\n\n\n\nExample - While Loops\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing while-loop pseudocode (while x &lt;= N) { # code that changes x in some way} and the program flow map expansion where we check if x &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\n\nx &lt;- 0\n\nwhile (x &lt; 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x &lt;- x + 1\n}\n## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n\n\n\n\nx = 0\n\nwhile x &lt; 10:\n  print(x)\n  x = x + 1\n## 0\n## 1\n## 2\n## 3\n## 4\n## 5\n## 6\n## 7\n## 8\n## 9\n\n\n\n\n\n\n\n\n\n\n\n\nTry it Out - While Loops\n\n\n\n\n\nProblem\nMath Notation\nR Solution\nPython solution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk &lt;- 1\nprod &lt;- 1\nans &lt;- (exp(pi) - exp(-pi))/(2*pi)\ndelta &lt;- 0.0001\n\nwhile (abs(prod - ans) &gt;= 0.0001) {\n  prod &lt;- prod * (1 + 1/k^2)\n  k &lt;- k + 1\n}\n\nk\n## [1] 36761\nprod\n## [1] 3.675978\nans\n## [1] 3.676078\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) &gt;= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k &gt; 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n## At  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Avoid Infinite Loops\n\n\n\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\n\nR\nPython\n\n\n\nThis while loop runs until either x &lt; 10 or n &gt; 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‚Äòrandom walk‚Äô) could theoretically continue forever, we add the n&gt;50 check to the loop so that we don‚Äôt tie up the computer for eternity.\n\nx &lt;- 0\nn &lt;- 0 # count the number of times the loop runs\n\nwhile (x &lt; 10) { \n  print(x)\n  x &lt;- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n &lt;- n + 1\n  if (n &gt; 50) \n    break # this stops the loop if n &gt; 50\n}\n## [1] 0\n## [1] 0.3517088\n## [1] 1.517403\n## [1] 2.810551\n## [1] 2.443797\n## [1] 2.813846\n## [1] 0.9007025\n## [1] 1.011412\n## [1] 0.961636\n## [1] 2.379953\n## [1] 2.714089\n## [1] 2.94099\n## [1] 2.227345\n## [1] 2.672468\n## [1] 4.240402\n## [1] 5.615584\n## [1] 5.204176\n## [1] 4.706667\n## [1] 5.582507\n## [1] 7.051302\n## [1] 8.839158\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x &lt; 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n &gt; 50:\n    break # this stops the loop if n &gt; 50\n## 0\n## [-0.88771665]\n## [-0.02517791]\n## [-0.55818398]\n## [0.7410756]\n## [1.0105944]\n## [0.70525995]\n## [1.18138021]\n## [0.6155277]\n## [-0.92180249]\n## [-0.44298115]\n## [0.04365226]\n## [-0.41922168]\n## [-0.33223168]\n## [0.04617224]\n## [-0.20752893]\n## [-0.46245587]\n## [-0.22026173]\n## [0.64728956]\n## [-1.70667016]\n## [-2.82031746]\n## [-3.76019061]\n## [-4.70790018]\n## [-5.83278641]\n## [-5.34132197]\n## [-7.60843189]\n## [-6.03492823]\n## [-6.06545976]\n## [-5.03359451]\n## [-3.01396455]\n## [-1.05511401]\n## [-0.5347353]\n## [-2.91148155]\n## [-1.854246]\n## [-2.00071402]\n## [-2.5173893]\n## [-3.66543687]\n## [-2.75343879]\n## [-1.94007636]\n## [-2.03480331]\n## [-1.4759154]\n## [-2.68606795]\n## [-3.26983128]\n## [-4.46714622]\n## [-3.52795086]\n## [-2.69546197]\n## [-1.72338727]\n## [-1.35948517]\n## [-2.15545937]\n## [0.47031341]\n## [-0.59488528]\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don‚Äôt run for all eternity.\n\n\n\n12.4.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nA visual demonstration of for loops iterating through a vector of monsters to dress them up for a parade. Image by Allison Horst.\n\n\n\n\n\n\n\nExample - For Loop Syntax\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\n\n\n\nfor i in range(5):\n  print(i)\n## 0\n## 1\n## 2\n## 3\n## 4\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is very commonly used to do a task for every value of a vector.\n\n12.4.2.1 Example - For Loops\n\n\nR\nPython\n\n\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n## [1] \"January\"\n## [1] \"February\"\n## [1] \"March\"\n## [1] \"April\"\n## [1] \"May\"\n## [1] \"June\"\n## [1] \"July\"\n## [1] \"August\"\n## [1] \"September\"\n## [1] \"October\"\n## [1] \"November\"\n## [1] \"December\"\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3\n\n# Create new vector of the correct length\nabbr3 &lt;- rep(\"\", length(month.name))\n\n# We have to iterate along the index (1 to length) instead of the name \n# in this case because we want to store the result in a corresponding\n# row of a new vector\nfor (i in 1:length(month.name))\n  abbr3[i] &lt;- substr(month.name[i], 1, 3)\n\n# We can combine the two vectors into a data frame \n# so that each row corresponds to a month and there are two columns:\n# full month name, and abbreviation\ndata.frame(full_name = month.name, abbrev = abbr3)\n##    full_name abbrev\n## 1    January    Jan\n## 2   February    Feb\n## 3      March    Mar\n## 4      April    Apr\n## 5        May    May\n## 6       June    Jun\n## 7       July    Jul\n## 8     August    Aug\n## 9  September    Sep\n## 10   October    Oct\n## 11  November    Nov\n## 12  December    Dec\n\n\n\nIn python, we have to define our vector or list to start out with, but that‚Äôs easy enough:\n\nimport calendar\n# Create a list with month names. For some reason, by default there's a \"\" as \n# the first entry, so we'll get rid of that\nmonth_name = list(calendar.month_name)[1:13]\n\nfor i in month_name:\n  print(i)\n## January\n## February\n## March\n## April\n## May\n## June\n## July\n## August\n## September\n## October\n## November\n## December\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3.\nPython handles lists best when you use pythonic expressions. The linked post has an excellent explanation of why enumerate works best here.\n\n# Create new vector of the correct length\nabbr3 = [\"\"] * len(month_name)\n\n# We have to iterate along the index because we want to \n# store the result in a corresponding row of a new vector\n# Python allows us to iterate along both the index i and the value val\n# at the same time, which is convenient.\nfor i, val in enumerate(month_name):\n  abbr3[i] = val[0:3:] # Strings have indexes by character, so this gets \n                       # characters 0, 1, and 2.\n  \nabbr3\n## ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#other-control-structures",
    "href": "part-gen-prog/04-control-struct.html#other-control-structures",
    "title": "12¬† Control Structures",
    "section": "\n12.5 Other Control Structures",
    "text": "12.5 Other Control Structures\n\n12.5.1 Conditional Statements\ncase statements, e.g.¬†case_when in tidyverse\n\n12.5.2 Loops\n\n12.5.2.1 Controlling Loops\nWhile I do not often use break, next, and continue statements, they do exist in both languages and can be useful for controlling the flow of program execution. I have moved the section on this to Section 31.2 for the sake of brevity and to reduce the amount of new material those without programming experience are being exposed to in this section.\n\n12.5.2.2 Other Types of Loops\nThere are other types of loops in most languages, such as the do-while loop, which runs the code first and then evaluates the logical condition to determine whether the loop will be run again.\n\n\n\n\n\n\nExample: do-while loops\n\n\n\n\n\nR\nPython\n\n\n\nIn R, do-while loops are most naturally implemented using a very primitive type of iteration: a repeat statement.\n\nrepeat {\n  # statements go here\n  if (condition)\n    break # this exits the repeat statement\n}\n\n\n\nIn python, do-while loops are most naturally implemented using a while loop with condition TRUE:\n\nwhile TRUE:\n  # statements go here\n  if condition:\n    break\n\n\n\n\n\n\nAn additional means of running code an indeterminate number of times is the use of recursion, which we cannot cover until we learn about functions. I have added an additional section, Section 31.3, to cover this topic, but it is not essential to being able to complete most basic data programming tasks. Recursion is useful when working with structures such as trees (including phylogenetic trees) and nested lists.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "href": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "title": "12¬† Control Structures",
    "section": "\n12.6 References",
    "text": "12.6 References",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Control Structures</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html",
    "href": "part-gen-prog/05-functions.html",
    "title": "13¬† Writing Functions",
    "section": "",
    "text": "13.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#objectives",
    "href": "part-gen-prog/05-functions.html#objectives",
    "title": "13¬† Writing Functions",
    "section": "",
    "text": "Identify the parts of a function from provided source code\nPredict what the function will return when provided with input values and source code\nGiven a task, lay out the steps necessary to complete the task in pseudocode\nWrite a function which uses necessary input values to complete a task",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "href": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "title": "13¬† Writing Functions",
    "section": "\n13.2 When to write a function?",
    "text": "13.2 When to write a function?\nIf you‚Äôve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you‚Äôre really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more about functions\n\n\n\nThere is some extensive material on this subject in R for Data Science [1] on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\n\n\n\n\n\nExample: Turning Code into Functions\n\n\n\nThis example is modified from R for Data Science [2, Ch. 19].\nWhat does this code do? Does it work as intended?\n\n\nR\nPython\n\n\n\n\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code‚Äôs author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has:\n\n\nR\nPython\n\n\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we start by rewriting it using general names:\n\n\nR\nPython\n\n\n\nIn this case, it might help to replace df$a with x.\n\nx &lt;- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n##  [1] 1.0000000 0.9105025 0.9720315 0.9766376 0.6076995 0.1629405 0.4346898\n##  [8] 0.6124091 0.0000000 0.3192607\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n## 0    0.183213\n## 1    0.267206\n## 2    1.000000\n## 3    0.055469\n## 4    0.567710\n## 5    0.317842\n## 6    0.011082\n## 7    0.000000\n## 8    0.146977\n## 9    0.733486\n## Name: a, dtype: float64\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\n\nR\nPython\n\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng &lt;- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n##  [1] 1.0000000 0.9105025 0.9720315 0.9766376 0.6076995 0.1629405 0.4346898\n##  [8] 0.6124091 0.0000000 0.3192607\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n## 0    0.183213\n## 1    0.267206\n## 2    1.000000\n## 3    0.055469\n## 4    0.567710\n## 5    0.317842\n## 6    0.011082\n## 7    0.000000\n## 8    0.146977\n## 9    0.733486\n## Name: a, dtype: float64\n\n\n\n\nFinally, we turn this code into a function:\n\n\nR\nPython\n\n\n\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n##  [1] 1.0000000 0.9105025 0.9720315 0.9766376 0.6076995 0.1629405 0.4346898\n##  [8] 0.6124091 0.0000000 0.3192607\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we‚Äôre working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n## 0    0.183213\n## 1    0.267206\n## 2    1.000000\n## 3    0.055469\n## 4    0.567710\n## 5    0.317842\n## 6    0.011082\n## 7    0.000000\n## 8    0.146977\n## 9    0.733486\n## Name: a, dtype: float64\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we‚Äôre working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#syntax",
    "href": "part-gen-prog/05-functions.html#syntax",
    "title": "13¬† Writing Functions",
    "section": "\n13.3 Syntax",
    "text": "13.3 Syntax\n\n\nR and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\nIn R, functions are defined as other variables, using &lt;-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return &lt;value&gt;, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don‚Äôt return anything; in R, this is a bit less common, for reasons we won‚Äôt get into here.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "href": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "title": "13¬† Writing Functions",
    "section": "\n13.4 Arguments and Parameters",
    "text": "13.4 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\n\n\n\n\n\nExample: Parts of a Function\n\n\n\nLet‚Äôs examine the difference between arguments and parameters by writing a function that takes a dog‚Äôs name and returns ‚Äú is a good pup!‚Äù.\n\n\nR\nPython\n\n\n\n\ndog &lt;- \"Eddie\"\n\ngoodpup &lt;- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n## [1] \"Eddie is a good pup!\"\n\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n## 'Eddie is a good pup!'\n\n\n\n\nIn this example function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer‚Äôs memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn‚Äôt defined outside of the function call: goodpup(\"Tesla\") produces ‚ÄúTesla is a good pup!‚Äù. Here, I do not have a variable storing the string ‚ÄúTesla‚Äù, but I can make the function run anyways. So ‚ÄúTesla‚Äù here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it‚Äôs ok if you only just sort of get what I‚Äôm trying to explain here. Hopefully it will become more clear as you write more code.\n\n\n\n\n\n\nTry it out: Function Parts\n\n\n\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be. Also determine whether the function output is stored in memory or just printed to the command line.\n\n\nFunction 1\nAnswer\n\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\n\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n## Hello World\n\n\nFunction output is not stored in memory and is printed to the command line.\n\n\n\n\n\n\nFunction 2\nAnswer\n\n\n\n\n\nmy_mean &lt;- function(x) {\n  censor_x &lt;- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n## [1] 6\n\n\nFunction output is saved to memory (x) and printed to the command line\n\n\n\n\n\n\n\n13.4.1 Named Arguments and Parameter Order\nIn the examples above, you didn‚Äôt have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\n\nR\nPython\n\n\n\n\n\ndivide &lt;- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\n\nR\nPython\n\n\n\n\ndivide(3, 6)\n## [1] 0.5\n\ndivide(x = 3, y = 6)\n## [1] 0.5\n\ndivide(y = 6, x = 3)\n## [1] 0.5\n\ndivide(6, 3)\n## [1] 2\n\ndivide(x = 6, y = 3)\n## [1] 2\n\ndivide(y = 3, x = 6)\n## [1] 2\n\n\n\n\ndivide(3, 6)\n## 0.5\n\ndivide(x = 3, y = 6)\n## 0.5\n\ndivide(y = 6, x = 3)\n## 0.5\n\ndivide(6, 3)\n## 2.0\n\ndivide(x = 6, y = 3)\n## 2.0\n\ndivide(y = 3, x = 6)\n## 2.0\n\n\n\n\nAs you can see, the order of the arguments doesn‚Äôt much matter, as long as you use named arguments, but if you don‚Äôt name your arguments, the order very much matters.\n\n13.4.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can‚Äôt guarantee that the person using your function knows that they need a certain type of input. In these cases, it‚Äôs best to validate your function input.\n\n\n\n\n\n\nInput Validation Example\n\n\n\n\n\nR\nPython\n\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in x + y: non-numeric argument to binary operator\n\nadd &lt;- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in add(\"tmp\", 3): is.numeric(x) is not TRUE\nadd(3, 4)\n## [1] 7\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we‚Äôre good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n## TypeError: can only concatenate str (not \"int\") to str\n\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n## x and y must be add-able\nadd(3, 4)\n## 7\n\nYou can read more about error handling in Python here\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don‚Äôt error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused [3].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#scope",
    "href": "part-gen-prog/05-functions.html#scope",
    "title": "13¬† Writing Functions",
    "section": "\n13.5 Scope",
    "text": "13.5 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you‚Äôve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I‚Äôll show you some examples of how scope works and the concepts that help you figure out what ‚Äúscope‚Äù actually means in practice.\n\n13.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you‚Äôll forgive my lack of creativity in this area so that you can see how name masking works.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nPython\n\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\nR\n\na &lt;- 10\n\nmyfun &lt;- function() {\n  a &lt;- 20\n  a\n}\n\nmyfun()\n## [1] 20\n\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n## 20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n13.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function‚Äôs environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat does this output?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other ‚Äì which means a = 1 every time.\n\n\n\n\nmyfun &lt;- function() {\n  if (!exists(\"aa\")) {\n    aa &lt;- 1\n  } else {\n    aa &lt;- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n## [1] 1\nmyfun()\n## [1] 1\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n## 1\nmyfun()\n## 1\n\nNote that the try command here is used to handle the case where a doesn‚Äôt exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used [4], Ch 17.\n\n\n\n\n\n\n13.5.3 Dynamic Lookup\nScoping determines where to look for values ‚Äì when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn‚Äôt exist in the function‚Äôs environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\n\n\n\nGuess and Check\n\n\n\nWhat will this code output?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\nmyfun &lt;- function() {\n  x + 1\n}\n\nx &lt;- 14\n\nmyfun()\n## [1] 15\n\nx &lt;- 20\n\nmyfun()\n## [1] 21\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n## 15\n\nx = 20\n\nmyfun()\n## 21\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Function Scope\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. From [2, Ch. 6]\n\n\nR code\nR solution\nPython code\nPython solution\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n## [1] 202\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n## 202",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#sec-functions-refs",
    "href": "part-gen-prog/05-functions.html#sec-functions-refs",
    "title": "13¬† Writing Functions",
    "section": "\n13.6 References",
    "text": "13.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O‚ÄôReilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[3] \nWikipedia Contributors, ‚ÄúDefensive programming,‚Äù Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[4] \nA. Martelli and D. Ascher, Python Cookbook. O‚ÄôReilly Media, 2002 [Online]. Available: https://learning.oreilly.com/library/view/python-cookbook/0596001673/ch05s24.html. [Accessed: May 31, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/05-functions.html#footnotes",
    "href": "part-gen-prog/05-functions.html#footnotes",
    "title": "13¬† Writing Functions",
    "section": "",
    "text": "This is not strictly true, you can of course use pass-by-reference, but we will not be covering that in this class as we are strictly dealing with the bare minimum of learning how to write a function here.‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html",
    "href": "part-gen-prog/06-debugging.html",
    "title": "14¬† Debugging",
    "section": "",
    "text": "14.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#objectives",
    "href": "part-gen-prog/06-debugging.html#objectives",
    "title": "14¬† Debugging",
    "section": "",
    "text": "Create reproducible examples of problems\nUse built in debugging tools to trace errors\nUse online resources to research errors\n\n\n\nThe faces of debugging (by Allison Horst)",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "href": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "title": "14¬† Debugging",
    "section": "\n14.2 Avoiding Errors: Defensive Programming",
    "text": "14.2 Avoiding Errors: Defensive Programming\nOne of the best debugging strategies (that isn‚Äôt a debugging strategy at all, really) is to code defensively [1]. By that, I mean, code in a way that you will make debugging things easier later.\n\nModularize your code. Each function should do only one task, ideally in the least-complex way possible.\nMake your code readable. If you can read the code easily, you‚Äôll be able to narrow down the location of the bug more quickly.\nComment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren‚Äôt just for your collaborators or others who see the code. They‚Äôre for future you.\nDon‚Äôt duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex.\nReduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse [2] is notorious for this.\n\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs ok to write code using lots of dependencies, but as you transition from ‚Äúexperimental‚Äù code to ‚Äúproduction‚Äù code (you‚Äôre using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constituent packages are some of the most installed R packages on CRAN.)\n\n\nAnother way to handle dependency management is to use the renv package [3], which creates a local package library with the appropriate versions of your packages stored in the same directory as your project. renv was inspired by the python concept of virtual environments, and it does also work with python if you‚Äôre using both R and python inside a project (e.g.¬†this book uses renv). renv will at the very least help you minimize issues with code not working after unintentional package updates.\n\nAdd safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g.¬†you don‚Äôt compute the derivative of a function and come up with ‚Äúa‚Äù.)\nDon‚Äôt reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that‚Äôs used by the community, don‚Äôt write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression.\nCollect your often-reused code in packages that you can easily load and make available to ‚Äúfuture you‚Äù. The process of making a package often encourages you to document your code better than you would a script. A good resource for getting started making R packages is [4], and a similar python book is [5].",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#working-through-errors",
    "href": "part-gen-prog/06-debugging.html#working-through-errors",
    "title": "14¬† Debugging",
    "section": "\n14.3 Working through Errors",
    "text": "14.3 Working through Errors\n\n14.3.1 First steps\n\n14.3.1.1 Get into the right mindset\nYou can‚Äôt debug something effectively if you‚Äôre upset. You have to be in a puzzle-solving, detective mindset to actually solve a problem. If you‚Äôre already stressed out, try to relieve that stress before you tackle the problem: take a shower, go for a walk, pet a puppy.\n\n\nA debugging manifesto [6]\n\n\n14.3.1.2 Check your spelling\nI‚Äôll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation.\n\n\nTitle: user.fist_name [7]\n\n\n14.3.2 General Debugging Strategies\n\n\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nWhile defensive programming is a nice idea, if you‚Äôre already at the point where you have an error you can‚Äôt diagnose, then‚Ä¶ it doesn‚Äôt help that much. At that point, you‚Äôll need some general debugging strategies to work with. The overall process is well described in [8]; I‚Äôve added some steps that are commonly overlooked and modified the context from the original package development to introductory programming. I‚Äôve also integrated some lovely illustrations from Julia Evans (@b0rk) to lighten the mood.\n\nRealize that you have a bug\nRead the error message\n\n\n\nDebugging strategy: Reread the error message[9]\n\n\n\nGoogle! Seriously, just google the whole error message.\nIn R you can automate this with the errorist and searcher packages. Python is so commonly used that you‚Äôll likely be able to find help for your issue if you are specific enough.\n\n\n\n\n\nDebugging strategy: Shorten your feedback loop [10]\n\n\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you‚Äôve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you‚Äôve been debugging the same error for a while. \nNote which inputs don‚Äôt generate the bug ‚Äì this negative ‚Äúdata‚Äù is helpful when asking for help.\n\n\n\nDebugging strategy: Change working code into broken code [12]\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nCome up with one question. If you‚Äôre stuck, it can be helpful to break it down a bit and ask one tiny question about the bug.\n\n\n\nDebugging strategy: Come up with one question [13]\n\n\n\nFix it and test it. The goal with tests is to ensure that the same error doesn‚Äôt pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you‚Äôre developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked.\n\n\n\n\n\nDebugging strategy: Write a unit test [14]\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it‚Äôs a short function or a couple of lines of code, but it‚Äôs less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - ‚Äúgot into function x‚Äù, ‚Äúreturning from function y‚Äù, and so on.\nRubber ducking. Have you ever tried to explain a problem you‚Äôre having to someone else, only to have a moment of insight and ‚Äúoh, never mind‚Äù? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck1. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you‚Äôve found the problem. See [15] for a more thorough explanation.\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that‚Äôs well-known enough that it has its own xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "href": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "title": "14¬† Debugging",
    "section": "\n14.4 Dividing Problems into Smaller Parts",
    "text": "14.4 Dividing Problems into Smaller Parts\n\n‚ÄúDivide each difficulty into as many parts as is feasible and necessary to resolve it.‚Äù -Ren√© Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solvable.\n\n\n\n\n\n\nExample: Exhaustion\n\n\n\nThis example inspired by [16].\n\n\nGeneral problem\nSpecific problem\nSubproblems\nBrainstorm\nSubproblem solutions\n\n\n\n‚ÄúI‚Äôm exhausted all the time‚Äù\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\n\n\n‚ÄúI wake up in the morning and I don‚Äôt have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I‚Äôm going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don‚Äôt have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.‚Äù\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\n\nMoving through the list in the previous tab, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don‚Äôt take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of ‚Äúfun‚Äù so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g.¬†no coffee after noon) so that caffeine doesn‚Äôt contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day‚Äôs work and wind down work well before bedtime\n\n\n\n\n\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don‚Äôt try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n[1] Make the doctor‚Äôs appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a ‚Äúno-work‚Äù period before bedtime.\n[1] Go to the doctor‚Äôs appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfullness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn‚Äôt work.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "href": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "title": "14¬† Debugging",
    "section": "\n14.5 Minimal Working (or Reproducible) Examples",
    "text": "14.5 Minimal Working (or Reproducible) Examples\n\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\nIf all else has failed, and you can‚Äôt figure out what is causing your error, it‚Äôs probably time to ask for help. If you have a friend or buddy that knows the language you‚Äôre working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you‚Äôre asking people who are much more knowledgeable about the topic - members of R core and really famous python developers browse stackoverflow and may drop in and help you out. Under those circumstances, it‚Äôs better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\nThere are numerous resources for writing what‚Äôs called a ‚Äúminimal working example‚Äù, ‚Äúreproducible example‚Äù (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you‚Äôd hope to see if the code were working.\n\n\n\n\n\n\nOther Minimum Working Example/Reprex resources\n\n\n\n\nreprex package: Do‚Äôs and Don‚Äôts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney\n\n\n\n\n\n\n\n\n\nExample: MWEs\n\n\n\n\n\nSAS markdown\nPython/Quarto\n\n\n\nNote: You don‚Äôt need to know anything about SAS to understand this example.\nA long time ago, when this book covered R and SAS, I had issues with SAS graphs rendering in black and white most of the time.\nI started debugging the issue with the following code chunk:\n```{r sas-cat-aes-map-07, engine=\"sashtml\", engine.path=\"sas\", fig.path = \"image/\"}\nlibname classdat \"sas/\";\n\nPROC SGPLOT data=classdat.fbiwide; \nSCATTER x = Population y = Assault /\n  markerattrs=(size=8pt symbol=circlefilled) \n  group = Abb; /* maps to point color by default */\nRUN;\nQUIT; \n  \nPROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */\nSCATTER x = Population y = Assault /\n  markercharattrs=(size=8) \n  markerchar = Abb /* specify marker character variable */\n    group = Abb\n  ; \nRUN;\nQUIT; \n```\nAfter running the code separately in SAS and getting a figure that looked like what I‚Äôd expected, I set out to construct a reproducible example so that I could post to the SASmarkdown github issues page and ask for help.\nThe first thing I did was strip out all of the extra stuff that didn‚Äôt need to be in the chunk - this chunk generates 2 pictures; I only need one. This chunk requires the fbiwide data from the classdata R package (that I exported to CSV); I replaced it with a dataset in the sashelp library.\nWhen I was done, the chunk looked like this:\nPROC SGPLOT data=sashelp.snacks;\nSCATTER x = date y = QtySold /\n  markerattrs=(size=8pt symbol=circlefilled)\n  group = product; /* maps to point color by default */\nRUN;\nQUIT;\nThen, I started constructing my reproducible example. I ran ?sas_enginesetup to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file).\nI copied the example from that page:\nindoc &lt;- '\n---\ntitle: \"Basic SASmarkdown Doc\"\nauthor: \"Doug Hemken\"\noutput: html_document\n---\n\n# I've deleted the intermediate chunks because they screw \n# everything up when I print this chunk out\n'\n\nknitr::knit(text=indoc, output=\"test.md\")\nrmarkdown::render(\"test.md\")\nThen, I created several chunks which would do the following: 1. Write the minimal example SAS code above to a file 2. Call that file in a SASmarkdown chunk using the %include macro, which dumps the listed file into the SAS program. This generates the plot using SASmarkdown. 3. Call the file using SAS batch mode\n(this runs the code and produces a plot outside of SASmarkdown, to prove that the issue is SASmarkdown itself)\nFinally, I included the image generated from the batch mode call manually.\nYou can see the resulting code here.\nI pasted my example into the issues page, and then included some additional information:\n\nA screenshot of the rendered page\nThe image files themselves\nA description of what happened\nMy suspicions (some obvious option I‚Äôm missing?)\nAn additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks.\n\nThis process took me about 45 minutes, but that was still much shorter than the time I‚Äôd spent rerunning code trying to get it to work with no success.\nIn less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. I had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do).\nThen, I had to tinker with the book a bit to figure out if there were easier ways to get the same result. The end result, though, was that I got what I wanted - color figures throughout the book!\n\n\nWhile converting the book from Rmarkdown to quarto, I ran into an issue setting up GitHub Actions (basically, when I push changes, GitHub rebuilds the book from scratch automatically).\nI found an issue describing the same segfault issue I had been getting, and so I made a post there with a new github repository containing a minimal working example that I set up to test the problem.\nWithin 24h, I had gotten replies from people working at RStudio, and one of them had diagnosed the problem. After I asked a few more questions, one of them submitted a pull request to my repository with a solution.\nI didn‚Äôt know enough python or enough about GitHub Actions to diagnose the problem myself, but because I managed to create a reproducible example, I got the answers I needed from people with more experience.\n\n\n\n\n\n\n\n\n\n\n\nTry It Out\n\n\n\nUse this list of StackOverflow posts to try out your new debugging techniques. Can you figure out what‚Äôs wrong? What information would you need from the poster in order to come up with a solution? How much time did you spend trying to figure out what the poster was actually asking?",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#debugging-tools",
    "href": "part-gen-prog/06-debugging.html#debugging-tools",
    "title": "14¬† Debugging",
    "section": "\n14.6 Debugging Tools",
    "text": "14.6 Debugging Tools\nNow that we‚Äôve discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging.\n\n14.6.1 Low tech debugging with print() and other tools\nSometimes called ‚Äútracing‚Äù techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. When the code is executed, you get a window into what the variables look like during execution.\nThis is called print debugging and it is an incredibly useful tool.\n\n\n\n\n\n\nExample: Nested Functions\n\n\n\n\n\nR\nPython\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n      z + y\n    }\n    cc(3) + 2\n  }\n  x + bb(4)\n}\n\naa(5)\n## [1] 14\n\nand the goal is to understand what‚Äôs happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  print(paste(\"Entering aa(). x = \", x))\n  bb &lt;- function(y) {\n    print(paste(\"Entering bb(). x = \", x, \"y = \", y))\n    cc &lt;- function(z) {\n      print(paste(\"Entering cc(). x = \", x, \"y = \", y, \"z = \", z))\n      cres &lt;- z + y\n      print(paste(\"Returning\", cres, \"from cc()\"))\n      cres\n    }\n    bres &lt;- cc(3) + 2\n    print(paste(\"Returning\", bres, \"from bb()\"))\n    bres\n  }\n  ares &lt;- x + bb(4)\n  print(paste(\"Returning\",ares, \"from aa()\"))\n  ares\n}\n\naa(5)\n## [1] \"Entering aa(). x =  5\"\n## [1] \"Entering bb(). x =  5 y =  4\"\n## [1] \"Entering cc(). x =  5 y =  4 z =  3\"\n## [1] \"Returning 7 from cc()\"\n## [1] \"Returning 9 from bb()\"\n## [1] \"Returning 14 from aa()\"\n## [1] 14\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      return z + y\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## 14\n\nand the goal is to understand what‚Äôs happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  print(\"Entering aa(). x = \" + str(x))\n  def bb(y):\n    print(\"Entering bb(). x = \" + str(x) + \", y = \" + str(y))\n    def cc(z):\n      print(\"Entering cc(). x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))\n      cres = z + y\n      print(\"Returning \" + str(cres) + \" from cc()\")\n      return cres\n    bres = cc(3) + 2\n    print(\"Returning \" + str(bres) + \" from bb()\")\n    return bres\n  ares = x + bb(4)\n  print(\"Returning \" + str(ares) + \" from aa()\")\n  return ares\n\naa(5)\n## Entering aa(). x = 5\n## Entering bb(). x = 5, y = 4\n## Entering cc(). x = 5, y = 4, z = 3\n## Returning 7 from cc()\n## Returning 9 from bb()\n## Returning 14 from aa()\n## 14\n\n\n\n\n\n\nFor more complex data structures, it can be useful to add str(), head(), or summary() functions.\n\n\n\n\n\n\nReal world example: Web Scraping\n\n\n\nIn fall 2020, I wrote a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the ‚Äútag‚Äù argument, that had the ‚Äúnode‚Äù argument as a descendant. I used print debugging to show the sequence of tags on the page.\nI was assuming that the order of the parents would be ‚Äúhtml‚Äù, ‚Äúbody‚Äù, ‚Äúdiv‚Äù, ‚Äútable‚Äù, ‚Äútbody‚Äù, ‚Äútr‚Äù - descending from outer to inner (if you know anything about HTML/XML structure).\nTo prevent the site from changing on me (as websites tend to do‚Ä¶), I‚Äôve saved the HTML file here.\n\n\nR\nPython\n\n\n\n\nlibrary(xml2) # read html\n\nsearch_for_parent &lt;- function(node, tag) {\n  # Get all of the parent nodes \n  parents &lt;- xml2::xml_parents(node)\n  # Get the tags of every parent node\n  tags &lt;- purrr::map_chr(parents, rvest::html_name)\n  print(tags)\n  \n  # Find matching tags\n  matches &lt;- which(tags == tag)\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match &lt;- min(matches)\n  if (length(matches) == 1) return(parents[min_match]) else return(NULL)\n}\n\npage &lt;- read_html(\"https://srvanderplas.github.io/stat-computing-r-python/files/realclearpolitics_frag.html\")\n# find all poll results in any table\npoll_results &lt;- xml_find_all(page, \"//td[@class='lp-results']\") \n# find the table that contains it\nsearch_for_parent(poll_results[1], \"table\") \n## [1] \"tr\"    \"tbody\" \"table\" \"div\"   \"body\"  \"html\" \n## [1] 3\n## {xml_nodeset (1)}\n## [1] &lt;table cellpadding=\"2\" cellspacing=\"0\" class=\"sortable\"&gt;\\n&lt;thead&gt;&lt;tr clas ...\n\n\n\nYou may need to pip install lxml requests bs4 to run this code.\n\n# !pip install lxml requests bs4\nfrom bs4 import BeautifulSoup\nimport requests as req\nimport numpy as np\n\n\ndef search_for_parent(node, tag):\n  # Get all of the parent nodes\n  parents = node.find_parents()\n  # get tag type for each parent node\n  tags = [x.name for x in parents]\n  print(tags)\n  \n  # Find matching tags\n  matches = np.array([i for i, val in enumerate(tags) if val == tag])\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match = np.min(matches)\n  if matches.size == 1:\n    ret = parents[min_match]\n  \n  return ret\n\n\nhtml_file = open('shorturl.at/jkS59', 'r')\n## FileNotFoundError: [Errno 2] No such file or directory: 'shorturl.at/jkS59'\npage = html_file.read() \n## NameError: name 'html_file' is not defined\n# Read the page as HTML\nsoup = BeautifulSoup(page, 'html')\n## NameError: name 'page' is not defined\n# Find all poll results in any table\npoll_results = soup.findAll('td', {'class': 'lp-results'})\n## NameError: name 'soup' is not defined\n# Find the table that contains the first poll result\nsearch_for_parent(poll_results[0], 'table')\n## NameError: name 'poll_results' is not defined\n\n\n\n\nBy printing out all of the tags that contain node, I could see the order ‚Äì inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node.\n\n\n\n\n\n\n\n\nTry it out: Hurricanes in R\n\n\n\nNot all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully.\n\n\nSetup\nBuggy code\nSolution 1: Identification\nSolution 2: Fixing\nSolution 3: Verifying\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(maps)\nlibrary(ggthemes)\nworldmap &lt;- map_data(\"world\")\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\n\n\nThe code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what‚Äôs wrong with my code.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nfor (i in 1:5) {\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, lets split the setup from the loop.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nprint(basemap) # make sure the basemap is fine\n\n\n\n\n\n\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\nstr(storms) # make sure the data exists and is formatted as expected\n## tibble [19,537 √ó 13] (S3: tbl_df/tbl/data.frame)\n##  $ name                        : chr [1:19537] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n##  $ year                        : num [1:19537] 1975 1975 1975 1975 1975 ...\n##  $ month                       : num [1:19537] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day                         : int [1:19537] 27 27 27 27 28 28 28 28 29 29 ...\n##  $ hour                        : num [1:19537] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat                         : num [1:19537] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n##  $ long                        : num [1:19537] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n##  $ status                      : Factor w/ 9 levels \"disturbance\",..: 7 7 7 7 7 7 7 7 8 8 ...\n##  $ category                    : num [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n##  $ wind                        : int [1:19537] 25 25 25 25 25 25 25 30 35 40 ...\n##  $ pressure                    : int [1:19537] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n##  $ tropicalstorm_force_diameter: int [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n##  $ hurricane_force_diameter    : int [1:19537] NA NA NA NA NA NA NA NA NA NA ...\n\nEverything looks ok in the setup chunk‚Ä¶\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, so from this we can see that something is going wrong with our filter statement - we have no rows of data.\n\n\n\nhead(storms)\n## # A tibble: 6 √ó 13\n##   name   year month   day  hour   lat  long status       category  wind pressure\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n## 1 Amy    1975     6    27     0  27.5 -79   tropical de‚Ä¶       NA    25     1013\n## 2 Amy    1975     6    27     6  28.5 -79   tropical de‚Ä¶       NA    25     1013\n## 3 Amy    1975     6    27    12  29.5 -79   tropical de‚Ä¶       NA    25     1013\n## 4 Amy    1975     6    27    18  30.5 -79   tropical de‚Ä¶       NA    25     1013\n## 5 Amy    1975     6    28     0  31.5 -78.8 tropical de‚Ä¶       NA    25     1012\n## 6 Amy    1975     6    28     6  32.4 -78.7 tropical de‚Ä¶       NA    25     1012\n## # ‚Ñπ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;\n\nWhoops. I meant ‚Äúcategory‚Äù when I typed ‚Äústatus‚Äù.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 13 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, that‚Äôs something, at least. We now have some data for category 1 storms‚Ä¶\n\nfilter(storms, year == 2013) %&gt;%\n  # Get max category for each named storm\n  group_by(name) %&gt;%\n  filter(category == max(category)) %&gt;%\n  ungroup() %&gt;%\n  # See what categories exist\n  select(name, category) %&gt;%\n  unique()\n## # A tibble: 0 √ó 2\n## # ‚Ñπ 2 variables: name &lt;chr&gt;, category &lt;dbl&gt;\n\nIt looks like 2013 was just an incredibly quiet year for tropical activity.\n\n\n2013 may have been a quiet year for tropical activity in the Atlantic, but 2004 was not. So let‚Äôs just make sure our code works by checking out 2004.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2004) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 49 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 51 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 43 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 49 ncol 13\"\n\n\n\n\n\n\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 12 ncol 13\"\n\n\n\n\n\n\n\nIf we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code).\n\nfor (i in 1:5) {\n  # print(paste0(\"Category \", i, \" storms\"))\n  \n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  # print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n  #       # str(subdata) works too, but produces more clutter. I started\n  #       # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  \n  if (nrow(subdata) &gt; 0) print(plot) \n}\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you‚Äôve found your problem, go back and delete or comment out your print statements, as they‚Äôre no longer necessary. If you think you may need them again, comment them out, otherwise, just delete them so that your code is neat, clean, and concise.\n\n14.6.2 After an error has occurred - traceback()\n\ntraceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This may help you identify which function is actually causing the problem, which is especially useful when you have nested functions or are using package functions that depend on other packages.\n\n14.6.2.1 Using traceback\n\n\nR\nPython\n\n\n\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n     stop('there was a problem')  # This generates an error\n    }\n    cc()\n  }\n  bb()\n}\n\naa()\n## Error in cc(): there was a problem\n\nFor more information, you could run traceback\n\ntraceback()\n\nWhich will provide the following output:\n4: stop(\"there was a problem\") at #4\n3: c() at #6\n2: b() at #8\n1: a()\nReading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It‚Äôs even kind enough to tell us that the error occurred at line 4 of the code.\nIf you are running this code interactively in RStudio, it‚Äôs even easier to run traceback() by clicking on the ‚ÄúShow Traceback‚Äù option that appears when there is an error.\n\n\nBoth Show Traceback and Rerun with Debug are useful tools\n\nIf you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. \n\n\n\nimport sys,traceback\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      try: \n        return y + z + tuple()[0] # This generates an error\n      except IndexError:\n        exc_type, exc_value, exc_tb = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_tb, file = sys.stdout)\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\nPython‚Äôs traceback information is a bit more low-level and requires a bit more from the programmer than R‚Äôs version.\n\n\n\n\n14.6.3 Interactive Debugging\n\n\n\n\n\n\nR browser()\nPython\n\n\n\nThe browser() function is useful for debugging your own code. If you‚Äôre writing a function and something isn‚Äôt working quite right, you can insert a call to browser() in that function, and examine what‚Äôs going on.\n\n\n\n\n\n\nExample : browser()\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in R.\nI start with\n\nlibrary(png)\nlibrary(xml2)\nlibrary(dplyr)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  \n  readPNG(source = image)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n## Error in readPNG(source = image): unable to open //imgs.xkcd.com/comics/uncanceled_units.png\n\nHere‚Äôs the final function\n\nlibrary(png)\nlibrary(xml2)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  \n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  # Fix image address so that we can access the image\n  image &lt;- substr(image, 3, nchar(image))\n  \n  # Download the file to a temp file and read from there\n  file_location &lt;- tempfile(fileext = \".png\")\n  download.file(image, destfile = file_location, quiet = T)\n  \n  readPNG(source = file_location)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\n\n\nIn python, the equivalent interactive debugger is ipdb. You can install it with pip install ipdb.\nIf you want to run Python in the interactive ipython console, then you can invoke the ipdb debugging with %debug get_xkcd(). This is similar to browser() in R. If you‚Äôre working in Python in RStudio, though, you have to get into debug mode in a more involved way.\nTo run code using ipdb when your code hits an error, add from ipdb import launch_ipdb_on_exception to the top of your python code chunk. Then, at the bottom, put any lines that may trigger the error after these two lines:\nif __name__ == \"__main__\":\n  with launch_ipdb_on_exception():\n    &lt;your properly indented code goes here&gt;\nThis ensures that ipdb is launched when an error is reached.\n\n\n\n\n\n\nExample using ipdb\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in python.\nI start with\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## ValueError: unknown url type: '//imgs.xkcd.com/comics/uncanceled_units.png'\nplt.show()\n\n\n\n\n\n\n\nHere‚Äôs the final function\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nEach xkcd has a corresponding ID number (ordered sequentially from 1 to 2722 at the time this was written). Modify the XKCD functions above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic.\nUse interactive debugging tools to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL.\nWhat things might you add to make this function ‚Äúdefensive programming‚Äù compatible?\n\n\n\n# get the most current xkcd or the specified number\nget_xkcd &lt;- function(id = NULL) {\n  if (is.null(id)) {\n    # Have to get the location of the image ourselves\n    url &lt;- \"http://xkcd.com\"\n  } else if (is.numeric(id)) {\n    url &lt;- paste0(\"http://xkcd.com/\", id, \"/\")\n  } else {\n    # only allow numeric or null input\n    stop(\"To get current xkcd, pass in NULL, otherwise, pass in a valid comic number\")\n  }\n\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  # Fix image address so that we can access the image\n  image &lt;- substr(image, 3, nchar(image)) # cut the first 2 characters off\n\n  # make temp file\n  location &lt;- tempfile(fileext = \"png\")\n  download.file(image, destfile = location, quiet = T)\n\n  # This checks to make sure we saved the file correctly\n  if (file.exists(location)) {\n    readPNG(source = location)\n  } else {\n    # Give a good informative error message\n    stop(paste(\"Something went wrong saving the image at \", image, \" to \", location))\n  }\n}\n\nget_xkcd(2259) %&gt;%\n  as.raster() %&gt;% \n  plot()\n\n\n\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd(id=''):\n  image = 0 # Defining a placeholder\n  \n  if id == '':\n    # Have to get the location of the image ourselves\n    url = \"http://xkcd.com\"\n  elif id.isnumeric():\n    url = \"http://xkcd.com/\" + id + \"/\"\n  else:\n    # only allow numeric or null input\n    raise TypeError(\"To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\")\n  \n  # Print debugging left in for your amusement\n  # print(type(id))\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imnode = soup.select('#comic &gt; img')\n  \n  try:\n    imlink = imnode[0].get('src')\n  except:\n    raise Exception(\"No comic could be found with number \" + id + \" (url = \"+ url+ \" )\")\n  \n  try: \n    # Format as a numpy array\n    image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n    return image\n  except: \n    raise Exception(\"Reading the image failed. Check to make sure an image exists at \" + url)\n    return(None)\n\n\nres = get_xkcd('')\nplt.imshow(res)\nplt.show()\n\n\n\n\n\n\nres = get_xkcd('3000')\n\nres = get_xkcd('abcd')\n\nTypeError: To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\n\n\n\n\n\n\n\n\n14.6.4 R debug()\n\nIn the traceback() Rstudio output, the other option is ‚Äúrerun with debug‚Äù. In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what‚Äôs going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow.\ndebug() is most useful when you‚Äôre working with code that you didn‚Äôt write yourself. So, if you can‚Äôt change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function‚Äôs source code.\n\n\n\n\n\n\ndebug() example\n\n\n\n\n\n\ndata(iris)\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n## \n## Call:\n## lm(formula = Species ~ ., data = iris)\n## \n## Residuals:\n## Error in quantile.default(resid): (unordered) factors are not allowed\n\nWe get this weird warning, and then an error about factors when we use summary() to look at the coefficients.\n\ndebug(lm) # turn debugging on\n\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n\nundebug(lm) # turn debugging off\n\n\n\nThe first thing I see when I run lm after turning on debug (screenshot)\n\n\n\nThe variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‚Äòdrive‚Äô through the function\n\nAfter pressing ‚Äúnext‚Äù a few times, you can see that I‚Äôve stepped through the first few lines of the lm function.\n\n\nStepping through the function. The arrow on the left side in the editor window shows which line of code we‚Äôre currently at.\n\nWe can see that once we‚Äôre at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we‚Äôve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it‚Äôs a factor, so our results aren‚Äôt going to mean much. We were using the function wrong.\nWe probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened.\n\n\nI can hit ‚ÄúStop‚Äù or type ‚ÄúQ‚Äù to exit the debug environment.\n\nBut, until I run undebug(lm), every call to lm will take me into the debug window.\n\n\n\nundebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run.\n\n\n\n\n\n\nTry it out: debug in R\n\n\n\n\n\nProblem\nSolution\n\n\n\nlarger(x, y) is supposed to return the elementwise maximum of two vectors.\n\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  x[y.is.bigger] &lt;- y[y.is.bigger] \n  x\n} \n\nlarger(c(1, 5, 10), c(2, 4, 11))\n## [1]  2  5 11\n\n\nlarger(c(1, 5, 10), 6)\n## [1]  6 NA 10\n\nWhy is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it.\n\n\nI‚Äôll replicate ‚Äúdebug‚Äù in non-interactive mode by setting up an environment where x and y are defined\n\n\nx &lt;- c(1, 5, 10)\ny &lt;- 6\n\n# Inside of larger() with x = c(1, 5, 10), y = 6\n(y.is.bigger &lt;- y &gt; x ) # putting something in () prints it out\n## [1]  TRUE  TRUE FALSE\ny[y.is.bigger] # This isn't quite what we were going for, but it's what's causing the issue\n## [1]  6 NA\nx[y.is.bigger] # What gets replaced\n## [1] 1 5\n\n\n# Better option\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  ifelse(y.is.bigger, y, x)\n}",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#sec-debugging-refs",
    "href": "part-gen-prog/06-debugging.html#sec-debugging-refs",
    "title": "14¬† Debugging",
    "section": "\n14.7 References",
    "text": "14.7 References\n\n\n\n\n[1] \nWikipedia Contributors, ‚ÄúDefensive programming,‚Äù Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[2] \nH. Wickham et al., ‚ÄúWelcome to the tidyverse,‚Äù Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686. \n\n\n[3] \nK. Ushey, Renv: Project environments. 2022 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[4] \nH. Wickham and J. Bryan, R Packages: Organize, Test, Document, and Share Your Code, 1st ed. Sebastopol, CA: O‚ÄôReilly, 2015 [Online]. Available: https://r-pkgs.org/. [Accessed: Sep. 23, 2022]\n\n\n[5] \nT. Beuzen and T. Timbers, Python Packages, 1st edition. Boca Raton: Chapman; Hall/CRC, 2022 [Online]. Available: https://py-pkgs.org/\n\n\n\n[6] \nJ. Evans, ‚ÄúA debugging manifesto https://t.co/3eSOFQj1e1,‚Äù Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570060516839641092. [Accessed: Sep. 21, 2022]\n\n\n[7] \nNasser_Junior, ‚ÄúUser.fist_name https://t.co/lxrf3IFO4x,‚Äù Twitter. Aug. 2020 [Online]. Available: https://twitter.com/Nasser_Junior/status/1295805928315531264. [Accessed: Sep. 21, 2022]\n\n\n[8] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nJ. Evans, ‚ÄúDebugging strategy: Reread the error message https://t.co/2BZHhPg04h,‚Äù Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570463473011920897. [Accessed: Sep. 21, 2022]\n\n\n[10] \nJ. Evans, ‚ÄúDebugging strategy: Shorten your feedback loop https://t.co/1cByDlafsK,‚Äù Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1549164800978059264. [Accessed: Sep. 21, 2022]\n\n\n[11] \nJ. Evans, ‚ÄúDebugging strategy: Write a tiny program https://t.co/Kajr5ZyeIp,‚Äù Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1547247776001654786. [Accessed: Sep. 21, 2022]\n\n\n[12] \nJ. Evans, ‚ÄúDebugging strategy: Change working code into broken code https://t.co/1T5uNDDFs0,‚Äù Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1545099244238946304. [Accessed: Sep. 21, 2022]\n\n\n[13] \nJ. Evans, ‚ÄúDebugging strategy: Come up with one question https://t.co/2Lytzl4laQ,‚Äù Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1554120424602193921. [Accessed: Sep. 21, 2022]\n\n\n[14] \nJ. Evans, ‚ÄúDebugging strategy: Write a unit test https://t.co/mC01DBNyM3,‚Äù Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1561718747504803842. [Accessed: Sep. 21, 2022]\n\n\n[15] \nT. Monteiro, ‚ÄúImprove how you code: Understanding rubber duck debugging. Duckly blog,‚Äù Oct. 31, 2019. [Online]. Available: https://duckly.com/blog/improve-how-to-code-with-rubber-duck-debugging/. [Accessed: Jan. 11, 2023]\n\n\n[16] \nS. Grimes, ‚ÄúThis 500-Year-Old Piece of Advice Can Help You Solve Your Modern Problems,‚Äù Forge. Dec. 2019 [Online]. Available: https://forge.medium.com/the-500-year-old-piece-of-advice-that-will-change-your-life-1e580f115731. [Accessed: Sep. 21, 2022]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#footnotes",
    "href": "part-gen-prog/06-debugging.html#footnotes",
    "title": "14¬† Debugging",
    "section": "",
    "text": "Some people use cats, but I find that they don‚Äôt meet the nonjudgmental criteria. Of course, they‚Äôre equally judgmental whether your code works or not, so maybe that works if you‚Äôre a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you‚Äôre upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html",
    "href": "part-gen-prog/07-prog-data.html",
    "title": "15¬† Programming With Data",
    "section": "",
    "text": "15.1  Objectives",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#objectives",
    "href": "part-gen-prog/07-prog-data.html#objectives",
    "title": "15¬† Programming With Data",
    "section": "",
    "text": "Write functions to create simple plots and data summaries\nApply syntax knowledge to reference variables and observations in common data structures\nCreate new variables and columns or reformat existing columns in provided data structures",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#artwork-dimensions",
    "href": "part-gen-prog/07-prog-data.html#artwork-dimensions",
    "title": "15¬† Programming With Data",
    "section": "\n15.2 Artwork Dimensions",
    "text": "15.2 Artwork Dimensions\nThe Tate Art Museum assembled a collection of 70,000 artworks (last updated in 2014). They cataloged information including accession number, artwork dimensions, units, title, date, medium, inscription, and even URLs for images of the art.\n\n15.2.1 Reading in the Data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\nartwork &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\nimport pandas as pd\nartwork = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\n\n15.2.2 Basic Summaries\nWhen you first access a new dataset, it‚Äôs fun to explore it a bit. I‚Äôve shown a summary of the variables (character variables summarized with completion rates and # unique values, numeric variables summarized with quantiles and mean/sd) generated using the R skimr and Python skimpy packages (which we‚Äôll talk about in the next chapter).\n\n\nR\nPython (pandas)\nPython (skimpy)\n\n\n\nYou may need to run install.packages(\"skimr\") in the R terminal if you have not used the package before.\n\nlibrary(skimr)\nskim(artwork)\n\n\nData summary\n\n\nName\nartwork\n\n\nNumber of rows\n69201\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nlogical\n1\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\naccession_number\n0\n1.00\n6\n7\n0\n69201\n0\n\n\nartist\n0\n1.00\n4\n120\n0\n3336\n0\n\n\nartistRole\n0\n1.00\n5\n24\n0\n19\n0\n\n\ntitle\n0\n1.00\n1\n320\n0\n43529\n0\n\n\ndateText\n0\n1.00\n4\n75\n0\n2736\n0\n\n\nmedium\n6384\n0.91\n3\n120\n0\n3401\n0\n\n\ncreditLine\n3\n1.00\n14\n820\n0\n3209\n0\n\n\ndimensions\n2433\n0.96\n4\n248\n0\n25575\n0\n\n\nunits\n3341\n0.95\n2\n2\n0\n1\n0\n\n\ninscription\n62895\n0.09\n14\n14\n0\n1\n0\n\n\nthumbnailUrl\n10786\n0.84\n55\n57\n0\n58415\n0\n\n\nurl\n0\n1.00\n48\n134\n0\n69201\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nthumbnailCopyright\n69201\n0\nNaN\n:\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1.00\n39148.03\n25980.47\n3\n19096.00\n37339\n54712\n129068\n‚ñá‚ñá‚ñÖ‚ñÅ‚ñÅ\n\n\nartistId\n0\n1.00\n1201.06\n2019.42\n0\n558.00\n558\n1137\n19232\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n5397\n0.92\n1867.23\n72.01\n1545\n1817.00\n1831\n1953\n2012\n‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÜ\n\n\nacquisitionYear\n45\n1.00\n1910.65\n64.20\n1823\n1856.00\n1856\n1982\n2013\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n\n\nwidth\n3367\n0.95\n323.47\n408.81\n3\n118.00\n175\n345\n11960\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nheight\n3342\n0.95\n346.44\n538.04\n6\n117.00\n190\n359\n37500\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ndepth\n66687\n0.04\n479.20\n1051.14\n1\n48.25\n190\n450\n18290\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\n\n# Base pandas\nartwork.describe()\n##                   id      artistId  ...         depth  thumbnailCopyright\n## count   69201.000000  69201.000000  ...   2514.000000                 0.0\n## mean    39148.026213   1201.063251  ...    479.197772                 NaN\n## std     25980.468687   2019.422535  ...   1051.141734                 NaN\n## min         3.000000      0.000000  ...      1.000000                 NaN\n## 25%     19096.000000    558.000000  ...     48.250000                 NaN\n## 50%     37339.000000    558.000000  ...    190.000000                 NaN\n## 75%     54712.000000   1137.000000  ...    450.000000                 NaN\n## max    129068.000000  19232.000000  ...  18290.000000                 NaN\n## \n## [8 rows x 8 columns]\n\n\n\nYou may need to run pip install skimpy in the terminal if you have not used the package before.\n\n\n# Skimpy package - like skimr\nfrom skimpy import skim\n## ModuleNotFoundError: No module named 'skimpy'\nskim(artwork)\n## NameError: name 'skim' is not defined\n\n\n\n\n\n15.2.3 Accessing one column\nFirst, let‚Äôs pull out the year for each piece of artwork in the dataset and see what we can do with it‚Ä¶\n\n\nR\nPython\n\n\n\n\nhead(artwork$year)\n## [1]   NA   NA 1785   NA 1826 1826\n\nWe reference a column of the dataset by name using dataset_name$column_name, and since our data is stored in artwork, and we want the column named year, we use artwork$year to get access to the data we want.\n\n\n\nartwork.year.head()\n## 0       NaN\n## 1       NaN\n## 2    1785.0\n## 3       NaN\n## 4    1826.0\n## Name: year, dtype: float64\n\nWe reference a column of the dataset by name using dataset_name.column_name or dataset_name['column_name'], and since our data is stored in artwork and we want the column year, we use artwork.year or artwork['year'] to access the data we want.\n\n\n\nI‚Äôve used the head command to show only the first few values (so that the output isn‚Äôt overwhelming).\n\n15.2.4 Variable Summary\nWhen we have output like this, it is useful to summarize the output in some way:\n\n\nR\nPython\n\n\n\n\nsummary(artwork$year)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    1545    1817    1831    1867    1953    2012    5397\n\nThat‚Äôs much less output, but we might want to instead make a chart:\n\nhist(artwork$year, breaks = 30)\n\n\n\n\n\n\n\n\n\n\nartwork.year.describe()\n## count    63804.000000\n## mean      1867.227823\n## std         72.012718\n## min       1545.000000\n## 25%       1817.000000\n## 50%       1831.000000\n## 75%       1953.000000\n## max       2012.000000\n## Name: year, dtype: float64\n\nThe df.describe() command provides us with a 5-number summary and then some additional statistics.\nWe can also create a chart:\n\nartwork.year.hist(bins = 30)\n\n\n\n\n\n\n\n\n\n\nPersonally, I much prefer the graphical version. It‚Äôs informative (though it does leave out NA values) and shows that there are pieces going back to the 1500s, but that most pieces were made in the early 1800s or late 1900s.\n\n15.2.5 Create a Histogram (base graphics/matplotlib)\nWe might be interested in the aspect ratio of the artwork - let‚Äôs take a look at the input variables and define new variables related to aspect ratio(s).\n\n\nR\nPython\n\n\n\n\npar(mfrow=c(1, 3)) # 3 plots on one row\nhist(artwork$width, main = \"width\", breaks = 30)\nhist(artwork$depth, main = \"depth\", breaks = 30)\nhist(artwork$height, main = \"height\", breaks = 30)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=3) # 3 subplots\n\nartwork.width.hist(bins = 30, ax = axes[0])\nartwork.depth.hist(bins = 30, ax = axes[1])\nartwork.height.hist(bins = 30, ax= axes[2])\n\n# Set subplot titles\naxes[0].title.set_text(\"width\")\naxes[1].title.set_text(\"depth\")\naxes[2].title.set_text(\"height\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSo all of our variables are skewed quite a bit, and we know from the existence of the units column that they may not be in the same unit, either.\n\n15.2.6 Summary Tables\nLet‚Äôs make a table of the units column so that we can see what the frequency of various units are in the dataset.\n\n\nR\nPython\n\n\n\n\ntable(artwork$units, useNA = 'ifany')\n## \n##    mm  &lt;NA&gt; \n## 65860  3341\n\n\n\n\nartwork.units.value_counts(dropna=False)\n## units\n## mm     65860\n## NaN     3341\n## Name: count, dtype: int64\n\n\n\n\nEverything that has specified units is in mm. That makes things easier.\n\n15.2.7 Defining a new variable\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\naspect_hw &lt;- artwork$height/artwork$width\npar(mfrow = c(1, 2))\nhist(aspect_hw, breaks = 30)\nhist(log(aspect_hw), breaks = 30)\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nfig, axes = plt.subplots(nrows=1, ncols=2) # 2 subplots\n\naspect_hw = artwork.height/artwork.width\naspect_hw.hist(bins = 30, ax = axes[0])\nnp.log(aspect_hw).hist(bins = 30, ax = axes[1])\n\n\n\n\n\n\n\n\n\n\nMost things are pretty square-ish, but there are obviously quite a few exceptions in both directions.\nThe one problem with how we‚Äôve done this is that we now have a data frame with all of our data in it, and a separate variable aspect_hw, that is not attached to our data frame. That‚Äôs not ideal - it‚Äôs easy to lose track of the variable, it‚Äôs easy to accidentally ‚Äúsort‚Äù the variable so that the row order isn‚Äôt the same as in the original data frame‚Ä¶ there are all sorts of potential issues.\n\n15.2.8 Adding a new column\nThe better way to define a new variable is to add a new column to the data frame:\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\nartwork$aspect_hw &lt;- artwork$height/artwork$width\n\n\n\n\nartwork['aspect_hw'] = artwork.height/artwork.width\n\nNote that when you create a new column in a pandas dataframe, you have to use df['colname'] on the left hand side, even if you use df.colname syntax on the right hand side.\n\n\n\n(We‚Äôll learn a shorter way to do this later, but this is functional, if not pretty, for now).\nThe downside to this is that we have to write out artwork$aspect_hw or artwork.aspect_hw each time we want to reference the variable. That is a pain, but one that‚Äôs relatively temporary (we‚Äôll get to a better way to do this in a couple of weeks). A little bit of extra typing is definitely worth it if you don‚Äôt lose data you want to keep.\n\n\n\n\n\n\nAssign your calculations to a variable or column!\n\n\n\nOne mistake I see people make frequently is to calculate height/width, but then not assign that value to a variable.\nIf you‚Äôre not using &lt;- in R1 or = in Python, then you‚Äôre not saving that information to be referenced later - you‚Äôre just calculating values temporarily and possibly printing them as output.\n\n\n\n15.2.9 Conclusions\nIt‚Äôs important to keep track of where you‚Äôre putting the pieces you create during an analysis - just as important as keeping track of the different sub-components when you‚Äôre putting a lego set together or making a complex recipe in the kitchen. Forgetting to assign your calculation to a variable is like dumping your glaze down the sink or throwing that small lego component into the trash.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#dogs-of-nyc",
    "href": "part-gen-prog/07-prog-data.html#dogs-of-nyc",
    "title": "15¬† Programming With Data",
    "section": "\n15.3 Dogs of NYC",
    "text": "15.3 Dogs of NYC\nNew York City provides a whole host of open-data resources, including a dataset of dogs licensed in the city on an annual basis (link is to the NYC Open Data Page).\nCSV link (this data is ~23 MB)\n\n15.3.1 Read in data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\n\nif (!file.exists(\"../data/NYC_dogs.csv\")) {\n  # if the file doesn't exist, download it!\n  download.file(\n    \"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\", # url for download\n    destfile = \"../data/NYC_dogs.csv\", # location to store the file\n    mode = \"wb\" # need this to get downloads to work on windows\n  )\n}\n\ndogs &lt;- read_csv(\"../data/NYC_dogs.csv\")\nhead(dogs)\n## # A tibble: 6 √ó 8\n##   AnimalName AnimalGender AnimalBirthYear BreedName    ZipCode LicenseIssuedDate\n##   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n## 1 PAIGE      F                       2014 American Pi‚Ä¶   10035 09/12/2014       \n## 2 YOGI       M                       2010 Boxer          10465 09/12/2014       \n## 3 ALI        M                       2014 Basenji        10013 09/12/2014       \n## 4 QUEEN      F                       2013 Akita Cross‚Ä¶   10013 09/12/2014       \n## 5 LOLA       F                       2009 Maltese        10028 09/12/2014       \n## 6 IAN        M                       2006 Unknown        10013 09/12/2014       \n## # ‚Ñπ 2 more variables: LicenseExpiredDate &lt;chr&gt;, `Extract Year` &lt;dbl&gt;\n\n\n\n\nfrom os.path import exists # to test whether files exist\nimport pandas as pd\nimport requests # to download a file\n## ModuleNotFoundError: No module named 'requests'\n\nif ~exists(\"../data/NYC_dogs.csv\"):\n  response = requests.get(\"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\")\n  open(\"../data/NYC_dogs.csv\", \"wb\").write(response.content)\n## NameError: name 'requests' is not defined\n\ndogs = pd.read_csv(\"../data/NYC_dogs.csv\")\ndogs.head()\n##   AnimalName AnimalGender  ... LicenseExpiredDate Extract Year\n## 0      PAIGE            F  ...         09/12/2017         2016\n## 1       YOGI            M  ...         10/02/2017         2016\n## 2        ALI            M  ...         09/12/2019         2016\n## 3      QUEEN            F  ...         09/12/2017         2016\n## 4       LOLA            F  ...         10/09/2017         2016\n## \n## [5 rows x 8 columns]\n\n\n\n\n\n15.3.2 Work with Dates\nOne thing we might want to do first is to transform the license dates (LicenseIssuedDate, LicenseExpiredDate) into actual dates instead of characters.\n\n\nR\nPython\n\n\n\nWe will use the lubridate package to do this, because it is designed to make working with dates and times very easy.\nYou may need to run install.packages(\"lubridate\") in the R console if you have not used the package before.\n\nlibrary(lubridate)\nhead(dogs$LicenseExpiredDate) # Dates are in month-day-year format\n## [1] \"09/12/2017\" \"10/02/2017\" \"09/12/2019\" \"09/12/2017\" \"10/09/2017\"\n## [6] \"10/30/2019\"\n\ndogs$LicenseExpiredDate &lt;- mdy(dogs$LicenseExpiredDate)\ndogs$LicenseIssuedDate &lt;- mdy(dogs$LicenseIssuedDate)\n\nhead(dogs$LicenseExpiredDate)\n## [1] \"2017-09-12\" \"2017-10-02\" \"2019-09-12\" \"2017-09-12\" \"2017-10-09\"\n## [6] \"2019-10-30\"\n\n\n\nYou may need to run pip install datetime in the terminal if you have not used the package before.\n\nfrom datetime import date\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # Before\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         09/12/2017        09/12/2014\n## 1         10/02/2017        09/12/2014\n## 2         09/12/2019        09/12/2014\n## 3         09/12/2017        09/12/2014\n## 4         10/09/2017        09/12/2014\n\nformat_str = \"%m/%d/%Y\" # date format in the dataset\n\ndogs['LicenseExpiredDate'] = pd.to_datetime(dogs.LicenseExpiredDate, format = format_str)\ndogs['LicenseIssuedDate'] = pd.to_datetime(dogs.LicenseIssuedDate, format = format_str)\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # After\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         2017-09-12        2014-09-12\n## 1         2017-10-02        2014-09-12\n## 2         2019-09-12        2014-09-12\n## 3         2017-09-12        2014-09-12\n## 4         2017-10-09        2014-09-12\n\n\n\n\nIt might be interesting to see when licenses have been issued over time, so let‚Äôs make a histogram. This time, I‚Äôm going to use ggplot graphics with the ggplot2 package in R and the plotnine package in python (which is the python version of the R package).\n\n15.3.3 Create a Histogram (ggplot2/plotnine)\n\n\nR\nPython\n\n\n\nYou may need to run install.packages(\"ggplot2\") in the R console if you have not used ggplot2 before.\n\nlibrary(ggplot2)\n\nggplot(\n  data = dogs, \n  aes(x = LicenseIssuedDate) # Specify we want LicenseIssueDate on the x-axis\n) + \n  geom_histogram() # Create a histogram\n\n\n\n\n\n\n\n\n\nYou may need to run pip install plotnine in the terminal if you have not used the package before.\n\nfrom plotnine import *\n\n(\n  ggplot(mapping = aes(x = 'LicenseIssuedDate'), data = dogs) + \n  geom_histogram() # Create a histogram\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nThere is an interesting periodicity to the license issue dates.\n\n15.3.4 Compute License Length\nI‚Äôm also curious about how long a license tends to be held for - we can get this information by subtracting the issue date from the expiration date.\n\n\nR\nPython\n\n\n\n\ndogs$LicenseLength &lt;- dogs$LicenseExpiredDate - dogs$LicenseIssuedDate\nsummary(dogs$LicenseLength)\n##   Length    Class     Mode \n##   616890 difftime  numeric\nhead(dogs$LicenseLength)\n## Time differences in days\n## [1] 1096 1116 1826 1096 1123 1874\n\nWe can see that directly subtracting date-times gives us a license length in days. That‚Äôs useful enough, I guess, but it might be more useful in years‚Ä¶ unfortunately, that‚Äôs not an option for difftime()\n\nlibrary(ggplot2)\ndogs$LicenseLength &lt;- difftime(dogs$LicenseExpiredDate, dogs$LicenseIssuedDate, units = \"weeks\")\n\n# 52 weeks in a year so we'll just convert as we plot\nggplot(data = dogs, aes(x = LicenseLength / 52 )) + geom_histogram() + \n  scale_x_continuous(limits = c(0,10))\n\n\n\n\n\n\n\n\n\n\ndogs[\"License_length\"] = dogs.LicenseExpiredDate - dogs.LicenseIssuedDate\n\ndogs.License_length.describe()\n## count                         616808\n## mean     513 days 21:59:06.164122400\n## std      389 days 21:45:53.476324672\n## min                  1 days 00:00:00\n## 25%                365 days 00:00:00\n## 50%                366 days 00:00:00\n## 75%                411 days 00:00:00\n## max               7913 days 00:00:00\n## Name: License_length, dtype: object\ndogs.License_length.head()\n## 0   1096 days\n## 1   1116 days\n## 2   1826 days\n## 3   1096 days\n## 4   1123 days\n## Name: License_length, dtype: timedelta64[ns]\n\ndogs[\"License_length_yr\"] = dogs.License_length.dt.days/365.25\n\n\n(\n  ggplot(mapping = aes(x = \"License_length_yr\"), data = dogs) + \n  geom_histogram(bins = 30)+\n  scale_x_continuous(limits = (0,10))\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nIn python, we have to first access the ‚Äúdays‚Äù attribute of the timedelta64 data type (this gives us a number) using dogs.Licence_length.dt.days and then divide by 365.25 (number of days in a year, on average).\n\n\n\n\n15.3.5 Explore Boroughs\nAnother question that I have when looking at this dataset is a bit more superficial - are the characteristics of different areas different? The dogs data frame has a Borough column, but it‚Äôs not actually filled in, so we‚Äôll need to get rid of it and then add Borough back in by zip code.\nTo look at this, we‚Äôll need a bit more data. I found a list of NYC zip codes by borough, which we can merge in with the data we already have to get puppy registrations by borough. Then, we can see if e.g.¬†the top 10 breeds are different for different boroughs. To simplify this, I‚Äôm going to link to a file to merge in, and not show you the specifics of how I read the table from this site.\n\n\nR\nPython\n\n\n\n\nborough_zip &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/nyc_zip_borough.csv\")\n\n# Remove the Borough column from dogs\ndogs &lt;- dogs[, which(names(dogs) != \"Borough\")]\ndogs &lt;- merge(dogs, borough_zip, by = \"ZipCode\")\nhead(dogs)\n##   ZipCode AnimalName AnimalGender AnimalBirthYear\n## 1   10001      UMEKO            F            2015\n## 2   10001        JOY            F            2017\n## 3   10001    UNKNOWN            M            2021\n## 4   10001    UNKNOWN            M            2021\n## 5   10001      BALOO            M            2019\n## 6   10001       MOMO            M            2020\n##                        BreedName LicenseIssuedDate LicenseExpiredDate\n## 1                 Boston Terrier        2019-09-19         2020-10-16\n## 2              Yorkshire Terrier        2017-09-07         2018-09-07\n## 3  Miniature Australian Shepherd        2021-08-19         2022-08-19\n## 4                   Goldendoodle        2022-03-25         2023-03-25\n## 5             Labrador Retriever        2020-02-16         2021-02-16\n## 6 American Staffordshire Terrier        2021-12-26         2022-12-10\n##   Extract Year  LicenseLength   Borough\n## 1         2022 56.14286 weeks Manhattan\n## 2         2017 52.14286 weeks Manhattan\n## 3         2022 52.14286 weeks Manhattan\n## 4         2023 52.14286 weeks Manhattan\n## 5         2022 52.28571 weeks Manhattan\n## 6         2022 49.85714 weeks Manhattan\n\n\n\n\nborough_zip = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/nyc_zip_borough.csv\")\ndogs = dogs.drop('Borough', axis = 1) # drop borough column\n## KeyError: \"['Borough'] not found in axis\"\ndogs = pd.merge(dogs, borough_zip, on = 'ZipCode')\ndogs.head()\n##   AnimalName AnimalGender  ... License_length_yr    Borough\n## 0      PAIGE            F  ...          3.000684  Manhattan\n## 1       YOGI            M  ...          3.055441      Bronx\n## 2        ALI            M  ...          4.999316  Manhattan\n## 3      QUEEN            F  ...          3.000684  Manhattan\n## 4       LOLA            F  ...          3.074606  Manhattan\n## \n## [5 rows x 11 columns]\n\n\n\n\nNow that we have borough, let‚Äôs write a function that will take a dataset and spit out a list of the top 5 dog breeds registered in that area.\n\n\nR\nPython\n\n\n\n\ntop_5_breeds &lt;- function(data) {\n  # Inside the function, our dataset is called data, not dogs\n  tmp &lt;- table(data$BreedName) \n  return(sort(tmp, decreasing = T)[1:5]) # top 5 breeds with counts\n}\n\n\n\n\n\ndef top_5_breeds(data):\n  tmp = pd.value_counts(data.BreedName)\n  return tmp.iloc[0:5]\n\n\n\n\nNow, using that function, lets write a for loop that loops through the 5 boroughs and spits out the top 5 breeds in each borough:\n\n\nR\nPython\n\n\n\n\nboroughs &lt;- unique(borough_zip$Borough) # get a list of the 5 boroughs\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- dogs[dogs$Borough == i,]\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  print(result)\n}\n##                Breed  Freq   Borough rank\n## 1            Unknown 16477 Manhattan    1\n## 2  Yorkshire Terrier  8121 Manhattan    2\n## 3          Chihuahua  7952 Manhattan    3\n## 4           Shih Tzu  7051 Manhattan    4\n## 5 Labrador Retriever  6765 Manhattan    5\n##                Breed Freq Borough rank\n## 1            Unknown 6006  Staten    1\n## 2           Shih Tzu 3611  Staten    2\n## 3  Yorkshire Terrier 3487  Staten    3\n## 4 Labrador Retriever 2391  Staten    4\n## 5            Maltese 1856  Staten    5\n##                                  Breed Freq Borough rank\n## 1                    Yorkshire Terrier 5911   Bronx    1\n## 2                              Unknown 5276   Bronx    2\n## 3                             Shih Tzu 5025   Bronx    3\n## 4                            Chihuahua 3272   Bronx    4\n## 5 American Pit Bull Mix / Pit Bull Mix 2499   Bronx    5\n##               Breed  Freq Borough rank\n## 1           Unknown 11942  Queens    1\n## 2 Yorkshire Terrier  8022  Queens    2\n## 3          Shih Tzu  7221  Queens    3\n## 4           Maltese  4987  Queens    4\n## 5         Chihuahua  4974  Queens    5\n##                           Breed  Freq  Borough rank\n## 1                       Unknown 14685 Brooklyn    1\n## 2             Yorkshire Terrier  9314 Brooklyn    2\n## 3                      Shih Tzu  9009 Brooklyn    3\n## 4                     Chihuahua  6504 Brooklyn    4\n## 5 Labrador Retriever Crossbreed  5376 Brooklyn    5\n\n\n\n\nboroughs = borough_zip.Borough.unique()\nfor i in boroughs:\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough == @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n\n  print(result)\n##                 count  count\n## 0             Unknown  16477\n## 1   Yorkshire Terrier   8121\n## 2           Chihuahua   7952\n## 3            Shih Tzu   7051\n## 4  Labrador Retriever   6765\n##             BreedName  count    Borough  rank\n## 0             Unknown  16477  Manhattan     1\n## 1   Yorkshire Terrier   8121  Manhattan     2\n## 2           Chihuahua   7952  Manhattan     3\n## 3            Shih Tzu   7051  Manhattan     4\n## 4  Labrador Retriever   6765  Manhattan     5\n##                 count  count\n## 0             Unknown   6006\n## 1            Shih Tzu   3611\n## 2   Yorkshire Terrier   3487\n## 3  Labrador Retriever   2391\n## 4             Maltese   1856\n##             BreedName  count Borough  rank\n## 0             Unknown   6006  Staten     1\n## 1            Shih Tzu   3611  Staten     2\n## 2   Yorkshire Terrier   3487  Staten     3\n## 3  Labrador Retriever   2391  Staten     4\n## 4             Maltese   1856  Staten     5\n##                                   count  count\n## 0                     Yorkshire Terrier   5911\n## 1                               Unknown   5276\n## 2                              Shih Tzu   5025\n## 3                             Chihuahua   3272\n## 4  American Pit Bull Mix / Pit Bull Mix   2499\n##                               BreedName  count Borough  rank\n## 0                     Yorkshire Terrier   5911   Bronx     1\n## 1                               Unknown   5276   Bronx     2\n## 2                              Shih Tzu   5025   Bronx     3\n## 3                             Chihuahua   3272   Bronx     4\n## 4  American Pit Bull Mix / Pit Bull Mix   2499   Bronx     5\n##                count  count\n## 0            Unknown  11942\n## 1  Yorkshire Terrier   8022\n## 2           Shih Tzu   7221\n## 3            Maltese   4987\n## 4          Chihuahua   4974\n##            BreedName  count Borough  rank\n## 0            Unknown  11942  Queens     1\n## 1  Yorkshire Terrier   8022  Queens     2\n## 2           Shih Tzu   7221  Queens     3\n## 3            Maltese   4987  Queens     4\n## 4          Chihuahua   4974  Queens     5\n##                            count  count\n## 0                        Unknown  14685\n## 1              Yorkshire Terrier   9314\n## 2                       Shih Tzu   9009\n## 3                      Chihuahua   6504\n## 4  Labrador Retriever Crossbreed   5376\n##                        BreedName  count   Borough  rank\n## 0                        Unknown  14685  Brooklyn     1\n## 1              Yorkshire Terrier   9314  Brooklyn     2\n## 2                       Shih Tzu   9009  Brooklyn     3\n## 3                      Chihuahua   6504  Brooklyn     4\n## 4  Labrador Retriever Crossbreed   5376  Brooklyn     5\n\nMore information on pandas query function (use \\@varname to use a variable in a query).\n\n\n\nIf we wanted to save these results as a summary data frame, we could totally do that!\n\n\nR\nPython\n\n\n\n\nbreeds_by_borough &lt;- data.frame() # create a blank data frame\n\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- subset(dogs, Borough == i)\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  breeds_by_borough &lt;- rbind(breeds_by_borough, result)\n}\n\nbreeds_by_borough\n##                                   Breed  Freq   Borough rank\n## 1                               Unknown 16477 Manhattan    1\n## 2                     Yorkshire Terrier  8121 Manhattan    2\n## 3                             Chihuahua  7952 Manhattan    3\n## 4                              Shih Tzu  7051 Manhattan    4\n## 5                    Labrador Retriever  6765 Manhattan    5\n## 6                               Unknown  6006    Staten    1\n## 7                              Shih Tzu  3611    Staten    2\n## 8                     Yorkshire Terrier  3487    Staten    3\n## 9                    Labrador Retriever  2391    Staten    4\n## 10                              Maltese  1856    Staten    5\n## 11                    Yorkshire Terrier  5911     Bronx    1\n## 12                              Unknown  5276     Bronx    2\n## 13                             Shih Tzu  5025     Bronx    3\n## 14                            Chihuahua  3272     Bronx    4\n## 15 American Pit Bull Mix / Pit Bull Mix  2499     Bronx    5\n## 16                              Unknown 11942    Queens    1\n## 17                    Yorkshire Terrier  8022    Queens    2\n## 18                             Shih Tzu  7221    Queens    3\n## 19                              Maltese  4987    Queens    4\n## 20                            Chihuahua  4974    Queens    5\n## 21                              Unknown 14685  Brooklyn    1\n## 22                    Yorkshire Terrier  9314  Brooklyn    2\n## 23                             Shih Tzu  9009  Brooklyn    3\n## 24                            Chihuahua  6504  Brooklyn    4\n## 25        Labrador Retriever Crossbreed  5376  Brooklyn    5\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\n\nbreeds_by_borough[order(breeds_by_borough$rank, \n                        breeds_by_borough$Borough),]\n##                                   Breed  Freq   Borough rank\n## 11                    Yorkshire Terrier  5911     Bronx    1\n## 21                              Unknown 14685  Brooklyn    1\n## 1                               Unknown 16477 Manhattan    1\n## 16                              Unknown 11942    Queens    1\n## 6                               Unknown  6006    Staten    1\n## 12                              Unknown  5276     Bronx    2\n## 22                    Yorkshire Terrier  9314  Brooklyn    2\n## 2                     Yorkshire Terrier  8121 Manhattan    2\n## 17                    Yorkshire Terrier  8022    Queens    2\n## 7                              Shih Tzu  3611    Staten    2\n## 13                             Shih Tzu  5025     Bronx    3\n## 23                             Shih Tzu  9009  Brooklyn    3\n## 3                             Chihuahua  7952 Manhattan    3\n## 18                             Shih Tzu  7221    Queens    3\n## 8                     Yorkshire Terrier  3487    Staten    3\n## 14                            Chihuahua  3272     Bronx    4\n## 24                            Chihuahua  6504  Brooklyn    4\n## 4                              Shih Tzu  7051 Manhattan    4\n## 19                              Maltese  4987    Queens    4\n## 9                    Labrador Retriever  2391    Staten    4\n## 15 American Pit Bull Mix / Pit Bull Mix  2499     Bronx    5\n## 25        Labrador Retriever Crossbreed  5376  Brooklyn    5\n## 5                    Labrador Retriever  6765 Manhattan    5\n## 20                            Chihuahua  4974    Queens    5\n## 10                              Maltese  1856    Staten    5\n\n\n\n\nbreeds_by_borough = pd.DataFrame() # Create a blank dataframe\n\nfor i in boroughs:\n  print(i)\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough== @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n  # Append to blank dataframe\n  breeds_by_borough = breeds_by_borough.append(result)\n## AttributeError: 'DataFrame' object has no attribute 'append'\n\nbreeds_by_borough.head()\n## Empty DataFrame\n## Columns: []\n## Index: []\nbreeds_by_borough.tail()\n## Empty DataFrame\n## Columns: []\n## Index: []\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\n\nbreeds_by_borough.sort_values(['rank', 'Borough'])\n## KeyError: 'rank'\n\n\n\n\nSoon we‚Äôll learn a much shorter set of commands to get these types of summaries, but it‚Äôs important to know how a for loop connects to the concept of summarizing data by a factor (in this case, by borough).\n\n\n\n\n\n\nTry it out: NYC dogs\n\n\n\nLook at the name, age, or gender of dogs registered in NYC and see if you can come up with a similar function and way of summarizing the data in a for-loop. You may want to calculate the mean or quantiles (for numeric variables), or list the most common dog names/genders in each borough.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#swearing-in-tarantino-films",
    "href": "part-gen-prog/07-prog-data.html#swearing-in-tarantino-films",
    "title": "15¬† Programming With Data",
    "section": "\n15.4 Swearing in Tarantino Films",
    "text": "15.4 Swearing in Tarantino Films\nContent warning: This section contains analysis of swear words and deaths. I will not censor the words used in these movies, as they are legitimate data and could lead to an interesting analysis. Feel free to skip this example if it makes you uncomfortable.\n\nQuentin Jerome Tarantino (/Àåt√¶r…ônÀàtiÀêno ä/; born March 27, 1963) is an American film director, screenwriter, producer, actor, and author. His films are characterized by stylized violence, extended dialogue including a pervasive use of profanity, and references to popular culture. [1]\n\n\n15.4.1 Read in data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\n\ntarantino &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/tarantino/tarantino.csv\")\nhead(tarantino)\n## # A tibble: 6 √ó 4\n##   movie          type  word     minutes_in\n##   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n## 1 Reservoir Dogs word  dick           0.4 \n## 2 Reservoir Dogs word  dicks          0.43\n## 3 Reservoir Dogs word  fucked         0.55\n## 4 Reservoir Dogs word  fucking        0.61\n## 5 Reservoir Dogs word  bullshit       0.61\n## 6 Reservoir Dogs word  fuck           0.66\n\n\n\n\nimport pandas as pd\n\ntarantino = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/tarantino/tarantino.csv\")\ntarantino.head()\n##             movie  type      word  minutes_in\n## 0  Reservoir Dogs  word      dick        0.40\n## 1  Reservoir Dogs  word     dicks        0.43\n## 2  Reservoir Dogs  word    fucked        0.55\n## 3  Reservoir Dogs  word   fucking        0.61\n## 4  Reservoir Dogs  word  bullshit        0.61\n\n\n\n\n\n15.4.2 Create a Density Plot (ggplot2/plotnine)\n\n\nR\nPython\n\n\n\nYou may need to run install.packages(\"ggplot2\") in the R console if you have not used ggplot2 before.\n\nlibrary(ggplot2)\n\nggplot(\n  data = tarantino, \n  aes(x = minutes_in, color = type)\n) + \n  geom_density() + \n  scale_color_manual(values = c(\"black\", \"grey\")) +\n  facet_wrap(~movie)\n\n\n\n\n\n\n\n\n\nYou may need to run pip install plotnine in the terminal if you have not used the package before.\n\nfrom plotnine import *\n\nplot = ggplot(data = tarantino, mapping = aes(x = 'minutes_in', color = \"type\")) \nplot = plot + geom_density()\nplot = plot + scale_color_manual(values = [\"black\", \"grey\"]) \nplot = plot + facet_wrap(\"movie\")\nplot.show()\n\n\n\n\n\n\n\n\n\n\nSo, from these plots, we can see that in at least two movies, there are high spikes in deaths about 1/3 and 2/3 of the way in; in another movie, most of the deaths occur in the first 25 minutes. Swearing, on the other hand, seems to be fairly evenly distributed throughout the movies.\n\n15.4.3 Compare Swear Words Used by Movie\nAs there are a very large number of swear words and variants in Tarantino movies, let‚Äôs work with only the 6 most common swear words in the data set. To do this, we have to:\n\nSelect only rows that have words (as opposed to deaths)\nAssemble a list of the 6 most common words\nSelect only rows with those words\n\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n# Step 1\ntarantino_words &lt;- tarantino[tarantino$type == \"word\",]\n\n# Step 2\nword_freq &lt;- sort(table(tarantino_words$word), decreasing = T)\n# word_freq has the counts of how many times the words appear\n# we need the names that are above those counts\nswear6 &lt;- names(word_freq)[1:6]\n\n# Step 3\nword_6 &lt;- tarantino_words[tarantino_words$word %in% swear6,]\n\n\n\nggplot(\n  data = word_6, \n  aes(x = movie, fill = word)\n) + \n  geom_bar() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\n# Step 1 - remove deaths\ntarantino_words = tarantino.query(\"type == 'word'\")\n\n# Step 2 - 6 most common words\n\nplot = ggplot(tarantino, aes(x = 'minutes_in', color = 'movie'))\nplot = plot + geom_density() \nplot = plot + facet_wrap(\"type\")\n\nplot.show()\n\n\n\n\n\n\n\n\n\n\nXXX Under construction - I will add more as I get time.\n\n\n\n\n[1] \nWikipedia contributors, ‚ÄúQuentin Tarantino,‚Äù Wikipedia. Aug. 2023 [Online]. Available: https://en.wikipedia.org/wiki/Quentin_Tarantino. [Accessed: Aug. 29, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/07-prog-data.html#footnotes",
    "href": "part-gen-prog/07-prog-data.html#footnotes",
    "title": "15¬† Programming With Data",
    "section": "",
    "text": "(or =, or -&gt; if you‚Äôre a total heathen)‚Ü©Ô∏é",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Programming With Data</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/99-extra.html",
    "href": "part-gen-prog/99-extra.html",
    "title": "16¬† Other Helpful Programming Resources",
    "section": "",
    "text": "16.1 MIT‚Äôs Missing Semester\nThis set of 11 1-hour lectures [1] covers topics that will help you develop general programming/computing skills. The topics covered are (mostly) adjacent to things covered in this book (with the exception of version control), but it seems like an excellent way to bone up on skills like how to work with the command line, how to accomplish basic tasks at the command line, and various other things that students tend to struggle with but that we don‚Äôt usually have time to go over in class in great detail.",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Other Helpful Programming Resources</span>"
    ]
  },
  {
    "objectID": "part-gen-prog/99-extra.html#sec-extra-gen-prog-refs",
    "href": "part-gen-prog/99-extra.html#sec-extra-gen-prog-refs",
    "title": "16¬† Other Helpful Programming Resources",
    "section": "16.2 References",
    "text": "16.2 References\n\n\n\n\n[1] Anish Athalye, Jon Gjengset, and Jose Javier, ‚ÄúThe missing semester of your CS education. Missing semester.‚Äù [Online]. Available: https://missing.csail.mit.edu/. [Accessed: Apr. 20, 2023]",
    "crumbs": [
      "Part II: General Programming",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Other Helpful Programming Resources</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html",
    "href": "part-wrangling/01-data-input.html",
    "title": "17¬† Data Input",
    "section": "",
    "text": "17.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#objectives",
    "href": "part-wrangling/01-data-input.html#objectives",
    "title": "17¬† Data Input",
    "section": "",
    "text": "Read in data from common formats into R or Python\nIdentify delimiters, headers, and other essential components of files\n\n\n\n\n\n\n\nCheatsheets!\n\n\n\nThese may be worth printing off as you work through this module.\n\nR - tidyverse\nPython",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#overview-data-formats",
    "href": "part-wrangling/01-data-input.html#overview-data-formats",
    "title": "17¬† Data Input",
    "section": "\n17.2 Overview: Data Formats",
    "text": "17.2 Overview: Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we‚Äôll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs.\n\n[1] describes why binary file formats exist, and why they‚Äôre not necessarily optimal.\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not binary formats, but they‚Äôre also not raw text files either. They‚Äôre a hybrid - a special type of markup that is specific to the filetype and the program it‚Äôs designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there‚Äôs this amazing comic:\n\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats ‚Äì some open and easy to work with, some inpenetrable. A few which you may come across:\n\nWeb data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. See Chapter 27 for some tools to work with these files.\nSpatial files: Shapefiles are the most common version of spatial files, though there are a seemingly infinite number of different formats, and new formats pop up at the most inconvenient times. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. Chapter 28 covers some of the tools available for working with spatial data.\n\nTo be minimally functional in R and Python, it‚Äôs important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly discuss binary files and databases, but it is less critical to remember how to read these in without consulting an online reference.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#text-files",
    "href": "part-wrangling/01-data-input.html#text-files",
    "title": "17¬† Data Input",
    "section": "\n17.3 Text Files",
    "text": "17.3 Text Files\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What‚Äôs the difference, you say?\n\n17.3.1 Fixed-width files\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don‚Äôt have to read the whole thing into memory).\n\n17.3.1.1 Fixed Width File IO\n\n\nBase R\nreadr\nPython\n\n\n\nIn base R (no extra packages), you can read fixed-width files in using read.fwf, but you must specify the column breaks yourself, which can be painful.\n\n## url &lt;- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\ndata &lt;- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you.\n\n\n\n\n\n\nCode for counting field width\n\n\n\n\n\n\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp &lt;- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars &lt;- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars &lt;- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space &lt;- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space &lt;- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks &lt;- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths &lt;- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata &lt;- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names &lt;- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) &lt;- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n\n\n\n\nYou can also write fixed-width files if you really want to:\n\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\nlibrary(gdata)\nwrite.fwf(mtcars, file = tempfile())\n\n\n\nThe readr package creates data-frame like objects called tibbles (a souped-up data frame), but it is much friendlier to use.\n\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 √ó 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # ‚Ñπ 110 more rows\n## # ‚Ñπ 12 more variables: PRES &lt;dbl&gt;, SRAD &lt;dbl&gt;, TA9M &lt;dbl&gt;, WS2M &lt;dbl&gt;,\n## #   TS10 &lt;dbl&gt;, TB10 &lt;dbl&gt;, TS05 &lt;dbl&gt;, TS25 &lt;dbl&gt;, TS60 &lt;dbl&gt;, TR05 &lt;dbl&gt;,\n## #   TR25 &lt;dbl&gt;, TR60 &lt;dbl&gt;\n\n\n\nBy default, pandas‚Äô read_fwf will guess at the format of your fixed-width file.\n\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\ndata.head()\n##    STID   STNM  TIME  RELH  TAIR  WSPD  ...  TS05  TS25   TS60  TR05  TR25    TR60\n## 0  ACME  110.0   0.0  53.0  31.8   5.2  ...  31.6  25.2   21.7  3.09  2.22    1.48\n## 1  ADAX    1.0   0.0  55.0  32.4   1.0  ...  29.6  26.8 -998.0  2.61  1.88 -998.00\n## 2  ALTU    2.0   0.0  31.0  35.6   8.9  ...  30.7  26.1 -998.0  3.39  2.47 -998.00\n## 3  ALV2  116.0   0.0  27.0  35.8   6.7  ...  25.6  22.6 -998.0  2.70  1.60 -998.00\n## 4  ANT2  135.0   0.0  73.0  27.8   0.0  ...  30.2  26.8   23.8  1.96  1.73    1.33\n## \n## [5 rows x 24 columns]\n\n\n\n\n\n17.3.2 Delimited Text Files\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file.\n\n17.3.2.1 Comma Delimited Files\n\n\nBase R\nreadr\nPython\n\n\n\n\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\npokemon_info &lt;- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n  gen pokedex_no\n1   1          1\n2   1          2\n3   1          3\n4   1          3\n5   1          4\n6   1          5\n                                                               img_link\n1     https://img.pokemondb.net/sprites/sword-shield/icon/bulbasaur.png\n2       https://img.pokemondb.net/sprites/sword-shield/icon/ivysaur.png\n3      https://img.pokemondb.net/sprites/sword-shield/icon/venusaur.png\n4 https://img.pokemondb.net/sprites/sword-shield/icon/venusaur-mega.png\n5    https://img.pokemondb.net/sprites/sword-shield/icon/charmander.png\n6    https://img.pokemondb.net/sprites/sword-shield/icon/charmeleon.png\n        name variant         type\n1  Bulbasaur    &lt;NA&gt; Grass,Poison\n2    Ivysaur    &lt;NA&gt; Grass,Poison\n3   Venusaur    &lt;NA&gt; Grass,Poison\n4   Venusaur    Mega Grass,Poison\n5 Charmander    &lt;NA&gt;         Fire\n6 Charmeleon    &lt;NA&gt;         Fire\n\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\nThe most common delimited text format is CSV: comma-separated value.\n\nlibrary(readr)\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\npokemon_info &lt;- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n# A tibble: 6 √ó 6\n    gen pokedex_no img_link                                  name  variant type \n  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1     1          1 https://img.pokemondb.net/sprites/sword-‚Ä¶ Bulb‚Ä¶ &lt;NA&gt;    Gras‚Ä¶\n2     1          2 https://img.pokemondb.net/sprites/sword-‚Ä¶ Ivys‚Ä¶ &lt;NA&gt;    Gras‚Ä¶\n3     1          3 https://img.pokemondb.net/sprites/sword-‚Ä¶ Venu‚Ä¶ &lt;NA&gt;    Gras‚Ä¶\n4     1          3 https://img.pokemondb.net/sprites/sword-‚Ä¶ Venu‚Ä¶ Mega    Gras‚Ä¶\n5     1          4 https://img.pokemondb.net/sprites/sword-‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire \n6     1          5 https://img.pokemondb.net/sprites/sword-‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire \n\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\n\nimport pandas as pd\n\nurl &lt;- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\nTypeError: bad operand type for unary -: 'str'\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n\n[123 rows x 0 columns]\n\n\n\n\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can‚Äôt let it keep us from accessing the data.\n\n17.3.2.2 Other Character Delimited Files\n\n\nBase R\nreadr\nPython\n\n\n\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n## Error in download.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", : cannot open URL 'https://geonames.usgs.gov/docs/stategaz/NE_Features.zip'\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations &lt;- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n## Error in download.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", : cannot open URL 'https://geonames.usgs.gov/docs/stategaz/NE_Features.zip'\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations &lt;- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 √ó 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre‚Ä¶ Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n\nWe can actually read in the file without unzipping it, so long as we download it first - readr does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\nnebraska_locations &lt;- read_delim(\"../data/NE_Features.zip\", delim = \"|\")\n## Error: '../data/NE_Features.zip' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-wrangling').\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 √ó 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre‚Ä¶ Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\nPandas can access zipped data files and unzip them while reading the data in, so we don‚Äôt have to download the file and unzip it first.\n\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\n## urllib.error.HTTPError: HTTP Error 503: Service Unavailable\nnebraska_locations\n## NameError: name 'nebraska_locations' is not defined\n\n\n\n\n\n\n\n\n\n\nTry it out: Reading CSV files\n\n\n\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveniently available at their download page. Because these data sets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV.\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe readr package and pandas can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n\n\nlibrary(readr)\nlegosets &lt;- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"../data/lego_sets.csv\")\n\n\n\n\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"../data/lego_sets_py.csv\")",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#spreadsheets",
    "href": "part-wrangling/01-data-input.html#spreadsheets",
    "title": "17¬† Data Input",
    "section": "\n17.4 Spreadsheets",
    "text": "17.4 Spreadsheets\n\n17.4.1 Spreadsheet IO\nThis example uses from NYC SLice. The author maintains a google sheet of the slices he has photographed, which we can download as an excel sheet and import.\n\n\nR readxl\nPython\n\n\n\nIn R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/1EY3oi9ttxybG0A0Obtwey6BFu7QLqdHe02JApijgztg/export?format=xlsx\"\n# Only download the data if it doesn't exist in the data folder\nif (!file.exists(\"../data/nyc_slice.xlsx\")) {\n  download.file(url, destfile = \"../data/nyc_slice.xlsx\", mode = \"wb\")\n}\n\n# Read in the downloaded data\npizza_data &lt;- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1)\npizza_data[1:10, 1:6]\n## # A tibble: 10 √ó 6\n##    `Link to IG Post`                       Name  location_lat location_lng Date \n##    &lt;chr&gt;                                   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;\n##  1 https://www.instagram.com/p/CjszJ-fOP5‚Ä¶ Ange‚Ä¶ 40.6232544   -73.9379223‚Ä¶ 2022‚Ä¶\n##  2 https://www.instagram.com/p/CjdcPNAufP‚Ä¶ Ozon‚Ä¶ 40.6808917   -73.8426307  2022‚Ä¶\n##  3 https://www.instagram.com/p/CjQdNsaOZl‚Ä¶ Pino‚Ä¶ 40.6000148   -73.9994551  2022‚Ä¶\n##  4 https://www.instagram.com/p/Ci5XblnOnM‚Ä¶ La R‚Ä¶ 40.7133354   -73.8294102  2022‚Ä¶\n##  5 https://www.instagram.com/p/CiiLAtkON_‚Ä¶ Rony‚Ä¶ 40.7482509   -73.9923498  2022‚Ä¶\n##  6 https://www.instagram.com/p/CiS-44nucN‚Ä¶ John‚Ä¶ 40.8545616   -73.8658818‚Ä¶ 2022‚Ä¶\n##  7 https://www.instagram.com/p/CiSmQnjutQ‚Ä¶ Preg‚Ä¶ 40.8631291   -73.8585108  2022‚Ä¶\n##  8 https://www.instagram.com/p/CiIO6oFuxp‚Ä¶ N & ‚Ä¶ 40.6004632   -73.9430723‚Ä¶ 2022‚Ä¶\n##  9 https://www.instagram.com/p/ChaZUsxuFs‚Ä¶ Pepp‚Ä¶ 40.9036613   -73.8504667‚Ä¶ 2022‚Ä¶\n## 10 https://www.instagram.com/p/ChNd9wqOqG‚Ä¶ Rocc‚Ä¶ 40.8676344   -73.8836046  2022‚Ä¶\n## # ‚Ñπ 1 more variable: `Date Expanded (times in EST)` &lt;chr&gt;\n\n\n\n\nimport pandas as pd\n\npizza_data = pd.read_excel(\"../data/nyc_slice.xlsx\")\n## ImportError: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.\npizza_data\n## NameError: name 'pizza_data' is not defined\n\n\n\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n\n\nurl &lt;- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates &lt;- read_xls(path = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 √ó 6\n##    Age   \\nOperator's \\nLicense ‚Ä¶¬π Operator's\\nLicense ‚Ä¶¬≤ Motor-\\ncycle\\nLicen‚Ä¶¬≥\n##    &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n##  1 &lt;NA&gt;                         NA                     NA                     NA\n##  2 14                            0                      0                      0\n##  3 15                            0                      0                      0\n##  4 16                            0                      0                      0\n##  5 17                          961                     33                      0\n##  6 18                        18903                    174                      0\n##  7 19                        22159                    251                      0\n##  8 20                        22844                    326                      1\n##  9 21                        21589                    428                      0\n## 10 22                        22478                    588                      0\n## # ‚Ñπ abbreviated names: ¬π‚Äã`\\nOperator's \\nLicense -\\nClass O`,\n## #   ¬≤‚Äã`Operator's\\nLicense - \\nClass O/\\nMotorcycle\\nClass M`,\n## #   ¬≥‚Äã`Motor-\\ncycle\\nLicense /\\nClass M`\n## # ‚Ñπ 2 more variables: `Commercial Driver's License` &lt;chr&gt;, ...6 &lt;chr&gt;\n\n\n\nYou may need to install xlrd via pip for this code to work.\n\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\n## ImportError: Missing optional dependency 'xlrd'. Install xlrd &gt;= 2.0.1 for xls Excel support Use pip or conda to install xlrd.\nne_plates\n## NameError: name 'ne_plates' is not defined\n\n\n\n\n\n\n\n17.4.2 Google Sheets\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We‚Äôre not going to get into the power of these packages just now, but it‚Äôs worth a look if you‚Äôre working with collaborators who use Google sheets.\n\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via GSpread or googlesheets4 in python and R respectively.\n\n\n17.4.2.1 Reading Google Sheets\nLet‚Äôs demonstrate reading in data from google sheets in R and python using the Data Is Plural archive.\n\n\nPython\nR\nR: googlesheets4\nPython: GSpread\n\n\n\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in [2].\n\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n\nThis method would likely work just as well in R and would not require the googlesheets4 package.\n\n\nThis method is described in [2] for Python, but I have adapted the code to use in R.\n\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n\n\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\nCopy this code and run it on your computer to read in a sheet from google drive directly. You will see a prompt in the R console that you‚Äôll have to interact with, and there may also be a browser pop-up where you will need to sign in to google.\n\n\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n\n\n\nThese instructions are derived from [3]. We will have to install the GSpread package: type pip install gspread into the terminal.\nThen, you will need to obtain a client token JSON file following these instructions.\n\nimport gspread as gs\nimport pandas as pd\n\nI‚Äôve stopped here because I can‚Äôt get the authentication working, but the method seems solid if you‚Äôre willing to fiddle around with it. \n\n\n\n\n\n\n\n\n\nTry It Out!\n\n\n\n\n\nProblem\nSolution\n\n\n\nUsing a method of your choice, read in this spreadsheet of dog sizes and make a useful plot of dog height and weight ranges by breed.\n\n\nComing soon!",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#binary-files",
    "href": "part-wrangling/01-data-input.html#binary-files",
    "title": "17¬† Data Input",
    "section": "\n17.5 Binary Files",
    "text": "17.5 Binary Files\nR has binary file formats which store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see [4].\n\n17.5.1 Binary File IO\n\n\nR formats in R\nR formats in Python\nSAS format in R\nSAS format in Python\n\n\n\n.Rdata is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\nlegos &lt;- read_csv(\"../data/lego_sets.csv\")\nmy_var &lt;- \"This variable contains a string\"\nsave(legos, my_var, file = \"../data/R_binary.Rdata\")\n\nIf we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious.\nWe can load the R binary file back in using the load() function.\n\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legosets\"           \"mesodata\"          \n##  [7] \"mesodata_names\"     \"ne_plates\"          \"nebraska_locations\"\n## [10] \"openai_key\"         \"pizza_data\"         \"pokemon_info\"      \n## [13] \"sheet_id\"           \"sheet_name\"         \"tmdb_key\"          \n## [16] \"tmp\"                \"tmp_chars\"          \"tmp_chars_space\"   \n## [19] \"tmp_space\"          \"url\"                \"widths\"\n\nload(\"../data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"legosets\"          \n##  [7] \"mesodata\"           \"mesodata_names\"     \"my_var\"            \n## [10] \"ne_plates\"          \"nebraska_locations\" \"openai_key\"        \n## [13] \"pizza_data\"         \"pokemon_info\"       \"sheet_id\"          \n## [16] \"sheet_name\"         \"tmdb_key\"           \"tmp\"               \n## [19] \"tmp_chars\"          \"tmp_chars_space\"    \"tmp_space\"         \n## [22] \"url\"                \"widths\"\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\nsaveRDS(legos, \"../data/RDSlego.rds\")\n\nother_lego &lt;- readRDS(\"../data/RDSlego.rds\")\n\nBecause RDS formats don‚Äôt save the object name, you can be sure that you‚Äôre not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately.\n\n\nWe first need to install the pyreadr package by running pip install pyreadr in the terminal.\n\nimport pyreadr\n\nrdata_result = pyreadr.read_r('../data/R_binary.Rdata')\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n##             set_num  ...                                            img_url\n## 0      0003977811-1  ...  https://cdn.rebrickable.com/media/sets/0003977...\n## 1             001-1  ...   https://cdn.rebrickable.com/media/sets/001-1.jpg\n## 2            0011-2  ...  https://cdn.rebrickable.com/media/sets/0011-2.jpg\n## 3            0011-3  ...  https://cdn.rebrickable.com/media/sets/0011-3.jpg\n## 4            0012-1  ...  https://cdn.rebrickable.com/media/sets/0012-1.jpg\n## ...             ...  ...                                                ...\n## 23860   YODACHRON-1  ...  https://cdn.rebrickable.com/media/sets/yodachr...\n## 23861        YOTO-1  ...  https://cdn.rebrickable.com/media/sets/yoto-1.jpg\n## 23862        YOTO-2  ...  https://cdn.rebrickable.com/media/sets/yoto-2.jpg\n## 23863    YTERRIER-1  ...  https://cdn.rebrickable.com/media/sets/yterrie...\n## 23864      ZX8000-1  ...  https://cdn.rebrickable.com/media/sets/zx8000-...\n## \n## [23865 rows x 6 columns]\nrdata_result[\"my_var\"]\n##                             my_var\n## 0  This variable contains a string\n\nrds_result = pyreadr.read_r('../data/RDSlego.rds')\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n##             set_num  ...                                            img_url\n## 0      0003977811-1  ...  https://cdn.rebrickable.com/media/sets/0003977...\n## 1             001-1  ...   https://cdn.rebrickable.com/media/sets/001-1.jpg\n## 2            0011-2  ...  https://cdn.rebrickable.com/media/sets/0011-2.jpg\n## 3            0011-3  ...  https://cdn.rebrickable.com/media/sets/0011-3.jpg\n## 4            0012-1  ...  https://cdn.rebrickable.com/media/sets/0012-1.jpg\n## ...             ...  ...                                                ...\n## 23860   YODACHRON-1  ...  https://cdn.rebrickable.com/media/sets/yodachr...\n## 23861        YOTO-1  ...  https://cdn.rebrickable.com/media/sets/yoto-1.jpg\n## 23862        YOTO-2  ...  https://cdn.rebrickable.com/media/sets/yoto-2.jpg\n## 23863    YTERRIER-1  ...  https://cdn.rebrickable.com/media/sets/yterrie...\n## 23864      ZX8000-1  ...  https://cdn.rebrickable.com/media/sets/zx8000-...\n## \n## [23865 rows x 6 columns]\n\n\n\nFirst, let‚Äôs download the NHTS data.\n\nlibrary(httr)\n# Download the file and write to disk\nres &lt;- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"../data/cen10pub.sas7bdat\", overwrite = T))\n\nYou can see more information about this data here [5].\n\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata &lt;- read.sas7bdat(\"../data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it‚Äôs enough that we can see roughly how it‚Äôs structured.\n\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"../data/cen10pub.sas7bdat\")\n## '../data/cen10pub.sas7bdat'\n\nYou can see more information about this data here [5].\nTo read SAS files, we use the read_sas function in Pandas.\n\nimport pandas as pd\n\ndata = pd.read_sas(\"../data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\nIn RStudio, go to Session -&gt; Clear Workspace. (This will clear your environment)\nNow, using your RDS files, load the objects back into R with different names.\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you‚Äôre curious)\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n\n\nlibrary(readxl)\nlibrary(readr)\npizza &lt;- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1, guess_max = 7000)\nlegos &lt;- read_csv(\"../data/lego_sets.csv\")\n\nsave(pizza, legos, file = \"../data/04_Try_Binary.Rdata\")\nsaveRDS(pizza, \"../data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"../data/04_Try_Binary2.rds\")\n\nrm(pizza, legos) # Limited clearing of workspace... \n\n\nload(\"../data/04_Try_Binary.Rdata\")\n\npizza_compare &lt;- readRDS(\"../data/04_Try_Binary1.rds\")\nlego_compare &lt;- readRDS(\"../data/04_Try_Binary2.rds\")\n\nall.equal(pizza, pizza_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n\n\n\n\nimport pyreadr\n\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary.Rdata' does not exist!\npizza = robjs[\"pizza\"]\n## NameError: name 'robjs' is not defined\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n## NameError: name 'robjs' is not defined\n\npizza_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary1.rds' does not exist!\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n## pyreadr.custom_errors.PyreadrError: File b'data/04_Try_Binary2.rds' does not exist!\n\npizza.equals(pizza_compare)\n## NameError: name 'pizza' is not defined\nlegos.equals(lego_compare)\n## NameError: name 'legos' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#learn-more",
    "href": "part-wrangling/01-data-input.html#learn-more",
    "title": "17¬† Data Input",
    "section": "\n17.6 Learn more",
    "text": "17.6 Learn more\n\n\nSlides from Jenny Bryan‚Äôs talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/01-data-input.html#references",
    "href": "part-wrangling/01-data-input.html#references",
    "title": "17¬† Data Input",
    "section": "\n17.7 References",
    "text": "17.7 References\n\n\n\n\n[1] \nBetterExplained, ‚ÄúA little diddy about binary file formats ‚Äì BetterExplained. Better explained,‚Äù 2017. [Online]. Available: https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/. [Accessed: Jan. 13, 2023]\n\n\n[2] \nM. Sch√§fer, ‚ÄúRead Data from Google Sheets into Pandas without the Google Sheets API,‚Äù Towards Data Science. Dec. 2020 [Online]. Available: https://towardsdatascience.com/read-data-from-google-sheets-into-pandas-without-the-google-sheets-api-5c468536550. [Accessed: Jun. 07, 2022]\n\n\n[3] \nM. Clarke, ‚ÄúHow to read Google Sheets data in Pandas with GSpread,‚Äù Practical Data Science. Jun. 2021 [Online]. Available: https://web.archive.org/web/20211025204025/https://practicaldatascience.co.uk/data-science/how-to-read-google-sheets-data-in-pandas-with-gspread. [Accessed: Jun. 07, 2022]\n\n\n[4] \nC. Maierle, ‚ÄúLoading binary data to NumPy/Pandas,‚Äù Towards Data Science. Jul. 2020 [Online]. Available: https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672. [Accessed: Jun. 07, 2022]\n\n\n[5] \nUS Department of Transportation, ‚ÄúNational Household Travel Survey (NHTS) 2009,‚Äù data.world. Mar. 2018 [Online]. Available: https://data.world/dot/national-household-travel-survey-nhts-2009. [Accessed: Jun. 13, 2022]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Data Input</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html",
    "href": "part-wrangling/02-basic-data-vis.html",
    "title": "18¬† Data Visualization Basics",
    "section": "",
    "text": "18.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#objectives",
    "href": "part-wrangling/02-basic-data-vis.html#objectives",
    "title": "18¬† Data Visualization Basics",
    "section": "",
    "text": "Use ggplot2/plotnine to create a chart\nBegin to identify issues with data formatting",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#package-installation",
    "href": "part-wrangling/02-basic-data-vis.html#package-installation",
    "title": "18¬† Data Visualization Basics",
    "section": "\n18.2 Package Installation",
    "text": "18.2 Package Installation\nYou will need the plotnine (python) and ggplot2 (R) packages for this section.\n\ninstall.packages(\"ggplot2\")\n\nTo install plotnine, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 9.8.3.1)\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install plotnine matplotlib\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don‚Äôt try to install packages this way).\n\nreticulate::py_install(c(\"plotnine\", \"matplotlib\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called ‚ÄúIPython magic‚Äù commands, so if it doesn‚Äôt work for you, try the System Terminal method instead.\n\n%pip install plotnine matplotlib\n\nOnce you have run this command, please comment it out so that you don‚Äôt reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#first-steps",
    "href": "part-wrangling/02-basic-data-vis.html#first-steps",
    "title": "18¬† Data Visualization Basics",
    "section": "\n18.3 First Steps",
    "text": "18.3 First Steps\nNow that you can read data in to R and python and define new variables, you can create plots! Data visualization is a skill that takes a lifetime to learn, but for now, let‚Äôs start out easy: let‚Äôs talk about how to make (basic) plots in R (with ggplot2) and in python (with plotnine, which is a ggplot2 clone).\n\n18.3.1 Graphing HBCU Enrollment\nLet‚Äôs work with Historically Black College and University enrollment.\n\n18.3.1.1 Loading Libraries\n\n\nR\nPython\n\n\n\n\nhbcu_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nlibrary(ggplot2)\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nhbcu_all = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\n\n\n\n\n18.3.2 Making a Line Chart\nggplot2 and plotnine work with data frames.\nIf you pass a data frame in as the data argument, you can refer to columns in that data with ‚Äúbare‚Äù column names (you don‚Äôt have to reference the full data object using df$name or df.name; you can instead use name or \"name\").\n\n\n\nR\nPython\n\n\n\n\n\nggplot(hbcu_all, aes(x = Year, y = `4-year`)) + geom_line() +\n  ggtitle(\"4-year HBCU College Enrollment\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(hbcu_all, aes(x = \"Year\", y = \"4-year\")) + geom_line() + \\\n  ggtitle(\"4-year HBCU College Enrollment\")\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n18.3.3 Data Formatting\nIf your data is in the right format, ggplot2 is very easy to use; if your data aren‚Äôt formatted neatly, it can be a real pain. If you want to plot multiple lines, you need to either list each variable you want to plot, one by one, or (more likely) you want to get your data into ‚Äúlong form‚Äù. We‚Äôll learn more about how to do this type of data transition when we talk about reshaping data.\n\nYou don‚Äôt need to know exactly how this works, but it is helpful to see the difference in the two datasets:\n\n\nR\nPython\nOriginal Data\nLong Data\n\n\n\n\nlibrary(tidyr)\nhbcu_long &lt;- pivot_longer(hbcu_all, -Year, names_to = \"type\", values_to = \"value\")\n\n\n\n\nhbcu_long = pd.melt(hbcu_all, id_vars = ['Year'], value_vars = hbcu_all.columns[1:11])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTotal enrollment\nMales\nFemales\n4-year\n2-year\nTotal - Public\n4-year - Public\n2-year - Public\nTotal - Private\n4-year - Private\n2-year - Private\n\n\n\n1976\n222613\n104669\n117944\n206676\n15937\n156836\n143528\n13308\n65777\n63148\n2629\n\n\n1980\n233557\n106387\n127170\n218009\n15548\n168217\n155085\n13132\n65340\n62924\n2416\n\n\n1982\n228371\n104897\n123474\n212017\n16354\n165871\n151472\n14399\n62500\n60545\n1955\n\n\n1984\n227519\n102823\n124696\n212844\n14675\n164116\n151289\n12827\n63403\n61555\n1848\n\n\n1986\n223275\n97523\n125752\n207231\n16044\n162048\n147631\n14417\n61227\n59600\n1627\n\n\n1988\n239755\n100561\n139194\n223250\n16505\n173672\n158606\n15066\n66083\n64644\n1439\n\n\n\n\n\n\n\n\n\n\n\nYear\ntype\nvalue\n\n\n\n1976\nTotal enrollment\n222613\n\n\n1976\nMales\n104669\n\n\n1976\nFemales\n117944\n\n\n1976\n4-year\n206676\n\n\n1976\n2-year\n15937\n\n\n1976\nTotal - Public\n156836\n\n\n\n\n\nIn the long form of the data, we have a row for each data point (year x measurement type), not for each year.\n\n\n\n\n\n18.3.4 Making a (Better) Line Chart\nIf we had wanted to show all of the available data before, we would have needed to add a separate line for each column, coloring each one manually, and then we would have wanted to create a legend manually (which is a pain). Converting the data to long form means we can use ggplot2/plotnine to do all of this for us with only a single geom_line statement. Having the data in the right form to plot is very important if you want to get the plot you‚Äôre imagining with relatively little effort.\n\n\n\nR\nPython\n\n\n\n\n\nggplot(hbcu_long, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(hbcu_long, aes(x = \"Year\", y = \"value\", color = \"variable\")) + geom_line() + \\\n  ggtitle(\"HBCU College Enrollment\") + \\\n  theme(subplots_adjust={'right':0.75}) # This moves the key so it takes up 25% of the area\n## &lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#sec-graphics-intro-refs",
    "href": "part-wrangling/02-basic-data-vis.html#sec-graphics-intro-refs",
    "title": "18¬† Data Visualization Basics",
    "section": "\n18.4 References",
    "text": "18.4 References",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Data Visualization Basics</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html",
    "href": "part-wrangling/02a-eda.html",
    "title": "19¬† Exploratory Data Analysis",
    "section": "",
    "text": "19.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#objectives",
    "href": "part-wrangling/02a-eda.html#objectives",
    "title": "19¬† Exploratory Data Analysis",
    "section": "",
    "text": "Understand the main goals of exploratory data analysis\nGenerate and answer questions about a new dataset using charts, tables, and numerical summaries",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#package-installation",
    "href": "part-wrangling/02a-eda.html#package-installation",
    "title": "19¬† Exploratory Data Analysis",
    "section": "\n19.2 Package Installation",
    "text": "19.2 Package Installation\nYou will need the plotnine, seaborn, matplotlib (python) and ggplot2 (R) packages for this section.\n\ninstall.packages(\"ggplot2\")\n\nTo install plotnine, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 9.8.3.1)\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install plotnine matplotlib seaborn\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don‚Äôt try to install packages this way).\n\nreticulate::py_install(c(\"plotnine\", \"matplotlib\", \"seaborn\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called ‚ÄúIPython magic‚Äù commands, so if it doesn‚Äôt work for you, try the System Terminal method instead.\n\n%pip install plotnine matplotlib seaborn\n\nOnce you have run this command, please comment it out so that you don‚Äôt reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#extra-reading",
    "href": "part-wrangling/02a-eda.html#extra-reading",
    "title": "19¬† Exploratory Data Analysis",
    "section": "Extra Reading",
    "text": "Extra Reading\nThe EDA chapter in R for Data Science [1] is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. Much of the material in this chapter is based at least in part on the R4DS chapter.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "href": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "title": "19¬† Exploratory Data Analysis",
    "section": "\n19.3 A Note on Language Philosophies",
    "text": "19.3 A Note on Language Philosophies\nIt is usually relatively easy to get summary statistics from a dataset, but the ‚Äúflow‚Äù of EDA is somewhat different depending on the language patterns.\n\nYou must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of ‚Äúgive me every possible statistic that could be calculated from this model, whether or not it makes sense‚Äù. The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. ‚Äì Douglas Bates\n\nI provide this as a historical artifact, but it does explain the difference between the approach to EDA and model output in R and Python, and the approach in SAS, which you may see in your other statistics classes. This is not (at least, in my opinion) a criticism ‚Äì the SAS philosophy dates back to the mainframe and punch card days, and the syntax and output still bear evidence of that ‚Äì but it is worth noting.\nIn R and in Python, you will have to specify each piece of output you want, but in SAS you will get more than you ever wanted with a single command. Neither approach is wrong, but sometimes one is preferable over the other for a given problem.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#generating-eda-questions",
    "href": "part-wrangling/02a-eda.html#generating-eda-questions",
    "title": "19¬† Exploratory Data Analysis",
    "section": "\n19.4 Generating EDA Questions",
    "text": "19.4 Generating EDA Questions\nI very much like the two quotes in the [1] section on EDA Questions:\n\nThere are no routine statistical questions, only questionable statistical routines. ‚Äî Sir David Cox\n\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. ‚Äî John Tukey\n\nAs statisticians, we are concerned with variability by default. This is also true during EDA: we are interested in variability (or sometimes, lack thereof) in the variables in our dataset, including the co-variability between multiple variables.\nWe may assess variability using pictures or numerical summaries:\n\nhistograms or density plots (continuous variables)\ncolumn plots (categorical variables)\nboxplots\n5 number summaries (min, 25%, mean, 75%, max)\ntabular data summaries (for categorical variables)\n\nIn many cases, this gives us a picture of both variability and the ‚Äútypical‚Äù value of our variable.\nSometimes we may also be interested in identifying unusual values: outliers, data entry errors, and other points which don‚Äôt conform to our expectations. These unusual values may show up when we generate pictures and the axis limits are much larger than expected.\nWe also are usually concerned with missing values - in many cases, not all observations are complete, and this missingness can interfere with statistical analyses. It can be helpful to keep track of how much missingness there is in any particular variable and any patterns of missingness that would impact the eventual data analysis1.\nIf you are having trouble getting started on EDA, [3] provides a nice checklist to get you thinking:\n\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat‚Äôs missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "href": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "title": "19¬† Exploratory Data Analysis",
    "section": "\n19.5 Useful EDA Techniques",
    "text": "19.5 Useful EDA Techniques\n\n\n\n\nNintendo, Creatures, Game Freak, The Pok√©mon Company, Public domain, via Wikimedia Commons\n\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\nSuppose we want to explore Pokemon. There‚Äôs not just the original 150 (gotta catch ‚Äôem all!) - now there are over 1000! Let‚Äôs start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %&gt;%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\n\n\nThis data has several categorical and continuous variables that should allow for a reasonable demonstration of a number of techniques for exploring data.\n\n19.5.1 Numerical Summary Statistics\n\n\nR: summary\nPython: describe\nR: skimr\npython: skimpy\n\n\n\nThe first, and most basic EDA command in R is summary().\nFor numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all ‚Äúother‚Äù category.\n\n# Make types into factors to demonstrate the difference\npoke &lt;- tidyr::separate(poke, type, into = c(\"type_1\", \"type_2\"), sep = \",\")\npoke$type_1 &lt;- factor(poke$type_1)\npoke$type_2 &lt;- factor(poke$type_2)\n\nsummary(poke)\n##       gen          pokedex_no       img_link             name          \n##  Min.   :1.000   Min.   :   1.0   Length:1526        Length:1526       \n##  1st Qu.:2.000   1st Qu.: 226.2   Class :character   Class :character  \n##  Median :4.000   Median : 484.0   Mode  :character   Mode  :character  \n##  Mean   :4.478   Mean   : 487.9                                        \n##  3rd Qu.:7.000   3rd Qu.: 726.8                                        \n##  Max.   :9.000   Max.   :1008.0                                        \n##                                                                        \n##    variant               type_1        type_2        total       \n##  Length:1526        Water   :179   Flying :157   Min.   : 175.0  \n##  Class :character   Normal  :156   Psychic: 61   1st Qu.: 345.8  \n##  Mode  :character   Psychic :123   Ghost  : 57   Median : 475.0  \n##                     Electric:119   Ground : 57   Mean   : 450.3  \n##                     Grass   :113   Steel  : 55   3rd Qu.: 525.0  \n##                     Bug     :107   (Other):466   Max.   :1125.0  \n##                     (Other) :729   NA's   :673                   \n##        hp             attack          defense         sp_attack     \n##  Min.   :  1.00   Min.   :  5.00   Min.   :  5.00   Min.   : 10.00  \n##  1st Qu.: 50.25   1st Qu.: 60.00   1st Qu.: 55.00   1st Qu.: 50.00  \n##  Median : 70.00   Median : 80.00   Median : 70.00   Median : 70.00  \n##  Mean   : 71.18   Mean   : 82.05   Mean   : 75.66   Mean   : 75.05  \n##  3rd Qu.: 85.00   3rd Qu.:100.00   3rd Qu.: 95.00   3rd Qu.: 98.00  \n##  Max.   :255.00   Max.   :190.00   Max.   :250.00   Max.   :194.00  \n##                                                                     \n##    sp_defense         speed         species             height_m     \n##  Min.   : 20.00   Min.   :  5.0   Length:1526        Min.   : 0.100  \n##  1st Qu.: 55.00   1st Qu.: 50.0   Class :character   1st Qu.: 0.500  \n##  Median : 70.00   Median : 70.0   Mode  :character   Median : 1.000  \n##  Mean   : 73.84   Mean   : 72.5                      Mean   : 1.233  \n##  3rd Qu.: 90.00   3rd Qu.: 95.0                      3rd Qu.: 1.500  \n##  Max.   :250.00   Max.   :200.0                      Max.   :20.000  \n##                                                                      \n##    weight_kg        generation \n##  Min.   :  0.10   1      :285  \n##  1st Qu.:  8.00   5      :237  \n##  Median : 29.25   3      :193  \n##  Mean   : 68.25   4      :178  \n##  3rd Qu.: 78.50   8      :134  \n##  Max.   :999.90   7      :133  \n##                   (Other):366\n\nOne common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis. We can see in this summary that 673 pokemon don‚Äôt have a second type.\nWe also look for extreme values. There is at least one pokemon who appears to have a weight of 999.90 kg. Let‚Äôs investigate further:\n\npoke[poke$weight_kg &gt; 999,] \n## # A tibble: 2 √ó 18\n##     gen pokedex_no img_link       name  variant type_1 type_2 total    hp attack\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1     7        790 https://img.p‚Ä¶ Cosm‚Ä¶ NA      Psych‚Ä¶ &lt;NA&gt;     400    43     29\n## 2     7        797 https://img.p‚Ä¶ Cele‚Ä¶ NA      Steel  Flying   570    97    101\n## # ‚Ñπ 8 more variables: defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;,\n## #   generation &lt;fct&gt;\n# Show any rows where weight_kg is extreme\n\nThis is the last row of our data frame, and this pokemon appears to have many missing values.\n\n\nThe most basic EDA command in pandas is df.describe() (which operates on a DataFrame named df). Like summary() in R, describe() provides a 5-number summary for numeric variables. For categorical variables, describe() provides the number of unique values, the most common value, and the frequency of that common value.\n\n# Split types into two columns\npoke[['type_1', 'type_2']] = poke.type.str.split(\",\", expand = True)\n# Make each one categorical\npoke['type_1'] = pd.Categorical(poke.type_1)\npoke['type_2'] = pd.Categorical(poke.type_2)\n\npoke.iloc[:,:].describe() # describe only shows numeric variables by default\n##                gen   pokedex_no  ...     height_m    weight_kg\n## count  1526.000000  1526.000000  ...  1526.000000  1526.000000\n## mean      4.477720   487.863041  ...     1.232962    68.249607\n## std       2.565182   290.328644  ...     1.289446   121.828015\n## min       1.000000     1.000000  ...     0.100000     0.100000\n## 25%       2.000000   226.250000  ...     0.500000     8.000000\n## 50%       4.000000   484.000000  ...     1.000000    29.250000\n## 75%       7.000000   726.750000  ...     1.500000    78.500000\n## max       9.000000  1008.000000  ...    20.000000   999.900000\n## \n## [8 rows x 11 columns]\n\n# You can get categorical variables too if that's all you give it to show\npoke['type_1'].describe()\n## count      1526\n## unique       18\n## top       Water\n## freq        179\n## Name: type_1, dtype: object\npoke['type_2'].describe()\n## count        853\n## unique        18\n## top       Flying\n## freq         157\n## Name: type_2, dtype: object\n\n\n\nAn R package that is incredibly useful for this type of dataset exploration is skimr.\n\nlibrary(skimr)\nskim(poke)\n\n\nData summary\n\n\nName\npoke\n\n\nNumber of rows\n1526\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nfactor\n3\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nimg_link\n0\n1\n59\n84\n0\n1192\n0\n\n\nname\n0\n1\n3\n12\n0\n1008\n0\n\n\nvariant\n0\n1\n2\n22\n0\n105\n0\n\n\nspecies\n0\n1\n11\n21\n0\n708\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ntype_1\n0\n1.00\nFALSE\n18\nWat: 179, Nor: 156, Psy: 123, Ele: 119\n\n\ntype_2\n673\n0.56\nFALSE\n18\nFly: 157, Psy: 61, Gho: 57, Gro: 57\n\n\ngeneration\n0\n1.00\nFALSE\n9\n1: 285, 5: 237, 3: 193, 4: 178\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ngen\n0\n1\n4.48\n2.57\n1.0\n2.00\n4.00\n7.00\n9.0\n‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ\n\n\npokedex_no\n0\n1\n487.86\n290.33\n1.0\n226.25\n484.00\n726.75\n1008.0\n‚ñá‚ñÜ‚ñá‚ñá‚ñÜ\n\n\ntotal\n0\n1\n450.29\n120.59\n175.0\n345.75\n475.00\n525.00\n1125.0\n‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nhp\n0\n1\n71.18\n26.53\n1.0\n50.25\n70.00\n85.00\n255.0\n‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\nattack\n0\n1\n82.05\n32.41\n5.0\n60.00\n80.00\n100.00\n190.0\n‚ñÇ‚ñá‚ñá‚ñÇ‚ñÅ\n\n\ndefense\n0\n1\n75.66\n30.21\n5.0\n55.00\n70.00\n95.00\n250.0\n‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nsp_attack\n0\n1\n75.05\n33.88\n10.0\n50.00\n70.00\n98.00\n194.0\n‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\nsp_defense\n0\n1\n73.84\n27.72\n20.0\n55.00\n70.00\n90.00\n250.0\n‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\nspeed\n0\n1\n72.50\n30.74\n5.0\n50.00\n70.00\n95.00\n200.0\n‚ñÉ‚ñá‚ñÜ‚ñÅ‚ñÅ\n\n\nheight_m\n0\n1\n1.23\n1.29\n0.1\n0.50\n1.00\n1.50\n20.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nweight_kg\n0\n1\n68.25\n121.83\n0.1\n8.00\n29.25\n78.50\n999.9\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nskim provides a beautiful table of summary statistics along with a sparklines-style histogram of values, giving you a sneak peek at the distribution.\n\n\nThere is a similar package to skimr in R called skimpy in Python.\n\nfrom skimpy import skim\nskim(poke)\n## ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skimpy summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n## ‚îÇ          Data Summary                Data Types                              ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì                       ‚îÇ\n## ‚îÇ ‚îÉ dataframe         ‚îÉ Values ‚îÉ ‚îÉ Column Type ‚îÉ Count ‚îÉ                       ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©                       ‚îÇ\n## ‚îÇ ‚îÇ Number of rows    ‚îÇ 1526   ‚îÇ ‚îÇ int64       ‚îÇ 9     ‚îÇ                       ‚îÇ\n## ‚îÇ ‚îÇ Number of columns ‚îÇ 19     ‚îÇ ‚îÇ string      ‚îÇ 5     ‚îÇ                       ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ category    ‚îÇ 3     ‚îÇ                       ‚îÇ\n## ‚îÇ                                ‚îÇ float64     ‚îÇ 2     ‚îÇ                       ‚îÇ\n## ‚îÇ                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ\n## ‚îÇ        Categories                                                            ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì                                                    ‚îÇ\n## ‚îÇ ‚îÉ Categorical Variables ‚îÉ                                                    ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©                                                    ‚îÇ\n## ‚îÇ ‚îÇ generation            ‚îÇ                                                    ‚îÇ\n## ‚îÇ ‚îÇ type_1                ‚îÇ                                                    ‚îÇ\n## ‚îÇ ‚îÇ type_2                ‚îÇ                                                    ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                    ‚îÇ\n## ‚îÇ                                   number                                     ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n## ‚îÇ ‚îÉ colu ‚îÉ    ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ     ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ  ‚îÇ\n## ‚îÇ ‚îÉ mn_n ‚îÉ    ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ     ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ  ‚îÇ\n## ‚îÇ ‚îÉ ame  ‚îÉ NA ‚îÉ NA % ‚îÉ mean ‚îÉ sd   ‚îÉ p0  ‚îÉ p25  ‚îÉ p50  ‚îÉ p75  ‚îÉ p100 ‚îÉ hist ‚îÉ  ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n## ‚îÇ ‚îÇ gen  ‚îÇ  0 ‚îÇ    0 ‚îÇ 4.47 ‚îÇ 2.56 ‚îÇ   1 ‚îÇ    2 ‚îÇ    4 ‚îÇ    7 ‚îÇ    9 ‚îÇ ‚ñá‚ñÉ‚ñÉ‚ñá ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ      ‚îÇ    ‚îÇ      ‚îÇ    8 ‚îÇ    5 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚ñÉ‚ñÖ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ poke ‚îÇ  0 ‚îÇ    0 ‚îÇ 487. ‚îÇ 290. ‚îÇ   1 ‚îÇ 226. ‚îÇ  484 ‚îÇ 726. ‚îÇ 1008 ‚îÇ ‚ñá‚ñÖ‚ñá‚ñá ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ dex_ ‚îÇ    ‚îÇ      ‚îÇ    9 ‚îÇ    3 ‚îÇ     ‚îÇ    2 ‚îÇ      ‚îÇ    8 ‚îÇ      ‚îÇ  ‚ñÜ‚ñÜ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ no   ‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ tota ‚îÇ  0 ‚îÇ    0 ‚îÇ 450. ‚îÇ 120. ‚îÇ 175 ‚îÇ 345. ‚îÇ  475 ‚îÇ  525 ‚îÇ 1125 ‚îÇ ‚ñÖ‚ñá‚ñá‚ñÅ ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ l    ‚îÇ    ‚îÇ      ‚îÇ    3 ‚îÇ    6 ‚îÇ     ‚îÇ    8 ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ hp   ‚îÇ  0 ‚îÇ    0 ‚îÇ 71.1 ‚îÇ 26.5 ‚îÇ   1 ‚îÇ 50.2 ‚îÇ   70 ‚îÇ   85 ‚îÇ  255 ‚îÇ ‚ñÅ‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ      ‚îÇ    ‚îÇ      ‚îÇ    8 ‚îÇ    3 ‚îÇ     ‚îÇ    5 ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ atta ‚îÇ  0 ‚îÇ    0 ‚îÇ 82.0 ‚îÇ 32.4 ‚îÇ   5 ‚îÇ   60 ‚îÇ   80 ‚îÇ  100 ‚îÇ  190 ‚îÇ ‚ñÇ‚ñá‚ñá‚ñÜ ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ck   ‚îÇ    ‚îÇ      ‚îÇ    5 ‚îÇ    1 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚ñÇ‚ñÅ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ defe ‚îÇ  0 ‚îÇ    0 ‚îÇ 75.6 ‚îÇ 30.2 ‚îÇ   5 ‚îÇ   55 ‚îÇ   70 ‚îÇ   95 ‚îÇ  250 ‚îÇ ‚ñÇ‚ñá‚ñÖ‚ñÅ ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ nse  ‚îÇ    ‚îÇ      ‚îÇ    6 ‚îÇ    1 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ sp_a ‚îÇ  0 ‚îÇ    0 ‚îÇ 75.0 ‚îÇ 33.8 ‚îÇ  10 ‚îÇ   50 ‚îÇ   70 ‚îÇ   98 ‚îÇ  194 ‚îÇ ‚ñÖ‚ñá‚ñÜ‚ñÉ ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ttac ‚îÇ    ‚îÇ      ‚îÇ    5 ‚îÇ    8 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚ñÅ   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ k    ‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ sp_d ‚îÇ  0 ‚îÇ    0 ‚îÇ 73.8 ‚îÇ 27.7 ‚îÇ  20 ‚îÇ   55 ‚îÇ   70 ‚îÇ   90 ‚îÇ  250 ‚îÇ ‚ñÖ‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ efen ‚îÇ    ‚îÇ      ‚îÇ    4 ‚îÇ    2 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ se   ‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ spee ‚îÇ  0 ‚îÇ    0 ‚îÇ 72.5 ‚îÇ 30.7 ‚îÇ   5 ‚îÇ   50 ‚îÇ   70 ‚îÇ   95 ‚îÇ  200 ‚îÇ ‚ñÉ‚ñÜ‚ñá‚ñÇ ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ d    ‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ    4 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚ñÅ   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ heig ‚îÇ  0 ‚îÇ    0 ‚îÇ 1.23 ‚îÇ 1.28 ‚îÇ 0.1 ‚îÇ  0.5 ‚îÇ    1 ‚îÇ  1.5 ‚îÇ   20 ‚îÇ  ‚ñá   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ht_m ‚îÇ    ‚îÇ      ‚îÇ    3 ‚îÇ    9 ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ weig ‚îÇ  0 ‚îÇ    0 ‚îÇ 68.2 ‚îÇ 121. ‚îÇ 0.1 ‚îÇ    8 ‚îÇ 29.2 ‚îÇ 78.5 ‚îÇ 999. ‚îÇ  ‚ñá‚ñÅ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ht_k ‚îÇ    ‚îÇ      ‚îÇ    5 ‚îÇ    8 ‚îÇ     ‚îÇ      ‚îÇ    5 ‚îÇ      ‚îÇ    9 ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ g    ‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n## ‚îÇ                                  category                                    ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n## ‚îÇ ‚îÉ column_name          ‚îÉ NA      ‚îÉ NA %     ‚îÉ ordered       ‚îÉ unique      ‚îÉ  ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n## ‚îÇ ‚îÇ generation           ‚îÇ       0 ‚îÇ        0 ‚îÇ False         ‚îÇ           9 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ type_1               ‚îÇ       0 ‚îÇ        0 ‚îÇ False         ‚îÇ          18 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ type_2               ‚îÇ     673 ‚îÇ     44.1 ‚îÇ False         ‚îÇ          19 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n## ‚îÇ                                   string                                     ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n## ‚îÇ ‚îÉ column_name     ‚îÉ NA     ‚îÉ NA %    ‚îÉ words per row     ‚îÉ total words    ‚îÉ  ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n## ‚îÇ ‚îÇ img_link        ‚îÇ      0 ‚îÇ       0 ‚îÇ                 1 ‚îÇ           1526 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ name            ‚îÇ      0 ‚îÇ       0 ‚îÇ                 1 ‚îÇ           1551 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ variant         ‚îÇ   1071 ‚îÇ   70.18 ‚îÇ              0.39 ‚îÇ            598 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ type            ‚îÇ      0 ‚îÇ       0 ‚îÇ                 1 ‚îÇ           1526 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ species         ‚îÇ      0 ‚îÇ       0 ‚îÇ               2.3 ‚îÇ           3469 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n## ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ End ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n\n\n19.5.2 Assessing Distributions\nWe are often also interested in the distribution of values.\n\n19.5.2.1 Categorical Variables\nOne useful way to assess the distribution of values is to generate a cross-tabular view of the data. This is mostly important for variables with a relatively low number of categories - otherwise, it is usually easier to use a graphical summary method.\nTabular Summaries\n\n\nR\nPython\n\n\n\nWe can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). We can even generate cross-tabular views for a combination of two variables (or theoretically more, but this gets hard to read and track).\n\ntable(poke$generation)\n## \n##   1   2   3   4   5   6   7   8   9 \n## 285 124 193 178 237 119 133 134 123\n\ntable(poke$type_1, poke$type_2)\n##           \n##            Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass\n##   Bug        0    1      0        4     2        5    2     14     1     8\n##   Dark       0    0      4        0     3        2    4      8     2     2\n##   Dragon     0    1      0        1     1        2    1      6     3     0\n##   Electric   0    4      3        0     2        2    6     19     6    10\n##   Fairy      0    0      0        0     0        1    0      6     0     0\n##   Fighting   0    3      1        1     0        0    4      3     2     0\n##   Fire       2    1      2        0     0        7    0     11     7     0\n##   Flying     0    1      2        0     0        1    0      0     0     0\n##   Ghost      0    1      4        0     3        0    3      6     0    11\n##   Grass      0    5      6        0     5        7    1      8     4     0\n##   Ground     0    3      2        2     0        1    1      6     5     2\n##   Ice        2    0      0        0     2        0    4      3     1     0\n##   Normal     0    0      1        0     5        5    0     33     4     8\n##   Poison     1    7      4        0     2        3    2      3     0     0\n##   Psychic    0    2      3        0    11        3    1     14     9     4\n##   Rock       2    2      2        7     3        1    2      8     0     2\n##   Steel      0    0      9        0     4        1    0      2     7     0\n##   Water      2    9      6        2     4        6    0      7     6     3\n##           \n##            Ground Ice Normal Poison Psychic Rock Steel Water\n##   Bug           4   0      0     12       3    4    13     3\n##   Dark          1   4      9      3       2    0     3     0\n##   Dragon       13  12      1      0       4    0     0     9\n##   Electric      1   7      2      5       2    0     4     6\n##   Fairy         0   0      0      0       1    0     5     0\n##   Fighting      0   1      0      2       3    0     4     6\n##   Fire          3   0      2      1       6    5     1     1\n##   Flying        0   0      0      0       0    0     1     3\n##   Ghost         2   0      0      4       0    0     0     0\n##   Grass         1   3      3     15       3    0     3     0\n##   Ground        0   0      1      0       2    3     8     0\n##   Ice           3   0      0      0       5    2     4     4\n##   Normal        1   0      0      0       6    0     0     1\n##   Poison        5   0      2      0       4    0     0     3\n##   Psychic       0   3      4      0       0    0     4     0\n##   Rock          9   2      0      3       2    0     4     6\n##   Steel         2   0      0      2       7    3     0     0\n##   Water        12   4      0      4      11    6     1     0\n\n\n\n\nimport numpy as np\n# For only one factor, use .groupby('colname')['colname'].count()\npoke.groupby(['generation'])['generation'].count()\n## generation\n## 1    285\n## 2    124\n## 3    193\n## 4    178\n## 5    237\n## 6    119\n## 7    133\n## 8    134\n## 9    123\n## Name: generation, dtype: int64\n\n# for two or more factors, use pd.crosstab\npd.crosstab(index = poke['type_1'], columns = poke['type_2'])\n## type_2    Bug  Dark  Dragon  Electric  ...  Psychic  Rock  Steel  Water\n## type_1                                 ...                             \n## Bug         0     1       0         4  ...        3     4     13      3\n## Dark        0     0       4         0  ...        2     0      3      0\n## Dragon      0     1       0         1  ...        4     0      0      9\n## Electric    0     4       3         0  ...        2     0      4      6\n## Fairy       0     0       0         0  ...        1     0      5      0\n## Fighting    0     3       1         1  ...        3     0      4      6\n## Fire        2     1       2         0  ...        6     5      1      1\n## Flying      0     1       2         0  ...        0     0      1      3\n## Ghost       0     1       4         0  ...        0     0      0      0\n## Grass       0     5       6         0  ...        3     0      3      0\n## Ground      0     3       2         2  ...        2     3      8      0\n## Ice         2     0       0         0  ...        5     2      4      4\n## Normal      0     0       1         0  ...        6     0      0      1\n## Poison      1     7       4         0  ...        4     0      0      3\n## Psychic     0     2       3         0  ...        0     0      4      0\n## Rock        2     2       2         7  ...        2     0      4      6\n## Steel       0     0       9         0  ...        7     3      0      0\n## Water       2     9       6         2  ...       11     6      1      0\n## \n## [18 rows x 18 columns]\n\n\n\n\nFrequency Plots\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\n\nplot(table(poke$generation)) # bar plot\n\n\n\n\n\n\n\n\n\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(generation))) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\")\n\n\n\n\n\n\n\n\n\nWe generate a bar chart using the contingency table we generated earlier combined with matplotlib‚Äôs plt.bar().\n\nimport matplotlib.pyplot as plt\n\ntab = poke.groupby(['generation'])['generation'].count()\n\nplt.bar(tab.keys(), tab.values, color = 'grey')\nplt.xlabel(\"Generation\")\nplt.ylabel(\"# Pokemon\")\nplt.show()\n\n\n\n\n\n\n\n\n\nPlotnine is a ggplot2 clone for python, and for the most part, the code looks almost exactly the same, minus a few python-specific tweaks to account for different syntax conventions in each language.\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nfrom plotnine import *\n\n(ggplot(mapping = aes(x = \"factor(generation)\"), data = poke) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\"))\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n19.5.2.2 Quantitative Variables\nWe covered some numerical summary statistics in the numerical summary statistic section above. In this section, we will primarily focus on visualization methods for assessing the distribution of quantitative variables.\n\n\n\n\n\n\nNote: R pipe\n\n\n\nThe code in this section uses the R pipe, %&gt;%. The left side of the pipe is passed as an argument to the right side. This makes code easier to read because it becomes a step-wise ‚Äúrecipe‚Äù instead of a nested mess of functions and parentheses.\n\n\nIn each step, the left hand side of the pipe is put into the first argument of the function. Source: Arthur Welle (Github)\n\n\n\nWe can generate histograms2 or kernel density plots (a continuous version of the histogram) to show us the distribution of a continuous variable.\n\n\nBase R\nPython: matplotlib\nR: ggplot2\nPython: plotnine\n\n\n\nBy default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desirable result. If we do not specify breaks, R will pick them for us.\n\nhist(poke$hp)\n\n\n\n\n\n\n\nFor continuous variables, we can use histograms, or we can examine kernel density plots.\nlibrary(magrittr) # This provides the pipe command, %&gt;%\n\nhist(poke$weight_kg)\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Take the log - will transformation be useful w/ modeling?\n  hist(main = \"Histogram of Log10 Weight (Kg)\") # create a histogram\n\npoke$weight_kg %&gt;%\n  density(na.rm = T) %&gt;% # First, we compute the kernel density\n  # (na.rm = T says to ignore NA values)\n  plot(main = \"Density of Weight (Kg)\") # Then, we plot the result\n\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Transform the variable\n  density(na.rm = T) %&gt;% # Compute the density ignoring missing values\n  plot(main = \"Density of Log10 pokemon weight in Kg\") # Plot the result,\n    # changing the title of the plot to a meaningful value\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots with separate axes\n# This uses python multi-assignment to assign figures, axes\n# variables all in one go\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\npoke.weight_kg.plot.hist(ax = ax1) # first plot\nax1.set_title(\"Histogram of Weight (kg)\")\n\n\nnp.log10(poke.weight_kg).plot.hist(ax = ax2)\nax2.set_title(\"Histogram of Log10 Weight (kg)\")\n\npoke.weight_kg.plot.density(ax = ax3)\nax3.set_title(\"Density of Weight (kg)\")\n\nnp.log10(poke.weight_kg).plot.density(ax = ax4)\nax4.set_title(\"Density of Log10 Weight (kg)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\nlibrary(ggplot2)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\nggplot(poke, aes(x = height_m)) +\n  geom_density()\nggplot(poke, aes(x = height_m)) +\n  geom_density() +\n  scale_x_log10()\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\nggplot(poke, aes(x = 'height_m')) + geom_histogram(bins = 30)\n## &lt;Figure Size: (640 x 480)&gt;\n\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_histogram(bins = 30) +\n  scale_x_log10())\n## &lt;Figure Size: (640 x 480)&gt;\n\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density())\n## &lt;Figure Size: (640 x 480)&gt;\n\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density() +\n  scale_x_log10())\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\n\n19.5.3 Relationships Between Variables\n\n19.5.3.1 Categorical - Categorical Relationships\n\n\nR: ggplot2\nBase R\nPython: matplotlib\nPython: plotnine\n\n\n\nWe can generate a (simple) mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I‚Äôve flipped the axes using coord_flip so that you can read the labels more easily.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip()\n\n\n\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\nggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +\n  # Shade tiles according to the number of things in the bin\n  geom_tile(aes(fill = after_stat(count)), stat = \"bin2d\") +\n  # Add the number of things in the bin to the top of the tile as text\n  geom_text(aes(label = after_stat(count)), stat = 'bin2d') +\n  # Scale the tile fill\n  scale_fill_gradient2(limits = c(0, 100), low = \"white\", high = \"blue\", na.value = \"white\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\n\n\n\n\n\nBase R mosaic plots aren‚Äôt nearly as pretty as the ggplot version, but I will at least show you how to create them.\n\nplot(table(poke$type_1, poke$type_2)) \n\n\n\n\n\n\n# mosaic plot - hard to read b/c too many categories\n\n\n\nTo get a mosaicplot, we need an additional library, called statsmodels, which we install with pip install statsmodels in the terminal.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(poke, ['type_1', 'type_2'], title = \"Pokemon Types\")\nplt.show()\n\n\n\n\n\n\n\nThis obviously needs a bit of cleaning up to remove extra labels, but it‚Äôs easy to get to and relatively functional. Notice that it does not, by default, show NA values.\n\n\nWe can generate a mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I‚Äôve flipped the axes using coord_flip so that you can read the labels more easily.\n\n# Convert everything to categorical/factor variable ahead of time\n# this stops an error: TypeError: '&lt;' not supported between instances of 'float' and 'str'\npoke['type_1'] = pd.Categorical(poke['type_1'].astype(str))\npoke['type_2'] = pd.Categorical(poke['type_2'].astype(str))\n\n( ggplot(mapping = aes(x = 'type_1', fill = 'type_2'), data = poke) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip() +\n  # This says 85% of the plot is for the main plot and 15% is for the legend.\n  theme(subplots_adjust={'right':0.85})\n  )\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\n(ggplot(mapping = aes(x = 'type_1', y = 'type_2'), data = poke) +\n  # Shade tiles according to the number of things in the bin\n  stat_bin2d(aes(fill = after_stat('count')), geom = 'tile'))\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n19.5.3.2 Categorical - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\nIn R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms.\nTo examine the relationship between a categorical variable and a continuous variable, we might look at box plots:\n\npar(mfrow = c(1, 2)) # put figures in same row\nboxplot(log10(height_m) ~ type_1, data = poke)\nboxplot(total ~ generation, data = poke)\n\n\n\n\n\n\n\nIn the second box plot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn‚Äôt usually about creating pretty plots (or we‚Äôd be using ggplot right now) but rather about identifying things which may come up in the analysis later.\n\n\n\nggplot(data = poke, aes(x = type_1, y = height_m)) + \n  geom_boxplot() + \n  scale_y_log10()\n\nggplot(data = poke, aes(x = factor(generation), y = total)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure()\n\n# Create a list of vectors of height_m by type_1\npoke['height_m_log'] = np.log(poke.height_m)\nheight_by_type = poke.groupby('type_1', group_keys = True).height_m_log.apply(list)\n\n# Plot each object in the list\nplt.boxplot(height_by_type, labels = height_by_type.index)\n## {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x7f139678b190&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139678a750&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353a5f710&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13539f4210&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f2110&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f0c10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353852b10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353956390&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353881ed0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353872710&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b2250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b37d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139606efd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396015fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961f8f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961fac10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538a1650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353a18ad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397faf310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397faddd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc3350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc21d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff32d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff0110&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396727510&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13967259d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396741310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396742b90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396742490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139674abd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353985050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396739c10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353912b10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353910310&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fea8d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397feb7d0&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x7f13960ba690&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13967667d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f0350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f3090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13a29988d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13a1b04e50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538226d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353a8fbd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396795290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b1ad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b8350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b8710&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13966a8450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13963f0450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13966b4c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961c2950&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353aa5890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353aa5190&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fac450&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fae510&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc1c50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc0350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff2550&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff1890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396727290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396724e50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396743350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396740790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139674b9d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396748cd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13967399d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396738c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353913490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13539135d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fe8590&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353aa2ad0&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x7f13960eccd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396518fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f14d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353960810&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353830490&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b2290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13963c2850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13960b7c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353ddfad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fae890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc0050&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353e01c10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396c2b090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396725150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396741ed0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396749850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353912dd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fea090&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x7f1396788190&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f0fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353a05f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353863890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b3010&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961ba550&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1395f050d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961c27d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396c7ef90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fad790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc2c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff3f90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396727c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396740b90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139674b650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139673bb50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353912fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353aa1f90&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x7f13a1b42a50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538f2810&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135389ea90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353862790&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961b24d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961bac90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139651acd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13961c6850&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353a49090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fad390&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fc3090&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff36d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396725a50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1396740250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13967483d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f139673a890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397feaad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397fd9190&gt;], 'means': []}\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure()\n\n# Create a list of vectors of total by generation\ntotal_by_gen = poke.groupby('generation', group_keys = True).total.apply(list)\n\n# Plot each object in the list\nplt.boxplot(total_by_gen, labels = total_by_gen.index)\n## {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x7f1397f9add0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f9bfd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ffa590&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ffaf10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6c350&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6d2d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f449d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f46b10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f4e890&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353854a10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f2ab50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f293d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f35bd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f37fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f5aad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391cf90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3f3d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3fb90&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x7f1397e1a610&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397e1bc90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ffbd10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff9650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6df50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6e810&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f47210&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f47250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1353854250&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538c06d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f284d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f286d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f36150&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f5af90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391d810&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391fb90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3e8d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3d710&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x7f1397f9bd50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ffa2d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6eb10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f47c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f4ff10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f29fd0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f34d10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f58a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391cf50&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x7f1397e1be50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff8f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f6f290&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f4db50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f13538c3ed0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f346d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f58390&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391de50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3f150&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x7f139648ce90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397ff8d10&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f44d90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f4cad0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f29f50&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f36c90&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f5a3d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7f135391d650&gt;, &lt;matplotlib.lines.Line2D object at 0x7f1397f3e2d0&gt;], 'means': []}\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nggplot(mapping = aes(x = \"type_1\", y = \"height_m\"), data = poke) +\\\ngeom_boxplot()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nggplot(mapping = aes(x = \"generation\", y = \"total\"), data = poke) +\\\ngeom_boxplot()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nYou can find more on boxplots and ways to customize boxplots in the Graphics chapter.\n\n19.5.3.3 Continuous - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: pandas\nPython: plotnine\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nplot(defense ~ attack, data = poke, type = \"p\")\n\n\n\n\n\n\n\ncor(poke$defense, poke$attack)\n## [1] 0.4259168\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. In other cases, like in the plot below, we may discover an interesting correlation that sticks out - the identity line \\(y=x\\) seems to stand out from the cloud here.\n\nplot(x = poke$sp_attack, y = poke$attack, type = \"p\")\n\n\n\n\n\n\n\nA scatterplot matrix can also be a useful way to visualize relationships between several variables.\n\npairs(poke[,c(\"hp\", \"attack\", \"defense\", \"sp_attack\", \"sp_defense\")]) # hp - sp_defense columns\n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere‚Äôs more information on how to customize base R scatterplot matrices here.\n\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nlibrary(ggplot2)\nggplot(poke, aes(x = attack, y = defense)) + geom_point()\n\n\n\n\n\n\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized. When this happens, it can be a good idea to use geom_jitter to provide some ‚Äúwiggle‚Äù in the data so that you can still see the point density. Changing the point transparency (alpha = .5) can also help with overplotting.\nIn other cases, we might find that there is a prominent feature of a scatterplot (in this case, the line \\(y=x\\) seems to stand out a bit from the overall point cloud). We can highlight this feature by adding a line at \\(y=x\\) in red behind the points.\n\nggplot(poke, aes(x = attack, y = sp_attack)) + geom_point()\n\n\n\n\n\n\n\nggplot(poke, aes(x = attack, y = sp_attack)) + \n  geom_abline(slope = 1, color = \"red\") + \n  geom_jitter(alpha = 0.5)\n\n\n\n\n\n\n\n\nlibrary(GGally) # an extension to ggplot2\nggpairs(poke[,c(\"hp\", \"attack\", \"defense\", \"sp_attack\", \"sp_defense\")], \n        # hp - sp_defense columns\n        lower = list(continuous = wrap(\"points\", alpha = .15)),\n        progress = F) \n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\nggpairs can also handle continuous variables, if you want to explore the options available.\n\n\nBelieve it or not, you don‚Äôt have to go to matplotlib to get plots in python - you can get some plots from pandas directly, even if you are still using matplotlib under the hood (this is why you have to run plt.show() to get the plot to appear if you‚Äôre working in markdown).\n\nimport matplotlib.pyplot as plt\n\npoke.plot.scatter(x = 'attack', y = 'defense')\nplt.show()\n\n\n\n\n\n\n\nPandas also includes a nice scatterplot matrix method.\n\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\n\nscatter_matrix(poke.iloc[:,15:19], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')\n## array([[&lt;Axes: xlabel='weight_kg', ylabel='weight_kg'&gt;]], dtype=object)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(poke, aes(x = \"attack\", y = \"sp_attack\")) + geom_point()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n# jitter in plotnine seems to use width and height jointly instead of \n# marginally\nggplot(poke, aes(x = \"attack\", y = \"sp_attack\")) + geom_jitter(alpha = 0.5, height = 5)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nWhile plotnine doesn‚Äôt have scatterplot matrices by default, you can create them using some clever code. This is obviously not as fancy as ggpairs but it works well enough.\n\nfrom plotnine import *\nimport itertools\n\ndef plot_matrix(df, columns):\n  pdf = []\n  for a1, b1 in itertools.combinations(columns, 2):\n    for (a,b) in ((a1, b1), (b1, a1)):\n      sub = df[[a, b]].rename(columns={a: \"x\", b: \"y\"}).assign(a=a, b=b)\n      pdf.append(sub)\n  \n  g = ggplot(pd.concat(pdf))\n  g += geom_point(aes('x','y'))\n  g += facet_grid('b~a', scales='free')\n  return g\n\n\nplot_matrix(poke, poke.columns[7:11])\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nIf you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I‚Äôm cheating a bit by mentioning it now, but it‚Äôs just so useful!)\n\n\n\n\n\n\nTry it out: EDA\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nExplore the variables present in the Lancaster County Assessor Housing Sales Data Documentation.\nNote that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\ndownload.file(\"https://github.com/srvanderplas/datasets/blob/main/raw/Lancaster%20County,%20NE%20-%20Assessor.xlsx?raw=true\", destfile = \"../data/lancaster-housing.xlsx\")\nhousing_lincoln &lt;- read_xlsx(\"../data/lancaster-housing.xlsx\", sheet = 1, guess_max = 7000)\n\n\nimport pandas as pd\nhousing_lincoln = pd.read_excel(\"../data/lancaster-housing.xlsx\")\n## ImportError: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.\n\n\n\n\nhousing_lincoln$TLA &lt;- readr::parse_number(housing_lincoln$`TLA (Sqft)`)\nhousing_lincoln$Assd_Value &lt;- readr::parse_number(housing_lincoln$Assd_Value)\n\nskim(housing_lincoln)\n\n\nData summary\n\n\nName\nhousing_lincoln\n\n\nNumber of rows\n6918\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nParcel_ID\n0\n1\n17\n17\n0\n6740\n0\n\n\nAddress\n0\n1\n29\n50\n0\n6740\n0\n\n\nOwner\n0\n1\n6\n67\n0\n6435\n0\n\n\nOwner Address\n0\n1\n25\n93\n0\n6184\n0\n\n\nImp_Type\n0\n1\n2\n3\n0\n39\n0\n\n\nTLA (Sqft)\n0\n1\n3\n5\n0\n1767\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nYr_Blt\n0\n1\n1950.85\n22.55\n1900\n1933\n1954.0\n1963\n2023\n‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÅ\n\n\nAssd_Value\n0\n1\n229956.23\n96272.53\n36500\n174925\n214500.0\n259175\n1404800\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nTLA\n0\n1\n1379.08\n599.09\n400\n966\n1235.5\n1612\n6819\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nLet‚Äôs examine the numeric variables first:\n\nhist(housing_lincoln$Assd_Value)\n\n\n\n\n\n\n\nhist(housing_lincoln$Yr_Blt)\n\n\n\n\n\n\n\nLet‚Äôs look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nhousing_lincoln$decade &lt;- 10*floor(housing_lincoln$Yr_Blt/10)\n\ntable(housing_lincoln$decade, useNA = 'ifany')\n## \n## 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 \n##  245  295  885  414  647 1949 1429  282  550  116   72   24   10\ntable(housing_lincoln$Imp_Type)\n## \n##   BL   BN   C1   C2   CA   CB  CXF  CXU  CYF  CYU   D1   D2   D3   D4   D5   D6 \n##  163  764   11   39   48   17    4    3    7   25    2   40    9  232   45   10 \n##   DA   HC   M1   R1   R2   RA   RB   RR   RS  RXF  RXU  RYF  RYU   T1   T2   T3 \n##    2  132    1 3165  492  628   23   15  218  257   79   31   73  160    9   14 \n##   T4   T5   T6   T7   TA   TS  TYF \n##   17   37    5   21  110    9    1\ntable(housing_lincoln$Imp_Type, housing_lincoln$decade)\n##      \n##       1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\n##   BL     0    0    0    0    0    0  119   42    0    1    1    0    0\n##   BN    77   84  413  176   14    0    0    0    0    0    0    0    0\n##   C1     1    0    8    0    1    0    0    0    1    0    0    0    0\n##   C2    14    9   12    1    1    2    0    0    0    0    0    0    0\n##   CA     7   14   17    5    4    1    0    0    0    0    0    0    0\n##   CB     5   11    0    0    0    1    0    0    0    0    0    0    0\n##   CXF    0    1    2    0    1    0    0    0    0    0    0    0    0\n##   CXU    0    0    1    2    0    0    0    0    0    0    0    0    0\n##   CYF    0    5    2    0    0    0    0    0    0    0    0    0    0\n##   CYU    7    9    7    2    0    0    0    0    0    0    0    0    0\n##   D1     0    0    0    0    0    0    1    1    0    0    0    0    0\n##   D2     0    0    0    0    0    5   26    8    1    0    0    0    0\n##   D3     0    0    0    0    0    0    1    1    2    3    2    0    0\n##   D4     0    0    1    3   29   91   87    8    5    5    1    2    0\n##   D5     0    2    1   10    9   12    0    5    1    2    3    0    0\n##   D6     0    0    0    0    0    2    0    2    0    6    0    0    0\n##   DA     0    0    1    0    0    1    0    0    0    0    0    0    0\n##   HC     0    0    0    0    0    0   83    8   41    0    0    0    0\n##   M1     0    0    0    1    0    0    0    0    0    0    0    0    0\n##   R1     3    1   10    8  382 1641  902  133   52   10    3   10   10\n##   R2    33   46   76   42   19   11   48   25  163   27    0    2    0\n##   RA    47   51  165   87  115  104   13    7   26    6    4    3    0\n##   RB     4    5    8    5    1    0    0    0    0    0    0    0    0\n##   RR     0    0    0    0    1    9    3    0    0    1    1    0    0\n##   RS     0    0    2    0    4   33  145   26    8    0    0    0    0\n##   RXF   16   13  101   53   45   28    1    0    0    0    0    0    0\n##   RXU    3    8   31   12   18    7    0    0    0    0    0    0    0\n##   RYF   12   11    6    0    1    1    0    0    0    0    0    0    0\n##   RYU   16   25   21    7    2    0    0    0    2    0    0    0    0\n##   T1     0    0    0    0    0    0    0    6  124    8   22    0    0\n##   T2     0    0    0    0    0    0    0    0    3    0    0    6    0\n##   T3     0    0    0    0    0    0    0    4   10    0    0    0    0\n##   T4     0    0    0    0    0    0    0    0   16    0    0    1    0\n##   T5     0    0    0    0    0    0    0    0    8   29    0    0    0\n##   T6     0    0    0    0    0    0    0    0    4    1    0    0    0\n##   T7     0    0    0    0    0    0    0    0    5   13    3    0    0\n##   TA     0    0    0    0    0    0    0    0   74    4   32    0    0\n##   TS     0    0    0    0    0    0    0    6    3    0    0    0    0\n##   TYF    0    0    0    0    0    0    0    0    1    0    0    0    0\n\nplot(table(housing_lincoln$decade, housing_lincoln$Imp_Type),\n     main = \"Year Built and Improvement Type\")\n\n\n\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_lincoln %&gt;%\n  subset(Imp_Type %in% c(\"BN\", \"R1\", \"R2\", \"RA\")) %&gt;%\n  boxplot(TLA ~ Imp_Type, data = .)\n\n\n\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house.\n\n\n\nhousing_lincoln[\"TLA\"] = housing_lincoln[\"TLA (Sqft)\"].str.replace(\"[,\\$]\", \"\", regex = True)\n## NameError: name 'housing_lincoln' is not defined\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA\"] = housing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA (Sqft)\"]\n## NameError: name 'housing_lincoln' is not defined\nhousing_lincoln[\"TLA\"] = pd.to_numeric(housing_lincoln[\"TLA\"], errors = 'coerce')\n## NameError: name 'housing_lincoln' is not defined\n\nhousing_lincoln[\"Assessed\"] = housing_lincoln[\"Assd_Value\"].str.replace(\"[,\\$]\", \"\", regex = True)\n## NameError: name 'housing_lincoln' is not defined\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assessed\"] = housing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assd_Value\"]\n## NameError: name 'housing_lincoln' is not defined\nhousing_lincoln[\"Assessed\"] = pd.to_numeric(housing_lincoln[\"Assessed\"], errors = 'coerce')\n## NameError: name 'housing_lincoln' is not defined\n\nhousing_lincoln = housing_lincoln.drop([\"TLA (Sqft)\", \"Assd_Value\"], axis = 1)\n## NameError: name 'housing_lincoln' is not defined\n\n# housing_lincoln.describe()\nskim(housing_lincoln)\n## NameError: name 'housing_lincoln' is not defined\n\nLet‚Äôs examine the numeric and date variables first:\n\nhousing_lincoln[\"TLA\"].plot.hist()\n## NameError: name 'housing_lincoln' is not defined\nplt.show()\n\n\n\n\n\n\n\nhousing_lincoln[\"Yr_Blt\"].plot.hist()\n## NameError: name 'housing_lincoln' is not defined\nplt.show()\n\n\n\n\n\n\n\nLet‚Äôs look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nimport numpy as np\nhousing_lincoln['decade'] = 10*np.floor(housing_lincoln.Yr_Blt/10)\n## NameError: name 'housing_lincoln' is not defined\n\nhousing_lincoln[\"decade\"].groupby(housing_lincoln[\"decade\"]).count()\n## NameError: name 'housing_lincoln' is not defined\nhousing_lincoln[\"Imp_Type\"].groupby(housing_lincoln[\"Imp_Type\"]).count()\n## NameError: name 'housing_lincoln' is not defined\n\npd.crosstab(index = housing_lincoln[\"decade\"], columns = housing_lincoln[\"Imp_Type\"])\n## NameError: name 'housing_lincoln' is not defined\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(housing_lincoln, [\"decade\", \"Imp_Type\"], title = \"Housing Built by Type, Decade\")\n## NameError: name 'housing_lincoln' is not defined\nplt.show()\n\n\n\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_subcat = [\"BN\", \"R1\", \"RA\", \"R2\"]\n\nhousing_sub = housing_lincoln.loc[housing_lincoln[\"Imp_Type\"].isin(housing_subcat)]\n## NameError: name 'housing_lincoln' is not defined\nhousing_sub = housing_sub.assign(Imp_cat = pd.Categorical(housing_sub[\"Imp_Type\"], categories = housing_subcat))\n## NameError: name 'housing_sub' is not defined\n\nhousing_sub.boxplot(\"TLA\", by = \"Imp_cat\")\n## NameError: name 'housing_sub' is not defined\nplt.show()\n\n\n\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house; we would expect an increase in square footage with each additional floor of the house (broadly speaking).\n\n\n\n\n\n\n\n\n\n\n\nLearn More: Janitor R package\n\n\n\nThe janitor package [4] has some very convenient functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing.\n\n\njanitor and clean_names() by Allison Horst",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#sec-eda-refs",
    "href": "part-wrangling/02a-eda.html#sec-eda-refs",
    "title": "19¬† Exploratory Data Analysis",
    "section": "\n19.6 References",
    "text": "19.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O‚ÄôReilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nN. Tierney, D. Cook, M. McBain, and C. Fay, Naniar: Data structures, summaries, and visualisations for missing data. 2021 [Online]. Available: https://CRAN.R-project.org/package=naniar\n\n\n\n[3] \nDaniel Bourke, ‚ÄúA Gentle Introduction to Exploratory Data Analysis,‚Äù Daniel Bourke. Jan. 2019 [Online]. Available: https://www.mrdbourke.com/a-gentle-introduction-to-exploratory-data-analysis/. [Accessed: Jun. 13, 2022]\n\n\n[4] \nS. Firke, Janitor: Simple tools for examining and cleaning dirty data. 2021 [Online]. Available: https://CRAN.R-project.org/package=janitor",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02a-eda.html#footnotes",
    "href": "part-wrangling/02a-eda.html#footnotes",
    "title": "19¬† Exploratory Data Analysis",
    "section": "",
    "text": "One package for this process in R is naniar [2].‚Ü©Ô∏é\nA histogram is a chart which breaks up a continuous variable into ranges, where the height of the bar is proportional to the number of items in the range. A bar chart is similar, but shows the number of occurrences of a discrete variable.‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html",
    "href": "part-wrangling/02b-graphics.html",
    "title": "20¬† Data Visualization",
    "section": "",
    "text": "20.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#objectives",
    "href": "part-wrangling/02b-graphics.html#objectives",
    "title": "20¬† Data Visualization",
    "section": "",
    "text": "Create charts designed to communicate specific aspects of the data\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#introduction",
    "href": "part-wrangling/02b-graphics.html#introduction",
    "title": "20¬† Data Visualization",
    "section": "\n20.2 Introduction",
    "text": "20.2 Introduction\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. I‚Äôm going to be opinionated on this one - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics specifically.\n\n\nVisualization and statistical graphics are also my research area, so I‚Äôm more passionate about this material, which means there‚Äôs going to be more to read. Sorry about that in advance. I‚Äôll do my best to indicate which content is actually mission-critical and which content you can skip if you‚Äôre not that interested.\nThis is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That‚Äôs why I‚Äôm showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.\n\n\n\n\n\n\nGuides and Resources\n\n\n\n\n\nGraph galleries contain sample code to create many different types of charts. Similar galaries are available in R and Python.\nCheat Sheets:\n\nPython\nGgplot2\nBase R\n\nYoutube Playlist: a bunch of different ‚Äúhow to plot‚Äù tutorials on YouTube I found helpful\n\n\n\n\n\n\n\n\n20.2.1 Package Installation\nThis chapter will cover the following graphics libraries:\n\n\nggplot2 in R [1]\n\n\nplotnine in python [2], which is a clone of ggplot2\n\n\nseaborn in python [3], with some information about its next-generation interface that is much closer to a grammar-of-graphics implementation [4]\n\n\nTo a lesser degree, we will also cover some details of more basic plotting libraries:\n\n\nmatplotlib in python [5]\n\nBase R plotting libraries [6]\n\n\n\ninstall.packages(\"ggplot2\")\n\nTo install plotnine, pick one of the following methods (you can read more about them and decide which is appropriate for you in Section 9.8.3.1)\n\n\nSystem Terminal\nR Terminal\nPython Terminal\n\n\n\n\npip3 install plotnine matplotlib seaborn\n\n\n\nThis package installation method requires that you have a virtual environment set up (that is, if you are on Windows, don‚Äôt try to install packages this way).\n\nreticulate::py_install(c(\"plotnine\", \"matplotlib\", \"seaborn\"))\n\n\n\nIn a python chunk (or the python terminal), you can run the following command. This depends on something called ‚ÄúIPython magic‚Äù commands, so if it doesn‚Äôt work for you, try the System Terminal method instead.\n\n%pip install plotnine matplotlib seaborn\n\nOnce you have run this command, please comment it out so that you don‚Äôt reinstall the same packages every time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "href": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "title": "20¬† Data Visualization",
    "section": "\n20.3 Why do we create graphics?",
    "text": "20.3 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (‚ÄúWhy, that scatter diagram has a hole in the middle!‚Äù) ‚Äì John W. Tukey [7]\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the ‚Äúreal world‚Äù into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is ‚Äútangible‚Äù in some way ‚Äì if not physical and literally touch-able, at least something we can view and ‚Äúwrap our heads around‚Äù.\n\n\n\n\n\n\nThought Experiment\n\n\n\nYou have a simple data set - 2 variables, about 150 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\n\n\nDataset\nSummary statistics\nPlot\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur brains are very good at processing large amounts of visual information quickly. Evolution is good at optimizing for survival, and it‚Äôs important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we‚Äôre not nearly as equipped to process quickly (and in parallel).\nIn addition, it‚Äôs a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "href": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "title": "20¬† Data Visualization",
    "section": "\n20.4 Thinking Critically About Graphics",
    "text": "20.4 Thinking Critically About Graphics\nWhen we create graphics, we want to enable people to think about relationships between the variables in our dataset.\nFor this example, let‚Äôs consider the relationship between different physical measurements of penguins. We‚Äôll use the palmerpenguins package in R, which is also available in python.\n\n20.4.1 Load (and examine) the data\n\n\nR\nPython\n\n\n\n\n## install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 √ó 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n## 1 Adelie  Torgersen           39.1          18.7               181        3750\n## 2 Adelie  Torgersen           39.5          17.4               186        3800\n## 3 Adelie  Torgersen           40.3          18                 195        3250\n## 4 Adelie  Torgersen           NA            NA                  NA          NA\n## 5 Adelie  Torgersen           36.7          19.3               193        3450\n## 6 Adelie  Torgersen           39.3          20.6               190        3650\n## # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nIn your system console:\n\npip3 install palmerpenguins\n\nIn Python:\n\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\npenguins.head()\n##   species     island  bill_length_mm  ...  body_mass_g     sex  year\n## 0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n## 1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n## 2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n## 3  Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n## 4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n## \n## [5 rows x 8 columns]\n\n\n\n\nHow do physical measurements (bill width, bill depth, flipper length, and body mass) relate to each other?\n\n20.4.2 Initial plots\n\n\nR - ggplot2\nPy - plotnine\nPy - Seaborn\nPy - Seaborn Objects\nR base\nPy - Matplotlib\n\n\n\n\nlibrary(ggplot2)\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm)) + geom_point()\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm, color = species)) + geom_point()\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in ggplot2 is mapped to a plot feature using the aes() (aesthetic) function. This function automatically constructs a scale that e.g.¬†converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in red, Chinstrap in green, and Gentoo in blue.\nThat is, ggplot2 allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\n\nfrom plotnine import ggplot,aes,geom_point\nggplot(data = penguins, \n       mapping = aes(x = \"body_mass_g\", y = \"bill_length_mm\")) +\\\n  geom_point()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith ggplot2/plotnine, you begin to construct a plot by identifying the primary relationship of interest: you define what variables go on each axis (body mass on x, bill length on y) and then create a layer that shows the actual data (geom_point). Note that the mapping argument must be explicitly labeled in python because of some differences in requirements for function arguments between R and python. In addition, when in python, the variable names must be passed in as strings.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nggplot(data = penguins, \n       mapping = aes(x = \"body_mass_g\", y = \"bill_length_mm\", color = \"species\")) +\\\n  geom_point()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in ggplot2/plotnine is mapped to a plot feature using the aes() (aesthetic) function. This function automatically constructs a scale that e.g.¬†converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in red, Chinstrap in green, and Gentoo in blue. Notice that the shades are slightly different, as plotnine uses matplotlib as a base plotting library, and the default colors are part of that library.\nThat is, ggplot2/plotnine allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\n\nimport seaborn as sns \nimport matplotlib.pyplot\n\nsns.set_theme() # default theme\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\"\n)\n\n\n\n\n\n\n\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). In seaborn, this is called a relational plot - that is, it shows the relationship between the variables on the x and y axis. We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn, you begin to construct a plot by identifying the type of plot you want - whether it‚Äôs a relational plot, like a scatterplot or a lineplot, a distributional plot, like a histogram, density, CDF, or rug plot, or a categorical plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\", hue = \"species\"\n)\n\n\n\n\n\n\n\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn relational plot is mapped to a plot feature. This function automatically constructs a scale that e.g.¬†converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2 and plotnine, seaborn allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\n\nimport seaborn.objects as so\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\")\n  .add(so.Dot())\n  .show()\n)\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn objects interface, you begin to construct a plot by identifying the primary relationship of interest: you define what variables go on each axis (body mass on x, bill length on y) and then create a layer that shows the actual data (so.Dot()).\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\", color = \"species\")\n  .add(so.Dot())\n  .show()\n)\n\n\n\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn objects plot declaration is mapped to a plot feature. Seaborn then automatically constructs a scale that e.g.¬†converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2 and plotnine, seaborn‚Äôs objects interface allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\nYou may notice that this is much more similar to ggplot2 in syntax than the default Seaborn interface.\n\n\n\nplot(penguins$body_mass_g, penguins$bill_length_mm)\n\n\n\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nNotice that in base R, you have to reference each column of the data frame separately, as there is not an included data argument in the plot function.\nWhen we add color to the plot, it gets a little more complicated - to get a legend, we have to know a bit about how base R plots are constructed. When we pass in a factor as a color, R will automatically assign colors to each level of the factor.\n\nplot(penguins$body_mass_g, penguins$bill_length_mm, col = penguins$species)\nlegend(5500, 40,unique(penguins$species),col=1:3,pch=1)\n\n\n\n\n\n\n\nTo create a legend, we need to then identify where the legend should go (x = 5500, y = 40), and then the factor labels (unique(penguins$species)), and then the color levels assigned to those labels (1:3), and finally the point shape (pch = 1).\nThis manual legend creation process is a bit odd if you start from a grammar-of-graphics approach, but was for a long time the only way to make graphics in essentially any plotting system.\nPersonally, I still think the base plotting system creates charts that look a bit ‚Ä¶ ancient ‚Ä¶, but there are some people who very much prefer it to ggplot2 [8].\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots() # Create a new plot\n\nax.scatter(penguins.body_mass_g, penguins.bill_length_mm)\nplt.show()\n\n\n\n\n\n\n\nplt.close() # close the figure\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWhen we add color to the plot, it gets a little more complicated. Matplotlib does not automatically assign factors to colors - we have to instead create a dictionary of colors for each factor label, and then use the .map function to apply our dictionary to our factor variable.\nTo create a legend, we define handles and use the dictionary to create the legend. Then we have to manually position the legend in the lower right of the plot. Note that here, the legend position is defined using bbox_to_anchor in plot coordinates (x = 1, y = 0), and then the orientation of that box is defined using the loc parameter.\n\nfrom matplotlib.lines import Line2D  # for legend handle\nfig, ax = plt.subplots() # Create a new plot\n\n# Define a color mapping\ncolors = {'Adelie':'tab:blue', 'Gentoo':'tab:orange', 'Chinstrap':'tab:green'}\n\nax.scatter(x = penguins.body_mass_g, y = penguins.bill_length_mm, c = penguins.species.map(colors))\n# add a legend\nhandles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in colors.items()]\nax.legend(title='Species', handles=handles, bbox_to_anchor=(1, 0), loc='lower right')\n\nplt.show()\n\n\n\n\n\n\n\nplt.close() # close the figure\n\nMatplotlib is prettier than the Base R graphics, but we again have to manually create our legend, which is not ideal. Matplotlib is great for lower-level control over your plot, but that means you have to do a lot of the work manually. Personally, I much prefer to focus my attention on creating the right plot, and let sensible defaults take over whenever possible - this is the idea with both ggplot2 and seaborn objects.\n\n\n\n\n20.4.3 A graphing template\nIt would be convenient if we could create a template for each of these libraries that would help us create many different types of plots. Unfortunately, that‚Äôs going to be a bit difficult, because there are a number of libraries covered here that are not built on the idea of a grammar of graphics - that is, a set of vocabulary that can define a bunch of different types of charts. In this section, I‚Äôll show you basic templates for charts where such templates exist, and otherwise, I will try to give you a set of functions that may get you started.\n\n\nggplot2/plotnine\nSeaborn\nSeaborn Objects\nR base\nMatplotlib\n\n\n\nThe gg in ggplot2 stands for the grammar of graphics. In ggplot2/plotnine, we can describe any plot using a template that looks like this:\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM&gt;(mapping = aes(&lt;MAPPINGS&gt;), \n         position = &lt;POSITION&gt;, \n         stat = &lt;STAT&gt;) + \n  &lt;FACET&gt; + \n  &lt;COORD&gt; + \n  &lt;THEME&gt;\nGeoms are geometric objects, like points (geom_point), lines (geom_line), and rectangles (geom_rect, geom_bar, geom_column). Mappings are relationships between plot characteristics and variables in the dataset - setting x, y, color, fill, size, shape, linetype, and more. Critically, when a visual characteristic, such as color, fill, shape, or size, is a constant, it is set outside of an aes() statement; when it is mapped to a variable, it must be provided inside the aes() statement. Read more about aesthetics\nIn ggplot2/plotnine, statistics can be used within geoms to summarize data. Additional modifications of plots in ggplot2 include changing the coordinate system, adjusting positions, and adding subplots using facets. Not all of these are fully implemented in plotnine, but most features are available in both systems.\n\n\nIn seaborn, we can describe plots using three different basic templates:\n\n\nSeaborn API module organization. Source\n\nsns.relplot(data = &lt;DATA&gt;, x = &lt;X var&gt;, y = &lt;Y var&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\nsns.displot(data = &lt;DATA&gt;, x = &lt;X var&gt;, y = &lt;Y var, optional&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\nsns.catplot(data = &lt;DATA&gt;, x = &lt;Categorical X var&gt;, y = &lt;Y var&gt;, \n            kind = &lt;PLOT TYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\n\n# General version\nsns.&lt;PLOTTYPE&gt;(data = &lt;DATA&gt;, &lt;MAPPINGS&gt;, \n               kind = &lt;PLOT SUBTYPE&gt;, &lt;ADDITIONAL ARGS&gt;)\nHere, the kind argument is somewhat similar to the geom argument in ggplot2, but there are different functions used for different purposes. This seems to be a way to handle statistical transformations without the full grammar-of-graphics implementation of statistics that is found in ggplot2. Read more about statistical relationships, distributions, and categorical data in the seaborn tutorial.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\nWe can come up with a generic plot template that looks something like this:\n(\n  so.Plot(&lt;DATA&gt;, &lt;MAPPINGS&gt;, &lt;GROUPS&gt;)\n  .add(so.&lt;GEOM&gt;(&lt;ARGS&gt;), so.&lt;TRANSFORMATION&gt;, so.&lt;POSITION&gt;)\n  .show()\n)\n\n\nR‚Äôs base graphics library is decidedly non-grammar-of-graphics like. Each plot is defined using plot-specific arguments that are not particularly consistent across different functions.\nThe following is as close as I can get, but it‚Äôs still not accurate for many plots.\n&lt;PLOT NAME&gt;(&lt;MAPPINGS&gt;, &lt;ARGS&gt;)\n&lt;LEGEND&gt;(&lt;MAPPINGS&gt;, &lt;ARGS&gt;)\nAny statistics are either computed as part of the plot function (e.g.¬†hist()) or there are plot methods to accompany the statistic calculation (plot(density(...))). Facets/subplots are sometimes created using par(mfrow=(...)) to define sub-plots, and are sometimes created by the plot command itself, in the case of e.g.¬†passing a numeric matrix to plot(), which creates a scatterplot matrix.\nUseful commands: plot() for scatterplots, lines (type = ‚Äòl‚Äô), scatterplot matrices, etc. hist() for histograms. plot(density(...)) for density plots. boxplot() for boxplots, barplot() for bar plots, ‚Ä¶\n\n\nI‚Äôm not even going to try to create a grammar summary for matplotlib‚Ä¶ there just isn‚Äôt one. It wasn‚Äôt designed with the grammar of graphics in mind, and was built by computer scientists [9], not statisticians. As a result, even retrofitting some sort of ‚Äúgrammar‚Äù interpretation doesn‚Äôt work so well.\nFor a primer on matplotlib, I recommend [10], Ch 9.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "href": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "title": "20¬† Data Visualization",
    "section": "\n20.5 Advanced: Charts, Graphics, and the Grammar of Graphics",
    "text": "20.5 Advanced: Charts, Graphics, and the Grammar of Graphics\nThere are two general approaches to generating statistical graphics computationally:\n\nManually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.\nBase R, matplotlib, old-style SAS graphics\nDescribe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.\nggplot2, plotnine, seaborn (sort of), seaborn objects\n\n\n\nThere is a difference between low-level plotting libraries (base R, matplotlib) and high-level plotting libraries (ggplot2, plotnine, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it‚Äôs probably a low level library.\nIn the introduction to The Grammar of Graphics [11], Leland Wilkinson suggests that the first approach is what we would call ‚Äúcharts‚Äù - pie charts, line charts, bar charts - objects that are ‚Äúinstances of much more general objects‚Äù. His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the ‚Äúgrammar of graphics‚Äù.\n\n\nThere are other graphics systems (namely, lattice in R, seaborn in Python, and some web-based rendering engines like Observable or d3) that you could explore, but it‚Äôs far more important that you know how to functionally create plots in R and/or Python. I don‚Äôt recommend you try to become proficient in all of them. Pick one (two at most) and get familiar with those libraries, then google for the rest.\nBefore we delve into the grammar of graphics, let‚Äôs motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we‚Äôve known it for 100 years[12], so in the interests of showing that we know that pie charts are awful, we‚Äôll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We‚Äôll talk about what makes pie charts terrible at the end of this module in Creating Good charts.\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\n\n\n\nSuppose we want to explore Pokemon. There‚Äôs not just the original 150 (gotta catch ‚Äôem all!) - now there are over 1000! Let‚Äôs start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %&gt;%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\nOnce the data is read in, we can start plotting:\n\n\nggplot2\nBase R\nMatplotlib\nPlotnine\nSeaborn\n\n\n\nIn ggplot2, we start by specifying which variables we want to be mapped to which features of the data.\nIn a pie or stacked bar chart, we don‚Äôt care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single ‚Äústack‚Äù. So it‚Äôs easiest to specify our x variable as a constant, ‚Äú‚Äú. We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable.\nThen, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want ‚Äì that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, ‚Äúdodge‚Äù).\n\nlibrary(ggplot2)\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") \n\n\n\n\n\n\n\nIf we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the ‚Äúy‚Äù coordinate, and that we want to start at \\(\\theta = 0\\).\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") + \n  coord_polar(\"y\", start = 0)\n\n\n\n\n\n\n\nNotice how the syntax and arguments to the functions didn‚Äôt change much between the bar chart and the pie chart? That‚Äôs because the ggplot package uses what‚Äôs called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it‚Äôs very easy to switch between plot types by changing one word or adding one transformation.\n\n\nLet‚Äôs start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\npie(tmp$n, labels = tmp$generation)\n\n\n\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs.¬†total) but does so in a linear fashion.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\n# Matrix is necessary for a stacked bar chart\nmatrix(tmp$n, nrow = 9, ncol = 1, dimnames = list(tmp$generation)) %&gt;%\nbarplot(beside = F, legend.text = T, main = \"Generations of Pokemon\")\n\n\n\n\n\n\n\nThere‚Äôs not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.\n\n\nLet‚Äôs start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)\n## ([&lt;matplotlib.patches.Wedge object at 0x7efb742d9850&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695bc8d0&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695be410&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695bda90&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695c9f10&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695cbf90&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695d5b10&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695d78d0&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb695e56d0&gt;], [Text(-0.6090072830464104, 0.9160295460280905, '1'), Text(-1.0954901625854225, -0.0995052947262842, '2'), Text(-0.6165300405105602, -0.9109833747923434, '3'), Text(0.18481484283490274, -1.0843631651194678, '4'), Text(0.7975738857504167, -0.7575459700697914, '5'), Text(1.0758366105622164, -0.22929367059298023, '6'), Text(1.044469949497543, 0.3450833589099894, '7'), Text(0.7443080435855409, 0.8099416869465756, '8'), Text(0.2667977873990754, 1.0671546001582704, '9')], [Text(-0.3321857907525874, 0.4996524796516857, '18.7%'), Text(-0.5975400886829577, -0.05427561530524592, '15.5%'), Text(-0.3362891130057601, -0.4969000226140054, '12.6%'), Text(0.10080809609176512, -0.5914708173378915, '11.7%'), Text(0.43504030131840904, -0.4132068927653407, '8.8%'), Text(0.5868199693975725, -0.1250692748688983, '8.7%'), Text(0.5697108815441142, 0.18822728667817604, '8.1%'), Text(0.40598620559211324, 0.44178637469813214, '8.1%'), Text(0.14552606585404113, 0.5820843273590565, '7.8%')])\nax1.axis('equal')\n## (-1.0999996160491716, 1.099999961997774, -1.099999694088651, 1.0999999854327929)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs.¬†total) but does so in a linear fashion.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts()\nsizes = sizes.sort_index()\n\n# Find location of bottom of the bar for each bar\ncumulative_sizes = sizes.cumsum() - sizes\nwidth = 1\n\nfig, ax = plt.subplots()\n\nfor i in sizes.index:\n  ax.bar(\"Generation\", sizes[i-1], width, label=i, bottom = cumulative_sizes[i-1])\n## KeyError: 0\n\nax.set_ylabel('# Pokemon')\nax.set_title('Pokemon Distribution by Generation')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs of January 2023, pie charts are still not supported in plotnine. So this demo will fall a bit flat.\n\nfrom plotnine import *\nplt.cla() # clear out matplotlib buffer\n\nggplot(poke, aes(x = \"1\", fill = \"generation\")) + geom_bar(position = \"stack\")\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs with Plotnine, seaborn does not support pie charts due to the same underlying issue. The best option is to create a pie chart in matplotlib and use seaborn colors [13].\n\n# Load the seaborn package\n# the alias \"sns\" stands for Samuel Norman Seaborn\n# from \"The West Wing\" television show\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Initialize seaborn styling; context\nsns.set_style('white')\nsns.set_context('notebook')\n\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n#define Seaborn color palette to use\ncolors = sns.color_palette()[0:9]\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, colors = colors, autopct='%1.1f%%', startangle = 90)\n## ([&lt;matplotlib.patches.Wedge object at 0x7efb6c131e90&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66125490&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66127190&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66125e50&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66136d90&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66138b90&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb6613a710&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66148410&gt;, &lt;matplotlib.patches.Wedge object at 0x7efb66136b90&gt;], [Text(-0.6090072830464104, 0.9160295460280905, '1'), Text(-1.0954901625854225, -0.0995052947262842, '2'), Text(-0.6165300405105602, -0.9109833747923434, '3'), Text(0.18481484283490274, -1.0843631651194678, '4'), Text(0.7975738857504167, -0.7575459700697914, '5'), Text(1.0758366105622164, -0.22929367059298023, '6'), Text(1.044469949497543, 0.3450833589099894, '7'), Text(0.7443080435855409, 0.8099416869465756, '8'), Text(0.2667977873990754, 1.0671546001582704, '9')], [Text(-0.3321857907525874, 0.4996524796516857, '18.7%'), Text(-0.5975400886829577, -0.05427561530524592, '15.5%'), Text(-0.3362891130057601, -0.4969000226140054, '12.6%'), Text(0.10080809609176512, -0.5914708173378915, '11.7%'), Text(0.43504030131840904, -0.4132068927653407, '8.8%'), Text(0.5868199693975725, -0.1250692748688983, '8.7%'), Text(0.5697108815441142, 0.18822728667817604, '8.1%'), Text(0.40598620559211324, 0.44178637469813214, '8.1%'), Text(0.14552606585404113, 0.5820843273590565, '7.8%')])\nax1.axis('equal')\n## (-1.0999996160491716, 1.099999961997774, -1.099999694088651, 1.0999999854327929)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeaborn doesn‚Äôt have the autopct option that matplotlib uses, so we have to aggregate the data ourselves before creating a barchart.\n\npoke['generation'] = pd.Categorical(poke['generation'])\n\nplt.cla() # clear out matplotlib buffer\nsns.catplot(data = poke, x = 'generation', kind = \"count\")\n\n\n\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNintendo, Creatures, Game Freak, The Pok√©mon Company, Public domain, via Wikimedia Commons\n\nThe grammar of graphics is an approach first introduced in Leland Wilkinson‚Äôs book [11]. Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the dataset itself relates to the components of the chart.\n\n\nBuilding a masterpiece, by Allison Horst\n\nThis has a few advantages:\n\nIt‚Äôs relatively easy to represent the same dataset with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: [14]\n\n\n\n\n\n\n\nNote\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g.¬†missing points - this is normal, I just didn‚Äôt want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you‚Äôre envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\n\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we‚Äôll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from [14]).\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing)\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point(alpha = .2)\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\nfrom plotnine.data import txhousing\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point(alpha = .1)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nfrom plotnine.data import txhousing\nimport seaborn.objects as so\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .show()\n)",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "href": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "title": "20¬† Data Visualization",
    "section": "\n20.6 Exploratory Data Analysis and Plot Customization",
    "text": "20.6 Exploratory Data Analysis and Plot Customization\nWhen you first get a new dataset, it is critical that you, as the analyst, get a feel for the data. John Tukey, in his book [15], likens exploratory data analysis to detective work, and says that in both cases the analyst needs two things: tools, and understanding. Tools, like fingerprint powder or types of charts, are essential to collect evidence; understanding helps the analyst to know where to apply the tools and what to look out for.\nTukey claims that while tools are constantly evolving, and there may not be one set of ‚Äúbest‚Äù tools, understanding is a bit different. There are situational elements ‚Äì you might need different base knowledge to solve a crime in a small village in the UK than you need to solve a crime in New York City or Buenos Aires. However, the broad strokes of what questions are asked during an investigation will be similar across different locations and types of crimes.\nThe same thing is true in exploratory data analysis - while it is helpful to have a basic understanding of the dataset, and being intimately familiar with the type of data and data collection processes can give the analyst an advantage, the same basic detective skills will be useful across a wide variety of data sets.\nThere is one other aspect of the analogy between EDA and detective work that is useful: the detective gathers evidence, but does not try the case or make decisions about what should happen to the accused. Similarly, the individual conducting an exploratory data analysis should not move too quickly to hypothesis testing and other confirmatory data analysis techniques. In exploratory data analysis, the goal is to lay the foundation, assess the evidence (data) for interesting clues, and to try to understand the whole story. Only once this process is complete should we move to any sort of confirmatory analysis.\nIn this section, we‚Äôll primarily use charts as a tool for exploratory data analysis. Pay close attention not only to the tools, but to the process of inquiry.\n\n20.6.1 Texas Housing Data\nLet‚Äôs explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of the features of each plotting library.\n\n20.6.2 Starting Chart\nBefore we start exploring, let‚Äôs start with a basic scatterplot, and add a title and label our axes, so that we‚Äôre creating good, informative charts.\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\n\n\n\n\n20.6.3 Exploring Trends\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we‚Äôve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"lm\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n# By default, geom_smooth in plotnine has a black line you can't see well\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .add(so.Line(color = \"black\"), so.PolyFit(order = 2))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\nI haven‚Äôt yet figured out a way to do locally weighted regression in Seaborn using the objects interface, but I‚Äôm sure that will come as the interface develops.\n\n\n\n\n20.6.4 Adding Complexity with Moderating Variables\nLooking at the plots here, it‚Äôs clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let‚Äôs see if we can figure out what those additional variables are‚Ä¶\nAs it happens, the best viable option is City.\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we‚Äôve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nThat‚Äôs a really crowded graph! It‚Äôs slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nThat‚Äôs a really crowded graph! It‚Äôs slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.5}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nThis is one of the first places we see differences in Python and R‚Äôs graphs - python doesn‚Äôt allocate sufficient space for the legend by default. In Python, you have to manually adjust the theme to show the legend (or plot the legend separately).\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\n\n\n\n\n20.6.5 Data Reduction Strategies\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn‚Äôt work well perceptually.\nSo let‚Äôs work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities).\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called ‚Äúfacets‚Äù. In visualization terms, we call this type of plot ‚Äúsmall multiples‚Äù - we have many small charts, each showing the trend for a subset of the data.\n\n\nggplot2\nplotnine\nSeaborn\n\n\n\n\ncitylist &lt;- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub &lt;- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nHere‚Äôs the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nNotice I‚Äôve removed the aesthetic mapping to color as it‚Äôs redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\nggplot(housingsub, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.75}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nHere‚Äôs the facetted version of the chart:\n\nggplot(housingsub, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nfacet_wrap(\"city\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n\n\n\n\n\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\")\n  .facet(col = \"city\")\n  .show()\n)\n\n\n\n\n\n\n\n\n\n\n\n20.6.6 Adding Additional Complexity\nNow that we‚Äôve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\n\nggplot2\nplotnine\nSeaborn\n\n\n\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  # Remove extra information from the legend -\n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',\n                             override.aes = list(linetype = NA,\n                                                 fill = 'transparent'))) +\n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nNotice I‚Äôve removed the aesthetic mapping to color as it‚Äôs redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"date\", y = \"median\", size = \"sales\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ facet_wrap(\"city\")\n+ guides(size = guide_legend(title = 'Number of Sales'))\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nNot all of the features we used in R are available in plotnine in Python (in part because of limitations of the underlying graphics interface that plotnine uses). This does somewhat limit the customization we can do with python, but for the most part we can still get the same basic information back out.\n\n\nComing soon!\n\n\n\n\n20.6.7 Exploring Other Variables and Relationships\nUp to this point, we‚Äôve used the same position information - date for the y axis, median sale price for the y axis. Let‚Äôs switch that up a bit so that we can play with some transformations on the x and y axis and add variable mappings to a continuous variable.\n\n\nggplot2\nplotnine\n\n\n\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation1. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n\n\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"listings\", y = \"sales\", color = \"city\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ scale_x_log10()\n+ scale_y_log10()\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nNotice that the gridlines included in python by default are different than those in ggplot2 by default (personally, I vastly prefer the python version - it makes it obvious that we‚Äôre using a log scale).\n\n\n\n\n20.6.8 Adding Additional Moderating Variables\nFor the next demonstration, let‚Äôs look at just Houston‚Äôs data. We can examine the inventory‚Äôs relationship to the number of sales by looking at the inventory-date relationship in x and y, and mapping the size or color of the point to number of sales.\n\n\nggplot2\nplotnine\n\n\n\n\nhouston &lt;- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) +\n  geom_point(shape = 1) +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_legend(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) +\n  geom_point() +\n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Date\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\n\n\nIs that easier or harder to read?\n\n\n\nhouston = txhousing[txhousing.city==\"Houston\"]\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", size = \"sales\"))\n  + geom_point(shape = 'o', fill = 'none')\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nIn plotnine, we have to use matplotlib marker syntax.\n\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", color = \"sales\"))\n  + geom_point()\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nPlotnine also defaults to different color schemes than ggplot2 ‚Äì just something to know if you want the plot to be exactly the same. Personally, I prefer the viridis color scheme (what plotnine uses) to the ggplot2 defaults.\nWhat happens if we move the variables around and map date to the point color?\n\n(\nggplot(houston, aes(x = \"sales\", y = \"inventory\", color = \"date\"))\n  + geom_point()\n  + xlab(\"Number of Sales\") + ylab(\"Months of Inventory\")\n  + guides(size = guide_colorbar(title = \"Date\"))\n  + ggtitle(\"Houston Housing Data\")\n  + theme(subplots_adjust={'right': 0.75})\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\nIs that easier or harder to read?",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "href": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "title": "20¬† Data Visualization",
    "section": "\n20.7 What type of chart to use?",
    "text": "20.7 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type.\nConsider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you‚Äôve thought through this, take a look through catalogs like the R Graph Gallery or the Python Graph Gallery to see what visualizations match your data and use-case.\nChapter 21 talks in more depth about considerations for creating good charts; these considerations may also inform your decisions.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#additional-reading",
    "href": "part-wrangling/02b-graphics.html#additional-reading",
    "title": "20¬† Data Visualization",
    "section": "\n20.8 Additional Reading",
    "text": "20.8 Additional Reading\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nR graph cookbook\n\nData Visualization in R (@ramnathv)\nPython graphics\n\nPlotnine documentation\n\nMatplotlib documentation - Matplotlib is the base that plotnine uses to replicate ggplot2 functionality\n\n\nVisualization with Matplotlib chapter of Python Data Science\n\n\nScientific Visualization with Python",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#sec-graphics-b-refs",
    "href": "part-wrangling/02b-graphics.html#sec-graphics-b-refs",
    "title": "20¬† Data Visualization",
    "section": "\n20.9 References",
    "text": "20.9 References\n\n\n\n\n[1] \nH. Wickham, ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016 [Online]. Available: https://ggplot2.tidyverse.org\n\n\n\n[2] \nH. Kibirige, ‚ÄúA grammar of graphics for python. Plotnine 0.10.1 documentation,‚Äù 2022. [Online]. Available: https://plotnine.readthedocs.io/en/stable/. [Accessed: Feb. 06, 2023]\n\n\n[3] \nM. Waskom, ‚ÄúAn introduction to seaborn. Seaborn 0.12.2 documentation,‚Äù 2022. [Online]. Available: https://seaborn.pydata.org/tutorial/introduction.html. [Accessed: Feb. 06, 2023]\n\n\n[4] \nM. Waskom, ‚ÄúNext-generation seaborn interface. Seaborn nextgen documentation,‚Äù 2022. [Online]. Available: https://seaborn.pydata.org/nextgen/. [Accessed: Aug. 29, 2022]\n\n\n[5] \nThe matplotlib development team, ‚ÄúMatplotlib ‚Äî visualization with python. Matplotlib,‚Äù 2023. [Online]. Available: https://matplotlib.org/. [Accessed: Feb. 06, 2023]\n\n\n[6] \nR. D. Peng, ‚ÄúThe base plotting system,‚Äù in Exploratory data analysis with r, 1st ed., leanpub, 2020 [Online]. Available: https://bookdown.org/rdpeng/exdata/. [Accessed: Feb. 06, 2023]\n\n\n[7] \nJ. W. Tukey, ‚ÄúData-Based Graphics: Visual Display in the Decades to Come,‚Äù Statistical Science, vol. 5, no. 3, pp. 327‚Äì339, Aug. 1990, doi: 10.1214/ss/1177012101. [Online]. Available: https://projecteuclid.org/journals/statistical-science/volume-5/issue-3/Data-Based-Graphics--Visual-Display-in-the-Decades-to/10.1214/ss/1177012101.full. [Accessed: Aug. 22, 2022]\n\n\n[8] \nN. Yau, ‚ÄúComparing ggplot2 and r base graphics. FlowingData,‚Äù Mar. 22, 2016. [Online]. Available: https://flowingdata.com/2016/03/22/comparing-ggplot2-and-r-base-graphics/. [Accessed: Feb. 06, 2023]\n\n\n[9] \nW. Koehrsen, ‚ÄúThe next level of data visualization in python. Medium,‚Äù Jan. 24, 2019. [Online]. Available: https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e. [Accessed: Feb. 06, 2023]\n\n\n[10] \nW. McKinney, Python for data analysis, 3rd ed. O‚ÄôReilly, 2022 [Online]. Available: https://wesmckinney.com/book/. [Accessed: Feb. 06, 2023]\n\n\n[11] \nL. Wilkinson, The grammar of graphics. New York: Springer, 1999. \n\n\n[12] \nF. E. Croxton and R. E. Stryker, ‚ÄúBar Charts Versus Circle Diagrams,‚Äù Journal of the American Statistical Association, vol. 22, no. 160, pp. 473‚Äì482, 1927, doi: 10.2307/2276829. \n\n\n[13] \nZach, ‚ÄúHow to Create a Pie Chart in Seaborn,‚Äù Statology. Jul. 2021 [Online]. Available: https://www.statology.org/seaborn-pie-chart/. [Accessed: Sep. 19, 2022]\n\n\n[14] \nD. (DJ). Sarkar, ‚ÄúA Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-dimensional‚Ä¶,‚Äù Medium. Sep. 2018 [Online]. Available: https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149. [Accessed: Apr. 11, 2022]\n\n\n[15] \nJ. Tukey, Exploratory data analysis. Addison-Wesley Publishing Company, 1977.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#footnotes",
    "href": "part-wrangling/02b-graphics.html#footnotes",
    "title": "20¬† Data Visualization",
    "section": "",
    "text": "This isn‚Äôt necessarily a good thing, but you should know how to do it. The jury is still very much out on whether log transformations make data easier to read and understand‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html",
    "href": "part-wrangling/02c-good-graphics.html",
    "title": "21¬† Creating Good Charts",
    "section": "",
    "text": "21.1  Objectives\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g.¬†the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#objectives",
    "href": "part-wrangling/02c-good-graphics.html#objectives",
    "title": "21¬† Creating Good Charts",
    "section": "",
    "text": "Evaluate existing charts and develop new versions that improve accessibility and readability",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#perceptual-and-cognitive-factors",
    "href": "part-wrangling/02c-good-graphics.html#perceptual-and-cognitive-factors",
    "title": "21¬† Creating Good Charts",
    "section": "\n21.2 Perceptual and Cognitive Factors",
    "text": "21.2 Perceptual and Cognitive Factors\n\n21.2.1 Preattentive Features\nYou‚Äôve almost certainly noticed that some graphical tasks are easier than others. Part of the reason for this is that certain tasks require active engagement and attention to search through the visual stimulus; others, however, just ‚Äúpop‚Äù out of the background. We call these features that just ‚Äúpop‚Äù without active work preattentive features; technically, they are detected within the first 250ms of viewing a stimulus [1].\nTake a look at Figure¬†21.1; can you spot the point that is different?\n\n\n\n\n\n\n\n\n\n(a) Shape\n\n\n\n\n\n\n\n\n\n(b) Color\n\n\n\n\n\n\nFigure¬†21.1: Two scatterplots with one point that is different. Can you easily spot the different point?\n\n\nColor and shape are commonly used graphical features that are processed pre-attentively. Some people suggest utilizing this to pack more dimensions into multivariate visualizations [2], but in general, knowing which features are processed more quickly (color/shape) and which are processed more slowly (combinations of preattentively processed features) allows you to design a chart that requires less cognitive effort to read.\nAs awesome as it is to be able to use preattentive features to process information, we should not use combinations of preattentive features to show different variables. Take a look at Figure¬†21.2 - part (a) shows the same grouping in color and shape, part (b) shows color and shape used to encode different variables.\n\n\n\n\n\n\n\n\n\n(a) Shape and Color (dual encoded)\n\n\n\n\n\n\n\n\n\n(b) Shape and Color (different variables)\n\n\n\n\n\n\nFigure¬†21.2: Two scatterplots. Can you easily spot the different point(s)?\n\n\nHere, it is easy to differentiate the points in Figure¬†21.2(a), because they are dual-encoded. However, it is very difficult to pick out the different groups of points in Figure¬†21.2(b) because the combination of preattentive features requires active attention to sort out.\nTakeaways\nCareful use of preattentive features can reduce the cognitive effort required for viewers to perceive a chart.\nEncode only one variable using preattentive features, as combinations of preattentive features are not processed preattentively.\n\n21.2.2 Color\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum, as shown in Figure¬†21.3. Why? Well, our sun produces yellow light, and plants tend to be green. It‚Äôs pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren‚Äôt that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\n\n\n\nFigure¬†21.3: Sensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient [3]. We have 3 cones used for color detection, as well as cells called rods, which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\n\n\n\n\nColorblindness Testing\n\n\n\n\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\n\n\n\n\n\n\n(a) The original illusion\n\n\n\n\n\n\n\n\n\n(b) The illusion with the checkerboard and shadow removed\n\n\n\n\n\n\nFigure¬†21.4: The color constancy illusion. The squares marked A and B are actually the same color.\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly ‚Äúremove‚Äù the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown in Figure¬†21.4 - because we‚Äôre correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\n\n21.2.2.1 Takeaways\n\nDo not use rainbow color gradient schemes\n\nbecause of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\n\n\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo ‚Äúcolorblind-proof‚Äù a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g.¬†showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors ‚Äúmean‚Äù\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g.¬†blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -&gt; blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.1\n\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women2.\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n21.2.3 Short Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\n\n\n\n\n\n\nTry it out!\n\n\n\n\nClick here, read the information, and then click to hide it.\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\nWhat was the third number?\n\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 ‚Äúbits‚Äù of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g.¬†a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n21.2.4 Grouping and Sense-making\nImposing order on visual chaos.\n\n\nAmbiguous Images\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does Figure¬†21.5 look like to you?\n\n\n\n\n\nFigure¬†21.5: Is it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\nDid you see something like ‚Äú3 circles, a triangle with a black outline, and a white triangle on top of that‚Äù? In reality, there are 3 angles and 3 pac-man shapes. But, it‚Äôs much more likely that we‚Äôre seeing layers of information, where some of the information is obscured (like the ‚Äúmouth‚Äù of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we‚Äôre not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving ‚Äúwith‚Äù the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\n\n\n\n\n\nCaution\n\n\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\n\nR\nPython\n\n\n\n\nfbiwide &lt;- read.csv(\"https://github.com/srvanderplas/Stat151/raw/main/data/fbiwide.csv\")\nlibrary(dplyr)\n\nfbiwide %&gt;%\n  filter(Year %in% c(1980, 2010)) %&gt;%\n  filter(State %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\")) %&gt;%\n  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfbiwide = r.fbiwide\nfbiwide = fbiwide.assign(YearFactor = pd.Categorical(fbiwide.Year))\nfbiwide = fbiwide.assign(Murder100k = fbiwide.Murder/fbiwide.Population * 100000)\n\nyr1980_2010 = fbiwide[fbiwide.Year.isin([1980,2010])]\nsubdata = yr1980_2010[yr1980_2010.State.isin([\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\"])]\n\n(\nggplot(subdata, aes(x = \"State\", y = \"Murder100k\", fill = \"YearFactor\")) +\n  geom_col(stat='identity', position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## NameError: name 'ggplot' is not defined\n\n\n\n\nOr, I could use a line chart\n\n\nR\nPython\n\n\n\n\nfbiwide %&gt;%\n  filter(Year %in% c(1980, 2010)) %&gt;%\n  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"Year\", y = \"Murder100k\", group = \"State\")) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## NameError: name 'ggplot' is not defined\n\n\n\n\nOr, I could use a box plot\n\n\nR\nPython\n\n\n\n\nfbiwide %&gt;%\n  filter(Year %in% c(1980, 2010)) %&gt;%\n  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"YearFactor\", y = \"Murder100k\")) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## NameError: name 'ggplot' is not defined\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g.¬†almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn‚Äôt allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can‚Äôt see the individual relationships.\n\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.3",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#general-guidelines-for-accuracy",
    "href": "part-wrangling/02c-good-graphics.html#general-guidelines-for-accuracy",
    "title": "21¬† Creating Good Charts",
    "section": "\n21.3 General guidelines for accuracy",
    "text": "21.3 General guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.4\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable ‚Äì they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n.\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\nExtra dimensions and other annotations are sometimes called ‚Äúchartjunk‚Äù and should only be used if they contribute to the overall numerical accuracy of the chart (e.g.¬†they should not just be for decoration).",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#sec-good-graphics-refs",
    "href": "part-wrangling/02c-good-graphics.html#sec-good-graphics-refs",
    "title": "21¬† Creating Good Charts",
    "section": "\n21.4 References",
    "text": "21.4 References\n\n\n\n\n[1] \nA. Treisman, ‚ÄúPreattentive processing in vision,‚Äù Computer Vision, Graphics, and Image Processing, vol. 31, no. 2, pp. 156‚Äì177, Aug. 1985, doi: 10.1016/S0734-189X(85)80004-9. \n\n\n[2] \nC. G. Healey, K. S. Booth, and J. T. Enns, ‚ÄúHigh-speed visual estimation using preattentive processing,‚Äù ACM Transactions on Computer-Human Interaction (TOCHI), vol. 3, no. 2, pp. 107‚Äì135, 1996, doi: 10.1145/230562.230563. \n\n\n[3] \nWikipedia contributors, ‚ÄúColor blindness,‚Äù Wikipedia, May 2023.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#footnotes",
    "href": "part-wrangling/02c-good-graphics.html#footnotes",
    "title": "21¬† Creating Good Charts",
    "section": "",
    "text": "When the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.‚Ü©Ô∏é\nLisa Charlotte Rost. What to consider when choosing colors for data visualization.‚Ü©Ô∏é\nSee this paper for more details. This is the last chapter of my dissertation, for what it‚Äôs worth. It was a lot of fun. (no sarcasm, seriously, it was fun!)‚Ü©Ô∏é\nSee this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged. Note that most of the items in this ranking were not examined in the linked paper, but are a synthesis of different experiments and conceptual knowledge in psychology as well as statistical graphics.‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Creating Good Charts</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html",
    "href": "part-wrangling/03-data-cleaning.html",
    "title": "22¬† Data Cleaning",
    "section": "",
    "text": "22.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#objectives",
    "href": "part-wrangling/03-data-cleaning.html#objectives",
    "title": "22¬† Data Cleaning",
    "section": "",
    "text": "Identify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately\nApply data manipulation verbs to prepare data for analysis\nUnderstand the consequences of data cleaning steps for statistical analysis\nCreate summaries of data appropriate for analysis or display using data manipulation techniques",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#introduction",
    "href": "part-wrangling/03-data-cleaning.html#introduction",
    "title": "22¬† Data Cleaning",
    "section": "\n22.2 Introduction",
    "text": "22.2 Introduction\nIn this section, we‚Äôre going start learning how to work with data. Generally speaking, data doesn‚Äôt come in a form suitable for analysis1 - you have to clean it up, create the variables you care about, get rid of those you don‚Äôt care about, and so on.\n\n\n\n\nData wrangling (by Allison Horst)\n\nSome people call the process of cleaning and organizing your data ‚Äúdata wrangling‚Äù, which is a fantastic way to think about chasing down all of the issues in the data.\nIn R, we‚Äôll be using the tidyverse for this. It‚Äôs a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You‚Äôve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a ‚Äúgrammar of data manipulation‚Äù to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I‚Äôm going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework).\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first ‚Äì literally, as it‚Äôs the first argument to any function. In addition, you don‚Äôt use df$variable to access a variable - you refer to the variable by its name alone (‚Äúbare‚Äù names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nIn Python, most data manipulation tasks are handled using pandas[1]. In the interests of using a single consistent ‚Äúlanguage‚Äù for describing data manipulation tasks, I‚Äôll use the tidyverse ‚Äúverbs‚Äù to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\nThere is also the datar python package[2], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, datar may be more useful if you prefer the dplyr way of handling things using a data-first API.\n\n\nI haven‚Äôt had the chance to add the datar package to this book, but it looks promising and may be worth your time to figure out. It‚Äôs a bit too new for me to teach right now - I want packages that will be maintained long-term if I‚Äôm going to teach them to others.\n\n\n\n\n\n\nNote\n\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.\nHere is a data wrangling with pandas cheatsheet that is formatted similarly to the dplyr cheat sheet.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#tidy-data",
    "href": "part-wrangling/03-data-cleaning.html#tidy-data",
    "title": "22¬† Data Cleaning",
    "section": "\n22.3 Tidy Data",
    "text": "22.3 Tidy Data\nThere are infinitely many ways to configure ‚Äúmessy‚Äù data, but data that is ‚Äútidy‚Äù has 3 attributes:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\nThese attributes aren‚Äôt sufficient to define ‚Äúclean‚Äù data, but they work to define ‚Äútidy‚Äù data (in the same way that you can have a ‚Äútidy‚Äù room because all of your clothes are folded, but they aren‚Äôt clean just because they‚Äôre folded; you could have folded a pile of dirty clothes).\nWe‚Äôll get more into how to work with different ‚Äúmessy‚Äù data configurations in Chapter 24 and Chapter 25, but it‚Äôs worth keeping rules 1 and 3 in mind while working through this module.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "href": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "title": "22¬† Data Cleaning",
    "section": "\n22.4 Filter: Subset rows",
    "text": "22.4 Filter: Subset rows\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we‚Äôre interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don‚Äôt change the original.\n\n\ndplyr filter() by Allison Horst\n\n\n\n\n\n\n\nExample: starwars\n\n\n\nLet‚Äôs explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies.\nIn the interests of demonstrating the process on the same data, I‚Äôve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn‚Äôt supported by CSV files. You can access the csv data here.\n\n\nR\nPython\n\n\n\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn‚Äôt complete until we actually use the dataset though‚Ä¶ so let‚Äôs print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 √ó 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n##  2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu‚Ä¶\n##  3 R2-D2        96    32 &lt;NA&gt;       white, bl‚Ä¶ red             33   none  mascu‚Ä¶\n##  4 Darth V‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n##  5 Leia Or‚Ä¶    150    49 brown      light      brown           19   fema‚Ä¶ femin‚Ä¶\n##  6 Owen La‚Ä¶    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n##  7 Beru Wh‚Ä¶    165    75 brown      light      blue            47   fema‚Ä¶ femin‚Ä¶\n##  8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu‚Ä¶\n##  9 Biggs D‚Ä¶    183    84 black      light      brown           24   male  mascu‚Ä¶\n## 10 Obi-Wan‚Ä¶    182    77 auburn, w‚Ä¶ fair       blue-gray       57   male  mascu‚Ä¶\n## # ‚Ñπ 77 more rows\n## # ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\nWe have to use the exported CSV data in python.\n\nimport pandas as pd\nstarwars = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padm√© Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\n\nfrom skimpy import skim\nskim(starwars)\n## ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skimpy summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n## ‚îÇ          Data Summary                Data Types                              ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì                       ‚îÇ\n## ‚îÇ ‚îÉ dataframe         ‚îÉ Values ‚îÉ ‚îÉ Column Type ‚îÉ Count ‚îÉ                       ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©                       ‚îÇ\n## ‚îÇ ‚îÇ Number of rows    ‚îÇ 87     ‚îÇ ‚îÇ string      ‚îÇ 8     ‚îÇ                       ‚îÇ\n## ‚îÇ ‚îÇ Number of columns ‚îÇ 11     ‚îÇ ‚îÇ float64     ‚îÇ 3     ‚îÇ                       ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ\n## ‚îÇ                                   number                                     ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n## ‚îÇ ‚îÉ colu ‚îÉ    ‚îÉ       ‚îÉ      ‚îÉ       ‚îÉ    ‚îÉ      ‚îÉ     ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ  ‚îÇ\n## ‚îÇ ‚îÉ mn_n ‚îÉ    ‚îÉ       ‚îÉ      ‚îÉ       ‚îÉ    ‚îÉ      ‚îÉ     ‚îÉ      ‚îÉ      ‚îÉ      ‚îÉ  ‚îÇ\n## ‚îÇ ‚îÉ ame  ‚îÉ NA ‚îÉ NA %  ‚îÉ mean ‚îÉ sd    ‚îÉ p0 ‚îÉ p25  ‚îÉ p50 ‚îÉ p75  ‚îÉ p100 ‚îÉ hist ‚îÉ  ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n## ‚îÇ ‚îÇ heig ‚îÇ  6 ‚îÇ   6.9 ‚îÇ 174. ‚îÇ 34.77 ‚îÇ 66 ‚îÇ  167 ‚îÇ 180 ‚îÇ  191 ‚îÇ  264 ‚îÇ  ‚ñÅ   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ht   ‚îÇ    ‚îÇ       ‚îÇ    4 ‚îÇ       ‚îÇ    ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ ‚ñÅ‚ñá‚ñÇ  ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ mass ‚îÇ 28 ‚îÇ 32.18 ‚îÇ 97.3 ‚îÇ 169.5 ‚îÇ 15 ‚îÇ 55.6 ‚îÇ  79 ‚îÇ 84.5 ‚îÇ 1358 ‚îÇ  ‚ñá   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ      ‚îÇ    ‚îÇ       ‚îÇ    1 ‚îÇ       ‚îÇ    ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ birt ‚îÇ 44 ‚îÇ 50.57 ‚îÇ 87.5 ‚îÇ 154.7 ‚îÇ  8 ‚îÇ   35 ‚îÇ  52 ‚îÇ   72 ‚îÇ  896 ‚îÇ  ‚ñá   ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ h_ye ‚îÇ    ‚îÇ       ‚îÇ    7 ‚îÇ       ‚îÇ    ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ ar   ‚îÇ    ‚îÇ       ‚îÇ      ‚îÇ       ‚îÇ    ‚îÇ      ‚îÇ     ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ  ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n## ‚îÇ                                   string                                     ‚îÇ\n## ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n## ‚îÇ ‚îÉ column_name      ‚îÉ NA   ‚îÉ NA %    ‚îÉ words per row      ‚îÉ total words    ‚îÉ  ‚îÇ\n## ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n## ‚îÇ ‚îÇ name             ‚îÇ    0 ‚îÇ       0 ‚îÇ                1.8 ‚îÇ            157 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ hair_color       ‚îÇ    5 ‚îÇ    5.75 ‚îÇ               0.98 ‚îÇ             85 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ skin_color       ‚îÇ    0 ‚îÇ       0 ‚îÇ                1.2 ‚îÇ            106 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ eye_color        ‚îÇ    0 ‚îÇ       0 ‚îÇ                  1 ‚îÇ             89 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ sex              ‚îÇ    4 ‚îÇ     4.6 ‚îÇ               0.95 ‚îÇ             83 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ gender           ‚îÇ    4 ‚îÇ     4.6 ‚îÇ               0.95 ‚îÇ             83 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ homeworld        ‚îÇ   10 ‚îÇ   11.49 ‚îÇ               0.98 ‚îÇ             85 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îÇ species          ‚îÇ    4 ‚îÇ     4.6 ‚îÇ               0.99 ‚îÇ             86 ‚îÇ  ‚îÇ\n## ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n## ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ End ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we‚Äôve talked about how to use logical indexing before in Section 10.5.1, but here we‚Äôll focus on using specific functions to perform the same operation.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nThe dplyr verb for selecting rows is filter. filter takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 √ó 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n##  2 Darth V‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n##  3 Leia Or‚Ä¶    150    49 brown      light      brown           19   fema‚Ä¶ femin‚Ä¶\n##  4 Owen La‚Ä¶    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n##  5 Beru Wh‚Ä¶    165    75 brown      light      blue            47   fema‚Ä¶ femin‚Ä¶\n##  6 Biggs D‚Ä¶    183    84 black      light      brown           24   male  mascu‚Ä¶\n##  7 Obi-Wan‚Ä¶    182    77 auburn, w‚Ä¶ fair       blue-gray       57   male  mascu‚Ä¶\n##  8 Anakin ‚Ä¶    188    84 blond      fair       blue            41.9 male  mascu‚Ä¶\n##  9 Wilhuff‚Ä¶    180    NA auburn, g‚Ä¶ fair       blue            64   male  mascu‚Ä¶\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu‚Ä¶\n## # ‚Ñπ 25 more rows\n## # ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 √ó 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n## 1 Luke Sky‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n## 2 Darth Va‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n## 3 Owen Lars    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n## 4 Beru Whi‚Ä¶    165    75 brown      light      blue            47   fema‚Ä¶ femin‚Ä¶\n## 5 Biggs Da‚Ä¶    183    84 black      light      brown           24   male  mascu‚Ä¶\n## 6 Anakin S‚Ä¶    188    84 blond      fair       blue            41.9 male  mascu‚Ä¶\n## 7 Shmi Sky‚Ä¶    163    NA black      fair       brown           72   fema‚Ä¶ femin‚Ä¶\n## 8 Cliegg L‚Ä¶    183    NA brown      fair       blue            82   male  mascu‚Ä¶\n## # ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cord√©   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dorm√©   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padm√© Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\n\n# Get only the people who come from Tattoine\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n\n\n\nIn base R, you would perform a filtering operation using subset\n\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 √ó 14\n##    name     height  mass hair_color skin_color eye_color birth_year sex   gender\n##    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n##  1 Luke Sk‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n##  2 Darth V‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n##  3 Leia Or‚Ä¶    150    49 brown      light      brown           19   fema‚Ä¶ femin‚Ä¶\n##  4 Owen La‚Ä¶    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n##  5 Beru Wh‚Ä¶    165    75 brown      light      blue            47   fema‚Ä¶ femin‚Ä¶\n##  6 Biggs D‚Ä¶    183    84 black      light      brown           24   male  mascu‚Ä¶\n##  7 Obi-Wan‚Ä¶    182    77 auburn, w‚Ä¶ fair       blue-gray       57   male  mascu‚Ä¶\n##  8 Anakin ‚Ä¶    188    84 blond      fair       blue            41.9 male  mascu‚Ä¶\n##  9 Wilhuff‚Ä¶    180    NA auburn, g‚Ä¶ fair       blue            64   male  mascu‚Ä¶\n## 10 Han Solo    180    80 brown      fair       brown           29   male  mascu‚Ä¶\n## # ‚Ñπ 25 more rows\n## # ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 √ó 14\n##   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n##   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n## 1 Luke Sky‚Ä¶    172    77 blond      fair       blue            19   male  mascu‚Ä¶\n## 2 Darth Va‚Ä¶    202   136 none       white      yellow          41.9 male  mascu‚Ä¶\n## 3 Owen Lars    178   120 brown, gr‚Ä¶ light      blue            52   male  mascu‚Ä¶\n## 4 Beru Whi‚Ä¶    165    75 brown      light      blue            47   fema‚Ä¶ femin‚Ä¶\n## 5 Biggs Da‚Ä¶    183    84 black      light      brown           24   male  mascu‚Ä¶\n## 6 Anakin S‚Ä¶    188    84 blond      fair       blue            41.9 male  mascu‚Ä¶\n## 7 Shmi Sky‚Ä¶    163    NA black      fair       brown           72   fema‚Ä¶ femin‚Ä¶\n## 8 Cliegg L‚Ä¶    183    NA brown      fair       blue            82   male  mascu‚Ä¶\n## # ‚Ñπ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nNotice that with subset, you have to use & to join two logical statements; it does not by default take multiple successive arguments.\n\n\n\n\n\n\n22.4.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I‚Äôll do my best to show you easy ways to handle each task in each language.\n\n22.4.1.1 Filtering by row number\n\n\nR: dplyr\nPython\nBase R\n\n\n\nrow_number() is a helper function that is only used inside of another dplyr function (e.g.¬†filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nlibrary(tidyverse)\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\nfilter(poke, (row_number() %% 2 == 0)) \n## # A tibble: 763 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          2 https://img.‚Ä¶ Ivys‚Ä¶ &lt;NA&gt;    Gras‚Ä¶   405    60     62      63\n##  2     1          3 https://img.‚Ä¶ Venu‚Ä¶ Mega    Gras‚Ä¶   625    80    100     123\n##  3     1          5 https://img.‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire    405    58     64      58\n##  4     1          6 https://img.‚Ä¶ Char‚Ä¶ Mega  X Fire‚Ä¶   634    78    130     111\n##  5     1          7 https://img.‚Ä¶ Squi‚Ä¶ &lt;NA&gt;    Water   314    44     48      65\n##  6     1          9 https://img.‚Ä¶ Blas‚Ä¶ &lt;NA&gt;    Water   530    79     83     100\n##  7     1         10 https://img.‚Ä¶ Cate‚Ä¶ &lt;NA&gt;    Bug     195    45     30      35\n##  8     1         12 https://img.‚Ä¶ Butt‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   395    60     45      50\n##  9     1         14 https://img.‚Ä¶ Kaku‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   205    45     25      50\n## 10     1         15 https://img.‚Ä¶ Beed‚Ä¶ Mega    Bug,‚Ä¶   495    65    150      40\n## # ‚Ñπ 753 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n\n\n\nIn python, the easiest way to accomplish filtering by row number is by using .iloc. But, up until now, we‚Äôve only talked about how Python creates slices using start:(end+1) notation. There is an additional option with slicing - start:(end+1):by. So if we want to get only even rows, we can use the index [::2], which will give us row 0, 2, 4, 6, ‚Ä¶ through the end of the dataset, because we didn‚Äôt specify the start and end portions of the slice.\nBecause Python is 0-indexed, using ::2 will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke.iloc[0::2]\n##       gen  pokedex_no  ... height_m weight_kg\n## 0       1           1  ...      0.7       6.9\n## 2       1           3  ...      2.0     100.0\n## 4       1           4  ...      0.6       8.5\n## 6       1           6  ...      1.7      90.5\n## 8       1           6  ...      1.7      90.5\n## ...   ...         ...  ...      ...       ...\n## 1516    9         999  ...      0.3       5.0\n## 1518    9        1001  ...      1.5      74.2\n## 1520    9        1003  ...      2.7     699.7\n## 1522    9        1005  ...      2.0     380.0\n## 1524    9        1007  ...      2.5     303.0\n## \n## [763 rows x 16 columns]\n\nIf we want to get only odd rows, we can use the index [1::2], which will start at row 1 and give us 1, 3, 5, ‚Ä¶\n\n\nIn base R, we‚Äôd use seq() to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use subset, which requires a logical condition, and use 1:nrow(poke) to create an index which we then use for deciding whether each row is even or odd.\n\npoke[seq(1, nrow(poke), 2),]\n## # A tibble: 763 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          1 https://img.‚Ä¶ Bulb‚Ä¶ &lt;NA&gt;    Gras‚Ä¶   318    45     49      49\n##  2     1          3 https://img.‚Ä¶ Venu‚Ä¶ &lt;NA&gt;    Gras‚Ä¶   525    80     82      83\n##  3     1          4 https://img.‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire    309    39     52      43\n##  4     1          6 https://img.‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire‚Ä¶   534    78     84      78\n##  5     1          6 https://img.‚Ä¶ Char‚Ä¶ Mega  Y Fire‚Ä¶   634    78    104      78\n##  6     1          8 https://img.‚Ä¶ Wart‚Ä¶ &lt;NA&gt;    Water   405    59     63      80\n##  7     1          9 https://img.‚Ä¶ Blas‚Ä¶ Mega    Water   630    79    103     120\n##  8     1         11 https://img.‚Ä¶ Meta‚Ä¶ &lt;NA&gt;    Bug     205    50     20      55\n##  9     1         13 https://img.‚Ä¶ Weed‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   195    40     35      30\n## 10     1         15 https://img.‚Ä¶ Beed‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   395    65     90      40\n## # ‚Ñπ 753 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## # A tibble: 763 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1          2 https://img.‚Ä¶ Ivys‚Ä¶ &lt;NA&gt;    Gras‚Ä¶   405    60     62      63\n##  2     1          3 https://img.‚Ä¶ Venu‚Ä¶ Mega    Gras‚Ä¶   625    80    100     123\n##  3     1          5 https://img.‚Ä¶ Char‚Ä¶ &lt;NA&gt;    Fire    405    58     64      58\n##  4     1          6 https://img.‚Ä¶ Char‚Ä¶ Mega  X Fire‚Ä¶   634    78    130     111\n##  5     1          7 https://img.‚Ä¶ Squi‚Ä¶ &lt;NA&gt;    Water   314    44     48      65\n##  6     1          9 https://img.‚Ä¶ Blas‚Ä¶ &lt;NA&gt;    Water   530    79     83     100\n##  7     1         10 https://img.‚Ä¶ Cate‚Ä¶ &lt;NA&gt;    Bug     195    45     30      35\n##  8     1         12 https://img.‚Ä¶ Butt‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   395    60     45      50\n##  9     1         14 https://img.‚Ä¶ Kaku‚Ä¶ &lt;NA&gt;    Bug,‚Ä¶   205    45     25      50\n## 10     1         15 https://img.‚Ä¶ Beed‚Ä¶ Mega    Bug,‚Ä¶   495    65    150      40\n## # ‚Ñπ 753 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nThis is less fun than using dplyr because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you‚Äôre going. The real power of dplyr is in the collection of the full set of verbs with a consistent user interface; nothing done in dplyr is so special that it can‚Äôt be done in base R as well.\n\n\n\n\n22.4.1.2 Sorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n\nR: dplyr\nPython\nBase R\n\n\n\narrange() is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\narrange(poke, desc(total))\n## # A tibble: 1,526 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n##  2     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n##  3     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n##  4     3        384 https://img.‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n##  5     3        382 https://img.‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n##  6     3        383 https://img.‚Ä¶ Grou‚Ä¶ Primal  Grou‚Ä¶   770   100    180     160\n##  7     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  8     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  9     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n## 10     4        493 https://img.‚Ä¶ Arce‚Ä¶ &lt;NA&gt;    Norm‚Ä¶   720   120    120     120\n## # ‚Ñπ 1,516 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\nIn pandas, we use the sort_values function, which has an argument ascending. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\npoke.sort_values(['total'], ascending = False)\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## ...   ...         ...  ...      ...       ...\n## 1336    8         872  ...      0.3       3.8\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## \n## [1526 rows x 16 columns]\n\n\n\nThe sort() function in R can be used to sort a vector, but when sorting a data frame we usually want to use the order() function instead. This is because sort() orders the values of the argument directly, where order() returns a sorted index.\n\nx &lt;- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n\nWhen working with a data frame, we want to sort the entire data frame‚Äôs rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\npoke[order(poke$total, decreasing = T),]\n## # A tibble: 1,526 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n##  2     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n##  3     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n##  4     3        384 https://img.‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n##  5     3        382 https://img.‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n##  6     3        383 https://img.‚Ä¶ Grou‚Ä¶ Primal  Grou‚Ä¶   770   100    180     160\n##  7     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  8     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  9     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n## 10     4        493 https://img.‚Ä¶ Arce‚Ä¶ &lt;NA&gt;    Norm‚Ä¶   720   120    120     120\n## # ‚Ñπ 1,516 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\n\n\n22.4.1.3 Keep the top \\(n\\) values of a variable\n\n\nR: dplyr\nPython\nBase R\n\n\n\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it‚Äôs a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nslice_max(poke, order_by = total, n = 5)\n## # A tibble: 6 √ó 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n## 2     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n## 3     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n## 4     3        384 https://img.p‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n## 5     3        382 https://img.p‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n## 6     3        383 https://img.p‚Ä¶ Grou‚Ä¶ Primal  Grou‚Ä¶   770   100    180     160\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nBy default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows.\n\nslice_max(poke, order_by = total, n = 5, with_ties = F) \n## # A tibble: 5 √ó 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n## 2     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n## 3     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n## 4     3        384 https://img.p‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n## 5     3        382 https://img.p‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nOf course, there is a similar slice_min() function as well:\n\nslice_min(poke, order_by = total, n = 5)\n## # A tibble: 5 √ó 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     7        746 https://img.p‚Ä¶ Wish‚Ä¶ Solo    Water   175    45     20      20\n## 2     7        746 https://img.p‚Ä¶ Wish‚Ä¶ Solo    Water   175    45     20      20\n## 3     2        191 https://img.p‚Ä¶ Sunk‚Ä¶ &lt;NA&gt;    Grass   180    30     30      30\n## 4     8        824 https://img.p‚Ä¶ Blip‚Ä¶ &lt;NA&gt;    Bug     180    25     20      20\n## 5     8        872 https://img.p‚Ä¶ Snom  &lt;NA&gt;    Ice,‚Ä¶   185    30     25      35\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nslice_max(poke, order_by = total, prop = .01)\n## # A tibble: 30 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n##  2     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n##  3     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n##  4     3        384 https://img.‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n##  5     3        382 https://img.‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n##  6     3        383 https://img.‚Ä¶ Grou‚Ä¶ Primal  Grou‚Ä¶   770   100    180     160\n##  7     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  8     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  9     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n## 10     4        493 https://img.‚Ä¶ Arce‚Ä¶ &lt;NA&gt;    Norm‚Ä¶   720   120    120     120\n## # ‚Ñπ 20 more rows\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\nIn Python, nlargest and nsmallest work roughly the same as dplyr‚Äôs slice_max and slice_min for integer counts.\n\npoke.nlargest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## \n## [5 rows x 16 columns]\npoke.nsmallest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## \n## [5 rows x 16 columns]\n\nTo get proportions, though, we have to do some math:\n\npoke.nlargest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## 582     3         383  ...      3.5     950.0\n## 1257    7         800  ...      2.4     230.0\n## 1258    7         800  ...      2.4     230.0\n## 1259    7         800  ...      2.4     230.0\n## 779     4         493  ...      3.2     320.0\n## 1126    6         718  ...      5.0     305.0\n## 1127    6         718  ...      5.0     305.0\n## 1128    6         718  ...      5.0     305.0\n## 405     2         248  ...      2.0     202.0\n## 567     3         373  ...      1.5     102.6\n## \n## [15 rows x 16 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## 465     3         298  ...      0.2       2.0\n## 616     4         401  ...      0.3       2.2\n## 13      1          10  ...      0.3       2.9\n## 16      1          13  ...      0.3       3.2\n## 431     3         265  ...      0.3       3.6\n## 446     3         280  ...      0.4       6.6\n## 248     1         129  ...      0.9      10.0\n## 524     3         349  ...      0.6       7.4\n## 1032    6         664  ...      0.3       2.5\n## 1237    7         789  ...      0.2       0.1\n## \n## [15 rows x 16 columns]\n\n\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing. In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\npoke[order(poke$total, decreasing = T)[1:5],]\n## # A tibble: 5 √ó 16\n##     gen pokedex_no img_link       name  variant type  total    hp attack defense\n##   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     8        890 https://img.p‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n## 2     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n## 3     1        150 https://img.p‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n## 4     3        384 https://img.p‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n## 5     3        382 https://img.p‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\npoke[order(poke$total, decreasing = T)[1:round(nrow(poke)*.01)],]\n## # A tibble: 15 √ó 16\n##      gen pokedex_no img_link      name  variant type  total    hp attack defense\n##    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1     8        890 https://img.‚Ä¶ Eter‚Ä¶ Eterna‚Ä¶ Pois‚Ä¶  1125   255    115     250\n##  2     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  X Psyc‚Ä¶   780   106    190     100\n##  3     1        150 https://img.‚Ä¶ Mewt‚Ä¶ Mega  Y Psyc‚Ä¶   780   106    150      70\n##  4     3        384 https://img.‚Ä¶ Rayq‚Ä¶ Mega    Drag‚Ä¶   780   105    180     100\n##  5     3        382 https://img.‚Ä¶ Kyog‚Ä¶ Primal  Water   770   100    150      90\n##  6     3        383 https://img.‚Ä¶ Grou‚Ä¶ Primal  Grou‚Ä¶   770   100    180     160\n##  7     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  8     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n##  9     7        800 https://img.‚Ä¶ Necr‚Ä¶ Ultra   Psyc‚Ä¶   754    97    167      97\n## 10     4        493 https://img.‚Ä¶ Arce‚Ä¶ &lt;NA&gt;    Norm‚Ä¶   720   120    120     120\n## 11     6        718 https://img.‚Ä¶ Zyga‚Ä¶ Comple‚Ä¶ Drag‚Ä¶   708   216    100     121\n## 12     6        718 https://img.‚Ä¶ Zyga‚Ä¶ Comple‚Ä¶ Drag‚Ä¶   708   216    100     121\n## 13     6        718 https://img.‚Ä¶ Zyga‚Ä¶ Comple‚Ä¶ Drag‚Ä¶   708   216    100     121\n## 14     2        248 https://img.‚Ä¶ Tyra‚Ä¶ Mega    Rock‚Ä¶   700   100    164     150\n## 15     3        373 https://img.‚Ä¶ Sala‚Ä¶ Mega    Drag‚Ä¶   700    95    145     130\n## # ‚Ñπ 6 more variables: sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   species &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\nTry it out: Filtering\n\n\n\n\n\nProblem\nR: dplyr\nPython\nBase R\n\n\n\nUse the Pokemon data to accomplish the following:\n\ncreate a new data frame that has only water type Pokemon\nwrite a filter statement that looks for any Pokemon which has water type for either type1 or type2\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nfilter(poke, type_1 == \"Water\")\n## Error in `filter()`:\n## ‚Ñπ In argument: `type_1 == \"Water\"`.\n## Caused by error:\n## ! object 'type_1' not found\n\nfilter(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in `filter()`:\n## ‚Ñπ In argument: `type_1 == \"Water\" | type_2 == \"Water\"`.\n## Caused by error:\n## ! object 'type_1' not found\n# The conditions have to be separated by |, which means \"or\"\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke.query(\"type_1=='Water'\")\n## pandas.errors.UndefinedVariableError: name 'type_1' is not defined\npoke.query(\"type_1=='Water'|type_2=='Water'\")\n## pandas.errors.UndefinedVariableError: name 'type_1' is not defined\n# The conditions have to be separated by |, which means \"or\"\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nsubset(poke, type_1 == \"Water\")\n## Error in eval(e, x, parent.frame()): object 'type_1' not found\n\nsubset(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in eval(e, x, parent.frame()): object 'type_1' not found\n# The conditions have to be separated by |, which means \"or\"",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "href": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "title": "22¬† Data Cleaning",
    "section": "\n22.5 Select: Pick columns",
    "text": "22.5 Select: Pick columns\nSometimes, we don‚Äôt want to work with a set of 50 variables when we‚Äôre only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g.¬†df[, c(1, 3, 5)]), but this can get tedious.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, ‚Ä¶)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of ‚Äúput in any additional arguments that make sense in context or might be passed on to other functions‚Äù.\nSo what can go in there?\n\n\n\n\n\n\nWays to select variables in dplyr\n\n\n\n\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with ‚Äúbare‚Äù variable names - you don‚Äôt need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they‚Äôre very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(&lt;set of variables&gt;) will give you any columns that aren‚Äôt in the set of variables in parentheses\n\n\n(&lt;set of vars 1&gt;) & (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection ‚Äúhelpers‚Äù that can be used inside select() statements. These statements work with bare column names (so you don‚Äôt have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you‚Äôd expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string ‚Äúxyz‚Äù. Note, contains does not work with regular expressions (you don‚Äôt need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn‚Äôt exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn‚Äôt exist in the data set.\n\nThere‚Äôs one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\n\n\nLet‚Äôs try these selector functions out and see what we can accomplish!\n\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 √ó 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\nWe‚Äôll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n\n\n\n\n\nTip\n\n\n\nYou might want to try out your EDA (Exploratory Data Analysis) skills to see what you can find out about the dataset, before seeing how select() works.\n\n\nWe could get a data frame of departure information for each flight:\n\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 √ó 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # ‚Ñπ 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nflights %&gt;%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 √ó 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\nNote that everything() won‚Äôt duplicate columns you‚Äôve already added.\nExploring the difference between bare name selection and all_of()/any_of()\n\nflights %&gt;%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 √ó 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 2 more variables: air_time &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nvarlist &lt;- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %&gt;%\n  select(all_of(varlist))\n## # A tibble: 336,776 √ó 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 1 more variable: air_time &lt;dbl&gt;\n\nvarlist &lt;- c(varlist, \"whoops\")\n\nflights %&gt;%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()` at magrittr/R/pipe.R:136:3:\n## ‚Ñπ In argument: `all_of(varlist)`.\n## Caused by error in `all_of()` at rlang/R/eval-tidy.R:121:3:\n## ! Can't subset elements that don't exist.\n## ‚úñ Element `whoops` doesn't exist.\n\nflights %&gt;%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 √ó 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228       517            515      830            819\n##  2 UA        1714 N24211       533            529      850            830\n##  3 AA        1141 N619AA       542            540      923            850\n##  4 B6         725 N804JB       544            545     1004           1022\n##  5 DL         461 N668DN       554            600      812            837\n##  6 UA        1696 N39463       554            558      740            728\n##  7 B6         507 N516JB       555            600      913            854\n##  8 EV        5708 N829AS       557            600      709            723\n##  9 B6          79 N593JB       557            600      838            846\n## 10 AA         301 N3ALAA       558            600      753            745\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 1 more variable: air_time &lt;dbl&gt;\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\nFirst, let‚Äôs install the nycflights13 package[3] in python by typing the following into your system terminal.\n\npip install nycflights13\n## Requirement already satisfied: nycflights13 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (0.0.3)\n## Requirement already satisfied: pandas&gt;=0.24.0 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from nycflights13) (2.2.2)\n## Requirement already satisfied: numpy&gt;=1.23.2 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (1.26.4)\n## Requirement already satisfied: python-dateutil&gt;=2.8.2 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2.9.0.post0)\n## Requirement already satisfied: pytz&gt;=2020.1 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2024.2)\n## Requirement already satisfied: tzdata&gt;=2022.7 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from pandas&gt;=0.24.0-&gt;nycflights13) (2024.1)\n## Requirement already satisfied: six&gt;=1.5 in /home/susan/.virtualenvs/book/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.24.0-&gt;nycflights13) (1.16.0)\n\nThen, we can load the flights data from the nycflights13 package.\n\nfrom nycflights13 import flights\n\nSelect operations are not as easy in python as they are when using dplyr::select() with helpers, but of course you can accomplish the same tasks.\n\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\nList Comprehensions\nIn Python, there are certain shorthands called ‚Äúlist comprehensions‚Äù [4] that can perform similar functions to e.g.¬†the matches() function in dplyr.\nSuppose we want to get all columns containing the word ‚Äòtime‚Äô. We could iterate through the list of columns (flights.columns) and add the column name any time we detect the word ‚Äòtime‚Äô within. That is essentially what the following code does:\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n\nExplaining the code:\n\n\nfor col in flights.columns iterates through the list of columns, storing each column name in the variable col\n\n\nif 'time' in col detects the presence of the word ‚Äòtime‚Äô in the column name stored in col\n\nthe col out front adds the column name in the variable col to the array of columns to keep\nSelecting columns in Python\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\nselcols.extend([\"whoops\"])\nselcols\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\n\n# Subset the data frame\nflights.loc[:,selcols]\n## KeyError: \"['whoops'] not in index\"\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\n\n\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g.¬†matches in dplyr.\ngrepl is a shorthand function for grep, which searches for a pattern in a vector of strings. grepl returns a logical vector indicating whether the pattern (\"dep\", in this case) was found in the vector (names(flights), in this case).\n\n\ndepcols &lt;- names(flights)[grepl(\"dep\", names(flights))]\ncollist &lt;- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 √ó 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # ‚Ñπ 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nnew_order &lt;- names(flights)\nnew_order &lt;- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 √ó 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\nThis is less convenient than dplyr::everything in part because it depends on us to get the column indexes right.\n\n\n\n\n22.5.1 Rearranging Columns\n\n\ndplyr::relocate\nPython\n\n\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I‚Äôm covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 √ó 19\n##    carrier flight tailnum origin dest   year month   day dep_time sched_dep_time\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1      517            515\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1      533            529\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1      542            540\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1      544            545\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1      554            600\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1      554            558\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1      555            600\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1      557            600\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1      557            600\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1      558            600\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 9 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n## #   time_hour &lt;dttm&gt;\n\n# move numeric variables to the front\nflights %&gt;% relocate(where(is.numeric))\n## # A tibble: 336,776 √ó 19\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # ‚Ñπ 336,766 more rows\n## # ‚Ñπ 11 more variables: arr_delay &lt;dbl&gt;, flight &lt;int&gt;, air_time &lt;dbl&gt;,\n## #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n## #   origin &lt;chr&gt;, dest &lt;chr&gt;, time_hour &lt;dttm&gt;\n\n\n\nThere are similar ways to rearrange columns in pandas, but they are a bit harder to work with - you have to specify the column names (in some way) and then perform the operation yourself.\n\nimport numpy as np\ncols = list(flights.columns.values) # get column names\n\n# Move flight specific info to the front\nflightcols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']\nflights[flightcols + list(flights.drop(flightcols, axis = 1))]\n##        carrier  flight tailnum  ... hour minute             time_hour\n## 0           UA    1545  N14228  ...    5     15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5     29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5     40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5     45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6      0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...    ...                   ...\n## 336771      9E    3393     NaN  ...   14     55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22      0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12     10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11     59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8     40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\n# move numeric variables to the front\nnumcols = list(flights.select_dtypes(include = np.number).columns.values)\nflights[numcols + list(flights.drop(numcols, axis = 1))]\n##         year  month  day  dep_time  ...  tailnum  origin  dest             time_hour\n## 0       2013      1    1     517.0  ...   N14228     EWR   IAH  2013-01-01T10:00:00Z\n## 1       2013      1    1     533.0  ...   N24211     LGA   IAH  2013-01-01T10:00:00Z\n## 2       2013      1    1     542.0  ...   N619AA     JFK   MIA  2013-01-01T10:00:00Z\n## 3       2013      1    1     544.0  ...   N804JB     JFK   BQN  2013-01-01T10:00:00Z\n## 4       2013      1    1     554.0  ...   N668DN     LGA   ATL  2013-01-01T11:00:00Z\n## ...      ...    ...  ...       ...  ...      ...     ...   ...                   ...\n## 336771  2013      9   30       NaN  ...      NaN     JFK   DCA  2013-09-30T18:00:00Z\n## 336772  2013      9   30       NaN  ...      NaN     LGA   SYR  2013-10-01T02:00:00Z\n## 336773  2013      9   30       NaN  ...   N535MQ     LGA   BNA  2013-09-30T16:00:00Z\n## 336774  2013      9   30       NaN  ...   N511MQ     LGA   CLE  2013-09-30T15:00:00Z\n## 336775  2013      9   30       NaN  ...   N839MQ     LGA   RDU  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "href": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "title": "22¬† Data Cleaning",
    "section": "\n22.6 Mutate: Add and transform variables",
    "text": "22.6 Mutate: Add and transform variables\nUp to this point, we‚Äôve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g.¬†BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate, but in base R and python, we‚Äôll simply use assignment to add columns to our data frames.\n\n\n\n\nMutate (by Allison Horst)\n\nWe‚Äôll use the Pokemon data to demonstrate. Some Pokemon have a single ‚Äútype‚Äù, which is usually elemental, such as Water, Ice, Fire, etc., but others have two. Let‚Äôs add a column that indicates how many types a pokemon has.\n\n\nBase R\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\n# This splits type_1,type_2 into two separate variables. \n# Don't worry about the string processing (gsub) just now\n# Focus on how variables are defined.\npoke$type_1 &lt;- gsub(\",.*$\", \"\", poke$type) # Replace anything after comma with ''\npoke$type_2 &lt;- gsub(\"^.*,\", \"\", poke$type) # Use the 2nd type\npoke$type_2[poke$type_1 == poke$type_2] &lt;- NA # Type 2 only exists if not same as Type 1\n\npoke$no_types &lt;- 1 # set a default value\npoke$no_types[grepl(\",\", poke$type)] &lt;- 2 # set the value if there's not a comma in type\n\n# This is a bit faster\npoke$no_types &lt;- ifelse(grepl(\",\", poke$type), 2, 1)\n\n# Sanity check\n# This checks number of types vs. value of type_2\n# If type 2 is NA, then number of types should be 1\nt(table(poke$type_2, poke$no_types, useNA = 'ifany'))\n##    \n##     Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass Ground Ice\n##   1   0    0      0        0     0        0    0      0     0     0      0   0\n##   2   9   40     49       17    47       47   31    157    57    50     57  36\n##    \n##     Normal Poison Psychic Rock Steel Water &lt;NA&gt;\n##   1      0      0       0    0     0     0  673\n##   2     24     51      61   23    55    42    0\n\nNotice that we had to type the name of the dataset at least 3 times to perform the operation we were looking for. I could reduce that to 2x with the ifelse function, but it‚Äôs still a lot of typing.\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke &lt;- poke %&gt;%\n  # This splits type into type_1,type_2 : two separate variables. \n  # Don't worry about the string processing (str_extract) just now\n  # Focus on how variables are defined: \n  #   we use a function on the type column\n  #   within the mutate statement.\n  mutate(type_1 = str_extract(type, \"^(.*),\", group = 1),\n         type_2 = str_extract(type, \"(.*),(.*)\", group = 2)) %&gt;%\n  mutate(no_types = if_else(is.na(type_2), 1, 2))\n\nselect(poke, type_2, no_types) %&gt;% table(useNA = 'ifany') %&gt;% t()\n##         type_2\n## no_types Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass Ground\n##        1   0    0      0        0     0        0    0      0     0     0      0\n##        2   9   40     49       17    47       47   31    157    57    50     57\n##         type_2\n## no_types Ice Normal Poison Psychic Rock Steel Water &lt;NA&gt;\n##        1   0      0      0       0    0     0     0  673\n##        2  36     24     51      61   23    55    42    0\n\nThe last 2 rows are just to organize the output - we keep only the two variables we‚Äôre working with, and get a crosstab.\n\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with).\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\n# This splits type into two columns, type_1 and type_2, based on \",\"\npoke[[\"type_1\", \"type_2\"]] = poke[\"type\"].apply(lambda x: pd.Series(str(x).split(\",\")))\n\n# This defines number of types\npoke[\"no_types\"] = 1 # default value\npoke.loc[~poke.type_2.isna(), \"no_types\"] = 2 # change those with a defined type 2\n\n\npoke.groupby([\"no_types\", \"type_2\"], dropna=False).size()\n## no_types  type_2  \n## 1         NaN         673\n## 2         Bug           9\n##           Dark         40\n##           Dragon       49\n##           Electric     17\n##           Fairy        47\n##           Fighting     47\n##           Fire         31\n##           Flying      157\n##           Ghost        57\n##           Grass        50\n##           Ground       57\n##           Ice          36\n##           Normal       24\n##           Poison       51\n##           Psychic      61\n##           Rock         23\n##           Steel        55\n##           Water        42\n## dtype: int64\n# When type_2 is NaN, no_types is 1\n# When type_2 is defined, no_types is 2\n\nAnother function that may be useful is the assign function, which can be used to create new variables if you don‚Äôt want to use the [\"new_col\"] notation. In some circumstances, .assign(var = ...) is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I‚Äôd like to not get into right now for simplicity‚Äôs sake).\n\n\n\nThe learning curve here isn‚Äôt actually knowing how to assign new variables (though that‚Äôs important). The challenge comes when you want to do something new and have to figure out how to e.g.¬†use find and replace in a string, or work with dates and times, or recode variables.\n\n\n\n\n\n\nMutate and new challenges\n\n\n\n\n\nI‚Äôm not going to be able to teach you how to handle every mutate statement task you‚Äôll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation, google things intelligently, and to understand what you‚Äôre reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\nSource\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We‚Äôre going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you‚Äôll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a quick table of places to look in R and python to solve some of the more common problems.\n\n\nProblem\nR\nPython\n\n\n\nDates and Times\n\nlubridate package (esp.¬†ymd_hms() and variants, decimal_date(), and other convenience functions)\n\npandas has some date time support by default; see the datetime module for more functionality.\n\n\nString manipulation\n\nstringr package\nQuick Tips [5], Whirlwind Tour of Python chapter [6]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarize",
    "href": "part-wrangling/03-data-cleaning.html#summarize",
    "title": "22¬† Data Cleaning",
    "section": "\n22.7 Summarize",
    "text": "22.7 Summarize\nThe next verb is one that we‚Äôve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we‚Äôve used e.g.¬†group_by + count in Python to perform certain tasks as well.\nHere (in a trivial example), I compute the overall average HP of a Pokemon in each generation, as well as the average number of characters in their name. Admittedly, that last computation is a bit silly, but it‚Äôs mostly for demonstration purposes.\n\n\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %&gt;%\n  mutate(name_chr = nchar(name)) %&gt;%\n  summarize(n = max(pokedex_no), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 1 √ó 3\n##       n    hp name_chr\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1008  71.2     7.55\n\n\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as mean. You can also build custom summary functions [7], or use the agg() function to define multiple summary variables. agg() will even let you use different summary functions for each variable, just like summarize.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke[['hp', 'name_length']].mean()\n## hp             71.178244\n## name_length     7.545216\n## dtype: float64\npoke[['hp', 'name_length']].agg(['mean', 'min'])\n##              hp  name_length\n## mean  71.178244     7.545216\n## min    1.000000     3.000000\npoke[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n## pokedex_no     1008.000000\n## hp               71.178244\n## name_length       7.545216\n## dtype: float64\n\n\n\n\nThe real power of summarize, though, is in combination with Group By. We‚Äôll see more summarize examples, but it‚Äôs easier to make good examples when you have all the tools - it‚Äôs hard to demonstrate how to use a hammer if you don‚Äôt also have a nail.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#group-by-power",
    "href": "part-wrangling/03-data-cleaning.html#group-by-power",
    "title": "22¬† Data Cleaning",
    "section": "\n22.8 Group By + (?) = Power!",
    "text": "22.8 Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\nsplit my dataset into smaller datasets - one for each day\ncompute summary values for each smaller dataset\nput my summarized data back together into a single dataset\n\nThis is known as the split-apply-combine [9] or sometimes, map-reduce [10] strategy (though map-reduce is usually on specifically large datasets and performed in parallel).\nIn tidy parlance, group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well.\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\n\nR: dplyr\nPython\n\n\n\n\npoke &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %&gt;%\n  mutate(name_chr = nchar(name)) %&gt;%\n  group_by(gen) %&gt;%\n  summarize(n = length(unique(pokedex_no)), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 9 √ó 4\n##     gen     n    hp name_chr\n##   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1   151  65.3     7.23\n## 2     2   100  71.0     7.36\n## 3     3   135  65.8     7.16\n## 4     4   107  69.4     6.85\n## 5     5   156  75.8     7.77\n## 6     6    72  72.9     7.47\n## 7     7    88  73.2     8.04\n## 8     8    96  77.9     8.01\n## 9     9   103  75.8     8.66\n\n\n\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke.groupby('gen')[['hp', 'name_length']].mean()\n##             hp  name_length\n## gen                        \n## 1    65.333333     7.231579\n## 2    71.024194     7.362903\n## 3    65.777202     7.160622\n## 4    69.382022     6.853933\n## 5    75.827004     7.767932\n## 6    72.882353     7.470588\n## 7    73.233083     8.037594\n## 8    77.940299     8.014925\n## 9    75.756098     8.658537\npoke.groupby('gen')[['hp', 'name_length']].agg(['mean', 'min'])\n##             hp     name_length    \n##           mean min        mean min\n## gen                               \n## 1    65.333333  10    7.231579   3\n## 2    71.024194  20    7.362903   4\n## 3    65.777202   1    7.160622   4\n## 4    69.382022  20    6.853933   4\n## 5    75.827004  30    7.767932   4\n## 6    72.882353  38    7.470588   5\n## 7    73.233083  25    8.037594   6\n## 8    77.940299  25    8.014925   4\n## 9    75.756098  10    8.658537   5\npoke.groupby('gen')[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n##      pokedex_no         hp  name_length\n## gen                                    \n## 1           151  65.333333     7.231579\n## 2           100  71.024194     7.362903\n## 3           135  65.777202     7.160622\n## 4           107  69.382022     6.853933\n## 5           156  75.827004     7.767932\n## 6            72  72.882353     7.470588\n## 7            88  73.233083     8.037594\n## 8            96  77.940299     8.014925\n## 9           103  75.756098     8.658537\n\n\n\n\nWhen you group_by a variable, your result carries this grouping with it. In R, summarize will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command. In Python, you should consider using reset_index or grouped_thing.obj() to access the original information[11].\n\n\n\n\n\n\nStorms Example\n\n\n\nLet‚Äôs try a non-trivial example, using the storms dataset that is part of the dplyr package.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 19,537 √ó 13\n##    name   year month   day  hour   lat  long status      category  wind pressure\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n##  1 Amy    1975     6    27     0  27.5 -79   tropical d‚Ä¶       NA    25     1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical d‚Ä¶       NA    25     1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical d‚Ä¶       NA    25     1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical d‚Ä¶       NA    25     1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical d‚Ä¶       NA    25     1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical d‚Ä¶       NA    25     1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical d‚Ä¶       NA    25     1011\n##  8 Amy    1975     6    28    18  34   -77   tropical d‚Ä¶       NA    30     1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical s‚Ä¶       NA    35     1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical s‚Ä¶       NA    40     1002\n## # ‚Ñπ 19,527 more rows\n## # ‚Ñπ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n\n\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\", on_bad_lines='skip')\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n\n\n\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes).\nOne thing we might want to know is at what point each storm was the strongest. Let‚Äôs define strongest in the following way:\n\nThe points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause).\nIf there‚Äôs a tie, we might want to know when the maximum wind speed occurred.\nIf that still doesn‚Äôt get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred.\n\nLet‚Äôs start by translating these criteria into basic operations. I‚Äôll use dplyr function names here, but I‚Äôll also specify what I mean when there‚Äôs a conflict (e.g.¬†filter in dplyr means something different than filter in python).\nInitial attempt:\n\n\nFor each storm (group_by),\nwe need the point where the storm has lowest atmospheric pressure. (filter - pick the row with the lowest pressure).\n\nThen we read the next part: ‚ÄúIf there is a tie, pick the maximum wind speed.‚Äù\n\ngroup_by\n\narrange by ascending pressure and descending wind speed\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\ngroup_by\n\narrange by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nsummarize - compute the average time and category (if there are multiple rows)\n\nLet‚Äôs write the code, now that we have the order of operations straight!\n\n\nR\nPython\n\n\n\n\nmax_power_storm &lt;- storms %&gt;%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %&gt;%\n  filter(pressure == min(pressure, na.rm = T)) %&gt;%\n  filter(wind == max(wind, na.rm = T)) %&gt;%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %&gt;%\n  arrange(time) %&gt;%\n  ungroup()\nmax_power_storm\n## # A tibble: 665 √ó 7\n##    name      year pressure  wind category status         time               \n##    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;          &lt;dttm&gt;             \n##  1 Amy       1975      981    60       NA tropical storm 1975-07-02 12:00:00\n##  2 Blanche   1975      980    75        1 hurricane      1975-07-28 00:00:00\n##  3 Caroline  1975      963   100        3 hurricane      1975-08-31 06:00:00\n##  4 Doris     1975      965    95        2 hurricane      1975-09-02 21:00:00\n##  5 Eloise    1975      955   110        3 hurricane      1975-09-23 12:00:00\n##  6 Faye      1975      977    75        1 hurricane      1975-09-28 18:00:00\n##  7 Gladys    1975      939   120        4 hurricane      1975-10-02 15:00:00\n##  8 Hallie    1975     1002    45       NA tropical storm 1975-10-27 03:00:00\n##  9 Belle     1976      957   105        3 hurricane      1976-08-09 00:00:00\n## 10 Dottie    1976      996    45       NA tropical storm 1976-08-20 06:00:00\n## # ‚Ñπ 655 more rows\n\n\n\n\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n\n\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nWe could also look to see whether there has been any change over time in pressure.\n\n\nR\nPython\n\n\n\n\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic.\n\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n\nR\nPython\n\n\n\n\nstorm_strength_duration &lt;- storms %&gt;%\n  group_by(name, year) %&gt;%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %&gt;%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n## Error in `geom_boxplot()`:\n## ! Problem while computing stat.\n## ‚Ñπ Error occurred in the 1st layer.\n## Caused by error in `if (...) NULL`:\n## ! missing value where TRUE/FALSE needed\n\n\n\n\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## TypeError: ggplot.__init__() got multiple values for argument 'data'\n\n\n\n\nYou don‚Äôt need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with.\nWe could also look to see how a storm‚Äôs diameter evolves over time, from when the storm is first identified (group_by + mutate)\nDiameter measurements don‚Äôt exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables).\n\n\nR\nPython\n\n\n\nNote the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted.\n\nstorm_evolution &lt;- storms %&gt;%\n  filter(!is.na(hurricane_force_diameter)) %&gt;%\n  group_by(name, year) %&gt;%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %&gt;%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\nFor this plot, I‚Äôve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms.\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n\nR\nPython\n\n\n\n\nstorm_evolution %&gt;%\n  filter(year == 2008) %&gt;%\n  arrange(desc(time_since_start))\n## # A tibble: 548 √ó 15\n##    name    year month   day  hour   lat  long status     category  wind pressure\n##    &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n##  1 Bertha  2008     7    21     6  58.5 -27   extratrop‚Ä¶       NA    45      990\n##  2 Bertha  2008     7    21     0  55.1 -31   extratrop‚Ä¶       NA    50      990\n##  3 Bertha  2008     7    20    18  53   -34   extratrop‚Ä¶       NA    55      990\n##  4 Bertha  2008     7    20    12  50   -37   extratrop‚Ä¶       NA    60      985\n##  5 Bertha  2008     7    20     6  47.6 -40   tropical ‚Ä¶       NA    60      985\n##  6 Bertha  2008     7    20     0  45.3 -42.4 tropical ‚Ä¶       NA    60      990\n##  7 Bertha  2008     7    19    18  43.7 -44.3 hurricane         1    65      989\n##  8 Bertha  2008     7    19    12  42.1 -46.3 hurricane         1    65      989\n##  9 Bertha  2008     7    19     6  40.4 -48   hurricane         1    65      989\n## 10 Bertha  2008     7    19     0  38.6 -49.7 hurricane         1    65      989\n## # ‚Ñπ 538 more rows\n## # ‚Ñπ 4 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;, time &lt;dttm&gt;, time_since_start &lt;drtn&gt;\n\nstorm_evolution %&gt;% filter(name == \"Ike\") %&gt;%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n##      name  year  ...                time              age\n## 8000  Ike  2008  ... 2008-09-14 06:00:00 13 days 00:00:00\n## 7999  Ike  2008  ... 2008-09-14 00:00:00 12 days 18:00:00\n## 7998  Ike  2008  ... 2008-09-13 18:00:00 12 days 12:00:00\n## 7997  Ike  2008  ... 2008-09-13 12:00:00 12 days 06:00:00\n## 7996  Ike  2008  ... 2008-09-13 07:00:00 12 days 01:00:00\n## \n## [5 rows x 12 columns]\n\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRadar coverage map from 1995, from [12]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "href": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "title": "22¬† Data Cleaning",
    "section": "\n22.9 Summarizing Across Multiple Variables",
    "text": "22.9 Summarizing Across Multiple Variables\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don‚Äôt want to write out all of the summarize statements individually, so we use across() instead (in dplyr).\n\n\nR\nPython\n\n\n\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time.\n\n\ndplyr‚Äôs across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)\n\n\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity &lt;- storms %&gt;%\n  filter(status == \"hurricane\") %&gt;%\n  group_by(name) %&gt;%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %&gt;%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %&gt;%\n  arrange(desc(wind)) %&gt;% \n  # get top 10\n  filter(row_number() &lt;= 10) %&gt;%\n  knitr::kable() # Make into a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nmonth\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\nAllen\n1980\n8.000000\n122.9688\n941.0312\nNaN\nNaN\n\n\nIrma\n2017\n8.942308\n118.8462\n941.6154\n249.4231\n75.96154\n\n\nAndrew\n1992\n8.000000\n118.2609\n946.6522\nNaN\nNaN\n\n\nMitch\n1998\n10.000000\n115.9091\n945.3182\nNaN\nNaN\n\n\nRita\n2005\n9.000000\n114.7368\n931.6316\n265.2941\n97.05882\n\n\nIsabel\n2003\n9.000000\n112.1875\n946.5417\nNaN\nNaN\n\n\nGilbert\n1988\n9.000000\n110.8929\n945.4286\nNaN\nNaN\n\n\nLuis\n1995\n8.928571\n110.5952\n948.6190\nNaN\nNaN\n\n\nWilma\n2005\n10.000000\n110.3030\n939.4242\n349.8333\n118.33333\n\n\nMatthew\n2016\n9.880952\n109.5238\n952.1190\n263.5714\n62.02381\n\n\n\n\n\n\n\nStackoverflow reference\nWe can use python‚Äôs list comprehensions in combination with .agg to accomplish the same task as dplyr‚Äôs across function.\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a lambda function - basically, a ‚Äúdummy‚Äù function that has some arguments but not all of the arguments. Here, our lambda function is a function of x, and we calculate x.mean(skipna=True) for each x passed in (so, for each column).\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "href": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "title": "22¬† Data Cleaning",
    "section": "\n22.10 Try it out - Data Cleaning",
    "text": "22.10 Try it out - Data Cleaning\nYou can read about the gapminder project here.\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country‚Äôs life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don‚Äôt have complete data. The gapminder package in python (install with pip install gapminder) is a port of the R package, but doesn‚Äôt contain the unfiltered data, so we‚Äôll instead use a CSV export.\n\n\n\n\n\n\nRead in the Data\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 √ó 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ‚Ñπ 3,303 more rows\n\n\n\n\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 1: How Bad is It?\n\n\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nUsing your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information.\n\n\n\ngapminder_unfiltered %&gt;% \n  group_by(country) %&gt;% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %&gt;%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 √ó 5\n##    country        n missinglifeExp missingpop missingGDP\n##    &lt;fct&gt;      &lt;int&gt;          &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # ‚Ñπ 73 more rows\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2: Exclude any data which isn‚Äôt at 5-year increments\n\n\n\n\n\nStart in 1952 (so 1952, 1957, 1962, ‚Ä¶, 2007).\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 √ó 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ‚Ñπ 2,003 more rows\n\n\n\nReminder about python list comprehensions\nExplanation of the query @ statement\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3: Exclude any countries that don‚Äôt have a full set of observations\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5)) %&gt;%\n  group_by(country) %&gt;%\n  mutate(nobs = n()) %&gt;% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %&gt;%\n  select(-nobs)\n## # A tibble: 1,704 √ó 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ‚Ñπ 1,694 more rows\n\n\n\n\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##                       country continent  year  lifeExp       pop   gdpPercap\n## country                                                                     \n## Afghanistan 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n##             1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n##             2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n##             3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n##             4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...                       ...       ...   ...      ...       ...         ...\n## Zimbabwe    3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n##             3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n##             3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n##             3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n##             3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#additional-resources",
    "href": "part-wrangling/03-data-cleaning.html#additional-resources",
    "title": "22¬† Data Cleaning",
    "section": "\n22.11 Additional Resources",
    "text": "22.11 Additional Resources\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nBase R data manipulation\n\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.\n\n\nTidyTuesday Python github repo - replicating Tidy Tuesday analyses in Python with Pandas",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#references",
    "href": "part-wrangling/03-data-cleaning.html#references",
    "title": "22¬† Data Cleaning",
    "section": "\n22.12 References",
    "text": "22.12 References\n\n\n\n\n[1] \nPandas, ‚ÄúIndexing and selecting data,‚Äù Pandas 1.4.3 Documentation. 2022 [Online]. Available: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing. [Accessed: Jun. 30, 2022]\n\n\n[2] \npwwang, ‚ÄúDatar: A Grammar of Data Manipulation in python.‚Äù May 2022 [Online]. Available: https://pwwang.github.io/datar/. [Accessed: Jun. 30, 2022]\n\n\n[3] \nM. Chow, ‚Äúnycflights13: A data package for nyc flights (the nycflights13 R package).‚Äù 2020 [Online]. Available: https://github.com/tidyverse/nycflights13. [Accessed: Jun. 30, 2022]\n\n\n[4] \nPython Foundation, ‚ÄúData Structures,‚Äù Python 3.10.5 documentation. Jun. 2022 [Online]. Available: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions. [Accessed: Jun. 30, 2022]\n\n\n[5] \nC. Nguyen, ‚ÄúTips for String Manipulation in Python,‚Äù Towards Data Science. Sep. 2021 [Online]. Available: https://towardsdatascience.com/tips-for-string-manipulation-in-python-92b1fc3f4d9f. [Accessed: Jul. 01, 2022]\n\n\n[6] \nJ. VanderPlas, ‚ÄúString Manipulation and Regular Expressions,‚Äù in A Whirlwind Tour of Python, O‚ÄôReilly Media, 2016 [Online]. Available: https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html. [Accessed: Jul. 01, 2022]\n\n\n[7] \nC. Whorton, ‚ÄúApplying Custom Functions to Groups of Data in Pandas,‚Äù Medium. Jul. 2021 [Online]. Available: https://towardsdatascience.com/applying-custom-functions-to-groups-of-data-in-pandas-928d7eece0aa. [Accessed: Jul. 01, 2022]\n\n\n[8] \nH. Wickham, ‚ÄúThe split-apply-combine strategy for data analysis,‚Äù Journal of statistical software, vol. 40, pp. 1‚Äì29, 2011. \n\n\n[9] \n\n‚ÄúGroup by: Split-apply-combine,‚Äù in Pandas 1.4.3 documentation, Python, 2022 [Online]. Available: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html. [Accessed: Jul. 01, 2022]\n\n\n[10] \nJ. Dean and S. Ghemawat, ‚ÄúMapReduce: Simplified data processing on large clusters,‚Äù Communications of the ACM, vol. 51, no. 1, pp. 107‚Äì113, Jan. 2008, doi: 10.1145/1327452.1327492. \n\n\n[11] \nM. Dancho, ‚ÄúAnswer to \"Is there an \"ungroup by\" operation opposite to .groupby in pandas?\",‚Äù Stack Overflow. Mar. 2021 [Online]. Available: https://stackoverflow.com/a/66879388/2859168. [Accessed: Jul. 01, 2022]\n\n\n[12] \nC. Mass, ‚ÄúThe pacific northwest has the worst coastal weather radar coverage in the continental u.s.: Documentation of the problem and a call for action. Department of atmospheric sciences,‚Äù Jan. 12, 2006. [Online]. Available: https://www.atmos.washington.edu/~cliff/coastalradarold.html. [Accessed: Jan. 14, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#footnotes",
    "href": "part-wrangling/03-data-cleaning.html#footnotes",
    "title": "22¬† Data Cleaning",
    "section": "",
    "text": "See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.‚Ü©Ô∏é\nThe philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.‚Ü©Ô∏é\nIt accomplishes this through the magic of quasiquotation, which we will not cover in this course because it‚Äôs basically witchcraft.‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html",
    "href": "part-wrangling/04-strings.html",
    "title": "23¬† Working with Strings",
    "section": "",
    "text": "23.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#objectives",
    "href": "part-wrangling/04-strings.html#objectives",
    "title": "23¬† Working with Strings",
    "section": "",
    "text": "Use functions to perform find-and-replace operations\nUse functions to split string data into multiple columns/variables\nUse functions to join string data from multiple columns/variables into a single column/variable\n\n\n\nPerhaps one day you‚Äôll be able to put this knowledge to use in a practical setting!",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#basic-operations",
    "href": "part-wrangling/04-strings.html#basic-operations",
    "title": "23¬† Working with Strings",
    "section": "\n23.2 Basic Operations",
    "text": "23.2 Basic Operations\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different ‚Äúlevels‚Äù of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think ‚ÄúI know, I‚Äôll use regular expressions.‚Äù Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\nThe stringr cheatsheet by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n\nTable¬†23.1: Table of string functions in R and python. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\n\n\nTask\nR\nPython\n\n\n\nReplace pattern with replacement\n\n\nbase: gsub(pattern, replacement, x)\nstringr: str_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\npandas: x.str.replace(pattern, replacement) (not vectorized over pattern or replacement)\n\n\nConvert case\n\nbase: tolower(x), toupper(x)\nstringr: str_to_lower(x), str_to_upper(x) , str_to_title(x)\n\npandas: x.str.lower(), x.str.upper()\n\n\n\nStrip whitespace from start/end\n\nbase: trimws(x)\nstringr: str_trim(x) , str_squish(x)\n\npandas: x.str.strip()\n\n\n\nPad strings to a specific length\n\nbase: sprintf(format, x)\nstringr: str_pad(x, ‚Ä¶)\n\npandas: x.str.pad()\n\n\n\nTest if the string contains a pattern\n\nbase: grep(pattern, x) or grepl(pattern, x)\nstringr: str_detect(x, pattern)\n\npandas: x.str.contains(pattern)\n\n\n\nCount how many times a pattern appears in the string\n\nbase: gregexpr(pattern, x) + sapply to count length of the returned list\nstringi: stri_count(x, pattern)\nstringr: str_count(x, pattern)\n\npandas: x.str.count(pattern)\n\n\n\nFind the first appearance of the pattern within the string\n\nbase: regexpr(pattern, x)\nstringr: str_locate(x, pattern)\n\npandas: x.str.find(pattern)\n\n\n\nFind all appearances of the pattern within the string\n\nbase: gregexpr\nstringr: str_locate_all(x, pattern)\n\npandas: x.str.findall(pattern)\n\n\n\nDetect a match at the start/end of the string\n\nbase: use regular expr.\nstringr: str_starts(x, pattern) ,str_ends(x, pattern)\n\npandas: x.str.startswith(pattern) , x.str.endswith(pattern)\n\n\n\nSubset a string from index a to b\n\nbase: substr(x, a, b)\nstringr: str_sub(x, a, b)\n\npandas: x.str.slice(a, b, step)\n\n\n\nConvert string encoding\n\nbase: iconv(x, encoding)\nstringr: str_conv(x, encoding)\n\npandas: x.str.encode(encoding)\n\n\n\n\n\n\n\nIn Table¬†23.1, multiple functions are provided for e.g.¬†common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the stringr package, which is included in the tidyverse1.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "href": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "title": "23¬† Working with Strings",
    "section": "\n23.3 Converting strings to numbers",
    "text": "23.3 Converting strings to numbers\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\nTable¬†23.2: Different ‚Äúmessy‚Äù number formats\n\n\n\n\nint_col\nfloat_col\nmix_col\nmissing_col\nmoney_col\neu_numbers\nboolean_col\ncustom\n\n\n\n0\n1\n1.1\na\n1\n¬£1,000.00\n1.000.000,00\nTrue\nY\n\n\n1\n2\n1.2\n2\n2\n¬£2,400.00\n2.000.342,00\nFalse\nY\n\n\n2\n3\n1.3\n3\n3\n¬£2,400.00\n3.141,59\nTrue\nN\n\n\n3\n4\n4.7\n4\nnan\n¬£2,400.00\n34,25\nTrue\nN\n\n\n\n\n\n\nNumbers, currencies, dates, and times are written differently based on what country you‚Äôre in [2]. In computer terms, this is the locale, and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [3].\nLocales are something you may want to skip if you‚Äôre just starting out and you don‚Äôt work with code written by people in other countries. If you‚Äôre collaborating internationally, however, you may want to at least skim the section below to be aware of potential issues when locale-related problems crop up.\nIf you‚Äôve never had to deal with the complexities of working on a laptop designed for one country using another country‚Äôs conventions, know that it isn‚Äôt necessarily the easiest thing to do.\nAdvanced: Locales\nFind your locale\n\n\n Type Get-WinSystemLocale into your CMD or powershell terminal.\n\n (10.4 and later) and  Type locale into your terminal\nGet set up to work with locales\nWhile this isn‚Äôt required, it may be useful and is definitely good practice if you‚Äôre planning to work with data generated internationally.\nThis article tells you how to set things up in linux . The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n\n Installing languages\n\n\n Change locales. Installing or creating new locales seems to be more complicated, and since I do not have a mac, I can‚Äôt test this out easily myself.\nWe‚Äôll use Table¬†23.2 to explore different string operations focused specifically on converting strings to numbers.\n\n\nGet the data: Python\nR\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/number-formats.csv\")\n\n\n\n\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/number-formats.csv\", colClasses = \"character\")\n\nBy default, R tries to outsmart us and read the data in as numbers. I‚Äôve disabled this behavior by setting colClasses='character' so that you can see how these functions work‚Ä¶ but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don‚Äôt know how to disable it.\n\n\n\n\n\n\n\n\n\nConverting Columns Using Your Best Guess\n\n\n\nBoth R and Python have ways to ‚Äúguess‚Äù what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you‚Äôre working with data that is already read in, how do you get R and Python to guess what type something is?\n\n\nR\nPython\n\n\n\nHere, R gets everything ‚Äúright‚Äù except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn‚Äôt clearly numeric or doesn‚Äôt match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle mix_col, we would have to explicitly convert to numeric, causing the a to be converted to NA\n\nlibrary(dplyr)\nlibrary(readr)\ndf_guess &lt;- type_convert(df)\nstr(df_guess)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"¬£1,000.00\" \"¬£2,400.00\" \"¬£2,400.00\" \"¬£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\nThe type_convert function has a locale argument; readr includes a locale() function that you can pass to type_convert that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\nlibrary(dplyr)\nlibrary(readr)\nfixed_df &lt;- type_convert(df) \nfixed_df2 &lt;- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"¬£1,000.00\" \"¬£2,400.00\" \"¬£2,400.00\" \"¬£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\n\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\nAs in R, we can set the locale in Python to change how things are read in.\n\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Columns Directly\n\n\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in Section 8.5. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of ‚Äòmissing‚Äô or NA variables that are not necessarily compatible.\n\n\nR\nPython\n\n\n\nHere, we use the across helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don‚Äôt work here, because they contain characters R doesn‚Äôt recognize as numeric characters.\n\nlibrary(dplyr)\n\ndf_numeric &lt;- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  NA NA NA NA\n##  $ custom     : num  NA NA NA NA\n\n\n\n\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Converting Y/N data\n\n\n\nThe next thing we might want to do is convert our custom column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\nWe could use factors/categorical variables, which have numeric values ‚Äúunder the hood‚Äù, but show up as labeled.\nWe could (in this particular case) test for equality with ‚ÄúY‚Äù, but this approach would not generalize well if we had more than 2 categories.\nWe could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I‚Äôve used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n\nR\nPython\n\n\n\n\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %&gt;%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %&gt;%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':    4 obs. of  11 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"¬£1,000.00\" \"¬£2,400.00\" \"¬£2,400.00\" \"¬£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n\n\n\nWe‚Äôve already done a brief demonstration of string methods in Python when we trimmed off the ¬£ character. In this situation, it‚Äôs better to use the pandas replace method, which allows you to pass in a list of values and a list of replacements.\n\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers      float64\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#find-and-replace",
    "href": "part-wrangling/04-strings.html#find-and-replace",
    "title": "23¬† Working with Strings",
    "section": "\n23.4 Find and replace",
    "text": "23.4 Find and replace\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution2, and may introduce bugs if you use the same code to analyze new data with characters you haven‚Äôt anticipated, but in so many cases it‚Äôs also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\nI‚Äôll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading [4] so that you know why find-and-replace isn‚Äôt (necessarily) the best option for locale-specific formatting.\n\n\n\n\n\n\nExample: find and replace\n\n\n\nLet‚Äôs start with the money column.\n\n\nR\nPython\n\n\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn‚Äôt work by default with type_convert, but as long as we mutate and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ‚Äò,‚Äô for decimal and ‚Äò.‚Äô for thousands separators.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n\n\n\nIn python, a similar approach doesn‚Äôt work out, because the pound sign is not handled correctly.\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## babel.numbers.NumberFormatError: '¬£1,000.00' is not a valid decimal number\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n# Remove ¬£ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"¬£\")\n# Then parse the number\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n23.4.1 Example: Locale find-and-replace\nWe could also handle the locale issues using find-and-replace, if we wanted to‚Ä¶\n\n\nR\nPython\n\n\n\nNote that str_remove is shorthand for str_replace(x, pattern, \"\"). There is a little bit of additional complexity in switching ‚Äú,‚Äù for ‚Äú.‚Äù and vice versa - we have to change ‚Äú,‚Äù to something else first, so that we can replace ‚Äú.‚Äù with ‚Äú,‚Äù. This is not elegant but it does work. It also doesn‚Äôt generalize - it will mess up numbers formatted using the US/UK convention, and it won‚Äôt handle numbers formatted using other conventions from other locales.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = str_remove(money_col, \"¬£\") %&gt;% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %&gt;%\n           parse_number())\n\n\n\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n## ValueError: Unable to parse string \"1.000.000.00\" at position 0\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"¬£\").\\\nstr.replace(',', '')\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...    eu_numbers  boolean_col custom\n## 0        1        1.1       a  ...  1.000.000.00         True      Y\n## 1        2        1.2       2  ...  2.000.342.00        False      Y\n## 2        3        1.3       3  ...      3.141.59         True      N\n## 3        4        4.7       4  ...         34.25         True      N\n## \n## [4 rows x 8 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "href": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "title": "23¬† Working with Strings",
    "section": "\n23.5 Separating multi-variable columns",
    "text": "23.5 Separating multi-variable columns\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\nIn pandas, we use x.str.split() to split columns in a DataFrame, in R we use the tidyr package‚Äôs separate_wider_xxx() series of functions.\n\n\n\n\n\n\nExample: Separating columns\n\n\n\nWe‚Äôll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to separate the rate column into two new columns, cases and population.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate_wider_delim(table3, rate, delim = \"/\", names = c('cases', 'pop'), cols_remove = F)\n## # A tibble: 6 √ó 5\n##   country      year cases  pop        rate             \n##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;            \n## 1 Afghanistan  1999 745    19987071   745/19987071     \n## 2 Afghanistan  2000 2666   20595360   2666/20595360    \n## 3 Brazil       1999 37737  172006362  37737/172006362  \n## 4 Brazil       2000 80488  174504898  80488/174504898  \n## 5 China        1999 212258 1272915272 212258/1272915272\n## 6 China        2000 213766 1280428583 213766/1280428583\n\n\n\n\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country    year               rate   cases         pop\n## 0  Afghanistan  1999.0       745/19987071     745    19987071\n## 1  Afghanistan  2000.0      2666/20595360    2666    20595360\n## 2       Brazil  1999.0    37737/172006362   37737   172006362\n## 3       Brazil  2000.0    80488/174504898   80488   174504898\n## 4        China  1999.0  212258/1272915272  212258  1272915272\n## 5        China  2000.0  213766/1280428583  213766  1280428583\n\nThis uses python‚Äôs multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g.¬†cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#joining-columns",
    "href": "part-wrangling/04-strings.html#joining-columns",
    "title": "23¬† Working with Strings",
    "section": "\n23.6 Joining columns",
    "text": "23.6 Joining columns\nIt‚Äôs also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a ‚Äòname‚Äô column that has both pieces of information together.\n\n\n\n\n\n\nExample: Joining columns\n\n\n\nWe‚Äôll use the table5 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table5.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to join the century and year columns into a new column, yyyy.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %&gt;%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 √ó 5\n##   country      yyyy century year  rate             \n##   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n\n\n\n\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#regular-expressions",
    "href": "part-wrangling/04-strings.html#regular-expressions",
    "title": "23¬† Working with Strings",
    "section": "\n23.7 Regular Expressions",
    "text": "23.7 Regular Expressions\nMatching exact strings is easy - it‚Äôs just like using find and replace.\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears &lt;- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n\n\n\n\n\nTo generate #1 albums, ‚Äòjay ‚Äìhelp‚Äô recommends the -z flag. XKCD comics by Randall Munroe CC-A-NC 2.5.\n\nA regular expression is a sequence of characters that specify a match pattern to search for in a larger text [5]. Regular expressions may be used to specify find or find-and-replace operations on strings.\nRegular expressions can be extremely useful for cleaning and extracting data: they can replace misspellings, extract pieces of information from longer strings, and flexibly handle different ways people may input data. They may be incredibly powerful, but they can also be complicated to create and the expressions themselves may be cryptic and nearly impossible to decode.\nBut, if you can master even a small amount of regular expression notation, you‚Äôll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you‚Äôre creative, but often they‚Äôre much simpler.\nHere are some useful regular expressions3:\n\nValidate a phone number [6]: ^\\(*\\d{3}\\)*( |-)*\\d{3}( |-)*\\d{4}$\n\nCheck for first and last names [7]: ^[\\w'\\-,.][^0-9_!¬°?√∑?¬ø/\\\\+=@#$%ÀÜ&*(){}|~&lt;&gt;;:[\\]]{2,}$\n(This is a tricky proposition and this regular expression does make some assumptions about what characters are valid for names.)\nMatch a 5 or 9 digit zip code: (^\\d{5}$)|(^\\d{9}$)|(^\\d{5}-\\d{4}$)\n\n\nThese tasks are all well-suited for regular expressions. More complicated tasks, such as validating an email address, are less suited for regular expressions, though there are regular expressions that exist [8] for that task.\n\n\nI‚Äôve assembled a YouTube playlist of different explanations of regular expressions, if you prefer that type of tutorial.\n\n\n\n\nThe following demonstrations are intended for advanced students: if you are just learning how to program, you may want to come back to these when you need them.\nThere is also an excellent site which helps you learn regular expressions via interactive tutorials, [9]. Another useful tool is [10]\n\n23.7.1 Regular Expression Basics\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found here. The subset of regular expression syntax we‚Äôre going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it‚Äôs useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\n\nR\nPython\n\n\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn &lt;- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n\n\n\nIn python, a regular expression is indicated by putting the character ‚Äòr‚Äô right before the quoted expression. This tells python that any backslashes in the string should be left alone ‚Äì if R had that feature, we wouldn‚Äôt have to escape all the backslashes!\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\n\n\n\n\n\n23.7.2 Specifying repetition\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times ‚Äì most useful when you‚Äôre looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means ‚Äú3 or more times‚Äù and {3} means ‚Äúexactly 3 times‚Äù\n\n\n\nR\nPython\n\n\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn &lt;- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone &lt;- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid &lt;- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance &lt;- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n\n\n\n\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## &lt;re.Match object; span=(0, 6), match='banana'&gt;\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## &lt;re.Match object; span=(0, 2), match='ba'&gt;\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## &lt;re.Match object; span=(1, 5), match='anan'&gt;\n\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## &lt;re.Match object; span=(7, 19), match='123-456-7890'&gt;\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## &lt;re.Match object; span=(27, 35), match='12345678'&gt;\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## &lt;re.Match object; span=(77, 91), match='$50,000,000.23'&gt;\n\n\n\n\n\n23.7.3 Matching Locations\nThere are also ways to ‚Äúanchor‚Äù a pattern to a part of the string (e.g.¬†the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it‚Äôs the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g.¬†[^abc], ^ means ‚Äúnot‚Äù - for instance, ‚Äúthe collection of all characters that aren‚Äôt a, b, or c‚Äù.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n\nR\nPython\n\n\n\n\naddress &lt;- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num &lt;- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet &lt;- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number\n\ncity &lt;- str_extract(address, \",.*,\") %&gt;% str_remove_all(\",\") %&gt;% str_trim()\n\nzip &lt;- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\nPython match objects contain 3 things: .span(), which has the start and end positions of the match, .string, which contains the original string passed into the function, and .group(), which contains the actual matching portion of the string.\n\nimport re\n\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n\n\n\n\n\n23.7.4 Capturing Information\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you‚Äôve captured information using (), you can reference that information using backreferences.\nIn most languages, backreferences look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\n\n\nR\nPython\n\n\n\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants &lt;- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears &lt;- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n\n\n\n\nimport pandas as pd\nimport re\n\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n## ValueError: Cannot use a compiled regex as replacement pattern with regex=False\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n\n\n\n\n\n23.7.5 Putting it all Together\nWe can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n\nR\nPython\n\n\n\n\nstrings &lt;- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex &lt;- \"(walk|treat)\"\naddr_regex &lt;- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex &lt;- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 √ó 5\n##   text                                                   phone dog   addr  abab \n##   &lt;chr&gt;                                                  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk‚Ä¶ TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b‚Ä¶ TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\nimport pandas as pd\nimport re\n\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#sec-strings-refs",
    "href": "part-wrangling/04-strings.html#sec-strings-refs",
    "title": "23¬† Working with Strings",
    "section": "\n23.8 References",
    "text": "23.8 References\n\n\n\n\n[1] \nP. Norvig, ‚ÄúHow to write a spelling corrector. Norvig.com,‚Äù Feb. 01, 2007. [Online]. Available: http://norvig.com/spell-correct.html. [Accessed: Mar. 08, 2023]\n\n\n[2] \nM. Ashour, ‚ÄúA Concise Guide to Number Localization,‚Äù Phrase. Feb. 2022 [Online]. Available: https://phrase.com/blog/posts/number-localization/. [Accessed: Jul. 25, 2022]\n\n\n[3] \nWikipedia Contributors, ‚ÄúLocale (computer software),‚Äù Wikipedia. Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Locale_(computer_software)&oldid=1082900932. [Accessed: Jul. 25, 2022]\n\n\n[4] \nA. Herrmann, ‚ÄúHow to deal with international data formats in Python,‚Äù herrmann.tech. Feb. 2021 [Online]. Available: https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html. [Accessed: Jul. 25, 2022]\n\n\n[5] \n\n‚ÄúRegular expression,‚Äù Wikipedia. Feb. 28, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Regular_expression&oldid=1142135804. [Accessed: Mar. 09, 2023]\n\n\n[6] \nJ. Atwood, ‚ÄúRegex use vs. Regex abuse. Coding horror,‚Äù Feb. 16, 2005. [Online]. Available: https://blog.codinghorror.com/regex-use-vs-regex-abuse/. [Accessed: Mar. 09, 2023]\n\n\n[7] \nCristianGuerrero, ‚ÄúAnswer to \"regular expression for first and last name\". Stack overflow,‚Äù Aug. 24, 2017. [Online]. Available: https://stackoverflow.com/a/45871742. [Accessed: Mar. 09, 2023]\n\n\n[8] \nL. Ristic, ‚ÄúValidate email addresses with regular expressions in JavaScript. Stack abuse,‚Äù Oct. 14, 2021. [Online]. Available: https://stackabuse.com/validate-email-addresses-with-regular-expressions-in-javascript/. [Accessed: Mar. 09, 2023]\n\n\n[9] \nRegexOne, ‚ÄúRegexOne.‚Äù [Online]. Available: https://regexone.com/. [Accessed: Apr. 20, 2023]\n\n\n[10] \nLea Verou, ‚ÄúReg explained. /a(b)/g,‚Äù 2017. [Online]. Available: https://projects.verou.me/regexplained/. [Accessed: Apr. 20, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/04-strings.html#footnotes",
    "href": "part-wrangling/04-strings.html#footnotes",
    "title": "23¬† Working with Strings",
    "section": "",
    "text": "Many functions from stringr have somewhat faster functional equivalents in the stringi package, but the stringi package has a less ‚Äútidy‚Äù API, so it may be worth the slight slowdown to use stringr if your data isn‚Äôt huge because your code will be more readable.‚Ü©Ô∏é\nIt‚Äôs particularly hackish when you‚Äôre working with locale-specific settings [4], and in many cases you can handle locale issues much more elegantly.‚Ü©Ô∏é\nNote that these are written in generic regular expression text - to use them in R you will have to escape each and every \\ with another \\.‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Working with Strings</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html",
    "href": "part-wrangling/05-data-reshape.html",
    "title": "24¬† Reshaping Data",
    "section": "",
    "text": "24.1  Objectives\nBroadly, your objective while reading this chapter is to be able to identify datasets which have ‚Äúmessy‚Äù formats and determine a sequence of operations to transition the data into ‚Äútidy‚Äù format. To do this, you should be master the following concepts:",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#objectives",
    "href": "part-wrangling/05-data-reshape.html#objectives",
    "title": "24¬† Reshaping Data",
    "section": "",
    "text": "Determine what data format is necessary to generate a desired plot or statistical model\nUnderstand the differences between ‚Äúwide‚Äù and ‚Äúlong‚Äù format data and how to transition between the two structures",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "href": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "title": "24¬† Reshaping Data",
    "section": "\n24.2 Tidy and Messy Data",
    "text": "24.2 Tidy and Messy Data\n\n24.2.1 Motivating Example\nConsider the spreadsheet screenshot in Figure¬†24.1.\n\n\n\n\n\nFigure¬†24.1: Spreadsheet intended for human consumption, from [1] (Chapter 3)\n\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. Figure¬†24.2 shows a restructured version of this data in a more compact rectangular format.\n\n\n\n\n\nFigure¬†24.2: Spreadsheet reorganized for data analysis\n\n\nIn Figure¬†24.2, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as ‚Äútidy‚Äù data: it has a single column for each variable and a single row for each observation.\n\n24.2.2 Defining Tidy data\nThe illustrations below are lifted from an excellent blog post [2] about tidy data; they‚Äôre reproduced here because\n\nthey‚Äôre beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\n\n\n\nFigure¬†24.3: Tidy data format, illustrated.\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the data set, you can borrow methods and tools from a collaborator‚Äôs analysis and easily apply them to your own data set.\n\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\n\nFigure¬†24.4: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\nExamples: Messy Data\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g.¬†regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g.¬†ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can‚Äôt do anything with this format as it stands, because we can‚Äôt do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year‚Äôs observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we‚Äôll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don‚Äôt always handle dates the way you‚Äôd hope they would.\n\n\n\n\n\n\n\n\n\nTry it out: Classifying Messy Data\n\n\n\n\n\nProblem\nTable 1\n2\n3\n4\n5\n\n\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nEach variable does not have its own column (so a single year‚Äôs observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you‚Äôd have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you‚Äôd need to ‚Äústack‚Äù the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren‚Äôt actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you‚Äôd need to combine the two year columns together first).\n\n\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach data set goes into its own table (or tibble, if you are using R)\nEach variable gets its own column",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#additional-reading",
    "href": "part-wrangling/05-data-reshape.html#additional-reading",
    "title": "24¬† Reshaping Data",
    "section": "\n24.3 Additional reading",
    "text": "24.3 Additional reading\n[3] - IBM SPSS ad that talks about the perils of spreadsheets\n[4] - assembled news stories involving spreadsheet mishaps",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#pivot-operations",
    "href": "part-wrangling/05-data-reshape.html#pivot-operations",
    "title": "24¬† Reshaping Data",
    "section": "\n24.4 Pivot operations",
    "text": "24.4 Pivot operations\nIt‚Äôs fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren‚Äôt necessarily the most friendly for analysis.\n\n\nWide and Long format data. Source\n\nThe two operations we‚Äôll learn here are wide -&gt; long and long -&gt; wide.\n\n\nPivoting from wide to long (and back) Source\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n24.4.1 Longer\nIn many cases, the data come in what we might call ‚Äúwide‚Äù form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTables 4a and 4b are good examples of data which is in ‚Äúwide‚Äù form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 √ó 3\n##   country     `1999` `2000`\n##   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 √ó 3\n##   country         `1999`     `2000`\n##   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into ‚Äúlong form‚Äù: to take the columns which contain values and ‚Äústack‚Äù them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn‚Äôt being stacked (e.g.¬†country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we‚Äôll have tidy(er) data.\n\n\n\ntba &lt;- table4a %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb &lt;- table4b %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %&gt;%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 √ó 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument ‚Äúnames_to‚Äù (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument ‚Äúvalues_to‚Äù (again, hopefully easy to remember).\nWe identify ID variables (variables which we don‚Äôt want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\n\nselect variables we don‚Äôt want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you‚Äôre pivoting (and how the columns are structured).\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values ‚Äúcases‚Äù and ‚Äúpopulation‚Äù respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x &lt;- table4a %&gt;% mutate(id = \"cases\")\ntable4b.x &lt;- table4b %&gt;% mutate(id = \"population\")\n# Create one table\ntable4 &lt;- bind_rows(table4a.x, table4b.x)\n\ntable4_long &lt;- table4 %&gt;%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %&gt;%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 √ó 4\n##    country     id         year       count\n##    &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy &lt;- table4_long %&gt;%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 √ó 4\n##   country     year   cases population\n##   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\nIn Pandas, pandas.melt(...) takes id_vars, value_vars, var_name, and value_name. Otherwise, it functions nearly exactly the same as pivot_longer; the biggest difference is that column selection works differently in python than it does in the tidyverse.\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n\nHere‚Äôs the melt/pivot_longer + pivot/pivot_wider version:\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year     cases    population\n## 0   Afghanistan  1999     745.0  1.998707e+07\n## 1   Afghanistan  2000    2666.0  2.059536e+07\n## 2        Brazil  1999   37737.0  1.720064e+08\n## 3        Brazil  2000   80488.0  1.745049e+08\n## 4         China  1999  212258.0  1.272915e+09\n## 5         China  2000  213766.0  1.280429e+09\n\n\n\n\n\n24.4.2 Wider\nWhile it‚Äôs very common to need to transform data into a longer format, it‚Äôs not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be ‚Äútidy‚Äù - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\ntable2 %&gt;%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 √ó 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country    year     cases    population\n## 0     Afghanistan  1999.0     745.0  1.998707e+07\n## 1     Afghanistan  2000.0    2666.0  2.059536e+07\n## 2          Brazil  1999.0   37737.0  1.720064e+08\n## 3          Brazil  2000.0   80488.0  1.745049e+08\n## 4           China  1999.0  212258.0  1.272915e+09\n## 5           China  2000.0  213766.0  1.280429e+09\n\n\n\n\n\n\n\n\n\n\nTry it Out!\n\n\n\nIn the next section, we‚Äôll be using the WHO surveillance of disease incidence data (link). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form (link). For demonstration purposes, I‚Äôll continue using the messier 2020 data, but the link is no longer available on the WHO‚Äôs site.\nIt will require some preprocessing before it‚Äôs suitable for a demonstration. I‚Äôll do some of it, but in this section, you‚Äôre going to do the rest.\n\n\nPreprocessing\nProblem\nR solution\nPython solution\n\n\n\nYou don‚Äôt have to understand what this code is doing just yet.\n\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\nlibrary(tibble)\nlibrary(dplyr)\n\ndownload.file(\"https://github.com/srvanderplas/datasets/raw/main/raw/2020_WHO_incidence_series.xls\", \"../data/2020_WHO_incidence_series.xls\")\nsheets &lt;- excel_sheets(\"../data/2020_WHO_incidence_series.xls\")\nsheets &lt;- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence &lt;- map_df(sheets, ~read_xls(path =\"../data/2020_WHO_incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 &lt;- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 &lt;- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"../data/2020_WHO_incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"../data/2020_who_disease_incidence.csv\")\n\n\n\nDownload the exported data here and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\nCan you make a line plot of cases of measles in Bangladesh over time?\n\nhead(disease_incidence)\n## # A tibble: 6 √ó 43\n##   WHO_REGION ISO_code Cname    Disease `2018` `2017` `2016` `2015` `2014` `2013`\n##   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 EMR        AFG      Afghani‚Ä¶ CRS         NA     NA     NA      0      0      0\n## 2 EUR        ALB      Albania  CRS          0      0     NA     NA     NA      0\n## 3 AFR        DZA      Algeria  CRS         NA     NA      0      0     NA     NA\n## 4 EUR        AND      Andorra  CRS          0      0      0     NA     NA      0\n## 5 AFR        AGO      Angola   CRS         NA     NA     NA     NA     NA     NA\n## 6 AMR        ATG      Antigua‚Ä¶ CRS          0      0      0      0      0      0\n## # ‚Ñπ 33 more variables: `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;,\n## #   `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;,\n## #   `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `1999` &lt;dbl&gt;,\n## #   `1998` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1994` &lt;dbl&gt;,\n## #   `1993` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1989` &lt;dbl&gt;,\n## #   `1988` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1984` &lt;dbl&gt;,\n## #   `1983` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1980` &lt;dbl&gt;\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(stringr)\nwho_disease &lt;- read_csv(\"../data/2020_who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long &lt;- who_disease %&gt;%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %&gt;%\n  rename(Country = Cname) %&gt;%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year),\n         cases = as.numeric(cases))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %&gt;%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"../data/2020_who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "href": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "title": "24¬† Reshaping Data",
    "section": "\n24.5 Example: Gas Prices Data",
    "text": "24.5 Example: Gas Prices Data\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table. \n\n\nGas prices at US EIA site\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\nBut looking at the chart at the top of the page, it‚Äôs not clear how we might get that chart from the data in the format it‚Äôs presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n24.5.1 Setup: Gas Price Data Cleaning\nFor the next example, we‚Äôll read the data in from the HTML table online and work to make it something we could e.g.¬†plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl &lt;- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc &lt;- read_html(url)\ngas_prices_html &lt;- html_table(htmldoc, fill = T, trim = T)[[5]][,1:11]\n\n\n\n\nFirst 6 rows of gas prices data as read into R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-Month\nWeek 1\nWeek 1\nWeek 2\nWeek 2\nWeek 3\nWeek 3\nWeek 4\nWeek 4\nWeek 5\nWeek 5\n\n\n\nYear-Month\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\n\n\n1994-Nov\n\n\n\n\n\n\n11/28\n1.175\n\n\n\n\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.110\n01/30\n1.109\n\n\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n## ImportError: Missing optional dependency 'lxml'.  Use pip or conda to install lxml.\n\nNameError: name ‚Äògas_prices_html‚Äô is not defined\n\n\n\n\n\n\n\n\n\nTry it out: Manual Formatting in Excel\n\n\n\n\n\nProblem\nSolution\nVideo\n\n\n\nAn excel spreadsheet of the data as downloaded in January 2023 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\nWhat steps are involved?\n\n\n\nCopy the year-month column, creating one vertical copy for every set of columns\nMove each block of two columns down to the corresponding vertical copy\nDelete empty rows\nFormat dates\nDelete empty columns\n\n\n\n\n\n\n\n\n\nFigure¬†24.5: Here is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting with Pivot Operations\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\nSteps to work through the gas prices data cleaning process\n\n\n\n\nlibrary(tidyverse)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names &lt;- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %&gt;%\n    # trim leading/trailing spaces\n    str_trim() %&gt;%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 √ó 11\n##   Year.Month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;      \n## 1 1994-Nov   \"\"          \"\"           \"\"          \"\"           \"\"         \n## 2 1994-Dec   \"12/05\"     \"1.143\"      \"12/12\"     \"1.118\"      \"12/19\"    \n## 3 1995-Jan   \"01/02\"     \"1.104\"      \"01/09\"     \"1.111\"      \"01/16\"    \n## 4 1995-Feb   \"02/06\"     \"1.103\"      \"02/13\"     \"1.099\"      \"02/20\"    \n## 5 1995-Mar   \"03/06\"     \"1.103\"      \"03/13\"     \"1.096\"      \"03/20\"    \n## 6 1995-Apr   \"04/03\"     \"1.116\"      \"04/10\"     \"1.134\"      \"04/17\"    \n## # ‚Ñπ 5 more variables: Week.3.Value &lt;chr&gt;, Week.4.Date &lt;chr&gt;,\n## #   Week.4.Value &lt;chr&gt;, Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;\n\n\n# gas_prices_raw &lt;- select(gas_prices_raw, -c(X, Date))\ngas_prices_long &lt;- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 √ó 3\n##   Year.Month variable     value\n##   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n\n\ngas_prices_sep &lt;- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %&gt;%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 √ó 4\n##   Year.Month week  variable value\n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n\n\ngas_prices_wide &lt;- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 √ó 4\n##   Year.Month week  Date    Value  \n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n\n\ngas_prices_date &lt;- gas_prices_wide %&gt;%\n  filter(nchar(Value) &gt; 0) %&gt;%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %&gt;%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %&gt;%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 √ó 2\n##   Date       Value\n##   &lt;chr&gt;      &lt;chr&gt;\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n\n\nlibrary(lubridate)\ngas_prices &lt;- gas_prices_date %&gt;%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %&gt;%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 √ó 2\n##   Date       Price.per.gallon\n##   &lt;date&gt;                &lt;dbl&gt;\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html.copy()\n## NameError: name 'gas_prices_html' is not defined\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n## NameError: name 'gas_prices_raw' is not defined\n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## NameError: name 'gas_prices_raw' is not defined\ncolnames\n## NameError: name 'colnames' is not defined\n\n# Set new column names\ngas_prices_raw.columns = colnames\n## NameError: name 'colnames' is not defined\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n## NameError: name 'gas_prices_raw' is not defined\n\n# Drop extra columns on the end\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\n## NameError: name 'gas_prices_raw' is not defined\ngas_prices_raw.head()\n## NameError: name 'gas_prices_raw' is not defined\n\n\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\n## NameError: name 'gas_prices_raw' is not defined\ngas_prices_long.head()\n## NameError: name 'gas_prices_long' is not defined\n\n\ngas_prices_sep = gas_prices_long\n## NameError: name 'gas_prices_long' is not defined\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\n## NameError: name 'gas_prices_sep' is not defined\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\n## NameError: name 'gas_prices_sep' is not defined\ngas_prices_sep.head()\n## NameError: name 'gas_prices_sep' is not defined\n\n\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\n## NameError: name 'gas_prices_sep' is not defined\ngas_prices_wide.head()\n## NameError: name 'gas_prices_wide' is not defined\n\n\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\n## NameError: name 'gas_prices_wide' is not defined\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\n## NameError: name 'gas_prices_date' is not defined\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\n## NameError: name 'gas_prices_date' is not defined\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n## NameError: name 'gas_prices_date' is not defined\n\ngas_prices_date.head()\n## NameError: name 'gas_prices_date' is not defined\n\n\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\n## NameError: name 'gas_prices_date' is not defined\ngas_prices['Price_per_gallon'] = gas_prices.Value\n## NameError: name 'gas_prices' is not defined\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\n## NameError: name 'gas_prices' is not defined\ngas_prices.head()\n## NameError: name 'gas_prices' is not defined\n\n\n\n\n\n\nWe‚Äôll return to this example in Section 25.5 to demonstrate how you can use pivot operations and database merges together to complete this operation in a slightly different way.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#other-resources",
    "href": "part-wrangling/05-data-reshape.html#other-resources",
    "title": "24¬† Reshaping Data",
    "section": "\n24.6 Other resources",
    "text": "24.6 Other resources\n[5] - very nice task-oriented chapter that‚Äôs below the level addressed in this course but still useful",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#references",
    "href": "part-wrangling/05-data-reshape.html#references",
    "title": "24¬† Reshaping Data",
    "section": "\n24.7 References",
    "text": "24.7 References\n\n\n\n\n[1] \nQ. E. McCallum, Bad data handbook: Mapping the world of data problems, 1. ed. Beijing, K√∂ln: O‚ÄôReilly, 2013. \n\n\n[2] \nJ. Lowndes and A. Horst, ‚ÄúTidy data for efficiency, reproducibility, and collaboration,‚Äù Openscapes. Oct. 2020 [Online]. Available: https://www.openscapes.org/blog/2020/10/12/tidy-data//. [Accessed: Jul. 21, 2022]\n\n\n[3] \nInternational Business Machines, ‚ÄúThe risks of using spreadsheets for statistical analysis,‚Äù The risks of using spreadsheets for statistical analysis. Nov. 2018 [Online]. Available: https://web.archive.org/web/20240415181938/https://www.statwks.com/wp-content/uploads/2018/11/ytw03240-usen-03\\_YTW03240USEN.pdf. [Accessed: Nov. 03, 2024]\n\n\n[4] \nP. O‚ÄôBeirne, F. Hermans, T. Cheng, and M. P. Campbell, ‚ÄúHorror Stories,‚Äù European Spreadsheet Risks Interest Group Horror Stories. Oct. 2020 [Online]. Available: https://eusprig.org/research-info/horror-stories/. [Accessed: Nov. 03, 2024]\n\n\n[5] \nJ. Dougherty and I. Ilyankou, ‚ÄúClean Up Messy Data,‚Äù in Hands-On Data Visualization, 1st ed., O‚ÄôReilly Media, 2021, p. 480 [Online]. Available: https://handsondataviz.org/. [Accessed: Nov. 03, 2024]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html",
    "href": "part-wrangling/06-data-join.html",
    "title": "25¬† Joining Data",
    "section": "",
    "text": "25.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#objectives",
    "href": "part-wrangling/06-data-join.html#objectives",
    "title": "25¬† Joining Data",
    "section": "",
    "text": "Identify columns (keys) which can be used to join separate but related tables\nSketch/plan out join operations based on matching keys and given objectives\nImplement planned join operations in R or python\nIdentify when join operations have not completed successfully by looking for duplicated rows, number of rows/columns in the finished object, and missing value counts.\n\n\n\n\n\n\n\nRelational Data Example: Primary School Records\n\n\n\n\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational:\n\neach class has both a teacher and a set of students\neach class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it can be done) because all of the teacher information would be duplicated for each student in each class; all of the student‚Äôs individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the ‚Äúindividual data‚Äù table to find any individuals with matching ID numbers, and keep those individuals‚Äô birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren‚Äôt well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#vocabulary",
    "href": "part-wrangling/06-data-join.html#vocabulary",
    "title": "25¬† Joining Data",
    "section": "\n25.2 Vocabulary",
    "text": "25.2 Vocabulary\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nKeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don‚Äôt change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g.¬†union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#illustrating-joins",
    "href": "part-wrangling/06-data-join.html#illustrating-joins",
    "title": "25¬† Joining Data",
    "section": "\n25.3 Illustrating Joins",
    "text": "25.3 Illustrating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nThe next several sections will show animations demonstrating the different types of joins.\n\n25.3.1 Mutating Joins\nWe‚Äôre primarily going to focus on mutating joins, as filtering joins can be accomplished by ‚Ä¶ filtering ‚Ä¶ rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but‚Ä¶ either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we‚Äôd do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a ‚Äúleft side‚Äù and a ‚Äúright side‚Äù - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\n25.3.1.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nJoins in base R are accomplished with the merge command.\nSpecify the keys to join by using by (or by.x and by.y if the column names are different in the two tables). Specify the rows to keep using all or all.x and all.y. By default, R will merge on any variables that have the same names in each table.\n\nmerge(x, y, by = \"v1\", all = F) # inner join\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nmerge(x, y, by = \"v1\", all = T) # full join\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\nmerge(x, y, by = \"v1\", all.x = T) # left join\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 &lt;NA&gt;\nmerge(x, y, by = \"v1\", all.y = T) # right join\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 &lt;NA&gt; y4\n\n\n\ndplyr contains functions that specifically implement mutating joins separately, primarily for code readability.\n\nlibrary(dplyr)\ninner_join(x, y)\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nleft_join(x, y)\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 &lt;NA&gt;\nright_join(x, y)\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 &lt;NA&gt; y4\nfull_join(x, y)\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\n\n\n\nMutating joins in pandas are accomplished with the merge command. The join type can be specified using the how parameter (left, right, outer, inner, cross). Specify the keys to join by using on (or left_on and right_on if the column names are different in the two tables).\n\nimport pandas as pd\npd.merge(x, y) # inner join (how = 'inner' is default)\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\npd.merge(x, y, how = 'left')\n##    v1  v2   v3\n## 0   1  x1   y1\n## 1   2  x2   y2\n## 2   3  x3  NaN\npd.merge(x, y, how = 'right')\n##     v1   v2  v3\n## 0  1.0   x1  y1\n## 1  2.0   x2  y2\n## 2  4.0  NaN  y4\npd.merge(x, y, how = 'outer') # full join\n##     v1   v2   v3\n## 0  1.0   x1   y1\n## 1  2.0   x2   y2\n## 2  3.0   x3  NaN\n## 3  4.0  NaN   y4\n\n\n\n\n\n25.3.2 Demo: Mutating Joins\n\n\nR\nPython\n\n\n\n\nlibrary(tibble)\nlibrary(dplyr)\nt1 &lt;- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 &lt;- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n## # A tibble: 2 √ó 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don‚Äôt match get filled in with NAs.\n\nleft_join(t1, t2)\n## # A tibble: 3 √ó 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 √ó 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n## # A tibble: 3 √ó 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 √ó 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it‚Äôs known as a full_join.\n\nfull_join(t1, t2)\n## # A tibble: 4 √ó 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n\n\n\n\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don‚Äôt match get filled in with NAs.\n\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n\nAn outer join keeps everything - all rows, all columns.\n\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  C  NaN  4.0\n## 3  D  3.0  5.0\n\n\n\n\nI‚Äôve included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins here [1].\n\n25.3.3 Filtering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\n25.3.3.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nSemi and anti joins aren‚Äôt available by default in base R. You have to do multiple stages of operations to get either one to work.\n\n## Semi-join\n# First, do an inner join\ninnerxy = merge(x, y, all = F)\ninnerxy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nsemixy = innerxy[,names(innerxy)%in% names(x)]\nsemixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n## Anti-join\n# First, do an outer join\nouterxy = merge(x, y, all = T)\nouterxy\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 &lt;NA&gt;\n## 4  4 &lt;NA&gt;   y4\n# Then, drop any rows with NAs\nantixy = na.omit(outerxy)\nantixy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nantixy = antixy[,names(antixy) %in% names(x)]\nantixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n\n\n\nlibrary(dplyr)\nsemi_join(x, y)\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\nanti_join(x, y)\n##   v1 v2\n## 1  3 x3\n\n\n\nIn pandas, we have to be a bit tricky to get semi and anti joins.\n\nimport pandas as pd\n# First, we merge the two data frames (inner by default)\nsemixy = pd.merge(x, y) # Semi join\nsemixy\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\n\n# Then, we drop the extra columns\nsemixy = semixy[semixy.columns.intersection(x.columns)]\nsemixy\n##    v1  v2\n## 0   1  x1\n## 1   2  x2\n\n\n# This syntax keeps track of which rows are from which table\nouter = x.merge(y, how='outer', indicator=True)\nouter\n##     v1   v2   v3      _merge\n## 0  1.0   x1   y1        both\n## 1  2.0   x2   y2        both\n## 2  3.0   x3  NaN   left_only\n## 3  4.0  NaN   y4  right_only\n# Then we drop any rows that aren't 'left_only'\nantixy = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\nantixy\n##     v1  v2   v3\n## 2  3.0  x3  NaN\n# Then we drop any cols that aren't in x\nantixy = antixy[antixy.columns.intersection(x.columns)]\nantixy\n##     v1  v2\n## 2  3.0  x3\n\n\n\n\n\n25.3.4 Set Operations\nWhen talking about set operations, we start with two different data frames than those used above:\n\n\n\nUnion\nUnion All\nIntersection\nSet Difference\n\n\n\nAll unique rows from x and y\n\nOr, all unique rows from y and x.\n\n\n\nAll rows from x and y, keeping duplicate rows.\n\nThis is fundamentally the same as an rbind or bind_rows operation.\n\n\nCommon rows in x and y, keeping only unique rows.\n\n\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n\n\n\n\n\n25.3.4.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\n\nunionxy = unique(rbind(x, y))\nunionxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 5  2  b\n\nunionallxy = rbind(x, y)\nunionallxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  1  a\n## 5  2  b\n\nintersectxy = merge(x, y, all = F)\nintersectxy\n##   v1 v2\n## 1  1  a\n\nIt is possible to get set difference and intersection for data frames by applying the base methods setdiff and intersect, but dplyr does this by overriding those defaults, so it‚Äôs easier to just use that.\n\n\n\nlibrary(dplyr)\nunion(x, y)\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  2  b\nunionall(x, y)\n## Error in unionall(x, y): could not find function \"unionall\"\nsetdiff(x, y)\n##   v1 v2\n## 1  1  b\n## 2  2  a\nsetdiff(y, x)\n##   v1 v2\n## 1  2  b\nintersect(x, y)\n##   v1 v2\n## 1  1  a\n\n\n\n\nimport pandas as pd\n\n# Union\npd.concat([x, y]).drop_duplicates(keep = False)\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n## 1  2.0  b\n\n# Union all\npd.concat([x, y])\n##     v1 v2\n## 0  1.0  a\n## 1  1.0  b\n## 2  2.0  a\n## 0  1.0  a\n## 1  2.0  b\n\n# Intersection\nintersect = x.merge(y, how='inner')\nintersect\n##     v1 v2\n## 0  1.0  a\n\n# Set Difference\nsetdiffxy = x.merge(y, how='outer', indicator=True)\nsetdiffxy = setdiffxy[(setdiffxy._merge=='left_only')].drop('_merge', axis = 1)\nsetdiffxy\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n\nsetdiffyx = x.merge(y, how='outer', indicator=True)\nsetdiffyx = setdiffyx[(setdiffyx._merge=='right_only')].drop('_merge', axis = 1)\nsetdiffyx\n##     v1 v2\n## 3  2.0  b",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#example-nyc-flights",
    "href": "part-wrangling/06-data-join.html#example-nyc-flights",
    "title": "25¬† Joining Data",
    "section": "\n25.4 Example: NYC Flights",
    "text": "25.4 Example: NYC Flights\nWe‚Äôll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you‚Äôll recall, I had to use a small sample the last time we played with this data‚Ä¶). So before we can work with this data, we have to load the tables into Python.\n\n\n\n\n\n\nLoading Data\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"../data/\")\n## &lt;SQLiteConnection&gt;\n##   Path: /home/susan/Projects/Class/stat-computing-r-python/data/nycflights13.sqlite\n##   Extensions: TRUE\n\n\n\n\nimport sqlite3\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\ncur = con.cursor()\n\n\n\n\n\n\nI am not going to cover SQLITE commands here - I‚Äôm just going to use the bare minimum, but you can find a very nice introduction to python and SQLITE at datacarpentry [2], and an introduction to the dbplyr package for a nice R-SQLITE interface.\n\n\n\n\n\n\nTry it out: Understanding Relational Data\n\n\n\n\n\nProblem\nSolution\n\n\n\nSketch a diagram of which fields in each table match fields in other tables. Use the data documentation to help you with your sketch.\n\n\nhere (scroll down a bit).\n\n\n\n\n\n\n\n\n\n\n\nExample: Mutating Joins\n\n\n\nThese functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let‚Äôs determine whether there‚Äôs a relationship between the age of a plane and its delays.\n\n\nR\nPython\n\n\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age &lt;- planes %&gt;%\n  mutate(age = 2013 - year) %&gt;% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane &lt;- flights %&gt;%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres &lt;- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n\n\n\n\n\n\n\n\n\n\nIt doesn‚Äôt look like there‚Äôs much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren‚Äôt enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## &lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "href": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "title": "25¬† Joining Data",
    "section": "\n25.5 Example: Gas Prices Data",
    "text": "25.5 Example: Gas Prices Data\nLet‚Äôs return to the gas price data introduced in Section 24.5. I‚Äôve repeated the setup chunks here for you to read in the data appropriately.\n\n25.5.1 Setup: Gas Price Data Cleaning\nFor the next example, we‚Äôll read the data in from the HTML table online and work to make it something we could e.g.¬†plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting using merge + pivot\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\nYou can start with the gas_prices_raw\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\n\nWe‚Äôll use the same data cleaning function as before:\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n## Error in set_names(., fix_gas_names(names(.))): could not find function \"set_names\"\n\nhead(gas_prices_raw)\n## Error: object 'gas_prices_raw' not found\n\n\ngas_prices_dates &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\n## Error: object 'gas_prices_raw' not found\ngas_prices_values &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n## Error: object 'gas_prices_raw' not found\n\nhead(gas_prices_dates)\n## Error: object 'gas_prices_dates' not found\nhead(gas_prices_values)\n## Error: object 'gas_prices_values' not found\n\n\ngas_prices_dates_long &lt;- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\n## Error: object 'gas_prices_dates' not found\ngas_prices_values_long &lt;- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n## Error: object 'gas_prices_values' not found\n\nhead(gas_prices_dates_long)\n## Error: object 'gas_prices_dates_long' not found\nhead(gas_prices_values_long)\n## Error: object 'gas_prices_values_long' not found\n\n\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean &lt;- gas_prices_dates_long %&gt;%\n  filter(month_day != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %&gt;% \n           ymd())\n## Error: object 'gas_prices_dates_long' not found\n\ngas_prices_values_long_clean &lt;- gas_prices_values_long %&gt;%\n  filter(price_per_gallon != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n## Error: object 'gas_prices_values_long' not found\n\nhead(gas_prices_dates_long_clean)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices_values_long_clean)\n## Error: object 'gas_prices_values_long_clean' not found\n\n\ngas_prices &lt;- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %&gt;%\n  select(Date, price_per_gallon)\n## Error: object 'gas_prices_dates_long_clean' not found\nhead(gas_prices)\n## Error: object 'gas_prices' not found\n\n\n\n\ngas_prices_raw = gas_prices_html.copy()\n## NameError: name 'gas_prices_html' is not defined\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n## NameError: name 'gas_prices_raw' is not defined\n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## NameError: name 'fix_gas_names' is not defined\ncolnames\n## NameError: name 'colnames' is not defined\n\n# Set new column names\ngas_prices_raw.columns = colnames\n## NameError: name 'colnames' is not defined\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_raw.head()\n## NameError: name 'gas_prices_raw' is not defined\n\n\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n## NameError: name 'gas_prices_raw' is not defined\n\ngas_prices_dates.head()\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values.head()\n## NameError: name 'gas_prices_values' is not defined\n\n\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\n## NameError: name 'gas_prices_dates' is not defined\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n## NameError: name 'gas_prices_values' is not defined\n\ngas_prices_dates_long.head()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_values_long.head()\n## NameError: name 'gas_prices_values_long' is not defined\n\n\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna().copy()\n## NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n## NameError: name 'gas_prices_dates_long_clean' is not defined\n\n\ngas_prices_values_long_clean = gas_prices_values_long.dropna().copy()\n## NameError: name 'gas_prices_values_long' is not defined\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## NameError: name 'gas_prices_values_long_clean' is not defined\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n## NameError: name 'gas_prices_values_long_clean' is not defined\n\ngas_prices_dates_long_clean.head()\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_values_long_clean.head()\n## NameError: name 'gas_prices_values_long_clean' is not defined\n\n\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\n## NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices.head()\n## NameError: name 'gas_prices' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/06-data-join.html#sec-data-join-refs",
    "href": "part-wrangling/06-data-join.html#sec-data-join-refs",
    "title": "25¬† Joining Data",
    "section": "\n25.6 References",
    "text": "25.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O‚ÄôReilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nThe Carpentries, ‚ÄúAccessing SQLite Databases Using Python and Pandas,‚Äù Data Analysis and Visualization in Python for Ecologists. 2022 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql.html. [Accessed: Nov. 03, 2024]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html",
    "href": "part-wrangling/07-datetime.html",
    "title": "26¬† Dates and Times",
    "section": "",
    "text": "26.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#objectives",
    "href": "part-wrangling/07-datetime.html#objectives",
    "title": "26¬† Dates and Times",
    "section": "",
    "text": "Understand the complexities of working with datetime data\nCreate datetime formatted data from character and numeric encodings\nFormat/print datetime data in the desired format",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "href": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "title": "26¬† Dates and Times",
    "section": "\n26.2 Why Dates and Times are hard",
    "text": "26.2 Why Dates and Times are hard\nI‚Äôm going to let Tom Scott deliver this portion of the material for me, as his times and timezones video is excellent and entertaining.\n\n\n\n\nThere is also an excellent StackOverflow question [1] and answers [3] demonstrating exactly how times and time zones can get very confusing even in relatively simple circumstances.\nLong story short, we will be using libraries in R and python which handle some of these complexities for us, because dates, times, and timezones are hard and we really don‚Äôt want to know exactly how hard they are. The libraries I‚Äôve chosen for this are datetime in Python (used by Pandas), and lubridate in R.\n\n\n\n\n\n\nTry It Out - Getting Set up\n\n\n\n\n\nR\nPython\n\n\n\n\n## install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# get current date/time\ntoday()\n## [1] \"2024-09-20\"\nnow()\n## [1] \"2024-09-20 13:06:53 CDT\"\n\nLubridate cheat sheet Lubridate documentation\n\n\n\npip install datetime\n\n\nimport datetime\n\ntoday = datetime.date.today()\ntoday\n## datetime.date(2024, 9, 20)\nprint(today)\n## 2024-09-20\n\nnow = datetime.datetime.now()\nnow\n## datetime.datetime(2024, 9, 20, 13, 6, 54, 708686)\nprint(now)\n## 2024-09-20 13:06:54.708686\n\npandas datetime documentation",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#getting-started",
    "href": "part-wrangling/07-datetime.html#getting-started",
    "title": "26¬† Dates and Times",
    "section": "\n26.3 Getting Started",
    "text": "26.3 Getting Started\nLet‚Äôs differentiate between three types of data which refer to a point in time:\n\na date\n\na time within a day\na date-time - a specific time on a specific date\n\nNow, let‚Äôs think about all of the different ways we can specify dates. The table below has examples along with strptime formats that are used in both R and python for telling the computer which date format is used.\n\n\nTable¬†26.1: Different ways to specify dates and times.\n\n\n\n\n\n\n\n\n\n\n\nExample\nType\nNotes\n\nstrptime format\n\n\n\n1\nJanuary 12, 2023\ndate\nCommon in US/N. America\n%B %d, %Y\n\n\n2\n12 January 2023\ndate\nCommon in Europe\n%d %B %Y\n\n\n3\n01/12/2023\ndate\nCommon in US\n%m/%d/%Y\n\n\n4\n1/12/23\ndate\nCommon in US\n%m/%d/%y\n\n\n5\n12/01/2023\ndate\nCommon in Europe/Asia\n%d/%m/%Y\n\n\n6\n2023-01-12\ndate\nISO 8601 standard\n(automatically sorts chronologically)\n%Y-%m-%d\nor %F\n\n\n7\n12 2023\ndate\nday of year + year\n%j %Y\n\n\n8\n9:23 PM\ntime\n12h time\n%I:%M %p\n\n\n9\n21:23\ntime\n24h time (military time)\n%H:%M\nor %R\n\n\n10\n21:23:05\ntime\n24h time (with seconds)\n%H:%M:%S\nor %T\n\n\n11\n2023-01-12T21:23:05\ndatetime\nISO 8601 international standard\n%FT%T\n\n\n\n\n\n\nNote that rows 4 and 5 of Table¬†26.1 are ambiguous if you don‚Äôt know what location your data comes from - the dates could refer to December 1, 2023 or January 12, 2023. This only gets worse if you use 2-digit years.\nThere are three main ways that you might want to create a date/time [4]:\n\nFrom a string\nFrom individual date/time components\nFrom an existing date/time object",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "href": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "title": "26¬† Dates and Times",
    "section": "\n26.4 Creating Dates and Times",
    "text": "26.4 Creating Dates and Times\n\n26.4.1 Creation from Strings\nDates and times are often stored in tabular formats as strings. In some cases, these are read in and automatically formatted as date-times, but in other situations, you have to specify the format yourself.\n\n26.4.1.1 Demo: Datetimes from Strings\nLet‚Äôs use some data from the US Geological Service with records of earthquakes with magnitude greater than 6 on the Richter scale that occurred between January 1, 2000 and January 1, 2023. You can pull this data yourself using https://earthquake.usgs.gov/earthquakes/map/, but you can also access a CSV of the data here.\n\n\nR + lubridate\nBase R\nPandas\n\n\n\n\nlibrary(lubridate)\nquake &lt;- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of T√©cpan de Galeana, Mexico\" ...\n\nBy default, read.csv reads the time information in as a character variable.\n\nlibrary(readr)\nquake2 &lt;- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake2)\n## spc_tbl_ [3,484 √ó 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ #EventID         : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : POSIXct[1:3484], format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n##  $ Latitude         : num [1:3484] -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num [1:3484] 171 -124 179 -101 -173 ...\n##  $ Depth/km         : num [1:3484] 10 17.9 73 18 38 ...\n##  $ Author           : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr [1:3484] \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num [1:3484] 6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr [1:3484] \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of T√©cpan de Galeana, Mexico\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `#EventID` = col_character(),\n##   ..   Time = col_datetime(format = \"\"),\n##   ..   Latitude = col_double(),\n##   ..   Longitude = col_double(),\n##   ..   `Depth/km` = col_double(),\n##   ..   Author = col_character(),\n##   ..   Catalog = col_character(),\n##   ..   Contributor = col_character(),\n##   ..   ContributorID = col_character(),\n##   ..   MagType = col_character(),\n##   ..   Magnitude = col_double(),\n##   ..   MagAuthor = col_character(),\n##   ..   EventLocationName = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\nHowever, if we use readr::read_csv, the data is correctly read in as a POSIXct format, which is how R indicates that something is a datetime object.\nIf we want to directly convert the Time column in quake to a datetime, we can use the lubridate package, which has helper functions ymd_hms, ymd, and more. Our data is formatted in ISO 8601 standard format, which means we can easily read it in with ymd_hms() .\n\nlibrary(lubridate)\nlibrary(dplyr)\nquake &lt;- quake %&gt;% \n  mutate(dateTime = ymd_hms(Time))\nstr(quake)\n## 'data.frame':    3484 obs. of  14 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of T√©cpan de Galeana, Mexico\" ...\n##  $ dateTime         : POSIXct, format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n\nWe can then test whether quake$dateTime is the same as quake2$Time :\n\nall.equal(quake2$Time, quake$dateTime)\n## [1] TRUE\n\nSo in the case that your data is not automatically read in as a date-time, you can use the helper functions from lubridate (ymd_hms, ymd, mdy, ‚Ä¶) to convert strings to date-time data.\n\n\nAs lovely as the lubridate package is, there are some situations where using the tidyverse may not be desirable or even allowed. It is helpful to know how to solve this problem in base R, even if 99% of the time we can use the much easier-to-remember lubridate package.\nIn this case, we would use the as.POSIXct function, and we probably want to have the reference page up (run ?strptime in the R console to pull up the help page).\nWe‚Äôll need to get the codes that tell R what format our datetimes use - you can use Table¬†26.1, if you like, or read the as.POSIXct help page to see all possible format codes.\n\nquake &lt;- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of T√©cpan de Galeana, Mexico\" ...\nquake$dateTime2 &lt;- as.POSIXct(quake$Time, \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\nSo using as.POSIXct we do not get the convenient handling of time zones that we got using ymd_hms, but we can set the time zone explicitly if we want to do so.\n\nquake$dateTime2 &lt;- as.POSIXct(quake$Time, tz = \"UTC\", \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\n\n\nIn pandas, we can use the to_datetime method. If the format is not specified, pandas will try to guess the date-time format; in this case, the guess works, but if not, you can provide a format = ‚Ä¶ argument that works the same way as R.\n\nimport pandas as pd\nquake = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nquake.dtypes\n## #EventID              object\n## Time                  object\n## Latitude             float64\n## Longitude            float64\n## Depth/km             float64\n## Author                object\n## Catalog               object\n## Contributor           object\n## ContributorID         object\n## MagType               object\n## Magnitude            float64\n## MagAuthor             object\n## EventLocationName     object\n## dtype: object\n\nquake.Time[0:10]\n## 0    2022-12-28T16:34:20Z\n## 1    2022-12-20T10:34:24Z\n## 2    2022-12-14T18:40:26Z\n## 3    2022-12-11T14:31:29Z\n## 4    2022-12-04T19:24:15Z\n## 5    2022-11-23T01:08:15Z\n## 6    2022-11-22T16:39:05Z\n## 7    2022-11-22T02:37:57Z\n## 8    2022-11-22T02:03:06Z\n## 9    2022-11-18T13:37:08Z\n## Name: Time, dtype: object\n\n# Convert to datetime\nquake['dateTime'] = pd.to_datetime(quake.Time)\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dtype: object\nquake.dateTime[0:10]\n## 0   2022-12-28 16:34:20+00:00\n## 1   2022-12-20 10:34:24+00:00\n## 2   2022-12-14 18:40:26+00:00\n## 3   2022-12-11 14:31:29+00:00\n## 4   2022-12-04 19:24:15+00:00\n## 5   2022-11-23 01:08:15+00:00\n## 6   2022-11-22 16:39:05+00:00\n## 7   2022-11-22 02:37:57+00:00\n## 8   2022-11-22 02:03:06+00:00\n## 9   2022-11-18 13:37:08+00:00\n## Name: dateTime, dtype: datetime64[ns, UTC]\n\n# Convert to datetime\nquake['dateTime2'] = pd.to_datetime(quake.Time, format = \"%Y-%m-%dT%H:%M:%S\")\n## ValueError: unconverted data remains when parsing with format \"%Y-%m-%dT%H:%M:%S\": \"Z\", at position 0. You might want to try:\n##     - passing `format` if your strings have a consistent format;\n##     - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n##     - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dtype: object\nquake.dateTime2[0:10]\n## AttributeError: 'DataFrame' object has no attribute 'dateTime2'\n\n\n\n\n\n\n\n\n\n\nTry it Out - Datetimes from Strings\n\n\n\nIt‚Äôs usually important for new parents to keep a log of the new baby‚Äôs feeds, to ensure that the baby is getting enough liquids and isn‚Äôt getting dehydrated. I used an app to keep track of my daughter‚Äôs feeds from birth (though here, we‚Äôll only work with the first 3 months of data), and it used a reasonable, if not standard way to store dates and times.\n\n\nProblem\nR solution\nPython solution\n\n\n\nTake a look at the first month of feeds. Note that these data are from August 7, 2021 to November 4, 2021 ‚Äì roughly baby‚Äôs first 90 days.\n\nConvert Start and End to datetime variables\nCan you plot the feeds somehow?\nCan you do arithmetic with datetimes to see if there are any user entry errors?\nThis data was created by a highly unreliable and error prone couple of individuals ‚Äì specifically, sleep-deprived new parents.\n\nTo do this, you may need to figure out how to specify a non-standard date format in R and/or python. The parse_date_time function is useful in R, and pd.to_datetime() takes a format argument in python.\n\n\nFirst, let‚Äôs read the data in and explore a bit.\n\nlibrary(lubridate)\nlibrary(readr)\nfeeds &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nhead(feeds)\n## # A tibble: 6 √ó 6\n##      id Start               End       Type  `Quantity (oz)` `Quantity (ml or g)`\n##   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n## 1  1368 20:03:30 11-04-2021 20:45:21‚Ä¶ Brea‚Ä¶              NA                   NA\n## 2  1366 18:00:29 11-04-2021 18:18:29‚Ä¶ Brea‚Ä¶              NA                   NA\n## 3  1365 16:27:29 11-04-2021 17:03:26‚Ä¶ Brea‚Ä¶              NA                   NA\n## 4  1364 14:30:01 11-04-2021 14:42:05‚Ä¶ Brea‚Ä¶              NA                   NA\n## 5  1367 12:48:29 11-04-2021 13:50:29‚Ä¶ Bott‚Ä¶               3                   88\n## 6  1363 10:59:18 11-04-2021 11:15:18‚Ä¶ Bott‚Ä¶               3                   88\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the parse_date_time function in lubridate\n\nfeeds &lt;- feeds %&gt;%\n  mutate(Start = parse_date_time(Start, orders = c(\"%H:%M:%S %m-%d-%Y\")),\n         End = parse_date_time(End, orders = c(\"%H:%M:%S %m-%d-%Y\")))\n\nLet‚Äôs then explore how we might plot this data:\n\nlibrary(ggplot2)\nggplot(feeds, aes(xmin = Start, xmax = End, fill = Type)) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"bottom\") + \n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nfeeds %&gt;%\n  mutate(day = floor_date(Start, \"day\"),\n         hour_start = Start - day,\n         hour_end = End - day) %&gt;%\n  mutate(across(starts_with(\"hour\"), ~as.numeric(., units = \"hours\"))) %&gt;%\n  mutate(doy = yday(day)) %&gt;%\nggplot(aes(ymin = day, ymax = day+days(1), xmin = hour_start, xmax = hour_end, fill = Type)) + \n  geom_rect() + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  scale_x_continuous(\"Hour of the day\") + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can also calculate the duration of each feed and look at the distributions for each type of feed.\n\nfeeds &lt;- feeds %&gt;%\n  mutate(duration = End - Start)\n\nggplot(feeds, aes(x = duration, fill = Type)) + geom_histogram(color = \"black\") + \n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"none\") + \n  xlab(\"Feed duration, in seconds\") + facet_wrap(~Type)\n\n\n\n\n\n\n\nWe can see a few suspiciously long feeds - 9000 seconds is 2.5 hours, which is not unheard of for a baby to breastfeed, but would be an exceptionally long bottle feed (unless a parent fell asleep before hitting ‚Äústop‚Äù on the feed, which is much more likely).\n\n\nFirst, let‚Äôs read the data in and explore a bit.\n\nimport pandas as pd\n\nfeeds = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nfeeds.head()\n##      id                Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368  20:03:30 11-04-2021  ...           NaN                NaN\n## 1  1366  18:00:29 11-04-2021  ...           NaN                NaN\n## 2  1365  16:27:29 11-04-2021  ...           NaN                NaN\n## 3  1364  14:30:01 11-04-2021  ...           NaN                NaN\n## 4  1367  12:48:29 11-04-2021  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the format argument to pd.to_datetime to specify this:\n\nfeeds[\"Start\"] = pd.to_datetime(feeds.Start, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds[\"End\"] = pd.to_datetime(feeds.End, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds.head()\n##      id               Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368 2021-11-04 20:03:30  ...           NaN                NaN\n## 1  1366 2021-11-04 18:00:29  ...           NaN                NaN\n## 2  1365 2021-11-04 16:27:29  ...           NaN                NaN\n## 3  1364 2021-11-04 14:30:01  ...           NaN                NaN\n## 4  1367 2021-11-04 12:48:29  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\nIn Python, it is helpful to do a bit of transformation first - this is partly because I‚Äôm not as good with Python plotting systems.\n\nimport datetime as dt\nfeeds[\"day\"] = feeds.Start.dt.strftime(\"%Y-%m-%d\")\nfeeds[\"day\"] = pd.to_datetime(feeds.day, format = \"%Y-%m-%d\")\nfeeds[\"day_end\"] = feeds.day + dt.timedelta(days = 1)\n\nfeeds[\"time_start\"] = feeds.Start - feeds.day\nfeeds[\"time_end\"] = feeds.End - feeds.day\nfeeds[\"duration\"] = feeds.time_end - feeds.time_start\n\nNote that as of January 2023, RStudio does not correctly display timedelta data types in python. They show up as NAs in the table, but are printed fine in the console. Don‚Äôt spend hours trying to figure out why it isn‚Äôt working ‚Äì it‚Äôs bad enough that I did.\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"Start\", xmax = \"End\", fill = \"Type\")) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw() + scale_y_continuous(breaks = [])\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"time_start\", xmax = \"time_end\", ymin = \"day\", ymax = \"day_end\", fill = \"Type\")) + \n  geom_rect() + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw()\n)\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n26.4.2 Creation from Components\nSometimes, instead of a single string, you‚Äôll have the individual components of the date-time spread across columns. The nycflights13 data is a good example of this.\n\n26.4.2.1 Demo: Datetimes from Components\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the make_date() and make_datetime() functions can be used to create date-times from component pieces.\n\nlibrary(nycflights13)\n\nflights %&gt;%\n  select(year, month, day, hour, minute) %&gt;% \n  head()\n## # A tibble: 6 √ó 5\n##    year month   day  hour minute\n##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1  2013     1     1     5     15\n## 2  2013     1     1     5     29\n## 3  2013     1     1     5     40\n## 4  2013     1     1     5     45\n## 5  2013     1     1     6      0\n## 6  2013     1     1     5     58\n\nflights &lt;- flights %&gt;%\n  mutate(date = make_date(year, month, day),\n         datetime = make_datetime(year, month, day, hour, minute))\n\nflights %&gt;% select(date, datetime, year, month, day, hour, minute)\n## # A tibble: 336,776 √ó 7\n##    date       datetime             year month   day  hour minute\n##    &lt;date&gt;     &lt;dttm&gt;              &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n##  1 2013-01-01 2013-01-01 05:15:00  2013     1     1     5     15\n##  2 2013-01-01 2013-01-01 05:29:00  2013     1     1     5     29\n##  3 2013-01-01 2013-01-01 05:40:00  2013     1     1     5     40\n##  4 2013-01-01 2013-01-01 05:45:00  2013     1     1     5     45\n##  5 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  6 2013-01-01 2013-01-01 05:58:00  2013     1     1     5     58\n##  7 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  8 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  9 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## 10 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## # ‚Ñπ 336,766 more rows\n\n\n\nIn base R, we can use the ISOdate function to create date times.\n\nflights$datetime_base = with(flights, ISOdatetime(year, month, day, hour, minute, sec= 0, tz=\"UTC\"))\nall.equal(flights$datetime, flights$datetime_base)\n## [1] TRUE\n\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n\nflights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##         year  month  day  hour  minute\n## 0       2013      1    1     5      15\n## 1       2013      1    1     5      29\n## 2       2013      1    1     5      40\n## 3       2013      1    1     5      45\n## 4       2013      1    1     6       0\n## ...      ...    ...  ...   ...     ...\n## 336771  2013      9   30    14      55\n## 336772  2013      9   30    22       0\n## 336773  2013      9   30    12      10\n## 336774  2013      9   30    11      59\n## 336775  2013      9   30     8      40\n## \n## [336776 rows x 5 columns]\n\nflights[\"date\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\"]])\nflights[\"datetime\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]])\n\n\nflights[[\"date\", \"datetime\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##              date            datetime  year  month  day  hour  minute\n## 0      2013-01-01 2013-01-01 05:15:00  2013      1    1     5      15\n## 1      2013-01-01 2013-01-01 05:29:00  2013      1    1     5      29\n## 2      2013-01-01 2013-01-01 05:40:00  2013      1    1     5      40\n## 3      2013-01-01 2013-01-01 05:45:00  2013      1    1     5      45\n## 4      2013-01-01 2013-01-01 06:00:00  2013      1    1     6       0\n## ...           ...                 ...   ...    ...  ...   ...     ...\n## 336771 2013-09-30 2013-09-30 14:55:00  2013      9   30    14      55\n## 336772 2013-09-30 2013-09-30 22:00:00  2013      9   30    22       0\n## 336773 2013-09-30 2013-09-30 12:10:00  2013      9   30    12      10\n## 336774 2013-09-30 2013-09-30 11:59:00  2013      9   30    11      59\n## 336775 2013-09-30 2013-09-30 08:40:00  2013      9   30     8      40\n## \n## [336776 rows x 7 columns]\n\n\n\n\n\n26.4.3 Creation from Other Objects\nSometimes, you may have information in one type of variable (e.g.¬†a datetime) and want to split it into a date and a time, separately.\nSome systems store datetimes as the number of seconds from a specific point (commonly, the Unix Epoch, midnight on 1970-01-01). You may have to convert from seconds since this epoch (or some other epoch [5]) to an actual date-time that is human readable.\nIf you ever have to convert dates and times that were stored in Microsoft Excel, it can be helpful to know that Microsoft stores dates as the number of days since January 1, 1900 [6] (or if the spreadsheet was created on a Mac, January 1, 1904) [7]. Yes, this is as confusing as it sounds. Don‚Äôt use MS Excel for handling dates [8], [9] (or really, at all, now that you know better tools). Geneticists have actually renamed genes because Microsoft won‚Äôt fix Excel to handle dates properly [10].\n\n26.4.3.1 Demo: Creation from Other Objects\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the as_date() and as_datetime() functions can be used to create date-times from other objects.\n\ntmp &lt;- flights %&gt;%\n  mutate(date2 = as_date(datetime))\n\n# Check that date and date2 are the same\nall.equal(flights$date, flights$date2)\n## [1] \"Modes: numeric, NULL\"                                \n## [2] \"Lengths: 336776, 0\"                                  \n## [3] \"Attributes: &lt; Modes: list, NULL &gt;\"                   \n## [4] \"Attributes: &lt; Lengths: 1, 0 &gt;\"                       \n## [5] \"Attributes: &lt; names for target but not for current &gt;\"\n## [6] \"Attributes: &lt; current is not list-like &gt;\"            \n## [7] \"target is Date, current is NULL\"\n\nHere‚Äôs a demonstration of epoch timekeeping.\n\ncurrent_time &lt;- now(tzone = \"UTC\")\n# This converts to the number of seconds since the Unix epoch\nseconds_since_epoch &lt;- current_time %&gt;% seconds()\n# Now let's convert back to a datetime\n(current_time2 &lt;- as_datetime(seconds_since_epoch))\n## [1] \"2024-09-20 18:07:06 UTC\"\n# Check to see that they're equal\nall.equal(current_time, current_time2)\n## [1] TRUE\n\n\n\nIn base R, we can use the as.Date function to create dates from datetimes.\n\nflights$date2 = as.Date(flights$date)\nall.equal(flights$date, flights$date2)\n## [1] TRUE\n\nWe can handle epochs as well:\n\n# Let's see what was 10000 days after the UNIX epoch\nas.Date(1e4, origin = \"1970-01-01\")\n## [1] \"1997-05-19\"\n\n# If we use as.POSIXct, we are counting in seconds from midnight\nas.POSIXct(1e4, origin = as.POSIXct(\"1970-01-01 00:00:00\"))\n## [1] \"1970-01-01 02:46:40 CST\"\n\nBy default, as.POSIXct will use the system‚Äôs time zone, which may not be desirable; you can always set the time zone yourself if you would like to do so.\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n\nflights[\"date2\"] = flights.date.dt.date # Convert datetime to date\n\n# They look the same\nflights[[\"date\", \"date2\"]]\n##              date       date2\n## 0      2013-01-01  2013-01-01\n## 1      2013-01-01  2013-01-01\n## 2      2013-01-01  2013-01-01\n## 3      2013-01-01  2013-01-01\n## 4      2013-01-01  2013-01-01\n## ...           ...         ...\n## 336771 2013-09-30  2013-09-30\n## 336772 2013-09-30  2013-09-30\n## 336773 2013-09-30  2013-09-30\n## 336774 2013-09-30  2013-09-30\n## 336775 2013-09-30  2013-09-30\n## \n## [336776 rows x 2 columns]\n\nflights.dtypes\n## year                       int64\n## month                      int64\n## day                        int64\n## dep_time                 float64\n## sched_dep_time             int64\n## dep_delay                float64\n## arr_time                 float64\n## sched_arr_time             int64\n## arr_delay                float64\n## carrier                   object\n## flight                     int64\n## tailnum                   object\n## origin                    object\n## dest                      object\n## air_time                 float64\n## distance                   int64\n## hour                       int64\n## minute                     int64\n## time_hour                 object\n## date              datetime64[ns]\n## datetime          datetime64[ns]\n## date2                     object\n## dtype: object\n# date2 is an object, date is a datetime64.\n\nWe created flights.date using pd.to_datetime(). Given this comparison, it may be better to use to.datetime() and append .dt.date on the end if you do not want to keep the time information that is provided by default.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "href": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "title": "26¬† Dates and Times",
    "section": "\n26.5 Working with Dates and Times",
    "text": "26.5 Working with Dates and Times\nIn this section, we‚Äôll work with comments by famous Reddit artist Shitty_Watercolour, who responds to people‚Äôs comments with a quickly created watercolor-style painting.\n\n\nHere‚Äôs one of Shitty Watercolour‚Äôs works: \n\n26.5.1 Getting the Data\n\n\nR\nPython\n\n\n\nNote: The textbook caches data, so your results may differ from those shown here because RedditExtractoR only acquires the last ~1000 comments from a user. In addition, in July 2003, reddit removed their API that allowed RedditExtractoR to function, so any future updates are sadly unlikely.\n\n# remotes::install_github(\"ivan-rivera/RedditExtractor\")\nlibrary(RedditExtractoR)\n\ncomment_list &lt;- get_user_content(\"Shitty_Watercolour\")\nwatercolour &lt;- comment_list$Shitty_Watercolour$comments\n\n\n\n\n# Get data from R directly, since redditExtractor package is in R\nwatercolour = r.watercolour\n\n\n\n\n\n26.5.2 Time Zones\nWe often store data in UTC1, but we may want to represent the data in a more familiar time zone for interpretation‚Äôs purposes.\n\nWorking with Time Zones. Here, time is a placeholder for whatever variable is being converted.\n\n\n\n\n\n\nTask\nLanguage\nFunction\n\n\n\nSet the time zone\nR\nas_datetime(time, tz = \"GMT\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_localize(\"GMT\"))\n\n\nDisplay the time in a diff TZ\nR\nwith_tz(time, tz = \"America/Chicago\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_convert(\"America/Chicago\"))\n\n\n\n\n\n\n\n\n\nTry it Out - Formatting Dates\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe watercolour dataset above contains 959 comments from Shitty_Watercolour, with the UTC date and timestamp of the comment.\nFigure out how to format these data into a proper date type and timestamp that is user-readable. Make sure you inform R or python that the timestamp is provided in UTC.\n Compare a couple of your timestamps to the timestamps provided by Reddit when you mouse over a comment.\nCan you get R or Python to output the date in your timezone?\n\n\n\nwatercolour &lt;- watercolour %&gt;%\n  mutate(date = ymd(date_utc, tz = \"UTC\"),\n         time = as_datetime(timestamp),\n         time_cst = with_tz(time, tzone_out = \"CST\"))\n\nwatercolour[,c(\"date\", \"time\", \"time_cst\")]\n## # A tibble: 959 √ó 3\n##    date                time                time_cst           \n##    &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;             \n##  1 2017-12-19 00:00:00 2017-12-19 22:08:19 2017-12-19 16:08:19\n##  2 2017-12-19 00:00:00 2017-12-19 22:07:03 2017-12-19 16:07:03\n##  3 2017-12-19 00:00:00 2017-12-19 20:47:55 2017-12-19 14:47:55\n##  4 2017-12-19 00:00:00 2017-12-19 20:01:19 2017-12-19 14:01:19\n##  5 2017-12-17 00:00:00 2017-12-17 19:58:51 2017-12-17 13:58:51\n##  6 2017-12-15 00:00:00 2017-12-15 02:53:23 2017-12-14 20:53:23\n##  7 2017-12-12 00:00:00 2017-12-12 16:56:23 2017-12-12 10:56:23\n##  8 2017-12-08 00:00:00 2017-12-08 17:48:05 2017-12-08 11:48:05\n##  9 2017-12-08 00:00:00 2017-12-08 03:50:17 2017-12-07 21:50:17\n## 10 2017-12-07 00:00:00 2017-12-07 18:00:58 2017-12-07 12:00:58\n## # ‚Ñπ 949 more rows\n\n\n\n\nfrom datetime import datetime\n\nwatercolour[\"date\"] = pd.to_datetime(watercolour.date_utc).dt.date\nwatercolour[\"time\"] = pd.to_datetime(watercolour.timestamp, unit = 's')\n\n# Tell Python the time is in UTC \nwatercolour[\"time\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_localize(\"UTC\"))\n\nwatercolour[\"time_cst\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_convert(\"US/Central\"))\n\nwatercolour[[\"date\", \"time\", \"time_cst\"]]\n##            date                      time                  time_cst\n## 0    2017-12-19 2017-12-19 22:08:19+00:00 2017-12-19 16:08:19-06:00\n## 1    2017-12-19 2017-12-19 22:07:03+00:00 2017-12-19 16:07:03-06:00\n## 2    2017-12-19 2017-12-19 20:47:55+00:00 2017-12-19 14:47:55-06:00\n## 3    2017-12-19 2017-12-19 20:01:19+00:00 2017-12-19 14:01:19-06:00\n## 4    2017-12-17 2017-12-17 19:58:51+00:00 2017-12-17 13:58:51-06:00\n## ..          ...                       ...                       ...\n## 954  2023-02-21 2023-02-21 16:55:03+00:00 2023-02-21 10:55:03-06:00\n## 955  2023-02-21 2023-02-21 15:59:51+00:00 2023-02-21 09:59:51-06:00\n## 956  2023-02-21 2023-02-21 15:09:19+00:00 2023-02-21 09:09:19-06:00\n## 957  2023-02-21 2023-02-21 14:45:10+00:00 2023-02-21 08:45:10-06:00\n## 958  2023-02-21 2023-02-21 11:27:52+00:00 2023-02-21 05:27:52-06:00\n## \n## [959 rows x 3 columns]\n\n\n\n\n\n\n\n26.5.3 Time Spans\nDates and times can be added and subtracted - after all, underneath, they‚Äôre usually implemented as a number of XXX from the reference time point, where XXX is usually seconds for datetimes and days for dates.\nIn R, the difference between two timestamps is called a duration and is implemented in the duration class See [11, Ch. 16.4.1] for more info. In Python, a similar class exists and is called a timedelta [12].\n\n\n\n\n\n\nTry it Out - Datetime Math\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nUse the watercolour data and plot the interval between successive Shitty_Watercolour posts in minutes. What can you conclude?\n\n\n\nwatercolour &lt;- watercolour %&gt;%\n  arrange(time) %&gt;%\n  mutate(diff = as.duration(time - lag(time, 1)),\n         diffmin = as.numeric(diff, \"minutes\"))\n\nlibrary(ggplot2)\nggplot(watercolour, aes(x = diffmin)) + \n  geom_histogram() + \n  xlab(\"Time between posts (minutes)\") + \n  ylab(\"# Posts\") + \n  scale_x_log10(breaks = c(1, 15, 30, 60, 1440, 10080))\n\n\n\n\n\n\n\nMost of the time, Shitty_Watercolour takes at least 15 minutes to generate a new comment. There is also a noticable peak just before 1440 minutes, indicating that as with most users, Shitty_Watercolour is active at approximately the same time each day for a few hours. The final break shown, 10080, is the number of minutes in a week, indicating that occasionally, Shitty_Watercolour goes more than a week between posts.\n\n\n\nfrom datetime import datetime\n\nwatercolour = watercolour.sort_values(by = 'time')\nwatercolour[\"diff\"] = watercolour.time.diff().astype('timedelta64')\n## ValueError: Cannot convert from timedelta64[ns] to timedelta64. Supported resolutions are 's', 'ms', 'us', 'ns'\n# This formats in minutes\nwatercolour[\"diffmin\"] = watercolour.time.diff().astype('timedelta64[m]')\n## ValueError: Cannot convert from timedelta64[ns] to timedelta64[m]. Supported resolutions are 's', 'ms', 'us', 'ns'\n# Remove negative minutes - something funky there?\nwatercolour = watercolour.query(\"diffmin &gt; 0\")\n## pandas.errors.UndefinedVariableError: name 'diffmin' is not defined\n\nimport seaborn.objects as so\np = (\n  so.\n  Plot(watercolour, watercolour[\"diffmin\"]).\n  add(so.Bars(width=.95), so.Hist(bins = 30)).\n  scale(x = so.Continuous(trans = \"log\").\n    tick(at = [1, 15, 30, 60, 1440, 10080]).\n    label(like=\"{x:d}\")).\n  label(x = \"Time between posts (minutes)\", y = \"# Posts\")\n)\n## KeyError: 'diffmin'\np.show()\n## NameError: name 'p' is not defined",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#references",
    "href": "part-wrangling/07-datetime.html#references",
    "title": "26¬† Dates and Times",
    "section": "\n26.6 References",
    "text": "26.6 References\n\n\n\n\n[1] \nFreewind, ‚ÄúWhy is subtracting these two times (in 1927) giving a strange result? Stack overflow,‚Äù May 22, 2021. [Online]. Available: https://stackoverflow.com/q/6841333/2859168. [Accessed: Jan. 21, 2023]\n\n\n[2] \nJ. Skeet, ‚ÄúAnswer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,‚Äù Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841479/2859168. [Accessed: Jan. 21, 2023]\n\n\n[3] \nM. Borgwardt, ‚ÄúAnswer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,‚Äù Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841572/2859168. [Accessed: Jan. 21, 2023]\n\n\n[4] \nH. W. {and}. G. Grolemund, ‚ÄúDates and times,‚Äù in R for data science, 1st ed., O‚ÄôReilly Media, p. 518 [Online]. Available: https://r4ds.had.co.nz/dates-and-times.html. [Accessed: Jan. 23, 2023]\n\n\n[5] \nWikipedia contributors, ‚ÄúEpoch (computing),‚Äù Wikipedia. Mar. 23, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Epoch_(computing)&oldid=1146251518. [Accessed: Apr. 04, 2023]\n\n\n[6] \nMicrosoft Support, ‚ÄúDATEVALUE function - Microsoft Support,‚Äù 2023. [Online]. Available: https://support.microsoft.com/en-us/office/datevalue-function-df8b07d4-7761-4a93-bc33-b7471bbff252. [Accessed: Apr. 04, 2023]\n\n\n[7] \nElizabeth Mott, ‚ÄúWhy Do Dates Come in Different in Excel From a Mac to a PC?‚Äù Jun. 12, 2013. [Online]. Available: https://smallbusiness.chron.com/dates-come-different-excel-mac-pc-68917.html. [Accessed: Apr. 04, 2023]\n\n\n[8] \nH. Caudill, ‚ÄúExcel Hell: A cautionary tale,‚Äù Dec. 19, 2018. [Online]. Available: https://medium.com/all-the-things/a-single-infinitely-customizable-app-for-everything-else-9abed7c5b5e7. [Accessed: Apr. 04, 2023]\n\n\n[9] \nChris88888888, ‚ÄúExcel Still Sucks at Recognizing Dates.‚Äù Apr. 15, 2020. [Online]. Available: https://answers.microsoft.com/en-us/msoffice/forum/all/excel-still-sucks-at-recognizing-dates/5305f6db-8211-49d5-932d-c4871df27fc7. [Accessed: Apr. 04, 2023]\n\n\n[10] \nJ. Vincent, ‚ÄúScientists rename human genes to stop Microsoft Excel from misreading them as dates,‚Äù Aug. 06, 2020. [Online]. Available: https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates. [Accessed: Apr. 04, 2023]\n\n\n[11] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O‚ÄôReilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[12] \nPython Foundation, ‚ÄúDatetime Basic date and time types,‚Äù Apr. 05, 2023. [Online]. Available: https://docs.python.org/3/library/datetime.html. [Accessed: Apr. 05, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/07-datetime.html#footnotes",
    "href": "part-wrangling/07-datetime.html#footnotes",
    "title": "26¬† Dates and Times",
    "section": "",
    "text": "Coordinated Universal Time, but the acronym is in a different language and the words are thus in a different order.‚Ü©Ô∏é",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html",
    "href": "part-wrangling/08-functional-prog.html",
    "title": "27¬† Functional Programming",
    "section": "",
    "text": "27.1  Objectives",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#objectives",
    "href": "part-wrangling/08-functional-prog.html#objectives",
    "title": "27¬† Functional Programming",
    "section": "",
    "text": "Use functional programming to replace for loops\nArticulate why functional programming can be preferable to using for loops\nUse functional programming to clean data, model data subsets, and assemble hierarchical data.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "href": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "title": "27¬† Functional Programming",
    "section": "\n27.2 Programming Philosophies",
    "text": "27.2 Programming Philosophies\n\nThis section is intended for everyone, but I do not expect that people who are just learning to program for the first time will fully absorb everything in this section. Get what you can out of this, use it to improve how you write code, and come back to it later if it‚Äôs too confusing.\n\nJust as spoken languages fall into families, like Indo-European or Sino-Tibetan, programming languages also have broad classifications. Here are a few ‚Äúfamilies‚Äù or classifications of programming languages [1]:\n\nMany languages are procedural: a program provides a list of instructions that tell the computer what to do with provided input. C, Pascal, Fortran, and UNIX shells are naturally procedural. JavaScript is also a fairly natural procedural language. Many R analysis scripts are also naturally written in a procedural style; SAS code is almost always procedural.\n\nDeclarative languages use code to describe the problem that needs to be solved, and the language figures out how to solve it. SQL is the most common declarative language you‚Äôll encounter for data-related tasks.\n\nObject oriented languages (sometimes abbreviated OOP, for object-oriented programming) manipulate collections of objects or classes. Data is stored in classes that have associated functions, which are often called methods. Java is explicitly object-oriented; C++ and Python support object-oriented programming but don‚Äôt force you to use those features.\n\nFunctional programming languages describe a problem using a set of functions, which only take inputs and produce outputs. Functions don‚Äôt have any internal tracking of state - purely functional languages move from input to output without storing variables or even printing output to the command line, but it is common to adopt a functional approach to programming without requiring strict adherence to all principles of a fully functional approach. Haskell and Rust are fairly standard functional programming languages.\n\n\n\nHadley‚Äôs talk on The Joy of Functional Programming for Data Science\n\n\nFunctional programming languages have a goal of writing pure functions - functions that do not change the global state (stuff stored in objects, memory, parameters, or files) of the program and thus have no side effects. The return value of a pure function is based solely on the inputs to the function. Not all functions can be pure functions - for instance, there‚Äôs no pure way to do file IO operations. But it is a nice goal to be able to move parameters into functions and have the correct object returned from that function, so that you can pipe multiple operations together into a pipeline.\nMost general-purpose languages like C++ and Python and even some domain languages like R support multiple different programming paradigms. While preparing to write this chapter, I saw functional programming books with examples in Java [2], JavaScript [3], and C# [4] - all languages that I would associate with OOP or procedural styles. I also found books teaching object oriented programming using Fortran 90-95 [5], which is something I wouldn‚Äôt have considered possible.\nAll of this is to say that while certain languages are built around principles like OOP or functional programming, almost every language has users who rely more heavily on one approach than the other. There are very few ‚Äúpure‚Äù programming languages, which reminds me of one of my favorite quotes about English:\n\n‚ÄúThe problem with defending the purity of the English language is that English is about as pure as a cribhouse whore. We don‚Äôt just borrow words; on occasion, English has pursued other languages down alleyways to beat them unconscious and rifle their pockets for new vocabulary.‚Äù ‚Äï James D. Nicoll\n\n\n27.2.1 Object Oriented Philosophy in R and Python\n\n\nPython\nR\n\n\n\nWhen you call df.size() in Python, you are calling the size method that is part of the df object, which is a DataFrame. This suggests that Pandas, at least, is programmed using an object-oriented paradigm.\n\n\n\n\n\nAn easy example of R‚Äôs object oriented nature is that when you fit different models or perform different tests, the default output is different.\n\ndata(mtcars)\n\nr1 &lt;- t.test(mtcars$mpg~mtcars$vs)\nprint(r1)\n## \n##  Welch Two Sample t-test\n## \n## data:  mtcars$mpg by mtcars$vs\n## t = -4.6671, df = 22.716, p-value = 0.0001098\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -11.462508  -4.418445\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        16.61667        24.55714\n\nr2 &lt;- lm(mtcars$mpg ~ mtcars$vs)\nprint(r2)\n## \n## Call:\n## lm(formula = mtcars$mpg ~ mtcars$vs)\n## \n## Coefficients:\n## (Intercept)    mtcars$vs  \n##       16.62         7.94\n\nThe output is different because each test/regression model object has a different print method, which allows R to create different output for each type of object.\n\n\n\nFunctional programming allows us to write programs that are more modular, and thus, are easier to test and debug. In addition, functional programming encourages you to think about data pipelines: a sequence of steps that are reproducible and reusable for different data sets and analyses. Functional programming is convenient for another (more esoteric, but important) reason - it allows you to prove that a function or series of functions is actually correct (rather than just testing input/output combinations).\n\n\nA pipeline of functional data analysis. Each function (station) modifies the data in some way and the returned result is passed into the next function (station) as input. This allows a sequence of functions to format data, visualize it, model it, and then package the results. While this paradigm doesn‚Äôt require a functional approach, functional programming does make it simpler. Modified from Allison Horst‚Äôs work\n\nIf you have been using the R pipe (|&gt; or %&gt;%), you didn‚Äôt realize it, but you were already using functional programming. Piping results from one function to another in a chain is a prime example of the ‚Äúpure function‚Äù idea - it allows us to chain each step of a sequence together to create a sequence that is modular and testable.\n\n27.2.2 A simple Functional Example\nA functional is a function that takes another function as input and returns a vector as output.\nOne simple example of a functional that is found in both R and Python is the apply function (or variants in R like lapply, sapply, tapply). In Python, .apply is a method in Pandas, but we can find an even more low-level equivalent in the ideas of list comprehensions and map functions.\nOne additional concept that is helpful before we start is the idea of a lambda function - a small anonymous function (that is, a function that is not named or stored in a variable). Lambda functions are great for filling in default arguments, but they have many other uses as well.\nCan you identify the lambda functions in each of the following examples?\n\n\nR\nPython\n\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nlapply(1:5, function(x) rnorm(5, mean = x, sd = 1))\n## [[1]]\n## [1] -2.16762210  1.99905314 -0.08759488  2.22009213  1.64652776\n## \n## [[2]]\n## [1] 2.475177 3.264478 1.280447 1.504710 1.371149\n## \n## [[3]]\n## [1] 2.476403 1.873314 3.501130 3.464979 3.618766\n## \n## [[4]]\n## [1] 4.135294 4.547488 5.259023 4.651852 4.428149\n## \n## [[5]]\n## [1] 4.201161 4.514928 3.938922 5.402455 5.304307\n\nOr, if you have R 4.1.0 or above, you can use a shorthand version:\n\nlapply(1:5, \\(x) rnorm(5, mean = x))\n## [[1]]\n## [1]  1.3395250 -0.1268683  0.4233265  1.3424008  1.8946875\n## \n## [[2]]\n## [1] 1.366347 1.849088 2.024996 2.130203 2.991602\n## \n## [[3]]\n## [1] 2.036583 3.385580 3.616681 3.802413 2.755437\n## \n## [[4]]\n## [1] 5.155240 4.172880 2.434981 4.995162 3.531419\n## \n## [[5]]\n## [1] 4.375204 5.086924 5.111008 4.633185 6.211562\n\nThe \\(x) is shorthand for function(x) and allows you to quickly and easily define anonymous functions in R.\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nimport numpy as np\n\n# List comprehension approach\nr1 = [np.random.normal(i, size = 5) for i in range(1, 6)]\nprint(r1) \n## [array([ 1.24857545,  1.49689476, -0.44984772,  0.20420764,  3.06522857]), array([0.71643789, 2.6619888 , 3.1023202 , 2.8393277 , 1.20607516]), array([1.93763234, 3.2012511 , 2.66445568, 3.26577021, 3.96375901]), array([4.57377777, 4.32844887, 3.99245661, 3.87456004, 2.45354337]), array([5.40400778, 4.79334176, 5.72749392, 4.78910161, 4.19462971])]\n\n# Functional approach\n# Defining a lambda function allows us to fill in non-default options\nr2 = map(lambda a: np.random.normal(a, size = 5), range(1, 6))\n\n# This is what map spits out by default\nprint(r2)\n## &lt;map object at 0x7f1610e8dff0&gt;\n# get your results back out with list()\nr2b = list(r2) \nprint(r2b)\n## [array([ 0.55513415,  1.51047086, -0.0692891 ,  0.52177732,  1.16883951]), array([2.26809068, 0.9552631 , 1.95039367, 2.21439357, 1.13564023]), array([2.5497408 , 3.82577465, 4.11895343, 2.16746233, 2.94641842]), array([3.76699069, 5.27928174, 5.9725059 , 3.82374019, 5.55132694]), array([5.70828473, 4.13119899, 4.49605868, 4.57137864, 3.21304835])]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "href": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "title": "27¬† Functional Programming",
    "section": "\n27.3 Replacing Loops with Functional Programming",
    "text": "27.3 Replacing Loops with Functional Programming\nOne really convenient application of functional programming is to replace loops. As Hadley Wickham says in [6],\n\nthe real downside of for loops is that they‚Äôre very flexible: a loop conveys that you‚Äôre iterating, but not what should be done with the results\n\nThat is, in many cases when programming with data, what we want is to iterate over a vector and return a vector of results. This is a perfect use case for functional programming, since we‚Äôre specifying both that we‚Äôre iterating AND more explicitly collecting the results into a form that makes sense.\nIf we work with this definition of functional programming, then python list comprehensions are also a functional approach: they specify how the results are collected (usually by putting [] around the statement) and how the iteration will occur [7].\n\n\nThere is an excellent vignette comparing Base R functional programming approaches to the purrr package that is worth a look if you‚Äôve used one and want to try the other [8].\nLet‚Äôs look at a few examples.\n\nSuppose we want to look at the Lego data and create a decade variable that describes the decade a set was first released.\n\n\nbase R\nR: purrr\nPython\n\n\n\n\nlego &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego$decade &lt;- sapply(lego$year, \\(x) floor(x/10)*10)\nhead(lego[,c(\"set_num\", \"name\", \"year\", \"decade\")])\n##   set_num                       name year decade\n## 1   001-1                      Gears 1965   1960\n## 2  0011-2          Town Mini-Figures 1979   1970\n## 3  0011-3 Castle 2 for 1 Bonus Offer 1987   1980\n## 4  0012-1         Space Mini-Figures 1979   1970\n## 5  0013-1         Space Mini-Figures 1979   1970\n## 6  0014-1         Space Mini-Figures 1979   1970\n\nStrictly speaking, this use of sapply isn‚Äôt necessary - because R is vectorized by default, we could also have used lego$decade &lt;- floor(lego$year/10)*10. However, there are functions in R that are not fully vectorized, and it is useful to know this approach for those use-cases as well, and it‚Äôs easier to demonstrate this approach with a relatively simple use case.\n\n\nIn purrr, you can create anonymous functions using ~ with . as a placeholder. If you need more parameters, you can use .x, .y and map2 (for now) or .1, .2, .3, ... with pmap.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\n\nlego &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego &lt;- lego |&gt; # Either pipe will work here\n  mutate(decade = purrr::map_int(year, ~floor(./10)*10))\n\nlego |&gt; \n  select(set_num, name, year, decade) |&gt; \n  head()\n## # A tibble: 6 √ó 4\n##   set_num name                        year decade\n##   &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt;  &lt;int&gt;\n## 1 001-1   Gears                       1965   1960\n## 2 0011-2  Town Mini-Figures           1979   1970\n## 3 0011-3  Castle 2 for 1 Bonus Offer  1987   1980\n## 4 0012-1  Space Mini-Figures          1979   1970\n## 5 0013-1  Space Mini-Figures          1979   1970\n## 6 0014-1  Space Mini-Figures          1979   1970\n\n\n\n\nimport pandas as pd\nimport math\n\nlego = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\nlego['decade'] = [math.floor(i/10)*10 for i in lego.year]\nlego[['set_num', 'name', 'year', 'decade']].head()\n##   set_num                        name  year  decade\n## 0   001-1                       Gears  1965    1960\n## 1  0011-2           Town Mini-Figures  1979    1970\n## 2  0011-3  Castle 2 for 1 Bonus Offer  1987    1980\n## 3  0012-1          Space Mini-Figures  1979    1970\n## 4  0013-1          Space Mini-Figures  1979    1970\n\n\n\n\n\nFor a more interesting example, though, let‚Äôs consider fitting a different linear regression for each generation of Pokemon, describing the relationship between HP (hit points) and CP (combat power, aka total in this dataset).\n\n\nI am sure that the python code I‚Äôve written here is a bit kludgy, so if you are more fluent in python/pandas than I am, please feel free to submit a pull request if you know a better or more ‚Äúpretty‚Äù way to do this.\n\n\n\n\n\n\nExample: Pokemon modeling\n\n\n\n\n\nbase R\nTidy R\nPython\n\n\n\n\npoke &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n# Get rid of mega pokemon - they're a different thing\npoke &lt;- subset(poke, !grepl(\"Mega\", poke$variant)) # step 1\n\n# Split into a list of data frames from each gen\npoke_gens &lt;- split(poke, poke$gen) # step 2\n\n# Fit linear regressions for each generation of pokemon\nmodels &lt;- lapply(poke_gens, \\(df) lm(total ~ hp, data = df)) # step 3\n\n# Pull out coefficients and r-squared values\nresults &lt;- lapply(models, \\(res) data.frame(coef1 = coef(res)[1], coef2 = coef(res)[2], rsq = summary(res)$r.squared))  # step 4\n\n# Join the results back into a data.frame\nresults &lt;- do.call(\"rbind\", results) # step 5\n\nresults\n##      coef1    coef2       rsq\n## 1 262.4730 2.394032 0.3895255\n## 2 258.5815 2.133868 0.3165292\n## 3 245.3375 2.753912 0.2686807\n## 4 268.5792 2.792350 0.3500187\n## 5 189.1339 3.506458 0.5578802\n## 6 252.7434 2.798942 0.5278159\n## 7 234.9329 3.293730 0.4382885\n## 8 205.2726 3.500811 0.6042219\n## 9 236.8656 2.757408 0.4757028\n\n\nData in data frame\nData split into a list of data frames\nModels in a list corresponding to data in step 2\nResults in a list of data frames corresponding to models in step 3\nBind results in step 4 back into a data frame\n\nIn each step, we specify not only what the iterative action should be, but also what form the results will take.\n\n\nIn the tidyverse, we use tidyr::nest() to accomplish a similar thing to split in base R.\nThis approach is designed to work entirely within a single data frame, which keeps the environment relatively clean and ensures that each step‚Äôs results are stored in a convenient, easy-to-find place.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\n\nres &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\") %&gt;%\n  # str_detect doesn't play nice with NAs, so replace NA with \"\"\n  mutate(variant = replace_na(variant, \"\")) %&gt;%\n  # Remove mega pokemon\n  filter(str_detect(variant, \"Mega\", negate = T)) %&gt;% # step 1\n  # Sub-data-frames\n  nest(.by = gen) %&gt;% # step 2\n  # Fit model\n  mutate(model = map(data, ~lm(total ~ hp, data = .))) %&gt;% # step 3\n  # Extract coefficients\n  mutate(res = map(model, ~data.frame(coef1 = coef(.)[1], \n                                      coef2 = coef(.)[2], \n                                      rsq = summary(.)$r.squared))) %&gt;% # step 4\n  # Bind together\n  unnest(c(res)) # step 5\nres\n## # A tibble: 9 √ó 6\n##     gen data                model  coef1 coef2   rsq\n##   &lt;dbl&gt; &lt;list&gt;              &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 &lt;tibble [269 √ó 15]&gt; &lt;lm&gt;    262.  2.39 0.390\n## 2     2 &lt;tibble [118 √ó 15]&gt; &lt;lm&gt;    259.  2.13 0.317\n## 3     3 &lt;tibble [173 √ó 15]&gt; &lt;lm&gt;    245.  2.75 0.269\n## 4     4 &lt;tibble [173 √ó 15]&gt; &lt;lm&gt;    269.  2.79 0.350\n## 5     5 &lt;tibble [236 √ó 15]&gt; &lt;lm&gt;    189.  3.51 0.558\n## 6     6 &lt;tibble [118 √ó 15]&gt; &lt;lm&gt;    253.  2.80 0.528\n## 7     7 &lt;tibble [133 √ó 15]&gt; &lt;lm&gt;    235.  3.29 0.438\n## 8     8 &lt;tibble [134 √ó 15]&gt; &lt;lm&gt;    205.  3.50 0.604\n## 9     9 &lt;tibble [123 √ó 15]&gt; &lt;lm&gt;    237.  2.76 0.476\n\nOur data takes the form:\n\nAn ungrouped data frame\nA data frame with 9 rows, one for each generation, with a list-column data that contains the full data for each generation\nWe fit our model and store the model results into another list-column named model that contains the fitted model object\nWe define some summary information and store it into a list-column containing each 1-row data frame\nWe ‚Äúunnest‚Äù the summary information, which is equivalent to bringing the columns we defined up to the primary level and binding the rows together.\n\nAt each step, we‚Äôre specifying the form of the results along with the contents.\n\n\nThis construct of storing everything inside a single data frame isn‚Äôt as common in Python, but we can make it work with only a little extra effort.\nYou will need to pip install statsmodels to get the statsmodels [9] package that implements many basic statistical models. The scikit-learn package [10] is another commonly used package [11], but it does not have the easy accessor functions to pull out e.g.¬†coefficients and r-squared values, so we‚Äôll use statsmodels here.\n\nimport pandas as pd\nfrom statsmodels.formula.api import ols\n\n# Create a function to fit a linear regression\n# There is probably a better way to do this flexibly,\n# but this approach is simple and useful for illustrative purposes\ndef pokereg(data):\n  x = data[[\"hp\"]].values\n  y = data[[\"total\"]].values\n  model = ols('total ~ hp', data)\n  results = model.fit()\n  return results\n\nres = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n\n# Replace NAs with \"\"\nres[\"variant\"] = [\"\" if pd.isna(i) else i for i in res.variant]\n# Remove mega pokemon\nres = res.query('~(variant.str.contains(\"(?:^[mM]ega)\"))')\n\n# Group data frames and apply regression function to each group\nres_reg = (\n  res # step 1\n    .groupby(\"gen\") # step 2\n    .apply(pokereg) # step 3\n)\n\n# Make results into a dataframe and rename the column as 'results'\nres_reg = pd.DataFrame(res_reg).rename(columns = {0:'results'}) # step 4\n\n# Get values of interest and store in new columns # step 5\nres_reg = res_reg.reset_index() # store gen in its own column\nres_reg['coef1'] = res_reg.results.map(lambda x: x.params[0])\nres_reg['coef2'] = res_reg.results.map(lambda x: x.params[1])\nres_reg['rsq'] = res_reg.results.map(lambda x: x.rsquared)\n\nres_reg[['gen', 'coef1', 'coef2', 'rsq']]\n##    gen       coef1     coef2       rsq\n## 0    1  262.472956  2.394032  0.389526\n## 1    2  258.581538  2.133868  0.316529\n## 2    3  245.337496  2.753912  0.268681\n## 3    4  268.579237  2.792350  0.350019\n## 4    5  189.133898  3.506458  0.557880\n## 5    6  252.743437  2.798942  0.527816\n## 6    7  234.932868  3.293730  0.438289\n## 7    8  205.272637  3.500811  0.604222\n## 8    9  236.865627  2.757408  0.475703\n\nWhile this doesn‚Äôt store our data in the same DataFrame as the model results, we do have a key that links the two: the gen variable is present in both res and res_reg and can be used to join the data to the regression results, if necessary.\n\nData in an ungrouped data frame\nWe group by gen (generation)\nWe apply the function pokereg to fit a linear regression, and the results are stored in an indexed Series where the index corresponds to gen.\nWe make the results into a DataFrame so that we can add extra columns, and rename the automatically created Series to results to be more descriptive\nWe create summary information and store the summaries in columns in the data frame.\n\nWhile the grouping and binding operations are in a different order in Python than in R, the basic specification of the structure of the output each time we iterate is similar.\n\n\n\n\n\n\n\nHere‚Äôs another demonstration of the use of the tidymodels package and purrr to fit multiple regression models to data subsets.",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "href": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "title": "27¬† Functional Programming",
    "section": "\n27.4 Complex Data Structures",
    "text": "27.4 Complex Data Structures\nNot all datasets are strictly tabular. One of the most common situations where we get data that can‚Äôt be made into a completely tabular structure is when we‚Äôre dealing with hierarchical data: tree structures, (network) graph structures, and even most webpages contain data that isn‚Äôt strictly tabular in nature. Sometimes, we can get that data into a tabular structure, but it generally depends on the data itself.\nOne of the most common structures for storing data on the web is JSON: JavaScript Object Notation[12].\n\n\n(JSON is pronounced ‚ÄúJason‚Äù, like the person‚Äôs name).\n In this section we‚Äôll work with some data gathered from TMDB (the movie database). I submitted a query for all movies that Patrick Stewart was involved with, and you can find the resulting JSON file here.\n\n27.4.1 JSON File Parsing\n\n\nR\nPython\n\n\n\nWe‚Äôll use the jsonlite package to read the data in, but invariably this package still requires us to do some post-processing ourselves.\n\nlibrary(jsonlite)\ndata_url &lt;- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\nps_json &lt;- fromJSON(data_url)\n\n\nExploring the output structure\n\n# head(ps_json) # This output is too long\nmap(ps_json, head) # show the first 6 rows of each element in the list\n## $cast\n##   adult                    backdrop_path       genre_ids    id\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg 878, 28, 12, 53   193\n## 2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg 878, 28, 12, 53   199\n## 3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg 878, 28, 12, 53   200\n## 4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg 878, 28, 12, 53   201\n## 5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg              99 10946\n## 6 FALSE                             &lt;NA&gt;              99 21746\n##   original_language              original_title\n## 1                en      Star Trek: Generations\n## 2                en    Star Trek: First Contact\n## 3                en     Star Trek: Insurrection\n## 4                en          Star Trek: Nemesis\n## 5                en                       Earth\n## 6                en The Secret of Life on Earth\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                overview\n## 1                                                                                                                                                                                                                                                                                                              Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2                                                                                                                                                                                                                                                    The Borg, a relentless race of cyborgs, are on a direct course for Earth. Violating orders to stay away from the battle, Captain Picard and the crew of the newly-commissioned USS Enterprise E pursue the Borg back in time to prevent the invaders from changing Federation history and assimilating the galaxy.\n## 3                                                                                                                                                                                                                                                                                 When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 4 En route to the honeymoon of William Riker to Deanna Troi on her home planet of Betazed, Captain Jean-Luc Picard and the crew of the U.S.S. Enterprise receives word from Starfleet that a coup has resulted in the installation of a new Romulan political leader, Shinzon, who claims to seek peace with the human-backed United Federation of Planets. Once in enemy territory, the captain and his crew make a startling discovery: Shinzon is human, a slave from the Romulan sister planet of Remus, and has a secret, shocking relationship to Picard himself.\n## 5                                                                                                                                                                                                                                                                                                 From the acclaimed team that brought you BBC's visual feast \"Planet Earth,\" this feature length film incorporates some of the same footage from the series with all new scenes following three remarkable, yet sadly endangered, families of animal across the globe.\n## 6                                                                                                                                                                                                                                                                          A breathtaking adventure across five continents and through time to reveal nature's most vital secret. Watch a flying fox gorge itself on a midnight snack of figs. Climb into the prickly jaws of insect-eating plants. Witness a mantis disguised as a flower petal lure its prey to doom.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     29.750 /vrC1lkTktFQ4AqBfqf4PXoDDLcw.jpg   1996-11-22\n## 3     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 4     33.614 /cldAwhvBmOv9jrd3bXWuqRHoXyq.jpg   2002-12-13\n## 5      9.188 /xybnXW6E28W9agiwUeGLbTYS454.jpg   2007-04-22\n## 6      1.831 /baa6T6noxiFUZcb6Jz8TjjlOoCH.jpg   1993-10-14\n##                         title video vote_average vote_count\n## 1      Star Trek: Generations FALSE        6.526       1126\n## 2    Star Trek: First Contact FALSE        7.305       1519\n## 3     Star Trek: Insurrection FALSE        6.425       1025\n## 4          Star Trek: Nemesis FALSE        6.293       1218\n## 5                       Earth FALSE        7.600        311\n## 6 The Secret of Life on Earth FALSE        6.000          1\n##                 character                credit_id order\n## 1 Captain Jean-Luc Picard 52fe4225c3a36847f80076d9     0\n## 2 Captain Jean-Luc Picard 52fe4226c3a36847f8007ba3     0\n## 3 Captain Jean-Luc Picard 52fe4226c3a36847f8007c27     0\n## 4 Captain Jean-Luc Picard 52fe4226c3a36847f8007cf1     0\n## 5                Narrator 52fe43d79251416c75020267     0\n## 6        Narrator (voice) 52fe4425c3a368484e01220f     0\n## \n## $crew\n##   adult                    backdrop_path            genre_ids      id\n## 1 FALSE                             &lt;NA&gt;               99, 35 1093380\n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg      878, 28, 12, 53     200\n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg 10770, 14, 18, 10751   48358\n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg                   37   47913\n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg 18, 36, 10770, 10749   37945\n## 6 FALSE /g5CMQPz5cqUHro9pNLBRW7cT8cY.jpg               18, 14   16716\n##   original_language          original_title\n## 1                en           Red Dwarf A-Z\n## 2                en Star Trek: Insurrection\n## 3                en   The Canterville Ghost\n## 4                en           King of Texas\n## 5                en      The Lion in Winter\n## 6                en       A Christmas Carol\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 overview\n## 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                       When a teenaged girl moves to England, with her brothers and parents into the ancient Canterville Hall, she's not at all happy. Especially as there's a ghost and a mysterious re-appearing bloodstain on the hearth. She campaigns to go back home, and her dad, believing the ghost's pranks are Ginny's, is ready to send her back. But then Ginny actually meets the elusive 17th-century Sir Simon de Canterville (not to mention the cute teenaged duke next door), and she sets her hand to the task of freeing Sir Simon from his curse.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this re-imagining of Shakespear's King Lear, Patrick Stewart stars as John Lear, a Texas cattle baron, who, after dividing his wealth among his three daughters, is rejected by them.\n## 5 King Henry II (Patrick Stewart) keeps his wife, Eleanor (Glenn Close) locked away in the towers because of her frequent attempts to overthrow him. With Eleanor out of the way he can have his dalliances with his young mistress (Yuliya Vysotskaya). Needless to say the queen is not pleased, although she still has affection for the king. Working through her sons, she plots the king's demise and the rise of her second and preferred son, Richard (Andrew Howard), to the throne. The youngest son, John (Rafe Spall), an overweight buffoon and the only son holding his father's affection is the king's choice after the death of his first son, young Henry. But John is also overly eager for power and is willing to plot his father's demise with middle brother, Geoffrey (John Light) and the young king of France, Phillip (Jonathan Rhys Meyers). Geoffrey, of course sees his younger brother's weakness and sees that route as his path to power. Obviously political and court intrigue ensues\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Scrooge is a miserly old businessman in 1840s London. One Christmas Eve he is visited by the ghost of Marley, his dead business partner. Marley foretells that Scrooge will be visited by three spirits, each of whom will attempt to show Scrooge the error of his ways. Will Scrooge reform his ways in time to celebrate Christmas?\n##   popularity                      poster_path release_date\n## 1      0.841                             &lt;NA&gt;   2003-02-10\n## 2     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 3      9.267 /m71l7oGGKLxdQaUceVTndg2qjJJ.jpg   1996-01-27\n## 4      3.951 /jFvDJsgnLRVuRpsbm3YHIn0dHxI.jpg   2002-06-02\n## 5      7.097 /f6yEfCBBMNp6jdny9AD4ZTG9tgi.jpg   2003-12-26\n## 6     13.427 /oi1NcVDXlFEsdpLp37BJmFbVlg9.jpg   1999-12-05\n##                     title video vote_average vote_count\n## 1           Red Dwarf A-Z FALSE        0.000          0\n## 2 Star Trek: Insurrection FALSE        6.425       1025\n## 3   The Canterville Ghost FALSE        6.042         48\n## 4           King of Texas FALSE        5.100         11\n## 5      The Lion in Winter FALSE        6.100         17\n## 6       A Christmas Carol FALSE        6.800        161\n##                  credit_id department                job\n## 1 63fead85699fb70096ff260e       Crew             Thanks\n## 2 52fe4226c3a36847f8007c1d Production           Producer\n## 3 5962f6d292514122510c57a0 Production        Co-Producer\n## 4 59807f88925141491d0113a0 Production Executive Producer\n## 5 5f72da29e4b5760039f36206 Production Executive Producer\n## 6 63c31ac8d46537007dbd999a Production Executive Producer\n## \n## $id\n## [1] 2387\n\nBy default, fromJSON does a LOT of heavy lifting for us:\n\nIdentifying the structure of the top-level data: cast, crew, and id information\nParses cast information into a data frame with list-columns\nParses crew information into a data frame with list-columns\n\nIt‚Äôs hard to explain how nice this is to someone who hasn‚Äôt had to parse this type of information by hand before‚Ä¶ so let‚Äôs briefly explore that process.\n\nlibrary(jsonlite)\n\nps_messy &lt;- fromJSON(data_url, simplifyVector = T, simplifyDataFrame = F)\n\n\nExploring the output structure (long version)\n\n# Top-level objects (show the first object in the list)\nps_messy$cast[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] \"/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg\"\n## \n## $genre_ids\n## [1] 878  28  12  53\n## \n## $id\n## [1] 193\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Star Trek: Generations\"\n## \n## $overview\n## [1] \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\"\n## \n## $popularity\n## [1] 19.783\n## \n## $poster_path\n## [1] \"/rHsCYDGHFUarGh5k987b0EFU6kC.jpg\"\n## \n## $release_date\n## [1] \"1994-11-18\"\n## \n## $title\n## [1] \"Star Trek: Generations\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 6.526\n## \n## $vote_count\n## [1] 1126\n## \n## $character\n## [1] \"Captain Jean-Luc Picard\"\n## \n## $credit_id\n## [1] \"52fe4225c3a36847f80076d9\"\n## \n## $order\n## [1] 0\nps_messy$crew[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99 35\n## \n## $id\n## [1] 1093380\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Red Dwarf A-Z\"\n## \n## $overview\n## [1] \"A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\"\n## \n## $popularity\n## [1] 0.841\n## \n## $poster_path\n## NULL\n## \n## $release_date\n## [1] \"2003-02-10\"\n## \n## $title\n## [1] \"Red Dwarf A-Z\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 0\n## \n## $vote_count\n## [1] 0\n## \n## $credit_id\n## [1] \"63fead85699fb70096ff260e\"\n## \n## $department\n## [1] \"Crew\"\n## \n## $job\n## [1] \"Thanks\"\nps_messy$id\n## [1] 2387\n\nLet‚Äôs start with the cast list. Most objects seem to be single entries; the only thing that isn‚Äôt is the genre_ids field. So let‚Äôs see whether we can just convert each list entry to a data frame, and then deal with the genre_ids column afterwards.\n\ncast_list &lt;- ps_messy$cast\n\n\nData frame conversion\n\nas.data.frame(cast_list[[1]])\n##   adult                    backdrop_path genre_ids  id original_language\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n##           original_title\n## 1 Star Trek: Generations\n## 2 Star Trek: Generations\n## 3 Star Trek: Generations\n## 4 Star Trek: Generations\n##                                                                                                                                                                                                                                                   overview\n## 1 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 3 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 4 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 3     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 4     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n##                    title video vote_average vote_count               character\n## 1 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 2 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 3 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 4 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n##                  credit_id order\n## 1 52fe4225c3a36847f80076d9     0\n## 2 52fe4225c3a36847f80076d9     0\n## 3 52fe4225c3a36847f80076d9     0\n## 4 52fe4225c3a36847f80076d9     0\n\n\nmap(cast_list, as.data.frame)\n## Error in `map()`:\n## ‚Ñπ In index: 6.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\nWell, that didn‚Äôt work, but the error message at least tells us what index is causing the problem: 6. Let‚Äôs look at that data:\n\nData frame conversion errors\n\ncast_list[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nOk, so backdrop_path is NULL, and as.data.frame can‚Äôt handle the fact that some fields are defined (length 1) and others are NULL (length 0). We could possibly replace the NULL with NA first?\n\nfix_nulls &lt;- function(x) {\n  lapply(x, \\(y) if (is.null(y)) NA else y)\n}\n\ncast_list_fix &lt;- map(cast_list, fix_nulls)\n\ncast_list_fix[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nmap(cast_list_fix, as.data.frame)\n## Error in `map()`:\n## ‚Ñπ In index: 8.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## list()\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\nOk, well, this time, we have an issue with position 8, and we have an empty list of genre_ids.\nAn empty list and NULL both have length 0, so let‚Äôs alter our fix_nulls function to test for things of length 0 instead of testing for nulls. That should fix both problems using the same code, and we‚Äôre trying to directly test for the issue which was causing problems, which is perhaps a better approach anyways.\n\nfix_nulls &lt;- function(x) {\n  lapply(x, \\(y) if (length(y) == 0) NA else y)\n}\n\ncast_list_fix &lt;- map(cast_list, fix_nulls)\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] NA\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\ncast_list_df &lt;- map_df(cast_list_fix, as.data.frame)\ncast_list_df[1:10, 1:5]\n##    adult                    backdrop_path genre_ids  id original_language\n## 1  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n## 5  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg       878 199                en\n## 6  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        28 199                en\n## 7  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        12 199                en\n## 8  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        53 199                en\n## 9  FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg       878 200                en\n## 10 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg        28 200                en\n\nWe still have too many rows for each entry because of the multiple genre_ids. But we can fix that with the nest command.\n\ncast_list &lt;- nest(cast_list_df, genre_ids = genre_ids )\ncast_list[1:10,c(1:4, 17)]\n## # A tibble: 10 √ó 5\n##    adult backdrop_path                       id original_language genre_ids\n##    &lt;lgl&gt; &lt;chr&gt;                            &lt;int&gt; &lt;chr&gt;             &lt;list&gt;   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg   193 en                &lt;tibble&gt; \n##  2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg   199 en                &lt;tibble&gt; \n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg   200 en                &lt;tibble&gt; \n##  4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg   201 en                &lt;tibble&gt; \n##  5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg 10946 en                &lt;tibble&gt; \n##  6 FALSE &lt;NA&gt;                             21746 en                &lt;tibble&gt; \n##  7 FALSE /cN4qq4B8JR4ekuKAIKGVa4bBssl.jpg 25224 en                &lt;tibble&gt; \n##  8 FALSE &lt;NA&gt;                             33335 en                &lt;tibble&gt; \n##  9 FALSE /89hVgLIH55PVE7wwLCVZF1j3ZGL.jpg 26950 en                &lt;tibble&gt; \n## 10 FALSE /x6f4Axjvr5Ybi2mfdpVSWvASdxX.jpg 28123 en                &lt;tibble&gt;\n\nThen, we‚Äôd have to apply this whole process to the crew list as well. Let‚Äôs see how robust our process actually is!\n\ncrew_list &lt;- ps_messy$crew\ncrew_list_fix &lt;- map(crew_list, fix_nulls)\ncrew_list_df &lt;- map_df(crew_list_fix, as.data.frame)\ncrew_list &lt;- nest(crew_list_df, genre_ids = genre_ids )\ncrew_list[1:5,c(1:4, 17)]\n## # A tibble: 5 √ó 5\n##   adult backdrop_path                         id original_language genre_ids\n##   &lt;lgl&gt; &lt;chr&gt;                              &lt;int&gt; &lt;chr&gt;             &lt;list&gt;   \n## 1 FALSE &lt;NA&gt;                             1093380 en                &lt;tibble&gt; \n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg     200 en                &lt;tibble&gt; \n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg   48358 en                &lt;tibble&gt; \n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg   47913 en                &lt;tibble&gt; \n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg   37945 en                &lt;tibble&gt;\n\nOk, so that actually worked, but only because the structure of the crew data is the same as the structure of the cast data.\nIt‚Äôs good to see what we‚Äôd have to do manually if fromJSON() failed on us. It‚Äôs also an excellent example of functional programming in a practical setting.\nLet‚Äôs finish this up by converting our cast and crew data frames into a single data frame with a variable indicating which source DF is relevant.\n\npatrick_stewart_movies &lt;- bind_rows(\n  mutate(cast_list, role = \"cast\"),\n  mutate(crew_list, role = \"crew\")\n)\npatrick_stewart_movies %&gt;%\n  arrange(id)\n## # A tibble: 156 √ó 20\n##    adult backdrop_path              id original_language original_title overview\n##    &lt;lgl&gt; &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;          &lt;chr&gt;   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW‚Ä¶   193 en                Star Trek: Ge‚Ä¶ \"Captai‚Ä¶\n##  2 FALSE /wygUDDRNpeKUnkekRGeLC‚Ä¶   199 en                Star Trek: Fi‚Ä¶ \"The Bo‚Ä¶\n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3m‚Ä¶   200 en                Star Trek: In‚Ä¶ \"When a‚Ä¶\n##  4 FALSE /vsjuHP9RQZJgYUvvSlO3m‚Ä¶   200 en                Star Trek: In‚Ä¶ \"When a‚Ä¶\n##  5 FALSE /6z9w8eidKWDDXwZNSVNaR‚Ä¶   201 en                Star Trek: Ne‚Ä¶ \"En rou‚Ä¶\n##  6 FALSE /2mEXtIjgsoe5uqH70CLps‚Ä¶   815 en                Animal Farm    \"An ani‚Ä¶\n##  7 FALSE /5wJ2tckpvwcxGCAgZicco‚Ä¶   841 en                Dune           \"In the‚Ä¶\n##  8 FALSE /92mpNNg6v2PN2HN2C2Z4g‚Ä¶  1273 en                TMNT           \"After ‚Ä¶\n##  9 FALSE /wvqdJLVh0mSblly7UnYFP‚Ä¶  2080 en                X-Men Origins‚Ä¶ \"After ‚Ä¶\n## 10 FALSE /hPDv0O8tvbEvcVVphIieS‚Ä¶  2107 en                L.A. Story     \"With t‚Ä¶\n## # ‚Ñπ 146 more rows\n## # ‚Ñπ 14 more variables: popularity &lt;dbl&gt;, poster_path &lt;chr&gt;, release_date &lt;chr&gt;,\n## #   title &lt;chr&gt;, video &lt;lgl&gt;, vote_average &lt;dbl&gt;, vote_count &lt;int&gt;,\n## #   character &lt;chr&gt;, credit_id &lt;chr&gt;, order &lt;int&gt;, genre_ids &lt;list&gt;,\n## #   role &lt;chr&gt;, department &lt;chr&gt;, job &lt;chr&gt;\n\nWe could theoretically clean this up so that movies where Patrick Stewart was in both the cast and crew are on a single row, but I think this is ‚Äúgood enough‚Äù for now.\n\n\nPandas includes a read_json function, so let‚Äôs try that and see if it works as well as fromJSON() did in R:\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\npd.read_json(data_url)\n## ValueError: All arrays must be of the same length\n\nIf we read the documentation for read_json, we can see that we have a few different options - maybe playing around with some of those options will help? Our top-level structure is a list with 3 values: cast, crew, and id. So let‚Äôs see if we can read things in as a series instead of a DataFrame first, and hopefully we can use that to get some traction on the situation.\n\npatrick_stewart = pd.read_json(data_url, typ='series', orient = 'records')\n\n# List the objects\npatrick_stewart.index\n## Index(['cast', 'crew', 'id'], dtype='object')\n\n# First item in the cast list\npatrick_stewart.cast[0]\n## {'adult': False, 'backdrop_path': '/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg', 'genre_ids': [878, 28, 12, 53], 'id': 193, 'original_language': 'en', 'original_title': 'Star Trek: Generations', 'overview': \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\", 'popularity': 19.783, 'poster_path': '/rHsCYDGHFUarGh5k987b0EFU6kC.jpg', 'release_date': '1994-11-18', 'title': 'Star Trek: Generations', 'video': False, 'vote_average': 6.526, 'vote_count': 1126, 'character': 'Captain Jean-Luc Picard', 'credit_id': '52fe4225c3a36847f80076d9', 'order': 0}\n\nSo now how do we get our data into a proper form? If we read the documentation a bit further, we can see a ‚ÄúSee also‚Äù section that has a json_normalize function which promises to ‚ÄúNormalize semi-structured JSON data into a flat table‚Äù. That sounds pretty good, let‚Äôs try it!\n\nps_cast = pd.json_normalize(patrick_stewart.cast)\nps_cast.head()\n##    adult                     backdrop_path  ...                 credit_id  order\n## 0  False  /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg  ...  52fe4225c3a36847f80076d9      0\n## 1  False  /wygUDDRNpeKUnkekRGeLCZM93tA.jpg  ...  52fe4226c3a36847f8007ba3      0\n## 2  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  52fe4226c3a36847f8007c27      0\n## 3  False  /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg  ...  52fe4226c3a36847f8007cf1      0\n## 4  False  /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg  ...  52fe43d79251416c75020267      0\n## \n## [5 rows x 17 columns]\n\nHuh, that actually worked! (I‚Äôm not used to this type of thing working on the first try).\n\nps_crew = pd.json_normalize(patrick_stewart.crew)\nps_crew.head()\n##    adult                     backdrop_path  ...  department                 job\n## 0  False                              None  ...        Crew              Thanks\n## 1  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  Production            Producer\n## 2  False  /ccsLztuF4cKlfnriitwdxs0coBa.jpg  ...  Production         Co-Producer\n## 3  False  /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg  ...  Production  Executive Producer\n## 4  False  /li27iYcGbSp89YTlVRmwujteykw.jpg  ...  Production  Executive Producer\n## \n## [5 rows x 17 columns]\n\nWe can combine these as we did in R into a single data frame, and sort by movie ID to simplify the list.\n\nps_cast['role'] = 'cast'\nps_crew['role'] = 'crew'\nps_movies = pd.concat([ps_cast, ps_crew])\nps_movies[['id', 'original_title', 'character', 'job']].sort_values(['id'])\n##           id               original_title                character       job\n## 0        193       Star Trek: Generations  Captain Jean-Luc Picard       NaN\n## 1        199     Star Trek: First Contact  Captain Jean-Luc Picard       NaN\n## 2        200      Star Trek: Insurrection  Captain Jean-Luc Picard       NaN\n## 1        200      Star Trek: Insurrection                      NaN  Producer\n## 3        201           Star Trek: Nemesis  Captain Jean-Luc Picard       NaN\n## ..       ...                          ...                      ...       ...\n## 46   1088162  The Elves and the Shoemaker         Narrator (voice)       NaN\n## 0    1093380                Red Dwarf A-Z                      NaN    Thanks\n## 140  1093380                Red Dwarf A-Z                     Self       NaN\n## 126  1095754           John Clare: \"I Am\"            Cyrus Redding       NaN\n## 47   1104829     In the Company of Whales                 Narrator       NaN\n## \n## [156 rows x 4 columns]\n\n\n\n\n\n\n\n\n\n\nTry It Out: JSON File Parsing\n\n\n\n\n\nThe Movie Database\n\nI used TMDB to find all movies resulting from the query ‚ÄúStar Trek‚Äù and stored the resulting JSON file here.\n\n\nProblem\nR solution\nPython solution\n\n\n\nCreate a data frame using the Star Trek query results. Because there were 6 pages of query results, the JSON file looks a bit different than the format used in the example above. Can you create a plot of the release date and rating of each movie?\n\n\n\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(dplyr)\n\nfile_loc &lt;- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\n\nstartrek &lt;- fromJSON(file_loc) |&gt;\n  unnest(results)\n\nstartrek |&gt;\n  select(title, release_date, popularity, vote_average, vote_count) |&gt;\n  head()\n## # A tibble: 6 √ó 5\n##   title                    release_date popularity vote_average vote_count\n##   &lt;chr&gt;                    &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;      &lt;int&gt;\n## 1 Star Trek                2009-05-06         51.2         7.43       9075\n## 2 Star Trek: Nemesis       2002-12-13         33.6         6.29       1218\n## 3 Star Trek Beyond         2016-07-07         40.3         6.78       6037\n## 4 Star Trek: Insurrection  1998-12-11         28.8         6.42       1025\n## 5 Star Trek Into Darkness  2013-05-05         41.3         7.33       8370\n## 6 Star Trek: First Contact 1996-11-22         29.8         7.30       1519\n\n# convert release_date to datetime\nlibrary(lubridate)\nstartrek &lt;- startrek |&gt;\n  mutate(rel_date = ymd(release_date))\n\nstartrek |&gt;\n  arrange(rel_date) |&gt;\n  select(title, rel_date, popularity, vote_average, vote_count) |&gt;\n  head()\n## # A tibble: 6 √ó 5\n##   title                            rel_date   popularity vote_average vote_count\n##   &lt;chr&gt;                            &lt;date&gt;          &lt;dbl&gt;        &lt;dbl&gt;      &lt;int&gt;\n## 1 Jr. Star Trek                    1969-01-01       0.6          0             0\n## 2 √ñmer the Tourist in Star Trek    1973-01-01       1.90         6.36         38\n## 3 Star Trek: The Motion Picture    1979-12-07      26.5          6.50       1480\n## 4 Star Trek II: The Wrath of Khan  1982-06-04      21.6          7.47       1655\n## 5 Leonard Nimoy: Star Trek Memori‚Ä¶ 1983-01-01       0.6          7             1\n## 6 Star Trek III: The Search for S‚Ä¶ 1984-06-01      17.0          6.62       1159\n\nlibrary(ggplot2)\nggplot(startrek, aes(x = rel_date, y = popularity)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\n\n\n\nggplot(startrek, aes(x = rel_date, y = vote_average)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfile_loc = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\ntrek = pd.read_json(file_loc)\n\ntrek['results2'] = trek.results.map(pd.json_normalize)\n\n# This doesn't actually keep the page info but I don't think we need that\ntrek_tidy = pd.concat(trek.results2.to_list())\ntrek_tidy['rel_date'] = pd.to_datetime(trek_tidy.release_date)\n\nimport matplotlib.pyplot as plt\np1 = trek_tidy.plot.scatter('rel_date', 'popularity')\nplt.show()\n\n\n\n\n\n\n\np2 = trek_tidy.plot.scatter('rel_date', 'vote_average')\nplt.show()",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "href": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "title": "27¬† Functional Programming",
    "section": "\n27.5 Assembling Hierarchical Data",
    "text": "27.5 Assembling Hierarchical Data\nAnother common situation we find ourselves in as analysts is to have multiple levels of data.\nLet‚Äôs start with a totally absurd hypothetical situation: Suppose I watched the documentary ‚ÄúChicken People‚Äù and became interested in the different breeds of chicken. As a data scientist, I want to assemble a dataset on chicken breeds that I might use to decide what breed(s) to raise.\nA site such as Cackle Hatchery has an overall summary table as well as pages for each individual breed. I‚Äôm not going to show you how to web scrape here - it‚Äôs not relevant to this chapter - but we can at least outline the process:\n\nAcquire the overall table\nUse the links to each breed in the overall table to get more specific information for each breed\n\nThis will require a function to scrape that individual data\nWe can use map to apply that function to acquire individual data from each breed\n\n\n\nI‚Äôve used this approach to generate two files:\n\n\nchicken-breeds.csv - the original table of breed information\n\nchicken-breed-details.json, which is a JSON file assembled by scraping information off of each breed‚Äôs individual page.\n\n\n\n\n\n\n\nTry It Out: Chicken Breed Data Assembly\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nCan you create a nested data frame that has all of the information from both the CSV and JSON file in a single tabular structure?\n\n\n\nlibrary(readr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(stringr)\n\noverall &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails &lt;- fromJSON(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\nhead(overall)\n## # A tibble: 6 √ó 9\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Austra White            220-280 eggs pe‚Ä¶ Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         &lt;NA&gt;        \n## 3 Barnevelder             150-200 eggs pe‚Ä¶ Dark Brown  Poor         &lt;NA&gt;        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         &lt;NA&gt;        \n## 5 Barred Cochin Standard  110-160 eggs pe‚Ä¶ Brown       Very         &lt;NA&gt;        \n## 6 Barred Old English Ban‚Ä¶ 120 eggs per ye‚Ä¶ Cream       Good         &lt;NA&gt;        \n## # ‚Ñπ 4 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;\nhead(details)\n##                             name\n## 1           Austra White Chicken\n## 2            Ayam Cemani Chicken\n## 3            Barnevelder Chicken\n## 4           Barred Cochin Bantam\n## 5 Barred Cochin Standard Chicken\n## 6 Barred Old English Game Bantam\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              description\n## 1 Our Austra Whites are a cross between one of our best Cackle Hatchery√Ç¬Æ production/bloodlines of Black Australorp rooster and one of our best Cackle Hatchery√Ç¬Æ production/bloodlines of White Leghorn hens (parent stock). They were first developed in the early 1900√¢\\u0080\\u0099s. This cross produces offspring that are very good layers throughout the year and lay a very light brown to off white egg shell color egg. The Austra White pullet will be white with little black specks in some feathers. This cross is considered to be a heavier breed and their disposition is calmer than the pure Leghorn chicken breed. There are many benefits to raising baby chickens from this cross. These hens will lay a lot of large eggs; have good feed to egg production ratios and good for chickens for backyards. Raising chickens for eggs at home can be fun and relaxing. Free range chicken eggs are great tasting eggs and this hybrid chicken breed has good reflexes for predator avoidance, however, they are white and easily seen.\\nWe also offer at limited times of the year Austra White Fertile Hatching Eggs\n## 2                                    The breed originated from the island of Java, Indonesia and has probably been used for centuries in religious and mystical purposes. The breed was first described by Dutch colonial settlers and first imported to Europe in 1998 by Dutch breeder Jan Steverink. Their beak and tongue, comb and wattles, even their meat bones and organs appear black. The blood of the Ayam Cemani is normally colored. The bird√¢\\u0080\\u0099s black color occurs as a result of excess pigmentation of the tissues, caused by a genetic condition known as fibro melanosis. This gene is also found in some other black fowl breeds. Roosters can get some mulberry upon maturity due to testosterone and other influences. The hens lay cream-colored eggs, although they are poor setters and rarely hatch their own brood. Our Ayam Cemani√¢\\u0080\\u0099s bloodline includes Raven and some Greenfire.\\n30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks.\\nWe now have Ayam Cemani Fertile Hatching Eggs for sale click here!\n## 3                                                                                                                                                                                                                                                                                                                                                                          The Barnevelder chicken originates from the Barneveld region of Holland and known for laying a dark brown egg. This beautiful bird has a single comb, is hardy and quiet and doesn√¢\\u0080\\u0099t mind being confined. The breed was first recognized by the American Standard of Perfection in 1991. Cackle Hatchery√Ç¬Æ√¢\\u0080\\u0099s Barnevelders breeding stock will produce feathering of partridge single laced and√Ç¬† double laced feather pattern. Each year breeding season our objective is to breed more for the double laced pattern. The Barnevelder chicken is rare to find in the USA but becoming more popular each year. To buy Barnevelder chickens, please select your quantity under 50 above.\\nAlso may like√Ç¬†Dark Brown Egg Female Surplus.\n## 4                                                                                                                                                                                                                                                                                                                                                                                          At Cackle Hatchery√Ç¬Æ, we offer several different types of the Cochin Bantam, including the Barred Cochin Bantam. If you√¢\\u0080\\u0099re not familiar with this chicken breed, it is a miniature version of the Standard Cochin. For more than a century these chickens were admitted to the American Poultry Standard of Perfection, and they make great pets and mothers for chicks. The standard version of the breed, the Barred Cochin Bantam, is an excellent choice, so place your order today. For more details about the Barred Cochin Chicken, please contact us!\\nMany people who like this breed of chicken also like the standard version of the breed, the√Ç¬†Barred Cochin Chicken.\\nAlso may like√Ç¬†Cochin Bantam Special Surplus.\n## 5                                                                                                                                          When it comes to rare breed chickens, the professionals at Cackle Hatchery√Ç¬Æ have a lot to offer. We have more than 200 breeds to choose from, including the Barred Cochin Standard. This bird is one of the many color types of Cochins that we have available and it is notable for several reasons. This is a very large chicken with a lot of unique feathering and feathered legs. This chicken is also great around children, making it a perfect pet for the family farm. This is a very hard color to find of the standard cochin with very few breeders in the USA. We further improved our flock by adding some of Roland Doerr bloodline into our flock in 2009. Make a great show and exhibition type chicken. You can place your order today or you can call us for more information.\\nMany people who like this breed of chicken also like the miniature version of the breed (bantam), the√Ç¬†Barred Cochin Bantam Chicken.\\nAlso, may like√Ç¬†Cochin Standard Surplus Special.\\n√Ç\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                             Cackle Hatchery√Ç¬Æ offers several varieties of the Old English Bantam, a miniature version of the Standard Old English Game chicken. The Barred Old English Game Bantam is just one of many high-quality chicken breeds we have available, and this variety is notable for its black and white spotted coloring. Because these chickens require little space and feed they make perfect pets, and they are generally well behaved. In fact, some Barred Old English Game Bantams can even become so tame that they will sit on your arm. Get started today by placing your order for baby chicks, and contact us if you have questions!\\nAlso may like√Ç¬†Old English Bantam Surplus Special.\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    breed_facts\n## 1                                                                                                                                                                                                                                                                                                                                                                : Not applicable, √Ç Hen√¢\\u0080\\u0094√¢\\u0080\\u00935 lbs\\nRooster√¢\\u0080\\u0094√¢\\u0080\\u00946 1/2 lbs\\nPullet√¢\\u0080\\u00944 1/2 lbs\\nCockerel√¢\\u0080\\u0094-5 1/2 lbs, √Ç Primary production, Egg Laying & Pet/Secondary meat source, Very light brown to off white egg shell color, √Ç √Ç 220-280 eggs per year (estimates only, see√Ç FAQ), √Ç Large, √Ç Active, √Ç 80-85%, Fertility Percentage:√Ç 65-80%, Non Setter, √Ç 12 Females to 1 Males, √Ç 4 to 8 feet, √Ç Hybrid, √Ç No, √Ç No, √Ç Cackle Hatchery√Ç¬Æ Poultry Breeding Farm has been developing our bloodline or strain of Austra White since 1939.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                         √Ç Not applicable, √Ç Hen √¢\\u0080\\u0094√¢\\u0080\\u0094-4.1/2 lbs\\nRooster√¢\\u0080\\u0094-7 lb\\nPullet√¢\\u0080\\u0094√¢\\u0080\\u00944√Ç lbs\\nCockerel√¢\\u0080\\u00945 lbs, √Ç Ornamental/ Meat and Egg, √Ç Cream, Light tan, √Ç 80-120 per year√Ç (estimates only, see√Ç FAQ), Docile, √Ç 8 Females to 1 Male, √Ç 4+ feet, Yes sometimes, √Ç Java, Indonesia, √Ç No, √Ç Not listed, Breeder Farm Source:√Ç Cackle Hatchery√Ç¬Æ Poultry Breeding Farm has been developing our bloodline or strain of pure Ayam Cemani since 2018\n## 3                                                                                                                                                                                                                                                                                                √Ç Continental Class, Weights:√Ç Hen√¢\\u0080\\u0094√¢\\u0080\\u00936 lbs\\nRooster√¢\\u0080\\u0094√¢\\u0080\\u00947 lbs\\nPullet√¢\\u0080\\u0094-5 lbs\\nCockerel√¢\\u0080\\u0094√¢\\u0080\\u00936 lbs, √Ç Egg Laying; Exhibition, √Ç Dark Brown, √Ç 150-200 eggs per year (estimates only, see FAQ), √Ç Large, √Ç Active, √Ç 80-85%, Fertility Percentage:√Ç 65-80%, Broody:√Ç Non Setter, √Ç 7 Females to 1 Male, √Ç 2 to 4 feet, Country of Origin:√Ç Holland, APA:√Ç Yes, Recognized by the American Standard of Perfection, TLC:√Ç Not Listed, BREEDER FARM SOURCE:√Ç Cackle Hatchery√Ç¬Æ Poultry Breeding Farm has been developing our bloodline/strain of pure Barnevelder chickens since 2008.\n## 4                                                                                                                                                                                                                                                                                                                                                                           Feather Legged Bantams, Hen √¢\\u0080\\u0094√¢\\u0080\\u0094-26 oz √Ç √Ç √Ç √Ç √Ç √Ç Rooster√¢\\u0080\\u0094√¢\\u0080\\u009330 oz, Purpose and Type:√Ç Pets,Very Broody, Ornamental; Exhibition, Egg Shell Color:√Ç Brown Bantam Sized Egg, Egg Production:√Ç Fair, Egg Size:√Ç Small, √Ç Docile, 75-80%, √Ç 40-55%, Broody:√Ç Setters, √Ç 6 Females to 1 Male, √Ç 0 to 2 feet, Country of Origin:√Ç Asia, Yes, Recognized by the Standard of Perfection in 1965, No, √¢\\u0080\\u009c Cackle Hatchery√Ç¬Æ Poultry Breeding Farm√¢\\u0080\\u009d developing our bloodline or strain of pure Cochin Bantams since 1971.\n## 5 √Ç Asiatic Class, Weights √¢\\u0080\\u0093√Ç Hen√¢\\u0080\\u0094√¢\\u0080\\u00938 1/2 lbs\\nRooster√¢\\u0080\\u0094√¢\\u0080\\u009411 lbs\\nPullet√¢\\u0080\\u00947 lbs\\nCockerel√¢\\u0080\\u0094-9 lbs, Purpose and Type √¢\\u0080\\u0093√Ç Ornamental√Ç and meat; Exhibition, √Ç Brown, Egg Production√Ç √¢\\u0080\\u0093 110-160 eggs per year (*estimates only, see√Ç FAQ), Egg Size:√Ç Medium-Large, √Ç Docile, √Ç 40-55%, Broody:√Ç Setter, √Ç 6 Females to 1 Male, √Ç 0 to 2 feet, Asia, Yes, Recognized by the American Standard of Perfection in 1982., Recovering Status, Considered a sustainable heritage chicken breed, √Ç √¢\\u0080\\u009cCackle Hatchery√Ç¬Æ√Ç Poultry Breeding Farm√¢\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975., Breeder Farm Source:√Ç Cackle Hatchery√Ç¬Æ√Ç Poultry Breeding Farm√¢\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975.\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Game Bantams, √¢\\u0080\\u0093√Ç Hen √¢\\u0080\\u0094√¢\\u0080\\u0094-22 oz√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç Rooster√¢\\u0080\\u0094-24 oz\\nPullet√¢\\u0080\\u0094√¢\\u0080\\u009420 oz√Ç √Ç √Ç √Ç √Ç √Ç √Ç √Ç Cockerel√¢\\u0080\\u009422 oz, Purpose and Type√Ç √¢\\u0080\\u0093 Ornamental; Exhibition, Egg Shell Color√Ç √¢\\u0080\\u0093 Cream or Tinted Bantam Sized Egg, √Ç √¢\\u0080\\u0093√Ç Poor, Egg Size:√Ç Small, √Ç Active, √Ç 40-55%, Broody:√Ç Setters, √Ç 9 Females to 1 Male, √Ç 3+ feet, √¢\\u0080\\u0093√Ç Europe, : No, No\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    availability\n## 1                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 2 04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, low-availability, unavailable, available, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, low-availability, low-availability, unavailable, low-availability, low-availability, low-availability, low-availability\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 5                     04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , low-availability, low-availability, unavailable, low-availability, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, available, unavailable, unavailable, unavailable, low-availability, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 6                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n##                                                                                                                            videos\n## 1                                                                                       https://www.youtube.com/embed/fnKRESJcpuY\n## 2                                            https://www.youtube.com/embed/AapH24ImQBo, https://www.youtube.com/embed/Y00jzCR7b-s\n## 3 https://www.youtube.com/embed/J7hIxdTgOPc, https://www.youtube.com/embed/sqhz7Rdc_0U, https://www.youtube.com/embed/T7UeVZe3j10\n## 4 https://www.youtube.com/embed/kr7huXt_-Fk, https://www.youtube.com/embed/dwsRHSr5scc, https://www.youtube.com/embed/k82eEH913Y8\n## 5                                            https://www.youtube.com/embed/iNwIbI3xbHA, https://www.youtube.com/embed/HbdlI_AEVQM\n## 6                                            https://www.youtube.com/embed/3tN2xqkoxLc, https://www.youtube.com/embed/dpLJK0Sc2xk\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            reviews\n## 1 364845, 326510, 326419, 172038, 171864, 171778, 171750, 171713, 171601, 171490, 171424, 171417, 171391, 171259, 171219, 170835, 170494, 170468, 170254, 170205, 170136, 5, 5, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, Andrea , Millermcnutt , Vinny , Sharon , Chad , Alexa , Lori , Mike , mmeyer , Kasen , Raymond , rhodyray , Sonia  , Chickens R Us , Darrell , Dorothy , Jill, Indiana April 2013 , Michelle, New York May 2013 , Beth Oklahoma February 2015 , Michael Georgia February 2014 , Janie, Missouri May 2014 , 2020-07-17T16:18:08-05:00, 2020-04-18T01:05:55-05:00, 2020-03-13T03:22:27-05:00, 2018-08-08T21:29:59-05:00, 2018-03-08T20:42:45-05:00, 2017-10-30T22:18:53-05:00, 2017-09-08T01:32:20-05:00, 2017-07-29T13:21:09-05:00, 2017-05-15T06:48:03-05:00, 2017-03-08T17:41:26-05:00, 2016-12-06T03:08:46-05:00, 2016-12-03T23:16:45-05:00, 2016-11-06T05:53:01-05:00, 2016-06-16T20:08:04-05:00, 2016-05-19T10:45:33-05:00, 2015-12-25T06:23:42-05:00, 2015-11-30T14:06:01-05:00, 2015-11-27T21:42:12-05:00, 2015-11-12T15:56:18-05:00, 2015-11-10T21:37:58-05:00, 2015-11-09T15:29:51-05:00, Very good, LOOOOVE my Austra White Hen, Good, Thank You, Austra Duds, Got my Chicks!, Nice birds!, Wonderful Chicks, Mother√¢\\u0080\\u0099s day surprise , Good, Like this bird, Good Hens, Adventurous birds! , Not your ordinary personalities, BEST\\n, Yum! We used to get , Cackle Hatchery, Cackle Hatchery, Testimonal, Cackle Hatchery, Cackle Hatchery, Very happy to see my 25 hens get here on time. All of them seem, so far happy and healthy., Purchased Austra White pullet last year. I LOOOVE her, she went broody late summer, hatched 2 eggs (which I had ordered) such a good Mommy bird.  Henny Penny is very friendly, she lets me pick her up and she comes running to me when I walk out the door.  She is a precious bird!, Want to say two years later they are great birds hardy and great free rangers. 5 eggs a week from my birds. Will get this breed again. , Got my chicks, all alive and doing well. I had 2 setters and they adopted them as their own. So precious. Thank you, a very pleased customer., I ordered a handful of these last spring and have to admit I am disappointed with them. When they do lay the eggs are super large and a beautiful cream color but mine do not lay often. Two of them are broody every couple months and never seem to lay and the other two only give me eggs once in awhile. My Easter Eggers, Ameraucanas, and Welsummers are by far better layers., Got my baby chicks this morning! All healthy, all doing fantastic! The tracking for my package was spot on with much appreciated detail. I got my chicks shipped to Hawaii which has additional fees and you have to have a higher minimum of birds and it was totally 100% worth it. I will be using your service in the future and will be recommending it to others. Thank you so very much!, I have 3 of these pullets and at about 21 weeks they are just starting to lay.  They are large, handsome and very friendly birds. As youngsters they were always flying up on my shoulders and are very adventurous.  Their eggs are still smallish and are light cream in color.  I√¢\\u0080\\u0099m very happy with them., I received 4 Austra White pullets as part of an order of 16 chicks on March 1st and they have made beautiful hens.  One began laying at 16 weeks of age which is the earliest I have ever heard of.  I would highly recommend this breed and Cackle Hatchery as this flock of hens that also contains 4 Buff Orpingtons, 3 Turkens, 3 Cuckoo Marans and 2 Black Golden Laced Wyandottes is the nicest flock I have ever had in my 50+ years of raising chickens., I ordered 15 chicks at the end of November 2016.  10 EE√¢\\u0080\\u0099s and 5\\n Austra Whites.  All 5 of the Whites survived, but I lost 5 EE in January.  The white chicks were more aggrisive then my Easter eggers and separated them for a month until bigger.  I kept them in a large brooder box in an enclosed porch area with an electric chicken heater until April. The whites were very hungry chicks.  I√¢\\u0080\\u0099ve never had a leg horn breed type before.  They are certainly more assertive than my Easter egger, RIR,  and Cinnamon Queens (they lay large to jumbo eggs, but are kind of dumb birds).  Today, Mother√¢\\u0080\\u0099s Day, one of my Austra Whites layed a small pretty cream egg!  Looking forward to seeing how big  and how many they  can lay., Good, Love this bird.  Not as skittish or as flighty as many suggest.  One was bullied by a Black Sex Link however I solved the issue by removing the Black Sex Link for three days and now everything is fine.  Lay some of the biggest eggs I√¢\\u0080\\u0099ve ever seen.  Two within the last two weeks measuring 3√¢\\u0080¬≥ X 2√¢\\u0080¬≥ (that√¢\\u0080\\u0099s big my friend).  Friendly at times and will eat from my hand., Received chicks in the middle of March and as they age have become more friendly.  Guess it takes time for them to trust humans.  Anyhow they have been consistent layers and one bird in particular has layed two eggs over the last month that measured 3√¢\\u0080¬≥ X 2√¢\\u0080¬≥.  Now they are humongeous eggs.  The Austra Whites share a coop with Red and Black Sex link hens are all are doing fine., We ordered 6 Austra Whites and received 7 on time and in excellent, healthy condition. We gave them water with electrolytes right away, and they were so much fun to handle as tiny chicks. They are now about 5 months old and we still have all 7. They haven√¢\\u0080\\u0099t started laying yet, but they are very adventurous and have been from the time they were baby chicks. They were the first of all 5 breeds we have to fly out of the trough we kept the babies in, they were the first to scale our fence, they were the first to find their way onto the roof of our house, and they are the only breed we have that will wander far from the house (which kinda worries us because we have had hawks get a couple other breeds, and Austra Whites are√¢\\u0080¬¶well√¢\\u0080¬¶white.) They definitely keep us on our toes! , Out of 4:  One was very mean at one month, pecking all her siblings so I had to get rid of her.  Another one escaped three weeks ago and never was seen again.  Another is an escape artists and ALWAYS climbs up over the chicken wire.  I have one ordinary one.  Phew!  This breed keeps me hopping., I am now on week number 2 with the 22 Chicks Cackle has sent me. They are all very healthy and beautiful birds. They are growing very nice and can√¢\\u0080\\u0099t wait for them to be able to move to the Big coop so they can roam around freely. More orders to come for sure. Friendly FAST Service for sure. Already made a pre-order for some Buff Orpingtons. Can not wait to get them., Yum! We used to get green eggs all the time but my green egg layers have been on strkie lately so all we have now is rose and brown √∞\\u009f\\u0099\\u0082 I Love green eggs!, Just wanted to tell you that we received our chicks midday on the 17th..and all were perfectly perfect and adorable! Thanks soooooo much for working with us on a quicker ship date:), They all arrived yesterday in great health and lots of vigor. By far the best order we have ever received from any hatchery. Thanks, Last year I purchased a dozen chickens six barred rocks and six Austra Whites. i am pleased with them. All of these girls are healthy and ornery at a year old. They are just now at nearly a year old starting to lay eggs. All of the eggs i have found have been quite large, surprising for pullets. They are all loaded with personality. With the exception of a few chickens I obtained from neighbors and friends, at least 3/4 of my flock came from Cackle Hatchery. Even the mixed breeds that I hatched out indirectly came from your hatchery since the parent birds were hatched in your facility. We have 23 chickens in all and I am always getting compliments on how good they look, as well as how friendly they are. I just wanted to say thank you for these fine quality birds that I get to enjoy having around. Thank you, Just a quick thank you. Once again your company delivered a box full of live healthy and vibrant chickens. All arrived doing fine. We ordered 60 and we√¢\\u0080\\u0099re not disappointed with the chicks in the least. Thanks to Cackle sending a few more chicks for warmth than we ordered.\\nWe ordered Egyptian for the first time. They are the most active chicks I have ever seen. The special heavy assorted was a great bargain. The chicks were even marked as requested!!!. You have earned my business once again. I have shared your catalog with friends and relatives and will continue to do so. Thanks again and keep up the excellent service., I received my chicks today and found all but 1 made it alive not bad odds for ordering 70 chicks. Thank you for the chicks and I am looking forward to ordering more. \n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       607315, 607316, Cackle Hatchery , Cackle Hatchery , 2022-10-05T11:35:27-05:00, 2022-10-05T11:36:06-05:00, Oh no so sorry to hear that. As stated on website possible white on wings, though rare,  is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks., Oh no so sorry to hear that. As stated on website possible white on wings, though rare, is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes. We cannot guarantee the distribution of black pigment on chicks.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  172155, 171588, 171517, 170545, 170528, 5, NA, NA, NA, NA, Mtviewranch , AnaMaria , T Rathjen , Jerry, Arizona March 2012 , Bill, Missouri February 2013 , 2019-01-07T02:35:08-05:00, 2017-05-05T20:35:00-05:00, 2017-03-20T19:00:11-05:00, 2015-12-02T21:11:34-05:00, 2015-11-30T17:33:10-05:00, Love them!!, Beautiful Birds, Beautiful Birds, Chicken order, Thank You, I√¢\\u0080\\u0099ve ordered from you guys twice now and I have not lost one chick. In the two orders you sent extra girls for warmth I think 7 all together bonus! Beautiful birds my barnevelders just started laying large dark speckled eggs. They have great personalities I highly recommend this breed., My 4 Barnevelders are 3 weeks old. They√¢\\u0080\\u0099ve arrived in 24 hours to Atlanta, in great health. They seem to grow by the second, but we√¢\\u0080\\u0099ve had no problems since their arrival. They are beutiful, full of personality, friendly and well behaved. Im very happy. Thank you. I will try to post a follow ip when they are older., I purchased my chicks in Sept of 2016.  Everyone of them were healthy and lived.  I purchased a combination of Sussex and Wyandotte√¢\\u0080\\u0099s.  They were all hand raised and are very friendly and love to √¢\\u0080\\u009ctalk√¢\\u0080\\u009d to you.  We had no issues with health or poop and they have grown into beautiful, large birds, and produce plenty of eggs.  We will definitely buy from you again when we are ready to expand our flock., I just wanted to send you some feedback on an order you sent me. They arrived all healthy and are doing great!!! My wife and I wanted to thank you for the way you handled the order and the quality of the chicks you sent us., I√¢\\u0080\\u0099m just wanting to express my thanks to you for the five different breeds of chicks that I got from your hatchery Feb 12th. They are doing quite well, one or two of the chicks has a soft poop but still seem to be full of energy. Being I never experienced chicks before, I was really surprised at how fast they grow. On the fifth day I had to extend my wall of the brooding area because they would jump and or fly over my twelve inch high pen. I did lose one of the barred rocks around the third or fourth day. I actually expected to lose two or three but I am blessed to have lost only one. Thanks for good service, information and kindly taking the time to answer my many questions.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     325390, 325352, 325322, 325320, 325106, 170959, 170836, 5, 5, 3, 5, 2, NA, NA, farmr john , Susannah C , mudman , Crystal , Grace , Sylvia June 2008 , Fares , 2019-10-31T16:58:50-05:00, 2019-10-30T16:33:57-05:00, 2019-10-29T21:24:13-05:00, 2019-10-29T21:00:55-05:00, 2019-07-01T14:24:36-05:00, 2016-01-11T16:37:32-05:00, 2015-12-25T06:46:45-05:00, Awesome birds, Beautiful, sweet birds / consistent layers, review, Beautiful chickens and fun pets !, Not my favs, Cackle Hatchery, We had a great year , These birds are awesome . They are lively from the start unlike to golden laced cochins. The show type bred into these birds is incredible. I have 6 pullets and 3 cockerels that could stand lots of competition. They are also heavy egg layers. So very impressed with them. They should be priced at double the cost!, We brought in barred cochin bantams last spring, and with thoughtful, gentle interaction, they have grown up to be beautiful, friendly, personable birds with very sweet personalities. I have many breeds in my flock, all with their own charm, but I do think these two Barred Cochins are perhaps the sweetest, cuddliest birds I√¢\\u0080\\u0099ve ever had.  Highly recommend for a gentle flock. (We have Polish, Silkies, Seramas, Brahmas, Wyandottes, Ameraucanas.) These birds aren√¢\\u0080\\u0099t bullies and would not do well with super aggressive flockmates., nice breed doing well, We absolutely love our Barred Cochin Bantams!  I bought them for my children to raise as beloved pets.  They are so beautiful and sweet tempered.  They love to be held and fed worms.  , We bought a few different kinds of bantams and these were definitely my least favorite. One died in transit, one died two days after we got them, one was a rooster, and one is a nice little pullet. We bought sexed females. All-in-all I like the one that we ended up with, but did not like how sickly the rooster and the one that died were. Our other bantams that we received, golden laced Cochin, Rhode island red, and barred Rock, have all been healthy and seem much stronger., Just wanted you to know how pleased I am with my chickens from your company. They arrived in great health and have grown into these beauties. Many Thanks!, We had a great year at county fair.  This was our fifth year doing 4-H, but our first year being on the hoetaemsd with 4-H.  We have done static (the cooking, rockets, sewing, etc) exhibits before and last year we took 3 meat goats, but we had to keep them a t a friends house for 2 months prior to fair.  Last year se moved to a rental house 1 week before fair and didn√¢\\u0080\\u0099t get our statics turned in.  So this year, now that we have a few acres we got a little carried away and took 38 statics   between 3 kids, and we took 3 dairy goats, 5 meat goats, 3 sheep, 6 chickens and 1 rabbit. Whoo!  that was probably a bit much!  We had a great time and the kids received 100 ribbons, plaques and awards in 4-H and our family got another 15 in open class.  We tried a lot of new things and the kids were pleased that for our first year in poultry they got either champion or reserve in showmanship.  We found out we don√¢\\u0080\\u0099t like sheep, and we like the chickens more than we thought we would.  We love 4-H and will be incorporating 4-H projects into our homeschool so that most of the projects are done by June and we don√¢\\u0080\\u0099t have the summer scramble to finish and then we can just concentrate on the livestock.\n## 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      326461, 326272, 326143, 171438, 170950, 5, 5, 5, NA, NA, Garcia  , PonyGirl , Ellen , Debbie North Carolina Jan 2017 , Larry December 2015 , 2020-04-02T06:12:50-05:00, 2020-01-11T22:50:46-05:00, 2019-12-10T23:38:08-05:00, 2017-01-04T15:23:10-05:00, 2016-01-11T15:51:18-05:00, Great Birds , Love my cacklehatchery chickens, Beautiful Birds, Cust Response, Cackle Hatchery, Great, beautiful, and wonderful mothers. My hen was only about 7 months old and she became broody and hatched out 7 chicks. They are great hens and let√¢\\u0080\\u0099s not forget the roosters! He is very protective of his hens but at the same time, he is very nice and gentle and likes to be held by humans. Great bred for first time chicken owners or if you live in cold areas like colorado , Our chickens arrived in excellent condition and have all been incredibly healthy.  The are now 6 months old and absolutely beautiful. I get compliments on how pretty my cochins are all the time.  I highly recommend buying all your chickens from Cacklehatchery.com!!! , Ordered Barred Cochins. I love these birds. Big, sturdy birds that are beautiful, fully feathered down to their feet, and so sweet. Hands down my favorite breed!, Just wanted to say that the Standard Golden Laced Cochins and the Standard Barred Cochins I ordered and received back in April are looking just beautiful! Very beautiful chickens, with great feathering! I am pleased with both breeds, and am especially excited about the standard sized Barred Cochins because it was only when I saw them on your list of cochins did I realize that the Barred even existed in the standard size! Thank you again, I would and do recommended your hatchery to my friends and acquaintances., Liked your youtube videos!\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          170939, Emily Ohio April 2008 , 2016-01-11T15:27:50-05:00, Thanks You Cackle Hatchery, I received my order this morning and could not be more pleased! I ordered 30 chicks and received 57. They are beautiful! Also I want to thank you for offering a discount to 4-Hers! I really appreciate it. I will be showing my birds at the fair this year. You folks also have AWESOME customer service, so keep up the good work! You have won my business!\n\nbreed_details &lt;- details |&gt;\nunnest(c(\"name\", \"description\")) |&gt;\n  mutate(name = str_remove_all(name, \" Chicken\") |&gt;\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |&gt; \n           str_remove_all(\"Standard|Game\") |&gt;\n           str_replace_all(\"D‚Äô\", \"d\") |&gt;\n           str_to_title() |&gt;\n           str_squish())\n\noverall &lt;- overall |&gt;\n  mutate(name = `Chicken Breed Name` |&gt;\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |&gt;\n           str_remove_all(\"Standard|Game\") |&gt;\n           str_replace_all(\"D‚Äô\", \"d\") |&gt;\n           str_to_title() |&gt;\n           str_squish())\n\n# Names aren't exactly the same, but close enough after some minor string manipulation\nanti_join(overall, breed_details)\n## # A tibble: 7 √ó 10\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Buff Ameraucanas        180-200 eggs pe‚Ä¶ Blue        Very         &lt;NA&gt;        \n## 2 Red Broiler             Poor             Brown       Good         Good        \n## 3 French Cuckoo Marans    180+ eggs per y‚Ä¶ Dark Brown  Moderate     Moderate    \n## 4 Saipan Jungle Fowl      Poor             Light Brown Poor         &lt;NA&gt;        \n## 5 Splash Old English Ban‚Ä¶ Poor             Cream       Good         &lt;NA&gt;        \n## 6 White Crested Blue Pol‚Ä¶ Good             White       Poor         &lt;NA&gt;        \n## 7 White Rock              200 ‚Äì 280 eggs ‚Ä¶ Brown       Very         &lt;NA&gt;        \n## # ‚Ñπ 5 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;, name &lt;chr&gt;\nanti_join(breed_details, overall)\n## # A tibble: 5 √ó 6\n##   name                description        breed_facts availability videos reviews\n##   &lt;chr&gt;               &lt;chr&gt;              &lt;list&gt;      &lt;list&gt;       &lt;list&gt; &lt;list&gt; \n## 1 Blue Polish         \"When it comes to‚Ä¶ &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;   \n## 2 Buff Ameraucana     \"Buff Ameraucana ‚Ä¶ &lt;df&gt;        &lt;named list&gt; &lt;chr&gt;  &lt;df&gt;   \n## 3 Cackles Red Broiler \"A great alternat‚Ä¶ &lt;df&gt;        &lt;named list&gt; &lt;chr&gt;  &lt;df&gt;   \n## 4 Saipan              \"Saipan chickens ‚Ä¶ &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;   \n## 5 White Plymouth Rock \"The White Plymou‚Ä¶ &lt;df&gt;        &lt;df&gt;         &lt;chr&gt;  &lt;df&gt;\n\nchickens &lt;- full_join(overall, breed_details)\nhead(chickens)\n## # A tibble: 6 √ó 15\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   &lt;chr&gt;                   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n## 1 Austra White            220-280 eggs pe‚Ä¶ Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         &lt;NA&gt;        \n## 3 Barnevelder             150-200 eggs pe‚Ä¶ Dark Brown  Poor         &lt;NA&gt;        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         &lt;NA&gt;        \n## 5 Barred Cochin Standard  110-160 eggs pe‚Ä¶ Brown       Very         &lt;NA&gt;        \n## 6 Barred Old English Ban‚Ä¶ 120 eggs per ye‚Ä¶ Cream       Good         &lt;NA&gt;        \n## # ‚Ñπ 10 more variables: Purpose &lt;chr&gt;, Broody &lt;chr&gt;, `Mating Ratio` &lt;chr&gt;,\n## #   `Roost Height` &lt;chr&gt;, name &lt;chr&gt;, description &lt;chr&gt;, breed_facts &lt;list&gt;,\n## #   availability &lt;list&gt;, videos &lt;list&gt;, reviews &lt;list&gt;\n\n\n\n\nimport pandas as pd\noverall = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails = pd.read_json(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\noverall['name'] = overall['Chicken Breed Name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D‚Äô\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n\ndetails = details.explode(['name', 'description'])\n\ndetails['name'] = details['name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D‚Äô\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n  \nchickens = overall.merge(details, how='outer', on = 'name',  indicator=True)\nchickens[(chickens._merge!='both')][['name', '_merge']]\n##                           name      _merge\n## 44                 Blue Polish  right_only\n## 53             Buff Ameraucana  right_only\n## 54            Buff Ameraucanas   left_only\n## 63         Cackles Red Broiler  right_only\n## 92        French Cuckoo Marans   left_only\n## 128                Red Broiler   left_only\n## 138                     Saipan  right_only\n## 139         Saipan Jungle Fowl   left_only\n## 162  Splash Old English Bantam   left_only\n## 176  White Crested Blue Polish   left_only\n## 185        White Plymouth Rock  right_only\n## 187                 White Rock   left_only\n\nchickens.head()\n##        Chicken Breed Name  ... _merge\n## 0            Austra White  ...   both\n## 1             Ayam Cemani  ...   both\n## 2             Barnevelder  ...   both\n## 3  Barred Cochin Standard  ...   both\n## 4    Barred Cochin Bantam  ...   both\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Cleaning Chicken Data\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nUnnest the chicken breed facts data, cleaning the responses. Which jobs are most suitable for a functional programming approach?\n\n\n\n# Column names in breed_facts are too different\n# chickens_exp &lt;- chickens |&gt; unnest('breed_facts', names_sep='facts')\n\nfix_names &lt;- function(df) {\n  if (!is.null(df)) {\n    names(df) &lt;- names(df) |&gt;\n      str_to_title() |&gt;\n      str_remove_all(\"[^A-z]\") |&gt; # Remove anything that isn't A-z, including spaces.\n      str_replace_all(c(\"CountryOfOrigin?\" = \"Origin\", \"Weights\" = \"Weight\", \"Tlc\" = \"TLC\", \"Albc\" = \"ALBC\", \"Apa\" = \"APA\", \"BroodyS\" = \"Broody\", \"Temperment\" = \"Temperament\", \"Broody\" = \"Broody_facts\", \"Purpose\" = \"Purpose_facts\")) |&gt;\n      str_remove_all(\"Shell|FarmSource|SourceFarm|Small|PoultryShow\") |&gt;\n      str_replace_all(\"^$\", \"xxx\") # replace blank names with xxx\n    df\n  } else {\n    return(NULL) \n  }\n}\nchickens_fix &lt;- chickens |&gt; \n  mutate(breed_facts = map(breed_facts, fix_names))\n\n# Test names\nchickens_fix$breed_facts %&gt;% map(names) |&gt; unlist() |&gt; unique()\n##  [1] \"Class\"                \"Weight\"               \"Purpose_factsAndType\"\n##  [4] \"EggColor\"             \"EggProduction\"        \"EggSize\"             \n##  [7] \"Temperament\"          \"GenderAccuracy\"       \"FertilityPercentage\" \n## [10] \"Broody_facts\"         \"MatingRatio\"          \"RoostHeight\"         \n## [13] \"Origin\"               \"APA\"                  \"TLC\"                 \n## [16] \"Breeder\"              \"Purpose_facts\"        \"xxx\"                 \n## [19] \"ALBC\"                 \"Rooster\"              \"Pullet\"              \n## [22] \"Cockerel\"             \"Exhibition\"\n\nWe‚Äôve fixed some of the misspellings and duplications. Rooster, Pullet, and Cockerel are all likely to be parsing issues stemming from Weight, but that‚Äôs the reality of working with data that is gathered from the internet.\n\nchickens_exp &lt;- chickens_fix |&gt; unnest(\"breed_facts\")\n\nhead(chickens_exp[,c(1, 16:37)])\n## # A tibble: 6 √ó 23\n##   `Chicken Breed Name`      EggProduction     EggSize Temperament GenderAccuracy\n##   &lt;chr&gt;                     &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;         \n## 1 Austra White              \"√Ç √Ç 220-280 egg‚Ä¶ √Ç Large √Ç Active    √Ç 80-85%      \n## 2 Ayam Cemani               \"√Ç 80-120 per ye‚Ä¶ &lt;NA&gt;    Docile      &lt;NA&gt;          \n## 3 Barnevelder               \"√Ç 150-200 eggs ‚Ä¶ √Ç Large √Ç Active    √Ç 80-85%      \n## 4 Barred Cochin Bantam      \"Egg Production:‚Ä¶ Egg Si‚Ä¶ √Ç Docile    75-80%        \n## 5 Barred Cochin Standard    \"Egg Production√Ç‚Ä¶ Egg Si‚Ä¶ √Ç Docile    &lt;NA&gt;          \n## 6 Barred Old English Bantam \"√Ç √¢\\u0080\\u0093‚Ä¶ Egg Si‚Ä¶ √Ç Active    &lt;NA&gt;          \n## # ‚Ñπ 18 more variables: FertilityPercentage &lt;chr&gt;, Broody_facts &lt;chr&gt;,\n## #   MatingRatio &lt;chr&gt;, RoostHeight &lt;chr&gt;, Origin &lt;chr&gt;, APA &lt;chr&gt;, TLC &lt;chr&gt;,\n## #   Breeder &lt;chr&gt;, Purpose_facts &lt;chr&gt;, xxx &lt;chr&gt;, ALBC &lt;chr&gt;, Rooster &lt;chr&gt;,\n## #   Pullet &lt;chr&gt;, Cockerel &lt;chr&gt;, Exhibition &lt;chr&gt;, availability &lt;list&gt;,\n## #   videos &lt;list&gt;, reviews &lt;list&gt;\n\nThere‚Äôs still quite a bit of cleaning left to do to get this data to be ‚Äúpretty‚Äù.\n\ntidy_col &lt;- function(x, text = \"(?:\\\\(estimates only, see FAQ\\\\))|(?:^APA)|(?:^TLC)|EggSize|(?:Fertility Percentage)|(?:Purpose and Type)\") {\n  str_remove_all(x, \"[\\u0600-\\u06FF]\") |&gt; # Remove non-ascii characters\n    str_remove_all(\"[√Ç¬Æ√¢¬Ñ¬¢√Ç√¢¬Ä¬ì]\") |&gt;\n    str_remove_all(text) |&gt;\n    str_remove_all(\"[:\\\\.\\\\?!\\\\*]\") |&gt;\n    str_replace_all(\"\\u0094\", \"-\") |&gt;\n    str_replace_all(\"-{1,}\", \"-\") |&gt;\n    str_squish()\n}\n\ntmp &lt;- mutate(chickens_exp, across(Class:Purpose_facts, tidy_col))\n\nhead(select(tmp, 1, Class:Purpose_facts))\n## # A tibble: 6 √ó 18\n##   `Chicken Breed Name`  Class Weight Purpose_factsAndType EggColor EggProduction\n##   &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;        \n## 1 Austra White          Not ‚Ä¶ Hen-5‚Ä¶ Primary production,‚Ä¶ Very li‚Ä¶ 220-280 eggs‚Ä¶\n## 2 Ayam Cemani           Not ‚Ä¶ Hen -‚Ä¶ &lt;NA&gt;                 Cream, ‚Ä¶ 80-120 per y‚Ä¶\n## 3 Barnevelder           Cont‚Ä¶ Weigh‚Ä¶ Egg Laying; Exhibit‚Ä¶ Dark Br‚Ä¶ 150-200 eggs‚Ä¶\n## 4 Barred Cochin Bantam  Feat‚Ä¶ Hen -‚Ä¶ Pets,Very Broody, O‚Ä¶ Egg She‚Ä¶ Egg Producti‚Ä¶\n## 5 Barred Cochin Standa‚Ä¶ Asia‚Ä¶ Weigh‚Ä¶ Ornamental and meat‚Ä¶ Brown    Egg Producti‚Ä¶\n## 6 Barred Old English B‚Ä¶ Game‚Ä¶ Hen -‚Ä¶ Ornamental; Exhibit‚Ä¶ Egg She‚Ä¶ Poor         \n## # ‚Ñπ 12 more variables: EggSize &lt;chr&gt;, Temperament &lt;chr&gt;, GenderAccuracy &lt;chr&gt;,\n## #   FertilityPercentage &lt;chr&gt;, Broody_facts &lt;chr&gt;, MatingRatio &lt;chr&gt;,\n## #   RoostHeight &lt;chr&gt;, Origin &lt;chr&gt;, APA &lt;chr&gt;, TLC &lt;chr&gt;, Breeder &lt;chr&gt;,\n## #   Purpose_facts &lt;chr&gt;\n\nIf we consider the use of across() as a functional programming technique (which it is), then it is much easier to create a generic tidy_col function than to tidy each column individually. There are probably a few things we‚Äôve missed, but the data looks decent for the amount of time we put in.\n\n\n\nimport pandas as pd\n\nXXX TODO",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#sec-functional-prog-refs",
    "href": "part-wrangling/08-functional-prog.html#sec-functional-prog-refs",
    "title": "27¬† Functional Programming",
    "section": "\n27.6 References",
    "text": "27.6 References\n\n\n\n\n[1] \nA. M. Kuchling, ‚ÄúFunctional programming HOWTO. cPython documentation,‚Äù Oct. 31, 2022. [Online]. Available: https://docs.python.org/3/howto/functional.html. [Accessed: Mar. 20, 2023]\n\n\n[2] \nR.-G. Urma, M. Fusco, and A. Mycroft, Modern java in action: Lambdas, streams, functional and reactive programming, 2nd edition. Shelter Island: Manning, 2018. \n\n\n[3] \nM. Fogus, Functional JavaScript: Introducing functional programming with underscore.js, 1st edition. Sebastopol, CA: O‚ÄôReilly Media, 2013. \n\n\n[4] \nE. Buonanno, Functional programming in c#: How to write better c# code, 1st edition. Shelter Island, NY: Manning, 2017. \n\n\n[5] \nE. Akin, Object-oriented programming via fortran 90-95. Cambridge University Press, 2003. \n\n\n[6] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[7] \nrituraj_jain, ‚ÄúNested list comprehensions in python. GeeksforGeeks,‚Äù Nov. 07, 2018. [Online]. Available: https://www.geeksforgeeks.org/nested-list-comprehensions-in-python/. [Accessed: Mar. 20, 2023]\n\n\n[8] \nHadley Wickham, Lionel Henry, and RStudio, ‚ÄúPurrr &lt;-&gt; base r. Purrr base r,‚Äù Oct. 10, 2022. [Online]. Available: https://purrr.tidyverse.org/articles/base.html. [Accessed: Apr. 10, 2023]\n\n\n[9] \nS. Seabold and J. Perktold, ‚ÄúStatsmodels: Econometric and statistical modeling with python,‚Äù in 9th python in science conference, 2010 [Online]. Available: https://proceedings.scipy.org/articles/Majora-92bf1922-011\n\n\n\n[10] \nF. Pedregosa et al., ‚ÄúScikit-learn: Machine learning in python,‚Äù Journal of Machine Learning Research, vol. 12, pp. 2825‚Äì2830, 2011. \n\n\n[11] \nA. Menon, ‚ÄúLinear Regression in 6 lines of Python,‚Äù Medium. Oct. 2018 [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[12] \nWikipedia contributors, ‚ÄúJSON,‚Äù Wikipedia. Apr. 05, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=JSON&oldid=1148380721. [Accessed: Apr. 10, 2023]",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "part-wrangling/09-spatial-formats.html",
    "href": "part-wrangling/09-spatial-formats.html",
    "title": "28¬† Spatial data",
    "section": "",
    "text": "28.1 References",
    "crumbs": [
      "Part III: Data Wrangling",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html",
    "href": "part-advanced-topics/01-simulation.html",
    "title": "29¬† Simulation and Reproducibility",
    "section": "",
    "text": "29.1  Objectives\nSimulation is an extremely important part of computational statistics. Bayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models. In this module, we‚Äôre going to touch on a few foundational pieces of simulation in computing, and you will get more exposure to simulation-based methods in other courses down the line.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#objectives",
    "href": "part-advanced-topics/01-simulation.html#objectives",
    "title": "29¬† Simulation and Reproducibility",
    "section": "",
    "text": "Program a simulation for a specific task, process, or model\nUnderstand the limitaitons of pseudorandom number generation",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#pseudorandom-number-generation",
    "href": "part-advanced-topics/01-simulation.html#pseudorandom-number-generation",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.2 Pseudorandom Number Generation",
    "text": "29.2 Pseudorandom Number Generation\nComputers are almost entirely deterministic, which makes it very difficult to come up with ‚Äúrandom‚Äù numbers. In addition to the deterministic nature of computing, it‚Äôs also somewhat important to be able to run the same code and get the same results every time, which isn‚Äôt possible if you rely on truly random numbers.\nHistorically, pseudorandom numbers were generated using linear congruential generators (LCGs) [1]. These algorithms aren‚Äôt typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem ‚Äúrandom‚Äù but are actually deterministic. LCGs use modular arithmetic: \\[X_{n+1} = (aX_n + c) \\mod m\\] where \\(X_0\\) is the start value (the seed), \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. When using a LCG, the user generally specifies only the seed.\n\n\nLCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia)\n\nThe important thing to note here is that if you specify the same generator values (\\(a\\), \\(c\\), \\(m\\), and \\(X_0\\)), you will always get the same series of numbers. Since \\(a\\), \\(c\\), \\(m\\) are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time.\n\n\n\n\n\n\nWarning\n\n\n\nIt is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.\n\n\nOnce you set your seed, the remaining results will only be reproducible if you generate the same set of random numbers every time.\n\n\nI once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds). We got brilliant results, but couldn‚Äôt ever reproduce them, because I hadn‚Äôt set the seed first and we never could figure out what the original seed was. üò≠\n\n\n\n\n\n\nExample: Setting Seeds for Reproducibility\n\n\n\n\n\nR\nPython\n\n\n\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(50))\n## [1] -0.1095366\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  4 57 69 10 76 15 67  1  3 91\n\n\n\n\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 500 std normal draws and take the mean\nnp.mean(rng.standard_normal(500))\n## -0.008197259441979758\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([33, 38,  3, 95,  3, 58, 79,  3, 77, 23])\n\n\n\n\nCompare the results above to these results:\n\n\nR\nPython\n\n\n\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(30))\n## [1] -0.1936645\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  49  37   6  34   9   3 100  43   7  29\n\n\n\n\nimport random\nimport numpy as np\n\n# Create a random generator with a specific seed\nrng = np.random.default_rng(342512)\n\n# Generate 10 integers\nrng.integers(low = 1, high = 100, size = 10)\n## array([18, 43, 71,  4, 35, 26, 41, 91, 42, 13])\n\n# Generate 30 std normal draws and take the mean\nnp.mean(rng.standard_normal(30))\n## 0.3016849078747997\n\n# Get 10 more numbers\nrng.integers(low = 1, high = 100, size = 10)\n## array([21, 49, 21, 99, 45,  1, 56, 70, 15, 82])\n\n\n\n\nNotice how the results have changed?\n\n\nTo make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I‚Äôve already set the seed earlier. This introduces certain ‚Äúfixed points‚Äù where results won‚Äôt change immediately after I‚Äôve re-set the seed. This is particularly important when I‚Äôm generating bootstrap estimates, fitting models, or simulating data for graphics experiments.\n\n\n\n\nIf you know, you know. If not, go watch some Dr.¬†Who sometime. Source\n\nPick your seed in any way you want. I tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people‚Äôs birthdays, the current time in hhmmss format‚Ä¶ basically, you can use anything.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#built-in-simulations-from-distributions",
    "href": "part-advanced-topics/01-simulation.html#built-in-simulations-from-distributions",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.3 Built-in simulations from distributions",
    "text": "29.3 Built-in simulations from distributions\nOften, we can get away with just simulating data from a known distribution. As both R and python are meant for statistical computing, this is extremely easy by design.\n\n\nR\nPython\n\n\n\nYou can see the various distribution options using ?Distributions. In general, dxxx is the PDF/PMF, pxxx is the CDF, qxxx is the quantile function, and rxxx gives you random nubmers generated from the distribution. (xxx, obviously, is whatever distribution you‚Äôre looking to use.)\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nset.seed(109025879)\n\ntibble(\n  norm = rnorm(500),\n  gamma = rgamma(500, shape = 3, scale = 1),\n  exp = rexp(500, rate = 1), # R uses a exp(-ax) \n  t = rt(500, df = 5),\n  chisq = rchisq(500, 5)\n) %&gt;%\n  pivot_longer(1:5, names_to = \"dist\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = \"free\", nrow = 1)\n\n\n\n\n\n\n\n\n\n\nimport random\nrandom.seed(109025879)\n\nimport pandas as pd\nimport numpy as np\n\nwide_df = pd.DataFrame({\n  \"norm\": np.random.normal(size=500),\n  \"gamma\": np.random.gamma(size=500, shape = 3, scale = 1),\n  \"exp\": np.random.exponential(size = 500, scale = 1),\n  \"t\": np.random.standard_t(df = 5, size = 500),\n  \"chisq\": np.random.chisquare(df = 5, size = 500)\n})\n\nlong_df = pd.melt(wide_df, id_vars = None, var_name = \"dist\", value_name = \"value\")\n\nfrom plotnine import *\n\n(ggplot(long_df, aes(x = \"value\")) + geom_density() + facet_wrap(\"dist\", scales=\"free\", nrow = 1))\n## &lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nGenerate variables x and y, where x is a sequence from -10 to 10 and y is equal to \\(x + \\epsilon\\), \\(\\epsilon \\sim N(0, 1)\\). Fit a linear regression to your simulated data (in R, lm, in Python, sklearn.linear_model‚Äôs LinearRegression).\nHint: Sample code for regression using sklearn [2].\n\n\n\nset.seed(20572983)\ndata &lt;- tibble(x = seq(-10, 10, .1), \n               y = x + rnorm(length(x)))\nregression &lt;- lm(y ~ x, data = data)\nsummary(regression)\n## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.14575 -0.70986  0.03186  0.65429  2.40305 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.01876    0.06869  -0.273    0.785    \n## x            0.99230    0.01184  83.823   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9738 on 199 degrees of freedom\n## Multiple R-squared:  0.9725, Adjusted R-squared:  0.9723 \n## F-statistic:  7026 on 1 and 199 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nrandom.seed(20572983)\n\ndata = pd.DataFrame({'x': np.arange(-10, 10, .1)})\ndata['y'] = data.x + np.random.normal(size = data.x.size)\n\n# Fitting the regression and predictions\n# scikit-learn requires that we reshape everything into\n# nparrays before we pass them into the model.fit() function.\nmodel = LinearRegression().\\\n  fit(data.x.values.reshape(-1, 1),\\\n      data.y.values.reshape(-1, 1))\ndata['pred'] = model.predict(data.x.values.reshape(-1, 1))\n\n# Plotting the results\nimport matplotlib.pyplot as plt\nplt.clf()\n\nplt.scatter(data.x, data.y)\nplt.plot(data.x, data.pred, color='red')\nplt.show()",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#simulation-to-test-model-assumptions",
    "href": "part-advanced-topics/01-simulation.html#simulation-to-test-model-assumptions",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.4 Simulation to test model assumptions",
    "text": "29.4 Simulation to test model assumptions\nOne of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model. Suppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors. How bad will your results be? Where can you trust the results, and where should you be cautious?\n\n\nThe purrr::map notation specifies that we‚Äôre using the map function from the purrr package. When functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function.\npurrr::map takes an argument and for each ‚Äúgroup‚Äù calls the compute_interval function, storing the results in res. So each row in res is a 1x2 tibble with columns lb and ub.\nThis pattern is very useful in all sorts of applications. You can read more about purrr in Chapter 27.\n\n\n\n\n\n\nExample: Confidence Interval coverage rates\n\n\n\n\n\nProblem\nSimulate Data\nFunction\nApply to Data\nResults\n\n\n\nSuppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our 25 observations.\nYou want to assess the coverage probability of a confidence interval computed under two different modeling scenarios:\n\nWorking with the log-transformed values, ln(x), and then transform the computed interval back\nWorking with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal.\n\nScenario 1:\n\nthe expected value of the standard normal deviates is 0\nthe variance of the data is 1\nthe SE(\\(\\overline x\\)) is \\(\\sqrt\\frac{1}{25} = \\frac{1}{5}\\)\n\n\nOur theoretical interval should be \\((\\exp(-1.96/5), \\exp(1.96/5)) = (0.6757, 1.4799)\\).\nScenario 2\n\nThe expected value of the lognormal distribution is \\(\\exp(1/2) = 1.6487213\\)\n\nThe variance of the data is \\((\\exp(1) - 1)(\\exp(1)) = 4.6707743\\)\n\nThe SE(\\(\\overline x\\)) is thus \\(\\sqrt{\\frac{(e^1 - 1)e^1}{25}} = \\frac{\\sqrt{(e^1 - 1)e^1}}{5} = 0.4322\\)\n\n\nOur theoretical interval should be \\((0.8015, 2.4959)\\). This interval could, if the circumstances were slightly different, contain 0, which is implausible for lognormally distributed data.\nOur expected values are different under scenario 1 and scenario 2:\n\nIn scenario 1 we are computing an interval for \\(\\mu\\)\n\nIn scenario 2, we are computing an interval for the population mean, which is \\(\\exp(\\mu + .5\\sigma^2)\\)\n\n\nBoth are valid quantities we might be interested in, but they do not mean the same thing.\n\n\n\nset.seed(40295023)\n\nsim &lt;- tibble(\n  id = rep(1:100, each = 25), # generate 100 samples of 25 points each\n  ln_x = rnorm(25*100), # generate 25 normal deviates for each sample\n  x = exp(ln_x), # transform into lognormal deviates\n) %&gt;%\n  # this creates a 100-row data frame, with one row for each id. \n  # the columns x, ln_x are stored in the data list-column as a tibble.\n  nest(data = c(x, ln_x))\n  \nhead(sim)\n## # A tibble: 6 √ó 2\n##      id data             \n##   &lt;int&gt; &lt;list&gt;           \n## 1     1 &lt;tibble [25 √ó 2]&gt;\n## 2     2 &lt;tibble [25 √ó 2]&gt;\n## 3     3 &lt;tibble [25 √ó 2]&gt;\n## 4     4 &lt;tibble [25 √ó 2]&gt;\n## 5     5 &lt;tibble [25 √ó 2]&gt;\n## 6     6 &lt;tibble [25 √ó 2]&gt;\nsim$data[[1]]\n## # A tibble: 25 √ó 2\n##        x    ln_x\n##    &lt;dbl&gt;   &lt;dbl&gt;\n##  1 0.310 -1.17  \n##  2 0.622 -0.475 \n##  3 0.303 -1.19  \n##  4 1.05   0.0525\n##  5 0.529 -0.636 \n##  6 1.09   0.0891\n##  7 1.97   0.676 \n##  8 8.94   2.19  \n##  9 0.598 -0.514 \n## 10 0.183 -1.70  \n## # ‚Ñπ 15 more rows\n\n\n\n\ncompute_interval &lt;- function(x) {\n  s1 &lt;- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x)))\n  s2 &lt;- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x))\n  tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n         mean = c(1, exp(1/2)),\n         lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]),\n         in_interval = (lb &lt; mean) & (ub &gt; mean))\n}\n\n\n\n\n\nsim_long &lt;- sim %&gt;%\n  # This line takes each data entry and computes an interval for x.\n  # .$x is code for take the argument you passed in to map and get the x column\n  mutate(res = purrr::map(data, ~compute_interval(.$x))) %&gt;%\n  # this \"frees\" res and we end up with two columns: lb and ub, for each scenario\n  unnest(res)\n  \n\nci_df &lt;- tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n                mu = c(1, exp(1/2)),\n                lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5),\n                ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5))\n\n\n\n\nggplot() + \n  geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), \n            data = ci_df,\n            fill = \"grey\", alpha = .5, color = NA) + \n  geom_vline(aes(xintercept = mu), data = ci_df) + \n  geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval),\n               data = sim_long) + \n  scale_color_manual(values = c(\"red\", \"black\")) + \n  theme_bw() + \n  facet_wrap(~scenario)\n\n\n\n\n\n\n\n\n\n\nFrom this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain.\n\n\n\n\nHere is a similar example worked through in SAS with IML. Note the use of BY-group processing to analyze each group at once - this is very similar to the use of purrr::map() in the R code.\n\n\n\n\n\n\nExample: Multilevel Regression and Post Stratification simulation\n\n\n\nMultilevel regression and post-stratification simulation with toddler bedtimes [3]\nThis example talks about how to take a biased sample and then recover the original unbiased estimates ‚Äì which is something you have to test using simulation to be sure it works, because you never actually know what the true population features are when you are working with real world data. When reading this example, you may not be all that interested with the specific model - but focus on the process of simulating data for your analysis so that you understand how and why you would want to simulate data in order to test a computational method.\n\n\n\n\n\n\n\n\nExample: Regression and high-leverage points\n\n\n\nWhat happens if we have one high-leverage point (e.g.¬†a point which is an outlier in both x and y)? How pathological do our regression coefficient estimates get?\nThe challenging part here is to design a data generating mechanism.\n\n\nData Generation\nData Checking\nModel Fitting\nResults\n\n\n\n\ngen_data &lt;- function(o = 1, n = 30, error_sd = 2) {\n  # generate the main part of the regression data\n  data &lt;- tibble(x = rnorm(n = n - o, \n                           mean = seq(-10, 10, length.out = n - o), \n                           sd = .1),\n                 y = x + rnorm(length(x), \n                               mean = 0, \n                               sd = error_sd))\n  # generate the outlier - make it at ~(-10, 5)\n  outdata &lt;- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd))\n  bind_rows(data, outdata)\n}\n\nsim_data &lt;- crossing(id = 1:100, outliers = 0:2) %&gt;%\n  mutate(\n  # call gen_data for each row in sim_data, \n  # but don't use id as a parameter.\n  data = purrr::map(outliers, gen_data) \n)\n\n\n\n\nhead(sim_data)\n## # A tibble: 6 √ó 3\n##      id outliers data             \n##   &lt;int&gt;    &lt;int&gt; &lt;list&gt;           \n## 1     1        0 &lt;tibble [30 √ó 2]&gt;\n## 2     1        1 &lt;tibble [30 √ó 2]&gt;\n## 3     1        2 &lt;tibble [30 √ó 2]&gt;\n## 4     2        0 &lt;tibble [30 √ó 2]&gt;\n## 5     2        1 &lt;tibble [30 √ó 2]&gt;\n## 6     2        2 &lt;tibble [30 √ó 2]&gt;\n\n# plot a few datasets just to check they look like we expect:\nsim_data %&gt;%\n  filter(id %% 100 &lt; 3) %&gt;%\n  unnest(data) %&gt;%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  facet_grid(id ~ outliers, labeller = label_both)\n\n\n\n\n\n\n\n\n\n\nlibrary(broom) # the broom package cleans up model objects to tidy form\n\nsim_data &lt;- sim_data %&gt;%\n  # fit linear regression\n  mutate(model = purrr::map(data, ~lm(y ~ x, data = .)))  %&gt;%\n  mutate(tidy_model = purrr::map(model, tidy))\n\n\n\n\n# Get the coefficients out\ntidy_coefs &lt;- select(sim_data, id, outliers, tidy_model) %&gt;%\n  unnest(tidy_model) %&gt;%\n  mutate(group = case_when(outliers == 0 ~ \"No HLPs\",\n                           outliers == 1 ~ \"1 HLP\",\n                           outliers == 2 ~ \"2 HLPs\") %&gt;%\n           factor(levels = c(\"No HLPs\", \"1 HLP\", \"2 HLPs\")))\n\nggplot(tidy_coefs, aes(x = estimate, color = group)) + \n  facet_grid(term ~ .) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\nObviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0).\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nHint\nGeneral Solution\nR Code\nPython Code\n\n\n\nLet‚Äôs explore what happens to estimates when certain observations are censored.\nSuppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102\\(^\\circ F\\); for these temperatures, the thermometer will record a value of 102.0.\nIt is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness. Given that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer‚Äôs censoring.\n\n\nIf most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution? What if you assume that 101 and 102.5 are the 2SD bounds?\n\n\nIf 101 and 102.5 are the anchor points we have, let‚Äôs assume that 95% of normal pet temperatures fall in that range. So our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375.\nWe can simulate 1000 observations from \\(N(101.75, 0.375)\\), create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are.\n\n\n\nset.seed(204209527)\ndogtemp &lt;- tibble(\n  actual = rnorm(1000, 101.75, 0.375),\n  read = pmin(actual, 102)\n) \ndogtemp %&gt;%\n  summarize_all(mean) %&gt;%\n  diff()\n## Error in r[i1] - r[-length(r):-(length(r) - lag + 1L)]: non-numeric argument to binary operator\n\nThe effect of the thermometer‚Äôs censoring is around 0.06 degrees F for animals that are not ill.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(204209527)\ndogtemp = pd.DataFrame({\n  \"actual\": np.random.normal(size = 1000, loc = 101.75, scale = 0.375)\n})\ndogtemp['read'] = np.minimum(dogtemp.actual, 102)\n\nnp.diff(dogtemp.mean())\n## array([-0.05730939])\n\nThe effect of the thermometer‚Äôs censoring is around 0.06 degrees F for animals that are not ill.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#monte-carlo-methods",
    "href": "part-advanced-topics/01-simulation.html#monte-carlo-methods",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.5 Monte Carlo methods",
    "text": "29.5 Monte Carlo methods\nMonte carlo methods [4] are methods which rely on repeated random sampling in order to solve numerical problems. Often, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically.\nIn general, a MC problem involves these steps:\n\nDefine the input domain\nGenerate inputs randomly from an appropriate probability distribution\nPerform a computation using those inputs\nAggregate the results.\n\n\n\n\n\n\n\nExample: Sum of Uniform Random Variables\n\n\n\n\n\n\n\nProblem\nDefining Steps\nR Code\nPython Code\nLearn More\n\n\n\nLet‚Äôs try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1.\nMore precisely, if \\(u_i \\sim U(0,1)\\), where _{i=1}^k u_i &gt; 1, what is the expected value of \\(k\\)?\n\n\n\nIn this simulation, our input domain is [0,1].\nOur input is \\(u_i \\sim U(0,1)\\)\n\nWe generate new \\(u_i\\) until \\(\\sum_{i=1}^k &gt; 1\\) and save the value of \\(k\\)\n\nWe average the result of \\(N\\) such simulations.\n\n\n\n\n# It's easier to think through the code if we write it inefficiently first\nsim_fcn &lt;- function() {\n  usum &lt;- 0\n  k &lt;- 0\n  # prevent infinite loops by monitoring the value of k as well\n  while (usum &lt; 1 & k &lt; 15) {\n    usum &lt;- runif(1) + usum\n    k &lt;- k + 1\n  }\n  return(k)\n}\n\nset.seed(302497852)\nres &lt;- tibble(k = replicate(1000, sim_fcn(), simplify = T))\n\nmean(res$k)\n## [1] 2.717\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nset.seed(20417023)\n\nsim_res &lt;- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) \n\nsim_res &lt;- sim_res %&gt;%\n  mutate(running_avg_est = cummean(samp),\n         N = row_number())\n\nggplot(aes(x = N, y = running_avg_est), data = sim_res) + \n  geom_hline(yintercept = exp(1), color = \"red\") + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport random\nimport pandas as pd\n\n\ndef sim_fcn():\n  usum = 0\n  k = 0\n  # prevent infinite loops by monitoring the value of k as well\n  while usum &lt; 1 and k &lt; 15:\n    # print(\"k = \", k)\n    usum = np.random.uniform(size=1) + usum\n    k += 1\n  return k\n\nrandom.seed(302497852)\nres = pd.DataFrame({\"k\": [sim_fcn() for _ in range(1000)]})\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nrandom.seed(20417023)\n\nsim_res = pd.DataFrame({\"k\": [sim_fcn() for _ in range(250000)]})\nsim_res['running_avg_est'] = sim_res.k.expanding().mean()\nsim_res['N'] = np.arange(len(sim_res))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = sim_res, x = 'N', y = 'running_avg_est', color = \"black\")\ngraph.axhline(y = np.exp(1), xmin = 0, xmax = 1, color = \"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe expected number of uniform RV draws required to sum to 1 is \\(e\\)!\nExplanation of why this works\n\n\n\n\n\n\nMonte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods).\n\n\n\n\n\n\nExample: Integration\n\n\n\n\n\nProblem\nR Code\nPython Code\nRiemann R Code\n\n\n\nSuppose you want to integrate \\[\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) dx\\]\n\n\n\n\nf(x) over the interval [0,1].\n\n\n\nYou could set up Riemann integration and evaluate the integral using a sum over \\(K\\) points, but that approach only converges for smooth functions (and besides, that‚Äôs boring Calc 2 stuff, right?).\nInstead, let‚Äôs observe that this is equivalent to \\(\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) \\cdot 1 dx\\), where \\(p(x) = 1\\) for a uniform random variable. That is, this integral can be written as the expected value of the function over the interval \\([0,1]\\). What if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at those points, and average the result?\nYou can use the law of large numbers to prove that this approach will converge. [5]\n\n\n\nset.seed(20491720)\nfn &lt;- function(x)  x^2 * sin(1/x)\n\nsim_data &lt;- tibble(x = runif(100000),\n                   y = fn(x))\nmean(sim_data$y)\n## [1] 0.28607461\n\n\n\n\nrandom.seed(20491720)\n\ndef fn(x):\n  return x**2 * np.sin(1/x)\n\nsim_data = pd.DataFrame({\"x\": np.random.uniform(size = 100000)})\nsim_data['y'] = fn(sim_data.x)\n\nsim_data.y.mean()\n## 0.2859042950973808\n\n\n\n\nfn &lt;- function(x)  x^2 * sin(1/x)\n\nriemann &lt;- tibble(x = seq(0, 1, length.out = 10000)[-1],\n                  y = fn(x))\nmean(riemann$y)\n## [1] 0.28657161\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Integration in 2d\n\n\n\n\n\nProblem\nR Code\nPython Code\nNumeric integration\n\n\n\nLet‚Äôs say that you want to find an estimate for \\(\\pi\\), and you know that a circle with radius 1 has an area of exactly that. You also know, that all of the points on this circle can be written as \\(x^2 + y^2 \\le 1\\).\n\n\n\n\nThe unit circle.\n\n\n\nEvaluating the area of the circle mathematically, would need us to either change to polar-coordinates or separate the graph into suitable functions (half-circles), and evaluate the integral between the top and the bottom: \\[\n\\int_{-1}^1 2 \\sqrt{1-x^2} dx\n\\] Instead, we note that the circle is encapsulated in a square with side length 2. We can reach all points in that square by using two independent uniform random random variables over the interval \\([-1,1]\\), i.e.¬†when we generate two random values from U[-1,1], and use one as the \\(x\\) coordinate and one as the \\(y\\) coordinate, we get a point in the square. If the sum of the squares of the coordinates are less than 1, the point will also fall inside the circle. If not, the point falls in one of the four corners of the square that are outside the circle.\n\n\n\n\nThe unit circle is encapsulated by a square and overlaid with uniform points from U[-1,1] x U[-1,1].\n\n\n\nHow do we get to an estimate of \\(\\pi\\) from there? We know that the area of the square is simply \\(2^2 = 4\\). The area of the circle is then directly proportional to the rate at which points fall into the circle, ie.\n\\[\n\\hat{\\pi} = 4 \\times \\frac{\\text{Number of points with } x^2+y^2 \\le 1}{\\text{Number of points generated}}.\n\\] The more points we generate, the closer our estimate will be to the real value.\nThis problem is an example for Monte-Carlo Integration using an Acceptance-Rejection approach: we can slightly re-write the simulation and think of the generation of a new point in the circle as a two step process, where we first generate a value for \\(x\\) from U[-1,1], and in second step generate a candidate \\(c\\) for \\(y\\) from U[-1, 1], which we will only accept as \\(y\\), if \\(|c| \\le \\sqrt{1-x^2}\\). Acceptance-Rejection sampling is the basis of a lot of Markov-Chain Monte-Carlo (MCMC) methods, such as e.g.¬†the Metropolis-Hastings algorithm.\n\n\n\nset.seed(20491720)\n\ncalculate_pi &lt;- function(R) {\n  x = runif(R, min=-1, max=1)\n  y = runif(R, min=-1, max=1)\n  in_circle = x^2+y^2&lt;1\n  \n  4 * sum(in_circle) / R\n}\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## [1] 3.16\ncalculate_pi(100)\n## [1] 3.2\ncalculate_pi(100)\n## [1] 2.96\n\n# Better with 10,000\ncalculate_pi(10000)\n## [1] 3.126\ncalculate_pi(10000)\n## [1] 3.1392\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## [1] 3.14344\n\npi\n## [1] 3.1415927\n\n\n\n\nrandom.seed(20491720)\n\ndef calculate_pi(R):\n  x = np.random.uniform(size = R)\n  y = np.random.uniform(size = R)\n  in_circle = x**2+y**2&lt;1\n  \n  return 4 * sum(in_circle) / R\n\n\n# Quite a bit of variability with just 100 values\ncalculate_pi(100)\n## 3.2\ncalculate_pi(100)\n## 3.2\ncalculate_pi(100)\n## 3.2\n\n# Better with 10,000\ncalculate_pi(10000)\n## 3.1652\ncalculate_pi(10000)\n## 3.14\n\n# Better, but still only good for about 2-3 digits\ncalculate_pi(1000000) \n## 3.14472\n\nnp.pi\n## 3.141592653589793\n\n\n\n\nset.seed(20491720)\nfn &lt;- function(x)  2*sqrt(1-x^2)\n\nintegrate(fn, lower=-1, upper=1)\n## 3.1415927 with absolute error &lt; 2e-09\n\npi\n## [1] 3.1415927\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR code\nPython Code\n\n\n\nBuffon‚Äôs needle is a mathematical problem which can be boiled down to a simple physical simulation. Read this science friday description of the problem and develop a monte carlo simulation method which estimates \\(\\pi\\) using the Buffon‚Äôs needle method. Your method should be a function which\n\nallows the user to specify how many sticks are dropped\nplots the result of the physical simulation\nprints out a numerical estimate of pi.\n\n\n\nLet‚Äôs start out with horizontal lines at 0 and 1, and set our stick length to 1. We need to randomly generate a position (of one end of the stick) and an angle. The position in \\(x\\) doesn‚Äôt actually make much of a difference (since what we care about is the \\(y\\) coordinates), but we can draw a picture if we generate \\(x\\) as well.\n\nneedle_sim &lt;- function(sticks = 100) {\n  df &lt;- tibble(xstart = runif(sticks, 0, 10), \n         ystart = runif(sticks, 0, 1), \n         angle = runif(sticks, 0, 360),\n         xend = xstart + cos(angle/180*pi), \n         yend = ystart + sin(angle/180*pi)\n  ) %&gt;%\n    # We can see if a stick crosses a line if the floor() function of ystart is \n    # different than floor(yend). \n    # Note this only works for integer line values.\n  mutate(crosses_line = floor(ystart) != floor(yend)) \n  \n  \n  gg &lt;- ggplot() + \n  geom_hline(yintercept = c(0, 1)) + \n  geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend,\n                   color = crosses_line), data = df) + \n  coord_fixed()\n  \n  return(list(est = 2 * sticks / sum(df$crosses_line), plot = gg))\n}\n\nneedle_sim(10)\n## $est\n## [1] 3.3333333\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(100)\n## $est\n## [1] 3.125\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(1000)\n## $est\n## [1] 3.1446541\n## \n## $plot\n\n\n\n\n\n\n\nneedle_sim(10000)\n## $est\n## [1] 3.1730922\n## \n## $plot\n\n\n\n\n\n\n\n\n\n\ndef needle_sim(sticks = 100):\n  df = pd.DataFrame({\n    \"xstart\": np.random.uniform(0, 10, size = sticks),\n    \"ystart\": np.random.uniform(0, 1, size = sticks),\n    \"angle\": np.random.uniform(0, 360, size = sticks)\n  })\n  \n  df['xend'] = df.xstart + np.cos(df.angle/180*np.pi)\n  df['yend'] = df.ystart + np.sin(df.angle/180*np.pi)\n  df['crosses_line'] = np.floor(df.ystart) != np.floor(df.yend)\n  \n  return df\n\ndata = needle_sim(100000)\ndata['N'] = np.arange(len(data)) + 1\ndata['cum_est'] = 2*data.N / data.crosses_line.expanding().sum()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = data, x = \"N\", y = \"cum_est\", color = \"black\")\ngraph.axhline(y = np.pi, xmin = 0, xmax = 1, color = \"red\")\nplt.show()",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#other-resources",
    "href": "part-advanced-topics/01-simulation.html#other-resources",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.6 Other Resources",
    "text": "29.6 Other Resources\n\nSimulation (R programming for Data Science chapter)\nSimulation - R Studio lesson\nSimulation, focusing on statistical modeling (R)\nSimulating Data with SAS (Excerpt)\nSimulating a Drunkard‚Äôs Walk in 2D in SAS\nSimulation from a triangle distribution (SAS)\nSimulating the Monty Hall problem (SAS)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/01-simulation.html#references",
    "href": "part-advanced-topics/01-simulation.html#references",
    "title": "29¬† Simulation and Reproducibility",
    "section": "\n29.7 References",
    "text": "29.7 References\n\n\n\n\n[1] \nWikipedia contributors, ‚ÄúLinear congruential generator.‚Äù Sep. 09, 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Linear_congruential_generator&oldid=1109285432\n\n\n\n[2] \nA. Menon, ‚ÄúLinear Regression in 6 lines of Python,‚Äù Medium. Oct. 2018 [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[3] \nR. Alexander, ‚ÄúTelling stories with data,‚Äù Jul. 27, 2023. [Online]. Available: https://tellingstorieswithdata.com/. [Accessed: Aug. 01, 2023]\n\n\n[4] \nWikipedia contributors, ‚ÄúMonte Carlo method,‚Äù Wikipedia. Oct. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Monte_Carlo_method&oldid=1116159451. [Accessed: Oct. 17, 2022]\n\n\n[5] \nY.-C. Chen, ‚ÄúLecture 2: Monte Carlo Simulation,‚Äù Monte Carlo Simulation. 2017 [Online]. Available: http://faculty.washington.edu/yenchic/17Sp_403/Lec2_MonteCarlo.pdf",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Simulation and Reproducibility</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html",
    "href": "part-advanced-topics/06-interactive-graphics.html",
    "title": "30¬† Interactive Graphics",
    "section": "",
    "text": "31 Animated and Interactive Graphics\nInteractive and animated graphics are one of the major advantages of using the Rmarkdown ecosystem - because you can easily create web pages in markdown (without the pain of HTML), you aren‚Äôt limited by paper any more. We‚Äôll cover two different technologies that allow you to create different types of interactive charts, graphs, and interfaces.\nIt is helpful to think about interactivity in a couple of different ways:\n(This is not a full list of all of the types of interactivity, just a few of the more common options)\nIn this section, we‚Äôll cover two ways to easily create interactive graphics or applets in R and python. There are, of course, many others ‚Äì many javascript libraries have extensions to R or python that may facilitate creating interactive graphics.",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html#objectives",
    "href": "part-advanced-topics/06-interactive-graphics.html#objectives",
    "title": "30¬† Interactive Graphics",
    "section": "\n31.1  Objectives",
    "text": "31.1  Objectives\n\nCreate animated and interactive charts using appropriate tools",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html#plotly",
    "href": "part-advanced-topics/06-interactive-graphics.html#plotly",
    "title": "30¬† Interactive Graphics",
    "section": "\n31.2 Plotly",
    "text": "31.2 Plotly\nPlotly PlotlyOpenSource2022? is a graphing library that uses javascript to add interactivity to graphics. There are several different ways to create plotly graphs in R or python. Here, we‚Äôll discuss 3 approaches: - Working with plotly in R directly - Working with plotly in python directly - Using ggplotly, which converts a ggplot to a plotly plot automatically\nResources:\n\nR Plotly cheat sheet\nPython Plotly cheat sheet\n\nWe‚Äôll demonstrate plotly‚Äôs capabilities using the volcanoes data from Tidy Tuesday.\n\n\n\n\n\n\nData set up\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"plotly\" %in% installed.packages()) \n  install.packages(\"plotly\")\n\nlibrary(plotly)\n\n\nlibrary(readr) # reading in data\nlibrary(dplyr) # cleaning data\nlibrary(tidyr) # merging data\nlibrary(lubridate) # dates and times\nlibrary(stringr) # string manipulation\nlibrary(ggplot2) # plotting\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano &lt;- read_csv(paste0(url_stub, \"volcano.csv\"))\neruptions &lt;- read_csv(paste0(url_stub, \"eruptions.csv\"))\nevents &lt;- read_csv(paste0(url_stub, \"events.csv\"))\nsulfur &lt;- read_csv(paste0(url_stub, \"sulfur.csv\"))\ntrees &lt;- read_csv(paste0(url_stub, \"tree_rings.csv\"))\n\n\n\n\n# Uncomment and run this line if you don't have plotly installed\n# %pip install plotly\n\nimport plotly.express as px\nimport plotly.io as pio # this allows plotly to play nice with markdown\n\nimport pandas as pd\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano = pd.read_csv(url_stub + \"volcano.csv\")\neruptions = pd.read_csv(url_stub + \"eruptions.csv\")\nevents = pd.read_csv(url_stub + \"events.csv\")\nsulfur = pd.read_csv(url_stub + \"sulfur.csv\")\ntrees = pd.read_csv(url_stub + \"tree_rings.csv\")\n\n\n\n\n\n\n\nLet‚Äôs try out plotly while doing a bit of exploratory data analysis on this dataset.\n\n\n\n\n\n\nCleaning up volcano\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nvolcano &lt;- volcano %&gt;%\n  filter(tectonic_settings != \"Unknown\") %&gt;%\n  separate(tectonic_settings, into = c(\"zone\", \"crust\"), \n           sep = \"/\", remove = F) %&gt;%\n  # Remove anything past the first punctuation character \n  # catch (xx) and ?\n  mutate(volcano_type = str_remove(primary_volcano_type, \n                                   \"[[:punct:]].*$\"))\n\n\n\n\nvolcano2 = volcano.query(\"tectonic_settings != 'Unknown'\")\nvolcano2[['zone', 'crust']] = volcano2.tectonic_settings.\\\n                              str.split(\" / \", expand = True)\n# Remove anything after ( as well as ? if it exists\nvolcano2 = volcano2.assign(volcano_type =\n            volcano2['primary_volcano_type'].\\\n            str.replace(r\"(\\(.*)?\\??$\", \"\", regex = True))\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship Between Elevation and Zone\n\n\n\n\n\nLet‚Äôs start by seeing whether the elevation of a volcano changes based on the type of zone it‚Äôs on - we might expect that Rift zone volcanos (where plates are pulling away from each other) might not be as high.\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = zone, fill = zone, y = elevation)) +\n  geom_boxplot() +\n  coord_flip()\nggplotly(p)\n\n\n\n\n\n\n\nThe plot_ly function is pipe friendly.\nVariable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\nlibrary(plotly)\nplot_ly(volcano, color= ~zone, x = ~elevation, type = \"box\")\n\n\n\n\n\n\n\n\nfig = px.box(volcano2, x = \"elevation\", color = \"zone\")\n\nfile = 'plotly-python/boxplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n## NameError: name 'pl' is not defined\n\n\n## Error: object 'py' not found\n\n\n\n\nIt doesn‚Äôt really look like there‚Äôs much difference.\n\n\n\n\n\n\n\n\n\nExamining Volcano Type\n\n\n\n\n\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug()\nggplotly(p)\n\n\n\n\n\n\n\nSince I‚Äôm trying to do this without the tidyverse, I‚Äôll try out the new base R pipe, |&gt;, and the corresponding new anonymous function notation, \\().1\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type))\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly as pl\n\n# This creates a list of vectors, one for each type of volcano\nvolcano = volcano.groupby(\"volcano_type\")[\"elevation\"].count()\n## KeyError: 'volcano_type'\ntype_list = volcano.groupby(\"volcano_type\").elevation.apply(list)\n## KeyError: 'volcano_type'\ntype_labels = volcano.volcano_type.unique()\n## AttributeError: 'DataFrame' object has no attribute 'volcano_type'\nfig = ff.create_distplot(type_list, group_labels = type_labels, show_hist=False)\n## NameError: name 'type_list' is not defined\n\nfile = 'plotly-python/distplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nHere, the interactivity actually helps a bit: we don‚Äôt need to use the legend to see what each curve corresponds to. We can see that submarine volcanoes are typically much lower in elevation (ok, duh), but also that subglacial volcanoes are found in a very limited range. If we double-click on a legend entry, we can get rid of all other curves and examine each curve one by one.\nI added the rug layer after the initial bout because I was curious how much data each of these curves were based on. If we want only curves with n &gt; 10 observations, we can do that:\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug(aes(text = paste0(volcano_name, \", \", country)))\nggplotly(p)\n\n\n\n\n\nIf we want to specify additional information that should show up in the tooltip, we can do that as well by adding the text aesthetic even though geom_rug doesn‚Äôt take a text aesthetic. You may notice that ggplot2 complains about the unknown aesthetic I‚Äôve added to geom_rug: That allows us to mouse over each data point in the rug plot and see what volcano it belongs to. So we can tell from the rug plot that the tallest volcano is Ojas de Salvado, in Chile/Argentina (I believe that translates to Eyes of Salvation?).\n\n\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n    tmp$n = nrow(df)\n    tmp\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type)) |&gt;\n  subset(n &gt; 15)\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, \n        type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\nvolcano = volcano.assign(count = volcano.groupby(\"volcano_type\").\\\n                                  volcano_type.transform(\"count\"))\n## KeyError: 'volcano_type'\n\ncommon_volcano = volcano.query(\"count &gt; 15\").sort_values([\"volcano_type\"])\n## pandas.errors.UndefinedVariableError: name 'count' is not defined\ncommon_volcano[\"label\"] = common_volcano.volcano_name + \", \" + common_volcano.country\n## NameError: name 'common_volcano' is not defined\n# This creates a list of vectors, one for each type of volcano\ntype_list = common_volcano.groupby(\"volcano_type\").elevation.apply(list)\n## NameError: name 'common_volcano' is not defined\n# rug_text = common_volcano.groupby(\"volcano_type\").label.apply(list)\ntype_labels = common_volcano.volcano_type.unique()\n## NameError: name 'common_volcano' is not defined\nfig = ff.create_distplot(type_list, group_labels = type_labels, rug_text = rug_text, show_hist=False)\n## NameError: name 'type_list' is not defined\nfig.show()\n\n                                    \n\n\n\n\n\n\n\n\n\nAt any rate, there isn‚Äôt nearly as much variation as I was expecting in the elevation of different types of volcanoes.\nggplotly makes it very easy to generate plots that have a ggplot2 equivalent; you can customize these plots further using plotly functions that we‚Äôll see in the next section. But first, try the interface out on your own.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nggplotly\nR + plotly\nPython\n\n\n\nConduct an exploratory data analysis of the eruptions dataset. What do you find?\n\n\n\nhead(eruptions)\n## # A tibble: 6 √ó 15\n##   volcano_number volcano_name eruption_number eruption_category area_of_activity\n##            &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n## 1         266030 Soputan                22354 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 2         343100 San Miguel             22355 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 3         233020 Fournaise, ‚Ä¶           22343 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 4         345020 Rincon de l‚Ä¶           22346 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 5         353010 Fernandina             22347 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 6         273070 Taal                   22344 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## # ‚Ñπ 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;,\n## #   start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;,\n## #   end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\nsummary(eruptions %&gt;% mutate(eruption_category = factor(eruption_category)))\n##  volcano_number   volcano_name       eruption_number\n##  Min.   :210010   Length:11178       Min.   :10001  \n##  1st Qu.:263310   Class :character   1st Qu.:12817  \n##  Median :290050   Mode  :character   Median :15650  \n##  Mean   :300284                      Mean   :15667  \n##  3rd Qu.:343030                      3rd Qu.:18464  \n##  Max.   :600000                      Max.   :22355  \n##                                                     \n##             eruption_category area_of_activity        vei       \n##  Confirmed Eruption  :9900    Length:11178       Min.   :0.000  \n##  Discredited Eruption: 166    Class :character   1st Qu.:1.000  \n##  Uncertain Eruption  :1112    Mode  :character   Median :2.000  \n##                                                  Mean   :1.948  \n##                                                  3rd Qu.:2.000  \n##                                                  Max.   :7.000  \n##                                                  NA's   :2906   \n##    start_year        start_month       start_day      evidence_method_dating\n##  Min.   :-11345.0   Min.   : 0.000   Min.   : 0.000   Length:11178          \n##  1st Qu.:   680.0   1st Qu.: 0.000   1st Qu.: 0.000   Class :character      \n##  Median :  1847.0   Median : 1.000   Median : 0.000   Mode  :character      \n##  Mean   :   622.8   Mean   : 3.451   Mean   : 7.015                         \n##  3rd Qu.:  1950.0   3rd Qu.: 7.000   3rd Qu.:15.000                         \n##  Max.   :  2020.0   Max.   :12.000   Max.   :31.000                         \n##  NA's   :1          NA's   :193      NA's   :196                            \n##     end_year      end_month         end_day         latitude      \n##  Min.   :-475   Min.   : 0.000   Min.   : 0.00   Min.   :-77.530  \n##  1st Qu.:1895   1st Qu.: 3.000   1st Qu.: 4.00   1st Qu.: -6.102  \n##  Median :1957   Median : 6.000   Median :15.00   Median : 17.600  \n##  Mean   :1917   Mean   : 6.221   Mean   :13.32   Mean   : 16.866  \n##  3rd Qu.:1992   3rd Qu.: 9.000   3rd Qu.:21.00   3rd Qu.: 40.821  \n##  Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   : 85.608  \n##  NA's   :6846   NA's   :6849     NA's   :6852                     \n##    longitude      \n##  Min.   :-179.97  \n##  1st Qu.: -77.66  \n##  Median :  55.71  \n##  Mean   :  31.57  \n##  3rd Qu.: 139.39  \n##  Max.   : 179.58  \n## \n\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet‚Äôs start out seeing what month most eruptions occur in‚Ä¶\n\n# Note, I'm using the original month, so 0 = unknown\np &lt;- ggplot(erupt, aes(x = factor(start_month))) + geom_bar()\nggplotly(p)\n\n\n\n\n# I could rename some of the factors to make this pretty, but... nah\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St.¬†Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn‚Äôt happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index,\np &lt;- ggplot(erupt, aes(x = vei)) + geom_bar()\nggplotly(p)\n\n\n\n\n\nWe can also look at the frequency of eruptions over time. We‚Äôll expect some historical bias - we don‚Äôt have exact dates for some of these eruptions, and if no one was around to write the eruption down (or the records were destroyed) there‚Äôs not going to be a date listed here.\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  filter(start_year &gt; 0) %&gt;%\n\nggplot(aes(x = start_date, xend = start_date,\n                  y = 0, yend = duration,\n                  color = evidence_method_dating)) +\n  geom_segment() +\n  geom_point(size = .5, aes(text = volcano_name)) +\n  xlab(\"Eruption Start\") +\n  ylab(\"Eruption Duration (days)\") +\n  facet_wrap(~vei, scales = \"free_y\")\nggplotly(p)\n\n\n\n\n\nAs expected, it‚Äôs pretty rare to see many eruptions before ~1800 AD, which is about when we have reliable historical records2 for most of the world (exceptions include e.g.¬†Vestuvius, which we have extensive written information about).\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\nggplot(aes(x = factor(vei), y = duration)) +\n  geom_violin() +\n  xlab(\"VEI\") +\n  ylab(\"Eruption Duration (days)\") +\n  scale_y_sqrt()\nggplotly(p)\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren‚Äôt that many of them (thankfully).\n\n\n\nhead(eruptions)\n## # A tibble: 6 √ó 15\n##   volcano_number volcano_name eruption_number eruption_category area_of_activity\n##            &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n## 1         266030 Soputan                22354 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 2         343100 San Miguel             22355 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 3         233020 Fournaise, ‚Ä¶           22343 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 4         345020 Rincon de l‚Ä¶           22346 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 5         353010 Fernandina             22347 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## 6         273070 Taal                   22344 Confirmed Erupti‚Ä¶ &lt;NA&gt;            \n## # ‚Ñπ 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;,\n## #   start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;,\n## #   end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet‚Äôs start out seeing what month most eruptions occur in‚Ä¶\n\n# Note, I'm using the original month, so 0 = unknown\nerupt %&gt;%\n  count(start_month) %&gt;%\n  plot_ly(\n    data = .,\n    x = ~start_month,\n    y = ~n,\n    type = \"bar\"\n)\n\n\n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St.¬†Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn‚Äôt happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nerupt %&gt;%\n  count(vei) %&gt;%\n  plot_ly(x = ~vei, y = ~n, type = \"bar\")\n\n\n\n\n\n\nerupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\n  plot_ly(x = ~ factor(vei),\n          y = ~ duration, \n          split = ~factor(vei),\n          type = \"violin\") %&gt;%\n  layout(yaxis = list(type=\"log\"))\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren‚Äôt that many of them (thankfully).\n\n\nIn Python, negative dates are even more of a pain to work with if you‚Äôre using standard libraries, so we‚Äôll install the astropy class with pip install astropy. BCE dates are still a pain in the ‚Ä¶ but they at least work.\n\neruptions.head()\n##    volcano_number            volcano_name  ...  latitude longitude\n## 0          266030                 Soputan  ...     1.112   124.737\n## 1          343100              San Miguel  ...    13.434   -88.269\n## 2          233020  Fournaise, Piton de la  ...   -21.244    55.708\n## 3          345020      Rincon de la Vieja  ...    10.830   -85.324\n## 4          353010              Fernandina  ...    -0.370   -91.550\n## \n## [5 rows x 15 columns]\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfrom astropy.time import Time,TimeDelta\nimport numpy as np\nimport math\n\ndef fix_date(yyyy, mm, dd):\n  # The zero, one columns allow using pd.max(axis = 1) where we'd use pmax in R\n  neg = yyyy &lt;= 0\n  nyear = -yyyy\n  \n  year = max([yyyy, 1])\n  subtract_year = max([nyear, 0]) + neg\n  \n  day = dd\n  month = mm\n  \n  if math.isnan(day): \n    day = 1\n  if math.isnan(month): \n    month = 1\n      \n  if day == 0: \n    day = 1\n  if month == 0: \n    month = 1\n  \n  dateformat = \"%04d-%02d-%02d\" % (year, month, day)\n  \n  date = Time(dateformat, format = \"iso\", scale = 'ut1')\n  datefix = date - TimeDelta(subtract_year*365, format= 'jd')\n  return datefix\n\nerupt = eruptions.query(\"eruption_category == 'Confirmed Eruption'\")\nerupt.fillna(0, inplace = True)\nerupt['start_date'] = erupt.apply(lambda x: fix_date(x.start_year, x.start_month, x.start_day), axis = 1)\nerupt['end_date'] = erupt.apply(lambda x: fix_date(x.end_year, x.end_month, x.end_day), axis = 1)\nerupt['duration'] = erupt.end_date - erupt.start_date\n# Convert back to numeric\nerupt['duration'] = erupt.duration.apply(lambda x: x.to_value(\"jd\", \"decimal\")) # Julian day\n\nLet‚Äôs start out seeing what month most eruptions occur in‚Ä¶\n\nimport plotly.express as px\ntmp = erupt.groupby(\"start_month\").count()\ntmp = tmp.reset_index()\n# Note, I'm using the original month, so 0 = unknown\nfig = px.bar(tmp, x = 'start_month', y = 'volcano_number')\n\nfile = 'plotly-python/eruptplot-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St.¬†Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn‚Äôt happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nfig = px.bar(\n  erupt.groupby(\"vei\").count().reset_index(),\n  x = \"vei\", y = \"volcano_number\")\n\nfile = 'plotly-python/vei-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\nerupt[\"duration_yr\"] = erupt.duration/365.25\n## TypeError: unsupported operand type(s) for /: 'decimal.Decimal' and 'float'\nfig = px.box(\n  erupt,\n  x = \"vei\",\n  y = \"duration_yr\",\n  points = \"all\"\n)\n## ValueError: Value of 'y' is not the name of a column in 'data_frame'. Expected one of ['volcano_number', 'volcano_name', 'eruption_number', 'eruption_category', 'area_of_activity', 'vei', 'start_year', 'start_month', 'start_day', 'evidence_method_dating', 'end_year', 'end_month', 'end_day', 'latitude', 'longitude', 'start_date', 'end_date', 'duration'] but received: duration_yr\n\nfile = 'plotly-python/vei-duration-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren‚Äôt that many of them (thankfully)\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Interactivity\n\n\n\n\n\nPlotly integration with ggplot2 is nice, but obviously not a universal summary of what it can do. Let‚Äôs look at another example of plotly in R/python without ggplot2 integration.\nWe start with a scatterplot of volcanoes along the earth‚Äôs surface:\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, lat = volcano$latitude)\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, lon = \"longitude\", lat = \"latitude\")\n\nfile = 'plotly-python/scatter-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nAnd then we can start customizing.\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, \n        lat = volcano$latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano$volcano_name, \"\\n\",\n                      \"Last Erupted: \", volcano$last_eruption_year),\n        # Change the markers because why not?\n        marker = list(color = \"#d00000\", opacity = 0.25)\n        )\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, \n  lon = \"longitude\", lat = \"latitude\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\nfig.update_traces(marker=dict(size=12, opacity = 0.25, color = 'red'),\n                  selector=dict(mode='markers'))\n\n                                    \n\n\n\nfile = 'plotly-python/scatter-hover-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)\n\n\n\n\n\n                                    \n\n\n\n\n\n\n\n\nPlotly will handle some variable mappings for you, depending on which ‚Äútrace‚Äù (type of plot) you‚Äôre using.\n\n\nR\nPython\n\n\n\nThe plot_ly function is also pipe friendly. Variable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\n# Load RColorBrewer for palettes\nlibrary(RColorBrewer)\n\nvolcano %&gt;%\n  group_by(volcano_type) %&gt;% \n  mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\nplot_ly(type = \"scattergeo\", lon = ~longitude, lat = ~latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano_name, \"\\n\",\n                      \"Last Erupted: \", last_eruption_year),\n        color = ~ volcano_type,\n        # Specify a palette\n        colors = brewer.pal(length(unique(.$volcano_type)), \"Paired\"),\n        # Change the markers because why not?\n        marker = list(opacity = 0.5)\n        )\n\n\n\n\n\n\n\n\nvolc_sub = volcano.groupby(\"volcano_type\").agg({'volcano_number': ['size']})\n## KeyError: 'volcano_type'\nvolc_sub.columns = [\"n\"]\n## NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.reset_index()\n## NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.query(\"n &gt;= 15\")\n## NameError: name 'volc_sub' is not defined\nvolc_sub = pd.merge(volc_sub['volcano_type'], volcano, on = 'volcano_type', how = 'inner')\n## NameError: name 'volc_sub' is not defined\n\nfig = px.scatter_geo(volc_sub, \n  lon = \"longitude\", lat = \"latitude\", \n  color = \"volcano_type\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\n## NameError: name 'volc_sub' is not defined\nfig.update_traces(marker=dict(size=12, opacity = 0.25),\n                  selector=dict(mode='markers'))\n\n                                    \n\n\n\nfile = 'plotly-python/scatter-hover-update-volcano.html'\npl.io.write_html(fig, file = file, auto_open = False)",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html#leaflet-maps",
    "href": "part-advanced-topics/06-interactive-graphics.html#leaflet-maps",
    "title": "30¬† Interactive Graphics",
    "section": "\n31.3 Leaflet maps",
    "text": "31.3 Leaflet maps\n\n\nI‚Äôm sorry, but I haven‚Äôt managed to redo this part of the chapter in python as well as R. Hopefully I‚Äôll get to it soon. You can find a tutorial for Python + Leaflet here [1].\nLeaflet is another javascript library that allows for interactive data visualization. We‚Äôre only going to briefly talk about it here, but there is extensive documentation that includes details of how to work with different types of geographical data, chloropleth maps, plugins, and more.\n\n\n\n\n\n\nBigfoot Sightings\n\n\n\n\n\nTo explore the leaflet package, we‚Äôll start out playing with a dataset of Bigfoot sightings assembled from the Bigfoot Field Researchers Organization‚Äôs Google earth tool\n\nif (!\"leaflet\" %in% installed.packages()) install.packages(\"leaflet\")\n\nlibrary(leaflet)\nlibrary(readr)\n\nbigfoot_data &lt;- read_csv(\"https://query.data.world/s/egnaxxvegdkzzrhfhdh4izb6etmlms\")\n\nWe can start out by plotting a map with the location of each sighting. I‚Äôve colored the points in a seasonal color scheme, and added the description of each incident as a mouseover label.\n\nbigfoot_data %&gt;%\n  filter(classification == \"Class A\") %&gt;%\n  mutate(seasoncolor = str_replace_all(season, c(\"Fall\" = \"orange\",\n                                                 \"Winter\" = \"skyblue\",\n                                                 \"Spring\" = \"green\",\n                                                 \"Summer\" = \"yellow\")),\n         # This code just wraps the description to the width of the R terminal\n         # and inserts HTML for a line break into the text at appropriate points\n         desc_wrap = purrr::map(observed, ~strwrap(.) %&gt;%\n                                  paste(collapse = \"&lt;br/&gt;\") %&gt;%\n                                  htmltools::HTML())) %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~longitude, ~latitude, color = ~seasoncolor, label = ~desc_wrap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSquirrels of New York City\n\n\n\n\n\nOf course, because this is an interactive map library, we aren‚Äôt limited to any one scale. We can also plot data at the city level:\n\n# library(nycsquirrels18)\n# data(squirrels)\n\nsquirrels &lt;- readr::read_csv(\"data/nycsquirrels.csv\")\n## Error: 'data/nycsquirrels.csv' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics').\nhead(squirrels)\n## Error: object 'squirrels' not found\n\nsquirrels %&gt;%\n  mutate(color = tolower(primary_fur_color)) %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~long, ~lat, color = ~color)\n## Error: object 'squirrels' not found\n\n\n\n\n\n\n\n\n\n\nEcological Regions\n\n\n\n\n\nWe can also plot regions, instead of just points. I downloaded a dataset released by the state of California, Crotch‚Äôs Bumble Bee Range - CDFW dataset, which shows the range of the Crotch‚Äôs Bumble Bee (Bombus crotchii).\nI‚Äôve set this chunk to not evaluate because it causes the book to be painfully large.\n\nlibrary(sf)\nbees &lt;- st_read(\"../data/Crotch_s_Bumble_Bee_Range_-_CDFW_[ds3095].geojson\")\nbees &lt;- sf::st_transform(bees, 4326)\n\nbees %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = F, fillOpacity = 0.25,\n              fillColor = \"yellow\")\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nStarter R code\nR solution\n\n\n\nDownload the Shapefiles for the 116th Congress Congressional Districts. Unzip the file and read it in using the code below (you‚Äôll have to change the file path). Use the MIT Election Data and Science Lab‚Äôs US House election results, and merge this data with the shapefiles to plot the results of the 2018 midterms in a way that you think is useful (you can use any of the available data).\nSome notes:\n\nFIPS codes are used to identify the state and district, with 00 indicating at-large districts (one district for the state) and 98 indicating non-voting districts.\nIf you would like to add in the number of citizens of voting age, you can get that information here but you will have to do some cleaning in order to join the table with the others.\nMinnesota‚Äôs Democratic-farmer-labor party caucuses with the Democrats but maintains its name for historical reasons. You can safely recode this if you want to.\n\n\n\n\nlibrary(sf)\n# Read in the districts\nziptemp &lt;- tempfile(fileext=\".zip\")\nshapeurl &lt;- \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_cd116_5m.zip\"\ndownload.file(shapeurl, destfile = ziptemp, mode = \"wb\")\nunzip(ziptemp, exdir = \"data/116_congress\")\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\")\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Read in the results\nelection_results &lt;- read_csv(\"data/1976-2020-house.csv\") %&gt;%\n  filter(year == 2018) %&gt;%\n  mutate(state_fips = sprintf(\"%02d\", as.integer(state_fips)),\n         district = sprintf(\"%02d\", as.integer(district)))\n## Error: 'data/1976-2020-house.csv' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics').\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n\n\n\nlibrary(sf)\nlibrary(htmltools) # to mark labels as html code\n\n# Read in the results\nelection_results &lt;- election_results %&gt;%\n  group_by(state, state_fips, state_po, district, stage) %&gt;%\n  arrange(candidatevotes) %&gt;%\n  mutate(pct = candidatevotes/totalvotes) %&gt;%\n  mutate(party = str_to_lower(party)) %&gt;%\n  # Keep the winner only\n  filter(pct == max(pct)) %&gt;%\n  # Fix Minnesota\n  mutate(party = ifelse(party == \"democratic-farmer-labor\", \"democrat\", party))\n## Error: object 'election_results' not found\n\n# Read in the districts\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\") %&gt;%\n  mutate(geometry = st_transform(geometry, crs = st_crs(\"+proj=longlat +datum=WGS84\")))\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/stat-computing-r-python/part-advanced-topics/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n# Merge\ncongress_districts &lt;- congress_districts %&gt;%\n  left_join(election_results, by = c(\"STATEFP\" = \"state_fips\", \"CD116FP\" = \"district\")) %&gt;%\n  mutate(party = factor(party, levels = c(\"republican\", \"democrat\")),\n         short_party = ifelse(party == \"republican\", \"R\", \"D\"),\n         label = paste0(state_po, \"-\", district, candidate, \" (\", short_party, \")\"))\n## Error: object 'election_results' not found\n\n# Define a palette\nregion_pal &lt;- colorFactor(c(\"#e9141d\", \"#0015bc\"), congress_districts$party)\n\ncongress_districts %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = TRUE, fillOpacity = ~pct/2,\n              # still want to see what's underneath, even in safe districts\n              fillColor = ~region_pal(party), color = ~region_pal(party),\n              label = ~label)\n## Error in eval(f[[2]], metaData(data), environment(f)): object 'label' not found",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html#shiny",
    "href": "part-advanced-topics/06-interactive-graphics.html#shiny",
    "title": "30¬† Interactive Graphics",
    "section": "\n31.4 Shiny",
    "text": "31.4 Shiny\nTake a few minutes and poke around the RStudio Shiny user showcase. It helps to have some motivation, and to get a sense of what is possible before you start learning something.\n\n\nOne of the more amusing ones I found was an exploration of lego demographics.\nShiny is a framework for building interactive web applications in R (and now in Python too!). Unlike plotly and other graphics engines, Shiny depends on an R instance on a server to do computations. This means Shiny is much more powerful and has more capabilities, but also that it‚Äôs harder to share and deploy - you have to have access to a web server with R installed on it. If you happen to have a server like that, though, Shiny is pretty awesome. Posit runs a service called shinyapps.io that will provide some limited free hosting, as well as paid plans for apps that have more web traffic, but you can also create Shiny apps for local use - I often do this for model debugging when I‚Äôm using neural networks, because they‚Äôre so complicated.\nPosit has a set of well produced video tutorials to introduce Shiny. I‚Äôd recommend you at least listen to the introduction if you‚Äôre a visual/audio learner (the whole tutorial is about 2 hours long). There is also a written tutorial if you prefer to learn in written form (7 lessons, each is about 20 minutes long).\nI generally think it‚Äôs better to send you to the source when there are well-produced resources, rather than trying to rehash something to put my own spin on it.\nOne other interesting feature to keep in mind when using Shiny - you can integrate Shiny reactivity into Rmarkdown by adding runtime: shiny to the markdown header.\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nShiny articles\nReactivity in Shiny\nLeaflet introduction for R\n\n\n31.4.1 Other interactive tools\n\nhtmlwidgets - a generic wrapper for any Javascript library (htmlwidgets is used under the hood in both Leaflet and Plotly R integration)\ndash - Another dashboard program supported by plotly. dash is the python equivalent of shiny, but also has R integration (though I‚Äôm not sure how well it‚Äôs supported).\n\n31.4.2 Debugging\n\nDebugging with Dean - Shiny debugging - YouTube video with debugging in realtime.\nShinyJS - Using Shiny and JavaScript together\nUsing Shiny in Production - Joe Cheng\n\n\n\n\n\n\n\n\n[1] \nK. Pham, ‚ÄúWeb mapping with python and leaflet,‚Äù Programming Historian, Aug. 2017 [Online]. Available: https://programminghistorian.org/en/lessons/mapping-with-python-leaflet. [Accessed: Aug. 01, 2023]",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "part-advanced-topics/06-interactive-graphics.html#footnotes",
    "href": "part-advanced-topics/06-interactive-graphics.html#footnotes",
    "title": "30¬† Interactive Graphics",
    "section": "",
    "text": "This is me experimentally trying to replace the tidyverse, and honestly, I‚Äôm not a fan.‚Ü©Ô∏é\nThere are obviously exceptions - we can figure out the exact date and approximate time that there was an earthquake along the Cascadia subduction zone based on a combination of oral histories of the indigenous people and records of a massive tsunami in Japan Excellent read, if you‚Äôre interested, and the Nature paper.‚Ü©Ô∏é",
    "crumbs": [
      "Part IV: Advanced Topics",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Interactive Graphics</span>"
    ]
  },
  {
    "objectID": "graveyard.html",
    "href": "graveyard.html",
    "title": "31¬† Other Topics",
    "section": "",
    "text": "31.1 Shell Commands\nWhen talking to computers, sometimes it is convenient to cut through the graphical interfaces, menus, and so on, and just tell the computer what to do directly, using the system shell (aka terminal, command line prompt, console).\nMost system shells are fully functioning programming languages in their own right. This section isn‚Äôt going to attempt to teach you those skills - we‚Äôll focus instead on the basics - how to change directories, list files, and run programs.",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-shell-commands",
    "href": "graveyard.html#sec-shell-commands",
    "title": "31¬† Other Topics",
    "section": "",
    "text": "31.1.1 Launching the system terminal\nIn RStudio, you can access a system terminal in the lower left corner by clicking on the tab labeled Terminal. If the tab does not exist, then go to Tools -&gt; Terminal -&gt; New Terminal in the main application toolbar.\nSometimes, it is preferable to launch a terminal separate from RStudio. Here‚Äôs how to do that:\n\n\n Windows\n Mac\n Linux\n\n\n\nOption 1: Default Windows terminal (cmd.exe)\n\nGo to the search bar/start menu\nType in cmd.exe\nA black window should appear.\n\nOption 2: Git bash (if you have git installed)\n\nGo to the search bar/start menu\nType in bash\nClick on the Git Bash application\n\nIf you choose option 2, use the commands for Bash/Linux below. Bash tends to be a bit less clunky than the standard windows terminal.\n\n\nOption 1: Dock\n\nClick the launchpad icon\nType Terminal in the search field\nClick Terminal\n\nOption 2: Finder\n\nOpen the Applications/Utilities folder\nDouble-click on Terminal\n\n\n\nOn most systems, pressing Ctrl-Alt-T or Super-T (Windows-T) will launch a terminal.\nOtherwise, launch your system menu (usually with the Super/Windows key) and type Terminal. You may have multiple options here; I prefer Konsole but I‚Äôm usually using KDE as my desktop environment. Other decent options include Gnome-terminal and xterm, and these are usually associated with Gnome and XFCE desktop environments, respectively.\n\n\n\n\n31.1.2 File Path Structure\nOn Windows, file paths are constructed as follows: C:\\Folder 1\\Folder_2\\file.R. Paths are generally not case sensitive, so you can reference the same file path as c:\\folder 1\\folder_2\\file.R. Usually, paths are encased in \"\" because spaces make interpreting file paths complicated and Windows paths have lots of spaces.\nOn Unix systems, file paths are constructed as follows: /home/user/folder1/folder2/file.R. Paths are case sensitive, so you cannot reach /home/user/folder1/folder2/file.R if you use /home/user/folder1/folder2/file.r. On Unix systems, spaces in file paths must be escaped with \\, so any space character in a terminal should be typed \\ instead.\nThis quickly gets complicated and annoying when working on code that is meant for multiple operating systems. These complexities are why when you‚Äôre constructing a file path in R or python, you should use commands like file.path(\"folder1\", \"folder2\", \"file.r\") or os.path.join(\"folder1\", \"folder2\", \"file.py\"), so that your code will work on Windows, Mac, and Linux by default.\n\n31.1.3 Basic Terminal Commands\nI have listed commands here for the most common languages used in each operating system. If you are using Git Bash on Windows, follow the commands for Linux/Bash. If you are using Windows PowerShell, google the commands.\nIn most cases, Mac/Zsh is similar to Linux/Bash, but there are a few differences1.\n\n\n\n\n\n\n\n\n\nTask\n\n Windows/CMD\n\n Mac/Zsh\n\n Linux/Bash\n\n\n\nList your current working directory\ncd\npwd\npwd\n\n\nChange directory\ncd &lt;path to new dir&gt;\ncd &lt;path to new dir&gt;\ncd &lt;path to new dir&gt;\n\n\nList files and folders in current directory\ndir\nls\nls\n\n\nCopy file\nxcopy &lt;source&gt; &lt;destination&gt; &lt;arguments&gt;\ncp &lt;arguments&gt; &lt;source&gt; &lt;destination&gt;\ncp &lt;arguments&gt; &lt;source&gt; &lt;destination&gt;\n\n\nCreate directory\nmkdir &lt;foldername&gt;\nmkdir &lt;foldername&gt;\nmkdir &lt;foldername&gt;\n\n\nDisplay file contents\ntype &lt;filename&gt;\ncat &lt;filename&gt;\ncat &lt;filename&gt;",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-controlling-loops",
    "href": "graveyard.html#sec-controlling-loops",
    "title": "31¬† Other Topics",
    "section": "\n31.2 Controlling Loops with Break, Next, Continue",
    "text": "31.2 Controlling Loops with Break, Next, Continue\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n31.2.1 Break Statement\n\n\nA break statement is used to exit a loop prematurely\n\n\n31.2.2 Next/Continue Statement\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\n\nExample: Next/continue and Break statements\n\n\n\nLet‚Äôs demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\n\nR\nPython\n\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n## [1] 1\n## [1] 2\n## [1] \"Divisible by 3\"\n## [1] 4\n## [1] \"Divisible by 5\"\n## [1] \"Divisible by 3\"\n## [1] 7\n## [1] 8\n## [1] \"Divisible by 3\"\n## [1] \"Divisible by 5\"\n## [1] 11\n## [1] \"Divisible by 3\"\n## [1] 13\n## [1] 14\n## [1] \"Exiting now\"\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n## 1\n## 2\n## Divisible by 3\n## 4\n## Divisible by 5\n## Divisible by 3\n## 7\n## 8\n## Divisible by 3\n## Divisible by 5\n## 11\n## Divisible by 3\n## 13\n## 14\n## Exiting now\n\n\n\n\n\n\nTo be quite honest, I haven‚Äôt really ever needed to use next/continue statements when I‚Äôm programming, and I rarely use break statements. However, it‚Äôs useful to know they exist just in case you come across a problem where you could put either one to use.",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-recursion",
    "href": "graveyard.html#sec-recursion",
    "title": "31¬† Other Topics",
    "section": "\n31.3 Recursion",
    "text": "31.3 Recursion\nUnder construction.\nIn the meantime, check out [1] (R) and [2] (Python) for decent coverage of the basic idea of recursive functions.",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-text-encoding",
    "href": "graveyard.html#sec-text-encoding",
    "title": "31¬† Other Topics",
    "section": "\n31.4 Text Encoding",
    "text": "31.4 Text Encoding\nI‚Äôve left this section in because it‚Äôs a useful set of tricks, even though it does primarily deal with SAS.\nDon‚Äôt know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding!\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters).\nWhile writing this code, I got an error of ‚ÄúInvalid logical name‚Äù because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS.\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv &gt; ../data/pokemon_gen_1-8.csv\";\nonly run this once to download the file... */\nfilename pokeloc '../data/pokemon_gen_1-8.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\nIf I fix the file in R (because I know how to fix it there‚Ä¶ another option is to fix it manually),\n\nlibrary(readr)\nlibrary(dplyr)\ntmp &lt;- read_csv(\"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\")[,-1]\nwrite_csv(tmp, \"../data/pokemon_gen_1-8.csv\")\n\ntmp &lt;- select(tmp, -japanese_name) %&gt;%\n  # iconv converts strings from UTF8 to ASCII by transliteration - \n  # changing the characters to their closest A-Z equivalents.\n  # mutate_all applies the function to every column\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\n\nwrite_csv(tmp, \"../data/pokemon_gen_1-8_ascii.csv\", na='.')\n\nThen, reading in the new file allows us to actually see the output.\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"../data/pokemon_gen_1-8_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \nThis trick works in so many different situations. It‚Äôs very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate.",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#sec-other-topics-refs",
    "href": "graveyard.html#sec-other-topics-refs",
    "title": "31¬† Other Topics",
    "section": "\n31.5 References",
    "text": "31.5 References\n\n\n\n\n[1] \nDataMentor, ‚ÄúR recursion. DataMentor,‚Äù Nov. 24, 2017. [Online]. Available: https://www.datamentor.io/r-programming/recursion/. [Accessed: Jan. 10, 2023]\n\n\n[2] \nParewa Labs Pvt. Ltd., ‚ÄúPython recursion. Learn python interactively,‚Äù 2020. [Online]. Available: https://www.programiz.com/python-programming/recursion. [Accessed: Jan. 10, 2023]",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "graveyard.html#footnotes",
    "href": "graveyard.html#footnotes",
    "title": "31¬† Other Topics",
    "section": "",
    "text": "Mac used to use bash but switched to Zsh in 2019 for licensing reasons.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Other Topics</span>"
    ]
  },
  {
    "objectID": "part-tools/00-tools-intro.html",
    "href": "part-tools/00-tools-intro.html",
    "title": "Part I: Tools",
    "section": "",
    "text": "This part of the textbook provides an overview of the different tools we will be using: R, python, quarto, markdown, pandoc, consoles, and so on. It can be a bit confusing at first, especially if you‚Äôre not familiar with how your computer works, where files are stored, and different ways to tell your computer what to do.\n1¬† Computer Basics gives you some important background material about how a computer functions.\n2¬† Setting Up Your Computer tells you exactly what software you need to install for the rest of this textbook.\n4¬† Scripts and Notebooks discusses the different ways we can talk to R and python, and the pros and cons of each.",
    "crumbs": [
      "Part I: Tools"
    ]
  },
  {
    "objectID": "part-gen-prog/00-gen-prog.html",
    "href": "part-gen-prog/00-gen-prog.html",
    "title": "Part II: General Programming",
    "section": "",
    "text": "In this portion of the textbook, we‚Äôll talk about the basics of programming in a general sense (that is, we‚Äôre not yet focusing on programming with data).\nBefore we start in on the hard stuff, we‚Äôll quickly go through what programming is and what the vocabulary of a programming language looks like in 7¬† Introduction to Programming.\n8¬† Variables and Basic Data Types will discuss the basics: variable types, how to assign variables, and how to convert between simple variable types.\n9¬† Using Functions and Libraries will discuss how to use built-in and package functions to make R and python more powerful. After this section, you should be able to use R or Python as a calculator.\n10¬† Data Structures will discuss the use of vectors and matrices in R and Python. Along the way, you‚Äôll get a quick refresher in mathematical logic - the use of And, Or, and Not.\nIf you‚Äôve had linear algebra, 11¬† Matrix Calculations will tell you how to use R and python to perform matrix calculations. If you haven‚Äôt had linear algebra yet, skip this section and move on to 12¬† Control Structures.\n12¬† Control Structures will discuss control structures - ways to change the flow of a program based on variable values and operating condition. This will include discussions of if-statements and different types of loops.\n13¬† Writing Functions will discuss writing your own functions.\nOnce we‚Äôve covered these topics, we should be ready to focus on programming with, for, and on data.",
    "crumbs": [
      "Part II: General Programming"
    ]
  },
  {
    "objectID": "part-wrangling/00-wrangling.html",
    "href": "part-wrangling/00-wrangling.html",
    "title": "Part III: Data Wrangling",
    "section": "",
    "text": "References",
    "crumbs": [
      "Part III: Data Wrangling"
    ]
  },
  {
    "objectID": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "href": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "title": "Part III: Data Wrangling",
    "section": "",
    "text": "[1] H. Wickham, ‚ÄúTidy data,‚Äù The Journal of Statistical Software, vol. 59, 2014 [Online]. Available: http://www.jstatsoft.org/v59/i10/",
    "crumbs": [
      "Part III: Data Wrangling"
    ]
  }
]